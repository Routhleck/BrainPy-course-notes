
<!doctype html>














<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/BrainPy-course-notes/assets/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/BrainPy-course-notes/assets/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/BrainPy-course-notes/assets/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Jekyll, NexT" />





  <link rel="alternate" href="/BrainPy-course-notes/atom.xml" title="BrainPy course notes" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/BrainPy-course-notes/assets/favicon.ico?v=5.1.1" />
















<meta name="description" content="[TOC] # 神经计算建模简介 ## 计算神经科学的背景与使命 计算神经科学是**脑科学**对**类脑智能**的**桥梁** ### 两大目标 - 用计算建模的方法来阐明大脑功能的计算原理 - 发展类脑智能的模型和算法 ### Prehistory - 1907 LIF model 神经计算的本质 - 1950s HH model 电位定量化模型 最fundamental的 - 1960s Roll&apos;s cable equation 描述信号在轴突和树突怎么传递 - 1970s Amari, Wilson, Cowan et al. 现今建模的基础 - 1982 Hopfield model(Amari-Hopfield model) 引入物理学技术，吸引子模型 - 1988 Sejnowski et al. &quot;Computational Neuroscience&quot;(science) 提出计算神经科学概念 **现在的计算神经科学对应于物理学的第谷-伽利略时代，对大脑工作原理还缺乏清晰的理论** ### Three levels of Brain Science ![image-20230823105226568](/BrainPy-course-notes/master_content/Notes.assets/image-20230823105226568.png) - 大脑做什么 Computational theory -&amp;gt; Psychology &amp; Cognitive Science -&amp;gt; Human-like Cognitive function - 大脑怎么做 Representation &amp; Algorithm -&amp;gt; Computational Neuroscience -&amp;gt; Brain-inspired model &amp; algorithm - 大脑怎么实现 Implementation -&amp;gt; Neuroscience -&amp;gt; Neuromorphic computing ### Mission of Computational Neuroscience &amp;gt; What I can not build a computational model, I do not understand ## 神经计算建模的目标与挑战 ### Limitation of Deep Learning - 不擅长对抗样本 - 对图像的理解有限 ![image-20230823105836259](/BrainPy-course-notes/master_content/Notes.assets/image-20230823105836259.png) ### Brain is for Processing Dynamical Information **We never &quot;see&quot; a static image** ![image-20230823105918336](/BrainPy-course-notes/master_content/Notes.assets/image-20230823105918336.png) ### The missing link a computational model of higher cognitive functior ![image-20230823110617639](/BrainPy-course-notes/master_content/Notes.assets/image-20230823110617639.png) 现在只是做的**局部**的网络，没有一个成功的模型，能**从神经元出发构建网络，到系统层面上** **原因**: 因为神经科学底层数据的缺失，可以考虑数据驱动、大数据的方式来加快发展 ## 神经计算建模的工具 &amp;gt; 工欲行其事，必先利其器 &amp;gt; We need &quot;PyTorch/TensorFlow&quot; in Computational Neuroscience! ### Challenges in neural modelling 有不同的尺度 - Mutiple-scale - Large-scale - Multiple purposes ![image-20230823111212460](/BrainPy-course-notes/master_content/Notes.assets/image-20230823111212460.png) &amp;gt; The modeling targets and methods are extremely complex, and we need a general framework. ### Limitations of Existing Brain Simulators 现今的框架不能满足以上 ![image-20230823111509523](/BrainPy-course-notes/master_content/Notes.assets/image-20230823111509523.png) ### What are needed for a brain simulator 1. Efficiency High-speed simulation on parallel computing devices, etc. 2. Integration Integrated modeling of simulation, training, and analysis 3. Flexibility New models at all scales can be accommodated 4. Extensibility Extensible to new modeling methods(machine learning) 需要新的范式 ### Our solution: BrainPy 4 levels ![image-20230823111903456](/BrainPy-course-notes/master_content/Notes.assets/image-20230823111903456.png) ## 神经计算建模举例 ### Image understanding: an ill-posed problem Image Understanding = image segmentation + image object recognition &amp;gt; Chicken vs. Egg dilemma &amp;gt; &amp;gt; - Without segmentation, how to recognize &amp;gt; - Without recognition, how to segment **The solution of brain:** Analysis-by-synthesis 猜测与验证方法 ### Reverse Hierarchy Theory 人的感知是整体到局部 ### Two pathways for visual information processing ![image-20230823114517888](/BrainPy-course-notes/master_content/Notes.assets/image-20230823114517888.png) ### Key Computational Issues for Global-to-local Neural Information Processing - What are global and local features - How to rapidly extract global features - How to generate global hypotheses - How to implement from global to local processing - The interplay between global and local features - Others #### How to extract global features **Global first = Topology first**(大范围首先，陈霖) 视觉系统更敏感于拓扑性质的差异 &amp;gt; DNNs has difficulty to recognize topology **A retina-SC network for topology detection** 视网膜到上丘的检测，Gap junction coupling ... ### A Model for Motion Pattern Recognition Reservoir Module Decision-making Module ### How to generate &quot;global&quot; hypotheses in the representation space Attractor neural network ![image-20230823115853980](/BrainPy-course-notes/master_content/Notes.assets/image-20230823115853980.png) Levy Flight in Animal Behaviors ![image-20230823120000911](/BrainPy-course-notes/master_content/Notes.assets/image-20230823120000911.png) ### How to process information from global to local Push-pull Feedback A hierarchical Hopfield Model ### Interplay between global and local features A two-pathway model for object recognition ![image-20230823120750349](/BrainPy-course-notes/master_content/Notes.assets/image-20230823120750349.png) Modeling visual masking 可以用two-pathway很好解释 # Programming basics ## Python Basics ### Values - Boolean - String - Integer - Float - ... ### Keywords Not allowed to use keywords, they define structure and rules of a language. ```python help(&quot;keywords&quot;) ``` ### Operators 数据之间的操作 #### For Integers and Floats ```python a=5 b=3 # addition + print(&quot;a+b=&quot;,atb) # subtraction - print(&quot;a-b=&quot;,a-b) # multiplication * print(&quot;axb=&quot;a*b) # division / print(&quot;a/b=&quot;,a/b) # power ** print(&quot;a**b=&quot;,a**b) ``` #### Booleans ```python #Boolean experssions # equals: == print(&quot;5==5&quot;,5==5) # do not equal: != print(&quot;5!-5&quot;,5!=5) # greater than: &amp;gt; print(&quot;5&amp;gt;5&quot;,5&amp;gt;5) # greater than or equal: &amp;gt;= print(&quot;5&amp;gt;=5”5&amp;gt;=5) ``` ```python # logica operators print(&quot;True and False:&quot;, True and False) print(&quot;True or False:&quot;, True or False) print(&quot;not False:&quot;, not False) ``` ### Modules Not all functionality available comes automatically when starting python. ```python import match import numpy as np print(math.pi) print(np.pi) from numpy import pi print(pi) from numpy import * print(pi) ``` ### Control statements #### If ```python a = 5 # In Python, blocks of code are defined using indentation. if a == 5: print(&quot;ok&quot;) ``` &amp;gt; ok #### For ```python # range(5) means a list with integers, 0, 1, 2, 3, 4 for i in range(5): print(i) ``` &amp;gt; 0 &amp;gt; 1 &amp;gt; 2 &amp;gt; 3 &amp;gt; 4 #### While ```python i = 1 while i 1 &amp;gt; 8 &amp;gt; 1000 ### Functions - Functions are used to abstract components of a program. - Much like a mathematical function, they take some input and then find the result. start a function definition with a keyword def - Then comes the function name, with arguments in braces, and then a colon. ```python def func(args1, args2): pass ``` ### Data types #### List - Group variables together - Specific order - Access item with brankets: [ ] - List can be sliced - List can be multiplied - List can be added - Lists are mutable - Copying a list ```python myList = [0, 1, 2, 0,&quot;name&quot;] print(&quot;myList[0]:&quot;, myList[0]) print(&quot;myList[1]:&quot;, myList[1]) print(&quot;myList[3]:&quot;, myList[3]) print(&quot;myList[-1]:&quot;, myList[-1]) print(&quot;myList[-2]:&quot;, myList[-2]) ``` &amp;gt; myList[0]: 0 &amp;gt; myList[1]: 1 &amp;gt; myList[3]: name &amp;gt; myList[-1]: name &amp;gt; myList[-2]: 2.0 ```python myList = [0, 1.0, &quot;hello&quot;] print(&quot;myList[0:2]:&quot;, mylist[0:2]) print(&quot;myList*2:&quot;, myList*2) myList2 = [2,&quot;yes&quot;] print(&quot;myList+myList2:&quot;, myList+myList2) ``` &amp;gt; myList[0:2]: [0，1.0] &amp;gt; myList*2: [0，1.0， hello&apos;，0，1.0， hello&apos;] &amp;gt; myList+myList2: [0，1.0，&apos;hello&apos;，2，yes&apos;] #### tuple Tuples are immutable. #### dictionary A dictionary is a collection of key-value pairs ```python d = {} d[1] = 2 d[&quot;a&quot;] = 3 print(&quot;d: &quot;, d) c = {1:2, &quot;a&quot;:3} print(&quot;c: &quot;, c) print(&quot;c[1]: &quot;, c[1]) ``` &amp;gt; d: {1: 2, &apos;a&apos;: 3} &amp;gt; c: {1: 2, &apos;a&apos;: 3} &amp;gt; c[1]: 2 ### Class In Python, everything is an object. Classes are objects, instances of classes are objects, modules are objects, and functions are objects. 1. a **type** 2. an internal **data representation** (primitive or composite) 3. a set of procedures for **interaction** with the object **a simple example** ```python # define class class Linear(): pass # instantiate object layer1 = Linear() print(layer1) ``` &amp;gt; `` #### Initializing an object ```python # define class class Linear(): # It refers to the object (instance) itself def __init__(self, n_input): self.n_input = n_input layer1 = Linear(100) layer2 = Linear(1000) print(&quot;layer1 : &quot;, layer1.n_input) print(&quot;layer2 : &quot;, layer2.n_input) ``` &amp;gt; layer1 : 100 &amp;gt; layer2 : 1000 #### Class has methods (similar to functions) ```python # define class class Linear(): ### It refers to the the object (instance) itself def __init__(self, n_input, n_output): self.n_input = n_input self.n_output = n_output def compute n params(self): num_params = self.n_input * self.n_output return num_params layerl = Linear(10,100) print(layerl.compute_n_params()) ``` &amp;gt; 1000 ## NumPy Basic ### Numpy Introduction - Fundamental package for scientific computing with Python - N-dimensional array object - Linear algebra, frontier transform, random number capacities - Building block for other packages (e.g. Scipy) ### Array - Arrays are mutable - Arrays attributes - ... ```python A = np.zeros((2, 2)) print(A) ``` &amp;gt; [[0. 0.] &amp;gt; [0. 0.]] ```python a.ndim # 2 dimension a.shape # (2, 5) shape of array a.size # 10 $ of elements a.T # transpose a.dtype # data type ``` #### Array broadcasting When operating on two arrays, numpy compares shapes. Two dimensions are compatible when 1. They are of equal size 2. One of them is 1 ![image-20230823143622229](/BrainPy-course-notes/master_content/Notes.assets/image-20230823143622229.png) ### Vector operations - Inner product - Outer product - Dot product (matrix multiplication) ```python u = [1, 2, 3] v = [1, 1, 1] np.inner(u, v) np.outer(u, v) np.dot(u, v) ``` &amp;gt; 6 &amp;gt; array([[1, 1, 1], &amp;gt; [2, 2, 2], &amp;gt; [3, 3, 3]]) &amp;gt; 6 ### Matrix operations - `np.ones` - `.T` - `np.dot` - `np.eye` - `np.trace` - `np.row_stack` - `np.column_stack` ### Operations along axes ```python a = np.ones((2, 3)) print(a) a.sum() a.sum(axis=0) a.cumsum() a.cumsum(axis=0) ``` ### Slicing arrays ```python a = np.random.random((2, 3)) print(a) a[0,:] # first row, all columns a[0:2] # first and second rows, al columns a[:,1:3]# all rows, second and third columns ``` ### Reshape ```python a = np.ones((10,1)) a.reshape(2,5) ``` ### Linear algebra ```python qr # Computes the QR decomposition cholesky # Computes the Cholesky decomposition inv(A) # Inverse solve(A,b) # Solves Ax = b for A full rank lstsq(A,b) # Solves arg minx //Ax - b//2 eig(A) # Eigenvalue decomposition eigvals(A) # Computes eigenvalues svd(A，full) # Sinqular value decomposition pinv(A) # Computes pseudo-inverse of A ``` ### Fourier transform ```python import numpy.fft fft # 1-dimensional DFT fft2 # 2-dimensional DFT fftn # N-dimensional DFT ifft # 1-dimensional inverse DFT (etc.) rfft # Real DFT (1-dim) ``` ### Random sampling ```python import numpy.random rand(d0, d1, ..., dn) # Random values in a given shape randn(d0, d1, ..., dn) # Random standard normal randint(lo, hi, size) # Random integers [lo hi) choice(a, size, repl, p) # Sample from a shuffle(a) # Permutation (in-place) permutation(a) # Permutation (new array) ``` ### Distributions in random ```python import numpy.random beta binomial chisquare exponential dirichlet gamma laplace lognormal ... ``` ### Scipy - `SciPy` is a library of algorithms and mathematical tools built to work with `NumPy ` arrays. - `scipy.linalg linear algebra` - `scipy.stats statistics` - `scipy.optimize optimization` - `scipy.sparse sparse matrices` - `scipy.signal signal processing` - etc. ## BrainPy introduction ### Modeling demands - Large-scale - Multi-scale - Methods ### BrainPy Architecture - Infrastructure - Functions - Just-in-time compilation - Devices ![image-20230823145349681](/BrainPy-course-notes/master_content/Notes.assets/image-20230823145349681.png) ### Main features #### Dense operators - Compatible with `NumPy`, `TensorFlow`, `PyTorch` and other dense matrix operator syntax. - Users do not need to learn and get started programming directly. #### Dedicated operatorsq - Applies brain dynamics sparse connectivity properties with event-driven computational features. - Reduce the complexity of brain dynamics simulations by several orders of magnitude. #### Numerical Integrators - Ordinary differential equations: brainpy.odeint - Stochastic differential equations: brainpy.sdeint - Fractional differential equations: brainpy.fdeint - Delayed differential equations #### Modular and composable 从微观到宏观 **brainpy.DynamicalSystem** ![image-20230823151159786](/BrainPy-course-notes/master_content/Notes.assets/image-20230823151159786.png) #### JIT of object-oriented BrainPy provides object-oriented transformations: - `brainpy.math.jit` - `brainpy.math.grad` - `brainpy.math.for_loop` - `brainpy.math.ifelse` ## BrainPy Programming Basics ### Just-in-Time compilation Just In Time Compilation (JIT, or Dynamic Translation), is compilation that is being done during the execution of a program. JIT compilation attempts to use **the benefits of both**. While the interpreted program is being run, the JIT compiler determines the most frequently used code and compiles it to machine code. The advantages of a JIT are due to the fact that since the compilation takes place in run time, a JIT compiler has access to dynamic runtime information enabling it to make better optimizations (such as inlining functions). ```python def gelu(x): sqrt = bm.sqrt(2 / bm.pi) cdf = 0.5 * (1.0 + bm.tanh(sqrt * (x + 0.044715 * (x ** 3)))) y = x *cdf return y &amp;gt;&amp;gt;&amp;gt; gelu_jit = bm.jit(gelu) # 使用JIT ``` ### Object-oriented JIT compilation - The class object must be inherited from brainpy.BrainPyObject, the base class of BrainPy, whose methods will be automatically JIT compiled. - All time-dependent variables must be defined as brainpy.math.Variable. ```python class LogisticRegression(bp.BrainPyObject): def __init__(self, dimension): super(LogisticRegression, self).__init__() # parameters self.dimension = dimension # variables self.w = bm.Variable(2.0 * bm.ones(dimension) - 1.3) def __call__(self, X, Y): u = bm.dot(((1.0 / (1.0 + bm.exp(-Y * bm.dot(X, self.w))) - 1.0) * Y), X) self.w.value = self.w - u # in-place update ``` **ExampleL Run a neuron model** ```python model = bp.neurons.HH(1000) #一共1000个神经元 runner = bp.DSRunner(target=model, inputs=(&apos;input&apos;, 10.)) # jit默认为True runner(duration=1000, eval_time=True) #模拟 1000ms ``` 禁用JIT来debug ### Data operations #### Array 等价于`numpy`的`array` #### BrainPy arrays &amp; JAX arrays ```python t1 = bm.arange(3) print(t1) print(t1.value) ``` &amp;gt; JaxArray([0, 1, 2], dtype=int32) &amp;gt; DeviceArray([0, 1, 2], dtype=int32) #### Variables Arrays that are not marked as dynamic variables will be JIT-compiled as static arrays, and modifications to static arrays will not be valid in the JIT compilation environment. ```python t = bm.arange(4) v = bm.Variable(t) print(v) print(v.value) ``` &amp;gt; Variable([0, 1, 2, 3], dtype=int32) &amp;gt; DeviceArray([0, 1, 2, 3], dtype=int32) ### Variables **In-place updating** 就地更新 #### Indexing and slicing - Indexing: `v[i] = a` or `v[(1, 3)] = c` - Slicing: `v[i:j] = b` - Slicing all values `v[:] = d`, `v[...] = e` #### Augmented assignment - add - subtract - divide - multiply - floor divide - modulo - power - and - or - xor - left shift - right shift #### Value assignment ```python v.value = bm.arange(10) check_no_change(v) ``` #### Update assignment ```python v.update(bm.random.randint(0, 20, size=10)) ``` ### Control flows #### If-else `brainpy.math.where` ```python a = 1. bm.where(a DeviceArray(1., dtype=float32, weak_type=True) `brainpy.math.ifelse` ```python def ifelse(condition, branches, operands): true_fun, false_fun = branches if condition: return true_fun(operands) else: return false_fun(operands) ``` #### For loop ```python import brainpy.math hist_of_out_vars = brainpy.math.for_loop(body_fun, operands) ``` #### While loop ```python i = bm.Variable(bm.zeros(1)) counter = bm.Variable(bm.zeros(1)) def cond_f(): return i[0] $$ (2\pi a\Delta x)c_{\mathrm{M}}\frac{\partial V(x,t)}{\partial t}+(2\pi a\Delta x)i_{\mathrm{ion}}=\frac{\pi a^{2}}{\rho_{\mathrm{L}}}\frac{\partial V(x+\Delta x,t)}{\partial x}-\frac{\pi a^{2}}{\rho_{\mathrm{L}}}\frac{\partial V(x,t)}{\partial x} $$ **Cable Equation** $$ c_\mathrm{M}\frac{\partial V(x,t)}{\partial t}=\frac{a}{2\rho_\mathrm{L}}\frac{\partial^2V(x,t)}{\partial x^2}-i_\mathrm{ion} $$ 电流在通过长直导体时会泄露电流，如何记录膜电位，可以使用此方程来描述 **Passive conduction:** ion currents are caused by leaky channels exclusively $$ i_{\mathrm{ion}}=V(x,t)/r_{\mathrm{M}} $$ -&amp;gt; $$ \begin{aligned}c_\mathrm{M}\frac{\partial V(x,t)}{\partial t}&amp;=\frac{a}{2\rho_\mathrm{L}}\frac{\partial^2V(x,t)}{\partial x^2}-\frac{V(x,t)}{r_\mathrm{M}}\\\\\tau\frac{\partial V(x,t)}{\partial t}&amp;=\lambda^2\frac{\partial^2V(x,t)}{\partial x^2}-V(x,t)\quad\lambda=\sqrt{0.5ar_\mathrm{M}/\rho_\mathrm{L}}\end{aligned} $$ 没有动作电位，单纯通过电缆传输 ![image-20230824102932665](/BrainPy-course-notes/master_content/Notes.assets/image-20230824102932665.png) If a constant external current is applied to 𝑥 = 0 the steady-state membrane potential $𝑉_{ss}(𝑥)$ is $$ \lambda^2\frac{\mathrm{d}^2V_{\mathrm{ss}}(x)}{\mathrm{d}x^2}-V_{\mathrm{ss}}(x)=0\longrightarrow V_{\mathrm{ss}}(x)=\frac{\lambda\rho_{\mathrm{L}}}{\pi a^2}I_0e^{-x/\lambda} $$ 电信号无衰减传播: 动作电位 ## Action potential &amp; active transport Steps of an action potential: - Depolarization - Repolarization - Hyperpolarization - Resting Characteristics: - All-or-none - Fixed shape - Active electrical property ![image-20230824103322522](/BrainPy-course-notes/master_content/Notes.assets/image-20230824103322522.png) How to simulate an action potential? $$ \begin{aligned} \frac{I(t)}{A}&amp; =c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+i_{\mathrm{ion}} \\ \Rightarrow\quad c_{\mathrm{M}}\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}&amp; =-g_{\mathrm{Cl}}(V_{\mathrm{M}}-E_{\mathrm{Cl}})-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}})-g_{\mathrm{Na}}(V_{\mathrm{M}}-E_{\mathrm{Na}})+\frac{I(t)}{A} \end{aligned} $$ 离子通道的开闭会随着电压而变化，电导也随着电压而变化 Mechanism: voltage-gated ion channels **HH建模思路：通过电导** ### Nodes of Ranvier Saltatory conduction with a much higher speed and less energy consumption 两个郎飞结之间会有离子通道，既有被动传导，也有主动的防止衰减 ![image-20230824104220106](/BrainPy-course-notes/master_content/Notes.assets/image-20230824104220106.png) ## The Hodgkin-Huxley Model ### Modeling of each ion channel Modeling of each ion channel: $$ g_m=\bar{g}_mm^x $$ Modeling of each ion gate: $$ \mathcal{C}\underset{}{\operatorname*{\overset{\alpha(\mathrm{V})}{\underset{\beta(\mathrm{V})}{\operatorname*{\longrightarrow}}}}\mathcal{O}} \\ \Rightarrow \begin{aligned} \frac{\mathrm{d}m}{\mathrm{d}t}&amp; =\alpha(V)(1-m)-\beta(V)m \\ &amp;=\frac{m_{\infty}(V)-m}{\tau_{m}(V)} \end{aligned} \\ \\ \begin{aligned}m_\infty(V)&amp;=\frac{\alpha(V)}{\alpha(V)+\beta(V)}.\\\tau_m(V)&amp;=\frac{1}{\alpha(V)+\beta(V)}\end{aligned} $$ $$ \text{If}\ V\text{ is constant:}m(t)=m_\infty(V)+(m_0-m_\infty(V))\mathrm{e}^{-t/\tau_m(V)} $$ ### Voltage clamp $$ \begin{aligned} \frac{I(t)}{A}&amp; =c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+i_{\mathrm{ion}} \\ \Rightarrow\quad c_{\mathrm{M}}\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}&amp; =-g_{\mathrm{Cl}}(V_{\mathrm{M}}-E_{\mathrm{Cl}})-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}})-g_{\mathrm{Na}}(V_{\mathrm{M}}-E_{\mathrm{Na}})+\frac{I(t)}{A} \end{aligned} $$ - The membrane potential is kept constant - The current from capacitors is excluded - Currents must come from leaky/voltage-gated ion channels $$ \begin{aligned}I_{\mathrm{cap}}&amp;=c\frac{dV}{dt}=0\\I_{\mathrm{fb}}&amp;=\quad i_{\mathrm{ion}}=g_{\mathrm{Na}}(V-E_{\mathrm{Na}})+g_{\mathrm{K}}(V-E_{\mathrm{K}})+g_{\mathrm{L}}(V-E_{\mathrm{L}})\end{aligned} $$ 只测量一个离子通道就可以很容易得到电导 ![image-20230824111620056](/BrainPy-course-notes/master_content/Notes.assets/image-20230824111620056.png) ### Leaky channel Hyperpolarization → the sodium and potassium channels are closed $$ I_{\mathrm{fb}}=g_{\mathrm{Na}}(V-E_{\mathrm{Na}})+g_{\mathrm{K}}(V-E_{\mathrm{K}})+g_{\mathrm{L}}(V-E_{\mathrm{L}}) $$ $$ \Rightarrow I_{\mathrm{fb}}=g_L(V-E_L) $$ $$ g_\mathrm{L}=0.3\mathrm{mS/cm}^2,E_\mathrm{L}=-54.4\mathrm{mV} $$ #### Potassium and sodium channels Potassium channels: Use choline to eliminate the inward current of Na + Na + current: $I_{fb} - I_{K}$ ![image-20230824112328953](/BrainPy-course-notes/master_content/Notes.assets/image-20230824112328953.png) ![image-20230824112333144](/BrainPy-course-notes/master_content/Notes.assets/image-20230824112333144.png) 转化速率和电导率两个因素 Potassium channels - Resting state (gate closed) - Activated state (gate open) → Activation gate: $g_{\mathrm{K}}=\bar{g}_{K}n^{x}$ Sodium channels - Resting state (gate closed) - Activated state (gate open) - Inactivated state (gate blocked) → Activation gate + inactivation gate: $g_{\mathrm{Na}}=\bar{g}_\text{Na}m^3h$ ![image-20230824113116329](/BrainPy-course-notes/master_content/Notes.assets/image-20230824113116329.png) The gates of sodium channels Modeling of each ion gate: $$ \begin{aligned} &amp;\text{gk}&amp;&amp; =\bar{g}_{K}n^{x} \\ &amp;\text{gNa}&amp;&amp; =\bar{g}_{\mathrm{Na}}m^{3}h \\ &amp;\frac{\mathrm{d}n}{\mathrm{d}t}&amp;&amp; =\alpha_{n}(V)(1-n)-\beta_{n}(V)n \\ &amp;\frac{\mathrm{d}m}{\mathrm{d}t}&amp;&amp; =\alpha_{m}(V)(1-m)-\beta_{m}(V)m \\ &amp;\frac{\mathrm{d}h}{\mathrm{d}t}&amp;&amp; =\alpha_{h}(V)(1-h)-\beta_{h}(V)h \end{aligned} $$ $$ \begin{aligned} \frac{\mathrm{d}m}{\mathrm{d}t}&amp; =\alpha(V)(1-m)-\beta(V)m \\ &amp;=\frac{m_{\infty}(V)-m}{\tau_{m}(V)} \end{aligned} $$ $$ \begin{aligned}m_\infty(V)&amp;=\frac{\alpha(V)}{\alpha(V)+\beta(V)}\\\tau_m(V)&amp;=\frac{1}{\alpha(V)+\beta(V)}\end{aligned}. $$ $$ m(t)=m_\infty(V)+(m_0-m_\infty(V))\mathrm{e}^{-t/\tau_m(V)} $$ ### The Hodgkin-Huxley(HH) Model $$ c_\mathrm{M}\frac{\mathrm{d}V_\mathrm{M}}{\mathrm{d}t}=-g_\mathrm{Cl}(V_\mathrm{M}-E_\mathrm{Cl})-g_\mathrm{K}(V_\mathrm{M}-E_\mathrm{K})-g_\mathrm{Na}(V_\mathrm{M}-E_\mathrm{Na})+\frac{I(t)}{A} $$ 本质是4个微分方程联立在一起 $$ \left\{\begin{aligned}&amp;c\frac{\mathrm{d}V}{\mathrm{d}t}=-\bar{g}_\text{Na}m^3h(V-E_\text{Na})-\bar{g}_\text{K}n^4(V-E_\text{K})-\bar{g}_\text{L}(V-E_\text{L})+I_\text{ext},\\&amp;\frac{\mathrm{d}n}{\mathrm{d}t}=\phi\left[\alpha_n(V)(1-n)-\beta_n(V)n\right]\\&amp;\frac{\mathrm{d}m}{\mathrm{d}t}=\phi\left[\alpha_m(V)(1-m)-\beta_m(V)m\right],\\&amp;\frac{\mathrm{d}h}{\mathrm{d}t}=\phi\left[\alpha_h(V)(1-h)-\beta_h(V)h\right],\end{aligned}\right. $$ $$ \begin{aligned}\alpha_n(V)&amp;=\frac{0.01(V+55)}{1-\exp\left(-\frac{V+55}{10}\right)},\quad\beta_n(V)&amp;=0.125\exp\left(-\frac{V+65}{80}\right),\\\alpha_h(V)&amp;=0.07\exp\left(-\frac{V+65}{20}\right),\quad\beta_n(V)&amp;=\frac{1}{\left(\exp\left(-\frac{V+55}{10}\right)+1\right)},\\\alpha_m(V)&amp;=\frac{0.1(V+40)}{1-\exp\left(-(V+40)/10\right)},\quad\beta_m(V)&amp;=4\exp\left(-(V+65)/18\right).\end{aligned} $$ $$ \phi=Q_{10}^{(T-T_{\mathrm{base}})/10} $$ 每一步符合生物学 ![image-20230824113714178](/BrainPy-course-notes/master_content/Notes.assets/image-20230824113714178.png) #### How to fit each gating variable? **Fitting n:** $g_{\mathbf{K}}=\bar{g}_{K}n^{x}\quad m(t)=m_{\infty}(V)+(m_{0}-\color{red}{\boxed{m_{\infty}(V)}})\mathrm{e}^{-t/\pi_{m}(V)}$ → $g_\mathrm{K}(V,t)=\bar{g}_\mathrm{K}\left[n_\infty(V)-(n_\infty(V)-n_0(V))\mathrm{e}^{-\frac{t}{\tau_n(V)}}\right]^x$ by $g_{\mathrm{K}\infty}=\bar{g}_{\mathrm{K}}n_{\infty}^{x},g_{\mathrm{K}0}=\bar{g}_{\mathrm{K}}n_{0}^{x}$ → $g_{\mathrm{K}}(V,t)=\left[g_{\mathrm{K}\infty}^{1/x}-(g_{\mathrm{K}\infty}^{1/x}-g_{\mathrm{K}0}^{1/x})\mathrm{e}^{-\frac{t}{\tau_{n}(V)}}\right]^{x}$ ![image-20230824114623467](/BrainPy-course-notes/master_content/Notes.assets/image-20230824114623467.png) # Hodgkin-Huxley brain dynamics programming ## Dynamics Programming Basics ### Integrators 微分器 ![image-20230824140806650](/BrainPy-course-notes/master_content/Notes.assets/image-20230824140806650.png) **example** FitzHugh-Nagumo equation $$ \begin{aligned}\tau\dot{w}&amp;=v+a-bw,\\\dot{v}&amp;=v-\frac{ u^3}{3}-w+I_{\mathrm{ext}}.\end{aligned} $$ ```python @bp.odeint(method=&apos;Euler&apos;, dt=0.01) def integral(V, w, t, Iext, a, b, tau): dw = (V + a - b * w) / tau dV = V - V * V * V / 3 - w + Iext return dV, dw ``` **JointEq** In a dynamical system, there may be multiple variables that change dynamically over time. Sometimes these variables are interrelated, and updating one variable requires other variables as inputs. For better integration accuracy, we recommend that you use `brainpy.JointEq` to jointly solve interrelated differential equations. ```python a, b = 0.02, 0.20 dV = lambda V, t, w, Iext: 0.04 * V * V + 5 * V + 140 - w + Iext # 第一个方程 dw = lambda w, t, V: a * (b * V - w) # 第二个方程 joint_eq = bp.JointEq(dV, dw) # 联合微分方程 integral2 = bp.odeint(joint_eq, method=&apos;rk2&apos;) # 定义该联合微分方程的数值积分方法 ``` ```python # 声明积分运行器 runner = bp.integrators.IntegratorRunner( integral, monitors=[&apos;V&apos;] inits=dict(V=0., w=0.) args=dict(a=a, b=b, tau=tau, Iext=Iext), dt=0.01 ) # 使用积分运行器来进行模拟100ms，结合步长dt=0.01 runner.run(100.) plt.plot(runner.mon.ts, runner.mon.V) plt.show() ``` ![image-20230824142019832](/BrainPy-course-notes/master_content/Notes.assets/image-20230824142019832.png) ### `DynamicalSystem` BrainPy provides a generic `SynamicalSystem` class to define various types of dynamical models. BrainPy supports modelings in brain simulation and brain-inspired computing. All these supports are based on one common concept: **Dynamical System** via `brainpy.DynamicalSystem`. #### What is `DynamicalSystem` A `DynamicalSystem` defines the updating rule of the model at single time step. 1. For models with state, `DynamicalSystem` defines the state transition from $t$ to $t + dt$, i.e., $S(t+dt)=F(S(t),x,t,dt)$, where $S$ is the state, $x$ is input, $t$ is the time, and $dt$ is the time step. This is the case for recurrent neural networks (like GRU, LSTM), neuron models (like HH, LIF), or synapse models which are widely used in brain simulation. 2. However, for models in deep learning, like convolution and fully-connected linear layers, `DynamicalSystem` defines the input-to-output mapping, i.e., $y=F(x,t)$. ![img](https://brainpy.readthedocs.io/en/latest/_images/dynamical_system.png) #### How to define `DynamicalSystem` ```python class YourDynamicalSystem(bp.DynamicalSystem): def update(self, x): ... ``` Instead of input x, there are shared arguments across all nodes/layers in the network: - the current time `t`, or - the current running index `i`, or - the current time step `dt`, or - the current phase of training or testing `fit=True/False`. Here, it is necessary to explain the usage of `bp.share`. - `bp.share.save( )`: The function saves shared arguments in the global context. User can save shared arguments in tow ways, for example, if user want to set the current time `t=100`, the current time step `dt=0.1`,the user can use `bp.share.save(&quot;t&quot;,100,&quot;dt&quot;,0.1)` or `bp.share.save(t=100,dt=0.1)`. - `bp.share.load( )`: The function gets the shared data by the `key`, for example, `bp.share.load(&quot;t&quot;)`. - `bp.share.clear_shargs( )`: The function clears the specific shared arguments in the global context, for example, `bp.share.clear_shargs(&quot;t&quot;)`. - `bp.share.clear( )`: The function clears all shared arguments in the global context. #### How to run `DynamicalSystem` As we have stated above that `DynamicalSystem` only defines the updating rule at single time step, to run a `DynamicalSystem` instance over time, we need a for loop mechanism. ![img](https://brainpy.readthedocs.io/en/latest/_images/dynamical_system_and_dsrunner.png) ##### `brainpy.math.for_loop` `for_loop` is a structural control flow API which runs a function with the looping over the inputs. Moreover, this API just-in-time compile the looping process into the machine code. ```python inputs = bp.inputs.section_input([0., 6.0, 0.], [100., 200., 100.]) indices = np.arange(inputs.size) def run(i, x): neu.step_run(i, x) return neu.V.value vs = bm.for_loop(run, (indices, inputs), progress_bar=True) ``` ##### `brainpy.LoopOverTime` Different from `for_loop`, `brainpy.LoopOverTime` is used for constructing a dynamical system that automatically loops the model over time when receiving an input. `for_loop` runs the model over time. While `brainpy.LoopOverTime` creates a model which will run the model over time when calling it. ```python net2.reset_state(batch_size=10) looper = bp.LoopOverTime(net2) out = looper(currents) ``` ##### `brainpy.DSRunner` **Initializing a `DSRunner`** Generally, we can initialize a runner for dynamical systems with the format of: ``` runner = DSRunner(target=instance_of_dynamical_system, inputs=inputs_for_target_DynamicalSystem, monitors=interested_variables_to_monitor, dyn_vars=dynamical_changed_variables, jit=enable_jit_or_not, progress_bar=report_the_running_progress, numpy_mon_after_run=transform_into_numpy_ndarray ) ``` - `target` specifies the model to be simulated. It must an instance of brainpy.DynamicalSystem. - `inputs` is used to define the input operations for specific variables. - It should be the format of `[(target, value, [type, operation])]`, where `target` is the input target, `value` is the input value, `type` is the input type (such as “fix”, “iter”, “func”), `operation` is the operation for inputs (such as “+”, “-”, “*”, “/”, “=”). Also, if you want to specify multiple inputs, just give multiple `(target, value, [type, operation])`, such as `[(target1, value1), (target2, value2)]`. - It can also be a function, which is used to manually specify the inputs for the target variables. This input function should receive one argument `tdi` which contains the shared arguments like time `t`, time step `dt`, and index `i`. - `monitors` is used to define target variables in the model. During the simulation, the history values of the monitored variables will be recorded. It can also to monitor variables by callable functions and it should be a `dict`. The `key` should be a string for later retrieval by `runner.mon[key]`. The `value` should be a callable function which receives an argument: `tdt`. - `dyn_vars` is used to specify all the dynamically changed [variables](https://brainpy.readthedocs.io/en/latest/tutorial_math/variables.html) used in the `target` model. - `jit` determines whether to use JIT compilation during the simulation. - `progress_bar` determines whether to use progress bar to report the running progress or not. - `numpy_mon_after_run` determines whether to transform the JAX arrays into numpy ndarray or not when the network finishes running. **Running a `DSRunner`** After initialization of the runner, users can call `.run()` function to run the simulation. The format of function `.run()` is showed as follows: ```python runner.run(duration=simulation_time_length, inputs=input_data, reset_state=whether_reset_the_model_states, shared_args=shared_arguments_across_different_layers, progress_bar=report_the_running_progress, eval_time=evaluate_the_running_time ) ``` - `duration` is the simulation time length. - `inputs` is the input data. If `inputs_are_batching=True`, `inputs` must be a PyTree of data with two dimensions: `(num_sample, num_time, ...)`. Otherwise, the `inputs` should be a PyTree of data with one dimension: `(num_time, ...)`. - `reset_state` determines whether to reset the model states. - `shared_args` is shared arguments across different layers. All the layers can access the elements in `shared_args`. - `progress_bar` determines whether to use progress bar to report the running progress or not. - `eval_time` determines whether to evaluate the running time. ### Monitors ```python # initialize monitor through a list of strings runner1 = bp.DSRunner(target=net, monitors=[&apos;E.spike&apos;, &apos;E.V&apos;, &apos;I.spike&apos;, &apos;I.V&apos;], # 4 elements in monitors inputs=[(&apos;E.input&apos;, 20.), (&apos;I.input&apos;, 20.)], jit=True) ``` Once we call the runner with a given time duration, the monitor will automatically record the variable evolutions in the corresponding models. Afterwards, users can access these variable trajectories by using .mon.[variable_name]. The default history times .mon.ts will also be generated after the model finishes its running. Let’s see an example. ```python runner1.run(100.) bp.visualize.raster_plot(runner1.mon.ts, runner1.mon[&apos;E.spike&apos;], show=True) ``` **Initialization with index specification** ```python monitors=[(&apos;E.spike&apos;, [1, 2, 3]), # monitor values of Variable at index of [1, 2, 3] &apos;E.V&apos;], # monitor all values of Variable &apos;V&apos; ``` &amp;gt; The monitor shape of &quot;E.V&quot; is (run length, variable size) = (1000, 3200) &amp;gt; The monitor shape of &quot;E.spike&quot; is (run length, index size) = (1000, 3) **Explicit monitor target** ```python monitors={&apos;spike&apos;: net.E.spike, &apos;V&apos;: net.E.V}, ``` &amp;gt; The monitor shape of &quot;V&quot; is = (1000, 3200) &amp;gt; The monitor shape of &quot;spike&quot; is = (1000, 3200) **Explicit monitor target with index specification** ```python monitors={&apos;E.spike&apos;: (net.E.spike, [1, 2]), # monitor values of Variable at index of [1, 2] &apos;E.V&apos;: net.E.V}, # monitor all values of Variable &apos;V&apos; ``` &amp;gt; The monitor shape of &quot;E.V&quot; is = (1000, 3200) &amp;gt; The monitor shape of &quot;E.spike&quot; is = (1000, 2) ### Inputs In brain dynamics simulation, various inputs are usually given to different units of the dynamical system. In BrainPy, `inputs` can be specified to runners for dynamical systems. The aim of `inputs` is to mimic the input operations in experiments like Transcranial Magnetic Stimulation (TMS) and patch clamp recording. `inputs` should have the format like `(target, value, [type, operation])`, where - `target` is the target variable to inject the input. - `value` is the input value. It can be a scalar, a tensor, or a iterable object/function. - `type` is the type of the input value. It support two types of input: `fix` and `iter`. The first one means that the data is static; the second one denotes the data can be iterable, no matter whether the input value is a tensor or a function. The `iter` type must be explicitly stated. - `operation` is the input operation on the target variable. It should be set as one of `{ + , - , * , / , = }`, and if users do not provide this item explicitly, it will be set to ‘+’ by default, which means that the target variable will be updated as `val = val + input`. #### Static inputs ```python runner6 = bp.DSRunner(target=net, monitors=[&apos;E.spike&apos;], inputs=[(&apos;E.input&apos;, 20.), (&apos;I.input&apos;, 20.)], # static inputs jit=True) runner6.run(100.) bp.visualize.raster_plot(runner6.mon.ts, runner6.mon[&apos;E.spike&apos;]) ``` #### Iterable inputs ```python I, length = bp.inputs.section_input(values=[0, 20., 0], durations=[100, 1000, 100], return_length=True, dt=0.1) runner7 = bp.DSRunner(target=net, monitors=[&apos;E.spike&apos;], inputs=[(&apos;E.input&apos;, I, &apos;iter&apos;), (&apos;I.input&apos;, I, &apos;iter&apos;)], # iterable inputs jit=True) runner7.run(length) bp.visualize.raster_plot(runner7.mon.ts, runner7.mon[&apos;E.spike&apos;]) ``` ## Run a built-in HH model [Using Built-in Models — BrainPy documentation](https://brainpy.readthedocs.io/en/latest/tutorial_building/overview_of_dynamic_model.html) ```python import brainpy as bp import brainpy.math as bm current, length = bp.inputs.section_input(values=[0., bm.asarray([1., 2., 4., 8., 10., 15.]), 0.], durations=[10, 2, 25], return_length=True) hh_neurons = bp.neurons.HH(current.shape[1]) runner = bp.DSRunner(hh_neurons, monitors=[&apos;V&apos;, &apos;m&apos;, &apos;h&apos;, &apos;n&apos;], inputs=(&apos;input&apos;, current, &apos;iter&apos;)) runner.run(length) ``` ## Run a HH model from scratch The mathematic expression of the HH model $$ \left\{\begin{aligned}&amp;c\frac{\mathrm{d}V}{\mathrm{d}t}=-\bar{g}_\text{Na}m^3h(V-E_\text{Na})-\bar{g}_\text{K}n^4(V-E_\text{K})-\bar{g}_\text{L}(V-E_\text{L})+I_\text{ext},\\&amp;\frac{\mathrm{d}n}{\mathrm{d}t}=\phi\left[\alpha_n(V)(1-n)-\beta_n(V)n\right]\\&amp;\frac{\mathrm{d}m}{\mathrm{d}t}=\phi\left[\alpha_m(V)(1-m)-\beta_m(V)m\right],\\&amp;\frac{\mathrm{d}h}{\mathrm{d}t}=\phi\left[\alpha_h(V)(1-h)-\beta_h(V)h\right],\end{aligned}\right. $$ $$ \begin{aligned}\alpha_n(V)&amp;=\frac{0.01(V+55)}{1-\exp\left(-\frac{V+55}{10}\right)},\quad\beta_n(V)&amp;=0.125\exp\left(-\frac{V+65}{80}\right),\\\alpha_h(V)&amp;=0.07\exp\left(-\frac{V+65}{20}\right),\quad\beta_n(V)&amp;=\frac{1}{\left(\exp\left(-\frac{V+55}{10}\right)+1\right)},\\\alpha_m(V)&amp;=\frac{0.1(V+40)}{1-\exp\left(-(V+40)/10\right)},\quad\beta_m(V)&amp;=4\exp\left(-(V+65)/18\right).\end{aligned} $$ $$ \phi=Q_{10}^{(T-T_{\mathrm{base}})/10} $$ V: the membrane potential n: activation variable of the Kt channel m: activation variable of the Nat channel h; inactivation variable of the Nat channe ### Define HH model `class` - Inherit `bp.dyn.NeuDyn` ```python import brainpy as bp import brainpy.math as bm class HH(bp.dyn.NeuDyn): def __init__(self, size, ENa=50., gNa=120., Ek=-77., gK=36., EL=-54.387, gL=0.03, V_th=0., C=1.0, T=6.3): super(HH, self).__init__(size=size) ``` ### Initialization ```python import brainpy as bp import brainpy.math as bm class HH(bp.dyn.NeuDyn): def __init__(self, size, ENa=50., gNa=120., Ek=-77., gK=36., EL=-54.387, gL=0.03, V_th=0., C=1.0, T=6.3): super(HH, self).__init__(size=size) # parameters self.ENa = ENa self.EK = EK self.EL = EL self.gNA = gNa self.gK = gK self.gL = gL self.C = C self.V_th = V_th self.T_base = 6.3 self.phi = 3.0 ** ((T - self.T_base) / 10.0) # variable self.V = bm.Variable(-70.68 * bm.ones(self.num)) self.m = bm.Variable(0.0266 * bm.ones(self.num)) self.h = bm.Variable(0.772 * bm.ones(self.num)) self.n = bm.Variable(0.235 * bm.ones(self.num)) self.input = bm.Variable(bm.zeros(self.num)) self.spike = bm.Variable(bm.zeros(self.num, dtype=bool)) self.t_last_spike = bm.Variable(bm.ones(self.num) * -1e7) # 定义积分函数 self.integral = bp.odeint(f=self.derivative, method=&apos;exp_auto&apos;) ``` ### Define the derivative function ```python @property def derivative(self): return bp.JointEq(self.dV, self.dm, self.dh, self.dn) def dV(self, V, t, m, h, n, Iext): I_Na = (self.gNa * m ** 3.0 * h) * (V - self.ENa) I_K = (self.gK * n ** 4.0) * (V - self.EK) I_leak = self.gL * (V - self.EL) dVdt = (- I_Na - I_K - I_leak + Iext) / self.C return dVdt def dm(self, m, t, V): alpha = 0.1 * (V + 40) / (1 - bm.exp(-(V + 40) / 10)) beta = 4.0 * bm.exp(-(V + 65) / 18) dmdt = alpha * (1 - m) - beta * m return self.phi * dmdt def dh(self, h, t, V): alpha = 0.07 * bm.exp(-(V + 65) / 20.) beta = 1 / (1 + bm.exp(-(V + 35) / 10)) dhdt = alpha * (1 - h) - beta * h return self.phi * dhdt def dn(self, n, t, V): alpha = 0.01 * (V + 55) / (1 - bm.exp(-(V + 55) / 10)) beta = 0.125 * bm.exp(-(V + 65) / 80) dndt = alpha * (1 - n) - beta * n return self.phi * dndt ``` ### Complete the `update()` function ```python def update(self, x=None): t = bp.share.load(&apos;t&apos;) dt = bp.share.load(&apos;dt&apos;) # TODO: 更新变量V, m, h, n, 暂存在V, m, h, n中 V, m, h, n = self.integral(self.V, self.m, self.h, self.n, t, self.input, dt=dt) #判断是否发生动作电位 self.spike.value = bm.logical_and(self.V = self.V_th) # 更新最后一次脉冲发放时间 self.t_last_spike.value = bm.where(self.spike, t, self.t_last_spike) # TODO: 更新变量V, m, h, n的值 self.V.value = V self.m.value = m self.h.value = h self.n.value = n #重置输入 self.input[:] = 0 ``` ### Simulation ```python current, length = bp.inputs.section_input(values=[0., bm.asarray([1., 2., 4., 8., 10., 15.]), 0.], durations=[10, 2, 25], return_length=True) hh_neurons = HH(current.shape[1]) runner = bp.DSRunner(hh_neurons, monitors=[&apos;V&apos;, &apos;m&apos;, &apos;h&apos;, &apos;n&apos;], inputs=(&apos;input&apos;, current, &apos;iter&apos;)) runner.run(length) ``` ### Visualization ```python import numpy as np import matplotlib.pyplot as plt bp.visualize.line_plot(runner.mon.ts, runner.mon.V, ylabel=&apos;V (mV)&apos;, plot_ids=np.arange(current.shape[1])) plt.plot(runner.mon.ts, bm.where(current[:, -1]&amp;gt;0, 10, 0) - 90.) plt.figure() plt.plot(runner.mon.ts, runner.mon.m[:, -1]) plt.plot(runner.mon.ts, runner.mon.h[:, -1]) plt.plot(runner.mon.ts, runner.mon.n[:, -1]) plt.legend([&apos;m&apos;, &apos;h&apos;, &apos;n&apos;]) plt.xlabel(&apos;Time (ms)&apos;) ``` ## Customize a conductance-based model 电路模拟，写成电导形式 ![image-20230824180831033](/BrainPy-course-notes/master_content/Notes.assets/image-20230824180831033.png) $$ \begin{aligned} \text{gK}&amp; =\bar{g}_\text{K}n^4, \\ \frac{\mathrm{d}n}{\mathrm{d}t}&amp; =\phi[\alpha_n(V)(1-n)-\beta_n(V)n], \end{aligned} $$ 动力学形式描述，引入门框变量$n$ $$ \begin{aligned} &amp;\alpha_{n}(V) =\frac{0.01(V+55)}{1-\exp(-\frac{V+55}{10})}, \\ &amp;\beta_{n}(V) =0.125\exp\left(-\frac{V+65}{80}\right). \end{aligned} $$ 由此式来建模钾离子通道 ### Programming an ion channel #### Three ion channel ```python import brainpy as bp import brainpy.math as bm class IK(bp.dyn.IonChannel): def __init__(self, size, E=-77., g_max=36., phi=1., method=&apos;exp_auto&apos;): super(IK, self).__init__(size) self.g_max = g_max self.E = E self.phi = phi self.n = bm.Variable(bm.zeros(size)) # variables should be packed with bm.Variable self.integral = bp.odeint(self.dn, method=method) def dn(self, n, t, V): alpha_n = 0.01 * (V + 55) / (1 - bm.exp(-(V + 55) / 10)) beta_n = 0.125 * bm.exp(-(V + 65) / 80) return self.phi * (alpha_n * (1. - n) - beta_n * n) def update(self, V): t = bp.share.load(&apos;t&apos;) dt = bp.share.load(&apos;dt&apos;) self.n.value = self.integral(self.n, t, V, dt=dt) def current(self, V): return self.g_max * self.n ** 4 * (self.E - V) ``` ```python class INa(bp.dyn.IonChannel): def __init__(self, size, E= 50., g_max=120., phi=1., method=&apos;exp_auto&apos;): super(INa, self).__init__(size) self.g_max = g_max self.E = E self.phi = phi self.m = bm.Variable(bm.zeros(size)) # variables should be packed with bm.Variable self.h = bm.Variable(bm.zeros(size)) self.integral_m = bp.odeint(self.dm, method=method) self.integral_h = bp.odeint(self.dh, method=method) def dm(self, m, t, V): # TODO: 计算dm/dt alpha_m = 0.11 * (V + 40) / (1 - bm.exp(-(V + 40) / 10)) beta_m = 4 * bm.exp(-(V + 65) / 18) return self.phi * (alpha_m * (1. - m) - beta_m * m) def dh(self, h, t, V): # TODO: 计算dh/dt alpha_h = 0.07 * bm.exp(-(V + 65) / 20) beta_h = 1. / (1 + bm.exp(-(V + 35) / 10)) return self.phi * (alpha_h * (1. - h) - beta_h * h) def update(self, V): t = bp.share.load(&apos;t&apos;) dt = bp.share.load(&apos;dt&apos;) # TODO: 更新self.m, self.h self.m.value = self.integral_m(self.m, t, V, dt=dt) self.h.value = self.integral_h(self.h, t, V, dt=dt) def current(self, V): return self.g_max * self.m ** 3 * self.h * (self.E - V) ``` ```python class IL(bp.dyn.IonChannel): def __init__(self, size, E=-54.39, g_max=0.03): super(IL, self).__init__(size) self.g_max = g_max self.E = E def current(self, V): return self.g_max * (self.E - V) def update(self, V): pass ``` #### Build a HH model with ion channels **Using customized ion channels** ```python class HH(bp.dyn.CondNeuGroup): def __init__(self, size): super(HH, self).__init__(size, V_initializer=bp.init.Uniform(-80, -60.)) # TODO: 初始化三个离子通道 self.IK = IK(size, E=-77., g_max=36.) self.INa = INa(size, E=50., g_max=120.) self.IL = IL(size, E=-54.39, g_max=0.03) ``` **Using built-in ion channels** ```python class HH(bp.dyn.CondNeuGroup): def __init__(self, size): super().__init__(size) self.INa = bp.channels.INa_HH1952(size) self.IK = bp.channels.IK_HH1952(size) self.IL = bp.cahnnels.IL(size, E=-54.387, g_max=0.03) ``` #### Simulation ```python neu = HH(1) runner = bp.DSRunner( neu, monitors=[&apos;V&apos;, &apos;IK.n&apos;, &apos;INa.m&apos;, &apos;INa.h&apos;], inputs=(&apos;input&apos;, 1.698) # near the threshold current ) runner.run(200) # the running time is 200 ms import matplotlib.pyplot as plt plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;V&apos;]) plt.xlabel(&apos;t (ms)&apos;) plt.ylabel(&apos;V (mV)&apos;) plt.savefig(&quot;HH.jpg&quot;) plt.show() plt.figure(figsize=(6, 2)) plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;IK.n&apos;], label=&apos;n&apos;) plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;INa.m&apos;], label=&apos;m&apos;) plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;INa.h&apos;], label=&apos;h&apos;) plt.xlabel(&apos;t (ms)&apos;) plt.legend() plt.savefig(&quot;HH_channels.jpg&quot;) plt.show() ``` ![image-20230824184016011](/BrainPy-course-notes/master_content/Notes.assets/image-20230824184016011.png) # Simple Neuron Modeling: Simplified Models ## The Leaky Integrate-and-Fire(LIF) Neuron Model ### The LIF neuron model $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-(V-V_{\mathrm{rest}})+RI(t)\\\\\mathrm{if}V&amp;&amp;gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\text{last}\ {t_{ref}}\end{aligned} $$ 只有一个微分方程，要加一个不应期(**t refractory period**)，膜电位不发生任何改变，认为离子通道只有泄露通道 ![image-20230825101057570](/BrainPy-course-notes/master_content/Notes.assets/image-20230825101057570.png) Given a constant current input: ![image-20230825101410745](/BrainPy-course-notes/master_content/Notes.assets/image-20230825101410745.png) 没有建模准确变化，只提供什么时候膜电位的变化 ### The dynamic features of the LIF model **General solution (constant input):**$V(t)=V_{\text{reset}}+RI_{\text{c}}(1-\mathrm{e}^{-\frac{t-t_0}{\tau}})$ **Firing frequency:** $$ \begin{aligned}T&amp;=-\tau\ln\left(1-\frac{V_{\phi h}-V_{\mathrm{rest}}}{RI_{\varsigma}}\right)\\f&amp;=\frac{1}{T+t_{\mathrm{ref}}}=\frac{1}{t_{\mathrm{ref}}-\tau\ln\left(1-\frac{V_{0}-V_{\mathrm{rest}}}{RI_{\varsigma}}\right)}\end{aligned} $$ **Rheobase current (minimal current):** $$ I_{\theta}=\frac{V_{\mathrm{th}}-V_{\mathrm{reset}}}{R} $$ 基强电流，如果小于它将无法发放 ### Strengths &amp; weaknesses of the LIF model #### Strengths - Simple, high simulation efficiency - Intuitive - Fits well the subthreshold membrane potential #### Weaknesses - The shape of action potentials is over-simplified - Has no memory of the spiking history - Cannot reproduce diverse firing patterns ### Other Univariate neuron models #### The Quadratic Integrate-and-Fire (QOF) model: $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=a_{0}(V-V_{\mathrm{re}t})(V-V_{\mathrm{c}})+RI(t)\\&amp;\text{if }V&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{re}set}\quad\text{last}\quad t_{\mathrm{ref}}\end{aligned} $$ ![image-20230825103243039](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103243039.png) 膜电位仍需要手动重置 #### The Theta neuron model $$ \frac{\mathrm{d}\theta}{\mathrm{d}t}=1-\cos\theta+(1+\cos\theta)(\beta+I(t)) $$ ![image-20230825103331170](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103331170.png) 隐式表达，不具有物理意义，但也会进行整合发放 #### The Exponential Integrate-and-Fire (ExpIF) model $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-\left(V-V_{\mathrm{res}t}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{3T}}+RI(t)\\\mathrm{if~}V&amp;&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{res}t}\mathrm{last}t_{\mathrm{ref}}\end{aligned} $$ ![image-20230825103501912](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103501912.png) 仍需要手动重置膜电位 ## The Adaptive Exponential Integrate-and-Fire(AdEx) Neuron Model ### The AdEx neuron model Two variables: - 𝑉: membrane potential - 𝑤: adaptation variable $$ \begin{aligned} \tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{S_{T}}}-Rw+RI(t) \\ \tau_{w}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{\mathrm{w}}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ \mathrm{if}V&amp; &amp;gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} \end{aligned} $$ 不为零，就会衰减到$-w$ ![image-20230825103840880](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103840880.png) - A larger 𝑤 suppresses 𝑉 from increasing - 𝑤 decays exponentially while having a sudden increase when the neuron fires **Firing patterns of the AdEx model** ![image-20230825104254936](/BrainPy-course-notes/master_content/Notes.assets/image-20230825104254936.png) **Categorization of firing patterns** According to the steady-state firing time intervals: - Tonic/regular spiking - Adapting - Bursting - Irregular spiking According to the initial-state features: - Tonic/classic spiking - Initial burst - Delayed spiking ### Other multivariate neuron models #### The Izhikevich model $$ \begin{aligned} &amp;\frac{dV}{dt} =0.04V^{2}+5V+140-u+I \\ &amp;\frac{\mathrm{d}u}{\mathrm{d}t} =a\left(bV-u\right) \\ &amp;\operatorname{if}V &amp;gt;\theta,\quad V\leftarrow c,u\leftarrow u+d\text{ last }t_{\mathrm{ref}} \end{aligned} $$ 二次整合发放多加了一个$u$ ![image-20230825104832770](/BrainPy-course-notes/master_content/Notes.assets/image-20230825104832770.png) #### The FitzHugh–Nagumo (FHN) model $$ \begin{aligned}\dot{v}&amp;=v-\frac{v^3}3-w+RI_{\mathrm{ext}}\\\tau\dot{w}&amp;=v+a-bw.\end{aligned} $$ 没有对膜电位进行人为的重置，可以更好的进行动力学分析，没有打破微分方程的连续性 ![image-20230825104922636](/BrainPy-course-notes/master_content/Notes.assets/image-20230825104922636.png) #### The Generalized Integrate-and-Fire (GIF) model n+2个变量 $$ \begin{aligned} &amp;\tau{\frac{\mathrm{d}V}{\mathrm{d}t}} =-\left(V-V_{\mathrm{rest}}\right)+R\sum_{j}I_{j}+RI \\ &amp;\frac{\mathrm{d}\Theta}{\mathrm{d}t} =a\left(V-V_{\mathrm{rest}}\right)-b\left(\Theta-\Theta_{\infty}\right) \\ &amp;\frac{\mathrm{d}l_{j}}{\mathrm{d}t} =-k_{j}I_{j},\quad j=1,2,...,n \\ &amp;\operatorname{if}V &amp;gt;\Theta,\quad I_{j}\leftarrow R_{j}I_{j}+A_{j},V\leftarrow V_{\mathrm{reset}},\Theta\leftarrow max(\Theta_{\mathrm{reset}},\Theta) \end{aligned} $$ 每个变量都是线性的，泛化性体现在重置条件上 ![image-20230825105035349](/BrainPy-course-notes/master_content/Notes.assets/image-20230825105035349.png) ## Dynamic analysis: phase-plane analysis ### Phase plane analysis 对动力学系统的行为来分析，普遍对两个变量来进行分析 Analyzes the behavior of a dynamical system with (usually two) variables described by ordinary differential equations $$ \begin{aligned} &amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp;&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{S_{T}}}-Rw+RI(t) \\ &amp;\tau_{W}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp;&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ &amp;\mathrm{if}V&amp;&amp; &amp;gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} \end{aligned} $$ **Elements:** - Nullclines: $\mathrm{d}V/\mathrm{d}t=0;\mathrm{d}w/\mathrm{d}t=0$ - Fixed points: $\mathrm{d}V/\mathrm{d}t=0\mathrm{~and~}\mathrm{d}w/\mathrm{d}t=0$ - The vector field - The trajectory of variables 假设外部电流恒定 ![image-20230825110708994](/BrainPy-course-notes/master_content/Notes.assets/image-20230825110708994.png) ### Phase plane analysis for the AdEx neuron model $$ \begin{aligned} &amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp;&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{\Lambda_{T}}}-Rw+RI(t) \\ &amp;\tau_{w}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp;&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ &amp;\text{ifV}&amp;&amp; &amp;gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} \end{aligned} $$ ![image-20230825110811399](/BrainPy-course-notes/master_content/Notes.assets/image-20230825110811399.png) #### Tonic ![image-20230825112857175](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112857175.png) #### Adaptation ![image-20230825112918815](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112918815.png) #### Bursting ![image-20230825112933938](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112933938.png) #### Transient spiking ![image-20230825112950297](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112950297.png) ## Dynamic analysis: bifurcation analysis ### Bifurcation analysis Quantitative analysis of the existence and the properties of fixed points in a dynamical system with a changing parameter 某个外界条件变化时，固定点的变化 Elements: - Lines of fixed points - Stability properties of fixed points ![image-20230825114510710](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114510710.png) ### Bifurcation analysis for the AdEx Neuron model bifurcation analysis for 2 variables Variables: 𝑉 and 𝑤 Parameters: $I_{ext}$ $$ \begin{aligned} &amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}=-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{{\frac{V-V_{T}}{ST}}}-Rw+RI(t) \\ &amp;\text{-} {\frac{\mathrm{d}w}{\mathrm{d}t}}=a(V-V_{\mathrm{rest}})-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ &amp;\mathrm{if}V&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{reset}}\ \mathrm{last}\ t_{\mathrm{ref}} \end{aligned} $$ ![image-20230825114801456](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114801456.png) ![image-20230825114742740](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114742740.png) **Subjects: two variables (𝑉 and 𝑤)** ![image-20230825114856403](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114856403.png) ### Extended: The limit cycle The FitzHugh–Nagumo (FHN) model $$ \begin{aligned}\dot{v}&amp;=v-\frac{v^3}3-w+RI_\mathrm{ext}\\\tau\dot{w}&amp;=v+a-bw.\end{aligned} $$ This dynamical system, in certain conditions, exhibits a cyclic pattern of variable changes which can be visualized as a closed trajectory in the phase plane. 变化锁定到环中 ![image-20230825115348008](/BrainPy-course-notes/master_content/Notes.assets/image-20230825115348008.png) ![image-20230825115354146](/BrainPy-course-notes/master_content/Notes.assets/image-20230825115354146.png) # Reduced Models - brain dynamics programming ## LIF neuron models programming ### Define LIF `class` $$ \begin{aligned}&amp;\tau\frac{\mathrm{d}V}{\mathrm{d}t}=-(V-V_{\mathrm{rest}})+RI(t)\\&amp;\text{if }V&amp;gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\text{last}t_{\mathrm{ref}}\end{aligned} $$ ```python class LIF(bp.dyn.NeuDyn): def __init__(self, size, V_rest=0, V_reset=-5, V_th=20, R=1, tau=10, t_ref=5., **kwargs): # 初始化父类 super(LIF, self).__init__(size=size, **kwargs) ``` ### Initialization ```python class LIF(bp.dyn.NeuDyn): def __init__(self, size, V_rest=0, V_reset=-5, V_th=20, R=1, tau=10, t_ref=5., **kwargs): # 初始化父类 super(LIF, self).__init__(size=size, **kwargs) # 初始化参数 self.V_rest = V_rest self.V_reset = V_reset self.V_th = V_th self.R = R self.tau = tau self.t_ref = t_ref # 不应期时长 # 初始化变量 self.V = bm.Variable(bm.random.randn(self.num) + V_reset) self.input = bm.Variable(bm.zeros(self.num)) self.t_last_spike = bm.Variable(bm.ones(self.num) * -1e7) # 上一次脉冲发放时间 self.refractory = bm.Variable(bm.zeros(self.num, dtype=bool)) # 是否处于不应期 self.spike = bm.Variable(bm.zeros(self.num, dtype=bool)) # 脉冲发放状态 # 使用指数欧拉方法进行积分 self.integral = bp.odeint(f=self.derivative, method=&apos;exponential_euler&apos;) ``` ### Define the derivative function ```python # 定义膜电位关于时间变化的微分方程 def derivative(self, V, t, Iext): dVdt = (-V + self.V_rest + self.R * Iext) / self.tau return dVdt ``` ### Complete the `update()` function ```python def update(self): t, dt = bp.share[&apos;t&apos;], bp.share[&apos;dt&apos;] # 以数组的方式对神经元进行更新 refractory = (t - self.t_last_spike) self.V_th # 将大于阈值的神经元标记为发放了脉冲 self.spike[:] = spike # 更新神经元脉冲发放状态 self.t_last_spike[:] = bm.where(spike, t, self.t_last_spike) # 更新最后一次脉冲发放时间 self.V[:] = bm.where(spike, self.V_reset, V) # 将发放了脉冲的神经元膜电位置为V_reset，其余不变 self.refractory[:] = bm.logical_or(refractory, spike) # 更新神经元是否处于不应期 self.input[:] = 0. # 重置外界输入 ``` ### Simulation ```python def run_LIF(): # 运行LIF模型 group = LIF(1) runner = bp.DSRunner(group, monitors=[&apos;V&apos;], inputs=(&apos;input&apos;, 22.)) runner(200) # 运行时长为200ms # 结果可视化 fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) plt.plot(runner.mon.ts, runner.mon.V) plt.xlabel(r&apos;$t$ (ms)&apos;) plt.ylabel(r&apos;$V$ (mV)&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) plt.show() ``` ![image-20230825141201825](/BrainPy-course-notes/master_content/Notes.assets/image-20230825141201825.png) ### Input current &amp; firing frequency $$ \begin{gathered} V(t)=V_{\mathrm{reset}}+RI_{\mathrm{c}}(1-\mathrm{e}^{-\frac{t-t_{0}}{\tau}}). \\ T=-\tau\ln\left[1-\frac{V_{\mathrm{th}}-V_{\mathrm{rest}}}{RI_{\mathrm{c}}}\right] \\ f={\frac{1}{T+t_{\mathrm{ref}}}}={\frac{1}{t_{\mathrm{ref}}-\tau\ln\left[1-{\frac{V_{\mathrm{th}}-V_{\mathrm{rest}}}{RI_{c}}}\right]}} \end{gathered} $$ ```python # 输入与频率的关系 current = bm.arange(0, 600, 2) duration = 1000 LIF_neuron = LIF(current.shape[0]) runner_2 = bp.dyn.DSRunner(LIF_neurons, monitors=[&apos;spike&apos;], inputs={&apos;input&apos;, current}, dt=0.01) runner_2.run(duration) freqs = runner_2.mon.spike.sum(axis=0) / (duration/1000) plt.figure() plt.plot(current, freqs) plt.xlabel(&apos;inputs&apos;) plt.ylabel(&apos;frequencies&apos;) ``` ![image-20230825143405952](/BrainPy-course-notes/master_content/Notes.assets/image-20230825143405952.png) ### Other Univariate neuron models **The Quadratic Integrate-and-Fire (QIF) model** $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=a_{0}(V-V_{\mathrm{res}t})(V-V_{c})+RI(t)\\\mathrm{if~}V&amp;&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{reset~last~}t_{\mathrm{ref}}}\end{aligned} $$ ```python def derivative(self, V, t, I): dVdt = (self.c * (V - self.V_reset) * (V - self.V_c) + self.R * I) / self.tau return dVdt ``` **The Exponential Integrate-and-Fire (ExpIF) model** $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{\delta_{T}}}+RI(t)\\&amp;\mathrm{if~}V&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{reset}}\mathrm{last}t_{\mathrm{ref}}\end{aligned} $$ ```python def derivative(self, V, t, I): exp_v = self.delta_T * bm.exp((V - self.V_T) / self.delta_T) dvdt = (- (V - self.V_rest) + exp_v + self.R * I) / self.tau return dvdt ``` ## AdEx neuron models programming $$ \begin{gathered} \tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}=-(V-V_{\mathrm{rest}})+\Delta_{T}\mathrm{e}^{{\frac{V-V_{T}}{\Delta T}}}-Rw+RI(t), \\ \tau_{w}\frac{\mathrm{d}w}{\mathrm{d}t}=a(V-V_{\mathrm{rest}})-w+b\tau_{w}\sum_{t^{(f)}}\delta(t-t^{(f)})), \\ \mathrm{if~}V&amp;gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\mathrm{last}t_{\mathrm{ref}}. \end{gathered} $$ ### Define AdEx `class` ```python class AdEx(bp.dyn.NeuDyn): def __init__(self, size, V_rest=-65, V_reset=-68, V_th=-30, V_T=-59.9, delta_T=3.48 a=1., b=1., R=1., tau=10., tau_w=30., tau_ref=0., **kwargs): # 初始化父类 super(AdEx, self).__init__(size=size, **kwargs) ``` ### Initialization ```python class AdEx(bp.dyn.NeuDyn): def __init__(self, size, V_rest=-65, V_reset=-68, V_th=-30, V_T=-59.9, delta_T=3.48 a=1., b=1., R=1., tau=10., tau_w=30., tau_ref=0., **kwargs): # 初始化父类 super(AdEx, self).__init__(size=size, **kwargs) # 初始化参数 self.V_rest = V_rest self.V_reset = V_reset self.V_th = V_th self.V_T = V_T self.delta_T = delta_T self.a = a self.b = b self.R = R self.tau = tau self.tau_w = tau_w self.tau_ref = tau_ref # 初始化变量 self.V = bm.Variable(bm.random.randn(self.num) - 65.) self.w = bm.Variable(bm.zeros(self.num)) self.input = bm.Variable(bm.zeros(self.num)) self.t_last_spike = bm.Variable(bm.ones(self.num) * -1e7) # 上一次脉冲发放时间 self.refractory = bm.Variable(bm.zeros(self.num, dtype=bool)) # 是否处于不应期 self.spike = bm.Variable(bm.zeros(self.num, dtype=bool)) # 脉冲发放状态 # 定义积分器 self.integral = bp.odeint(f=self.derivative, method=&apos;exp_auto&apos;) ``` ### Define the derivative function ```python def dV(self, V, t, w, I): exp = self.delta_T * bm.exp((V - self.V_T) / self.delta_T) dVdt = (-V + self.V_rest + exp - self.R * w + self.R * I) / self.tau return dVdt def dw(self, w, t, V): dwdt = (self.a * (V - self.V_rest) - w) / self.tau_w return dwdt @property def derivative(self): return bp.JointEq([self.dV, self.dw]) ``` ### Complete the `update()` function ```python def update(self): t, dt = bp.share[&apos;t&apos;], bp.share[&apos;dt&apos;] V, w = self.integral(self.V.value, self.w.value, t, self.input, dt=dt) # 以数组的方式对神经元进行更新 refractory = (t - self.t_last_spike) self.V_th # 将大于阈值的神经元标记为发放了脉冲 self.spike[:] = spike # 更新神经元脉冲发放状态 self.t_last_spike[:] = bm.where(spike, t, self.t_last_spike) # 更新最后一次脉冲发放时间 self.V[:] = bm.where(spike, self.V_reset, V) # 将发放了脉冲的神经元膜电位置为V_reset，其余不变 self.w[:] = bm.where(spike, w + self.b, w) #更新自适应电流 self.refractory[:] = bm.logical_or(refractory, spike) # 更新神经元是否处于不应期 self.input[:] = 0. # 重置外界输入 ``` ### Simulation ![image-20230825145518709](/BrainPy-course-notes/master_content/Notes.assets/image-20230825145518709.png) ### Other multivariate neuron models **The Izhikevich model** $$ \begin{aligned} &amp;\frac{dV}{dt} =0.04V^{2}+5V+140-u+I \\ &amp;\frac{\mathrm{d}u}{\mathrm{d}t} =a\left(bV-u\right) \\ &amp;\operatorname{if}V &amp;gt;\theta,\quad V\leftarrow c,u\leftarrow u+d\mathrm{last}t_{\mathrm{ref}} \end{aligned} $$ ```python def dV(self, V, t, u, I): dVdt = 0.04 * V * V + 5 * V + 140 - u + I return dVdt def du(self, u, t, V): dudt = self.a * (self.b * V - u) return dudt ``` **The Generalized Integrate-and-Fire (GIF) model** $$ \begin{aligned} &amp;\tau{\frac{\mathrm{d}V}{\mathrm{d}t}} =-\left(V-V_{\mathrm{rest}}\right)+R\sum_{j}I_{j}+RI \\ &amp;\frac{\mathrm{d}\Theta}{\mathrm{d}t} =a\left(V-V_{\mathrm{est}}\right)-b\left(\Theta-\Theta_{\infty}\right) \\ &amp;\frac{\mathrm{d}I_j}{\mathrm{d}r} =-k_jI_j,\quad j=1,2,\ldots,n \\ &amp;\text{if V} &amp;gt;\Theta,\quad I_{j}\leftarrow R_{j}I_{j}+A_{j},V\leftarrow V_{\mathrm{reset}},\Theta\leftarrow max\left(\Theta_{\mathrm{reset}},\Theta\right) \end{aligned} $$ ```python def dI1(self, I1, t): return - self.k1 * I1 def dI2(self, I2, t): return - self.k2 * I2 def dVth(self, V_th, t, V): return self.a * (V - self.v_rest) - self.b * (V_th - self.V_th_inf) def dV(self, V, t, I1, I2, I): return (- (V - self.V_rest) + self.R * (I + I1 + I2)) / self.tau ``` **Built-in reduced neuron models** ![image-20230825145947800](/BrainPy-course-notes/master_content/Notes.assets/image-20230825145947800.png) ## Dynamic analysis: phase-plane analysis ### Simple case $$ \frac{dx}{dt}=\sin(x)+I, $$ ```python @bp.odeint def int_x(x, t, Iext): return bp.math.sin(x) + Iext ``` ```python pp = bp.analysis.PhasePlane1D( model=int_x, target_vars={&apos;x&apos;: [-10, 10]}, pars_update={&apos;Iext&apos;: 0.}, resolutions={&apos;x&apos;: 0.01} ) pp.plot_vector_field() pp.plot_fixed_point(show=True) ``` ![image-20230825152003373](/BrainPy-course-notes/master_content/Notes.assets/image-20230825152003373.png) - Nullcline: The zero-growth isoclines, such as $f(x,y) = 0$ and $g(x,y) = 0$ - Fixed points: The equilibrium points of the system, which are located at all the nullclines intersect. - Vector field: The vector field of the system. - Limit cycles: The limit cycles. - Trajectories: A simulation trajectory with the given initial values ### Phase plane analysis for AdEx ```python def ppa_AdEx(group): bm.enable_x64() v_range = [-70., -40.] w_range = [-10., 50.] phase_plane_analyzer = bp.analysis.PhasePlane2D( model=group, target_vars={&apos;V&apos;: v_range, &apos;w&apos;: w_range, }, # 待分析变量 pars_update={&apos;I&apos;: Iext}, # 需要更新的变量 resolutions=0.05 ) # 画出V, w的零增长曲线 phase_plane_analyzer.plot_nullcline() # 画出奇点 phase_plane_analyzer.plot_fixed_point() # 画出向量场 phase_plane_analyzer.plot_vector_field() # 分段画出V, w的变化轨迹 group.V[:], group.w[:] = group.V_reset, 0 runner = bp.DSRunner(group, monitors=[&apos;V&apos;, &apos;w&apos;, &apos;spike&apos;], inputs=(&apos;input&apos;, Iext)) runner(500) spike = runner.mon.spike.squeeze() s_idx = np.where(spike)[0] # 找到所有发放动作电位对应的index s_idx = np.concatenate(([0], s_idx, [len(spike) - 1])) # 加上起始点和终止点的index for i in range(len(s_idx) - 1): vs = runner.mon.V[s_idx[i]: s_idx[i + 1]] ws = runner.mon.w[s_idx[i]: s_idx[i + 1]] plt.plot(vs, ws, color=&apos;darkslateblue&apos;) # 画出虚线 x = V_reset plt.plot([group.V_reset, group.V_reset], w_range, &apos;--&apos;, color=&apos;grey&apos;, zorder=-1) plt.show() ``` ![image-20230825152925463](/BrainPy-course-notes/master_content/Notes.assets/image-20230825152925463.png) ## Dynamic analysis: bifurcation analysis ### Simple case $$ \frac{dx}{dt}=\sin(x)+I, $$ ```python bif = bp.analysis.Bifurcation1D( model=int_x, target_vars={&apos;x&apos;: [-10, 10]}, target_pars={&apos;Iext&apos;: [0., 1.5]}, resolutions={&apos;Iext&apos;: 0.005, &apos;x&apos;: 0.05} ) bif.plot_bifurcation(show=True) ``` ![image-20230825154227567](/BrainPy-course-notes/master_content/Notes.assets/image-20230825154227567.png) # Synapse models and their programming ## The biology of synapses ### Neurotransmitter &amp; Synapse When the action potential invades the axon terminals, it causes voltage-gated 𝐶𝐶𝑎𝑎 2+ channels to open (1), which triggers vesicles to bind to the presynaptic membrane (2). Neurotransmitter is released into the synaptic cleft by exocytosis and diffuses across the cleft (3). Binding of the neurotransmitter to receptor molecules in the postsynaptic membrane completes the process of transmission (4). 去极化时钙离子内流，与囊泡相结合，...，与受体结合，打开离子通道，超极化、去极化现象 ![image-20230826100321307](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100321307.png) ![image-20230826100418911](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100418911.png) **Neurotransmitter leading to postsynaptic potential.** The binding of neurotransmitter to the postsynaptic membrane receptors changes the membrane potential ($V_m$). These postsynaptic potentials can be either excitatory (depolarizing the membrane), as shown here, or inhibitory (hyperpolarizing the membrane). ![image-20230826100531535](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100531535.png) ### Neurotransmitters 兴奋性神经递质： - 乙酰胆碱 (ACh) - 儿茶酚胺 (catecholamines) - 谷氨酸 (glutamate) - 组胺 (histamine) - 5-羟色胺 (serotonin) - 某些神经肽类 (some of neuropeptides) 抑制性神经递质： - GABA - 甘氨酸 (glycine) - 某些神经肽类 (some of peptides) ![image-20230826100609904](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100609904.png) ### The postsynaptic response The aim of a synapse model is to describe accurately the postsynaptic response generated by the arrival of an action potential at a presynaptic terminal. 1. The fundamental quantity to be modelled is the time course of the postsynaptic receptor conductance 2. The models: - Simple phenomenological waveforms - More complex kinetic schemes that are analogous to the models of membrane- bound ion channels ![image-20230826100701580](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100701580.png) 建模这种响应模式，打开关闭的概率... ## Phenomenological synapse models ### Exponential Model ![image-20230826100738460](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100738460.png) **Assumption**: - The release of neurotransmitter, its diffusion across the cleft, the receptor binding, and channel opening all happen very quickly, so that the channels instantaneously jump from the closed to the open state. channel会瞬间增加然后逐渐关闭 $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}e^{-(t-t_{0})/\tau} \\ \begin{matrix}\bullet&amp;\tau \ \text{is the time constant}\\\bullet&amp;t_0 \ \text{is the time of the pre-synaptic spike}\\\bullet&amp;\bar{g_{syn}}\ \text{is the maximal conductance}\end{matrix} $$ -&amp;gt; corresponding differential equation $$ \tau\frac{dg_{\mathrm{syn}}(t)}{dt}=-g_{\mathrm{syn}}(t)+\bar{g}_{\mathrm{syn}}\delta\left(t_{0}-t\right) $$ - Can fit with experimental data. - A good approximation for GABA A and AMPA, because the rising phase is much shorter than their decay phase. ### Dual Exponential Model ![image-20230826101203059](/BrainPy-course-notes/master_content/Notes.assets/image-20230826101203059.png) exponential model上升的太快，不太符合某些synapse Dual exponential synapse provides a general way to describe the synaptic conductance with different rising and decay time constants. $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}\frac{\tau_{1}\tau_{2}}{\tau_{1}-\tau_{2}}\left(\exp\left(-\frac{t-t_{0}}{\tau_{1}}\right)-\exp\left(-\frac{t-t_{0}}{\tau_{2}}\right)\right) \\ \begin{matrix} \bullet &amp;t_1\ \text{is the decay synaptic time constant} \\ \bullet &amp;\tau_2\ \text{is the rise synaptic time constant} \\ \bullet &amp;t_0\ \text{is the time of the pre-synaptic spike} \\ \bullet &amp;\bar{g}_{syn}\ \text{is the maximal conductance} \end{matrix} $$ -&amp;gt;corresponding differential equation $$ \begin{aligned} &amp;g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}g \\ &amp;\frac{dg}{dt}=-\frac{g}{\tau_{\mathrm{decay}}}+h \\ &amp;\frac{dh}{dt}&amp; =-\frac{h}{\tau_{\mathrm{rise}}}+\delta\left(t_{0}-t\right), \end{aligned} $$ The time course of most synaptic conductance can be well described by this sum of two exponentials. ### Synaptic time constants ![image-20230826101544786](/BrainPy-course-notes/master_content/Notes.assets/image-20230826101544786.png) http://compneuro.uwaterloo.ca/research/constants-constraints/neurotransmitter-time-constants-pscs.html #### AMPA synapse - $t_{decay}$ = 0.18 ms in the auditory system of the chick nucleus magnocellularis (Trussell, 1999). - $t_{rise}$ 25 ms and $\tau_{decay}$ =0.77 ms in dentate gyrus basket cells (Geiger et al., 1997). - $t_{rise}$ = 0.2 ms and $\tau_{decay}$ =1.7 ms in in neocortical layer 5 pyramidal neurons (Hausser and Roth, 1997b). - Reversal potential is nearly 0 mV. #### NMDA synapse - The decay time constants (at near-physiological temperature): - 19 ms in dentate gyrus basket cells (Geiger et al., 1997), - 26 ms in neocortical layer 2/3 pyramidal neurons (Feldmeyer et al., 2002), - 89 ms in CA1 pyramidal cells (Diamond, 2001). - The rise time constants are about 2 ms (Feldmeyer et al., 2002). - Reversal potential is nearly 0 mV. #### GABA$_A$ synapse - GABAergic synapses from dentate gyrus basket cells onto other basket cells are faster: $t_{rise}$ = 0.3 ms and $t_{decay}$ = 2.5 ms (Bartos et al., 2001) than synapses from basket cells to granule cells: $t_{rise}$ = 0.26 ms and $t_{decay}$ = 6.5 ms (Kraushaar and Jonas, 2000). - Reversal potential is nearly -80 mV. #### GABA$_B$ synapse - Common models use models with a rise time of about 25-50 ms, a fast decay time in the range of 100-300ms and a slow decay time of 500-1000 ms. - Reversal potential is nearly -90 mV. ### General property of synaptic time constants - The time constants of synaptic conductance vary widely among synapse types. - The synaptic kinetics tends to accelerate during development (T. Takahashi, Neuroscience Research, 2005) . - The synaptic kinetics becomes substantially faster with increasing temperature. ![image-20230826102033433](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102033433.png) ### Current- and Conductance-based Response ![image-20230826102042614](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102042614.png) #### Conductance-based Response Most synaptic ion channels, such as AMPA and GABA, display an approximately linear current-voltage relationship when they open. ![image-20230826102113670](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102113670.png) **For example**: The synapse is located on a thin dendrite, because the local membrane potential V changes considerably when the synapse is activated. #### Current-based Response In some case, we can also approximate the synapses as sources of current and not a conductance. ![image-20230826102150487](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102150487.png) **For example**: The excitatory synapse on a large compartment, because the depolarization of the membrane is small. ## Programming of phenomenological synapse models ### `ProjAlignPostMg2` ![Image Name](https://cdn.kesci.com/upload/rzz4o4uyar.png?imageView2/0/w/960/h/960) ```python brainpy.dyn.ProjAlignPostMg2( pre, delay, comm, syn, out, post ) ``` - ``pre (JointType[DynamicalSystem, AutoDelaySupp])``: The pre-synaptic neuron group. - ``delay (Union[None, int, float])``: The synaptic delay. - ``comm (DynamicalSystem)``: The synaptic communication. - ``syn (ParamDescInit)``: The synaptic dynamics. - ``out (ParamDescInit)``: The synaptic output. - ``post (DynamicalSystem)`` The post-synaptic neuron group. 只需要建模所有post的neurons ### CSR matrix ![Image Name](https://cdn.kesci.com/upload/rzz4on32hr.png?imageView2/0/w/960/h/960) ### Exponential Model The single exponential decay synapse model assumes the release of neurotransmitter, its diffusion across the cleft, the receptor binding, and channel opening all happen very quickly, so that the channels instantaneously jump from the closed to the open state. Therefore, its expression is given by $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} e^{-\left(t-t_{0}\right) / \tau} $$ where $\tau$ is the time constant, $t_0$ is the time of the pre-synaptic spike, $\bar{g}_{\mathrm{syn}}$ is the maximal conductance. The corresponding differential equation: $$ \frac{d g}{d t} = -\frac{g}{\tau_{decay}}+\sum_{k} \delta(t-t_{j}^{k}). $$ #### COBA Given the synaptic conductance, the COBA model outputs the post-synaptic current with $$ I_{syn}(t) = g_{\mathrm{syn}}(t) (E - V(t)) $$ ```python class ExponSparseCOBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.COBA.desc(E=E), post=post, ) ``` ```python class SimpleNet(bp.DynSysGroup): def __init__(self, E=0.): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = ExponSparseCOBA(self.pre, self.post, delay=None, prob=1., g_max=1., tau=5., E=E) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` ```python def run_a_net(net): indices = np.arange(1000) # 100 ms conductances, currents, potentials = bm.for_loop(net.step_run, indices, progress_bar=True) ts = indices * bm.get_dt() # --- similar to: # runner = bp.DSRunner(net) # conductances, currents, potentials = runner.run(100.) fig, gs = bp.visualize.get_figure(1, 3, 3.5, 4) fig.add_subplot(gs[0, 0]) plt.plot(ts, conductances) plt.title(&apos;Syn conductance&apos;) fig.add_subplot(gs[0, 1]) plt.plot(ts, currents) plt.title(&apos;Syn current&apos;) fig.add_subplot(gs[0, 2]) plt.plot(ts, potentials) plt.title(&apos;Post V&apos;) plt.show() ``` #### CUBA Given the conductance, this model outputs the post-synaptic current with a identity function: $$ I_{\mathrm{syn}}(t) = g_{\mathrm{syn}}(t) $$ ```python class ExponSparseCUBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.CUBA.desc(), post=post, ) ``` ```python class SimpleNet2(bp.DynSysGroup): def __init__(self, g_max=1.): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = ExponSparseCUBA(self.pre, self.post, delay=None, prob=1., g_max=g_max, tau=5.) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` #### Dense connections Exponential synapse model with the conductance-based (COBA) output current and dense connections. ```python class ExponDenseCOBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.MaskedLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.COBA.desc(E=E), post=post, ) ``` ![Image Name](https://cdn.kesci.com/upload/rzz4p7x6dl.png?imageView2/0/w/960/h/960) Exponential synapse model with the current-based (COBA) output current and dense connections. ```python class ExponDenseCUBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.MaskedLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.CUBA.desc(), post=post, ) ``` ### `ProjAlignPreMg2` Synaptic projection which defines the synaptic computation with the dimension of presynaptic neuron group. ![Image Name](https://cdn.kesci.com/upload/rzz4pj1qmk.png?imageView2/0/w/960/h/960) ```python brainpy.dyn.ProjAlignPreMg2( pre, delay, syn, comm, out, post ) ``` - ``pre (JointType[DynamicalSystem, AutoDelaySupp])``: The pre-synaptic neuron group. - ``delay (Union[None, int, float])``: The synaptic delay. - ``syn (ParamDescInit)``: The synaptic dynamics. - ``comm (DynamicalSystem)``: The synaptic communication. - ``out (ParamDescInit)``: The synaptic output. - ``post (DynamicalSystem)`` The post-synaptic neuron group. ### Dual Exponential Model The dual exponential synapse model, also named as **difference of two exponentials model**, is given by: $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} \frac{\tau_{1} \tau_{2}}{\tau_{1}-\tau_{2}}\left(\exp \left(-\frac{t-t_{0}}{\tau_{1}}\right)-\exp \left(-\frac{t-t_{0}}{\tau_{2}}\right)\right) $$ where $\tau_1$ is the time constant of the decay phase, $\tau_2$ is the time constant of the rise phase, $t_0$ is the time of the pre-synaptic spike, $\bar{g}_{\mathrm{syn}}$ is the maximal conductance. The corresponding differential equation: $$ \begin{aligned} &amp;g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} g \\ &amp;\frac{d g}{d t}=-\frac{g}{\tau_{\mathrm{decay}}}+h \\ &amp;\frac{d h}{d t}=-\frac{h}{\tau_{\text {rise }}}+ \delta\left(t_{0}-t\right), \end{aligned} $$ The alpha function is retrieved in the limit when both time constants are equal. ```python class DualExpSparseCOBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau_decay, tau_rise, E): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.DualExpon.desc(pre.num, tau_decay=tau_decay, tau_rise=tau_rise), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.COBA(E=E), post=post, ) ``` ```python class SimpleNet4(bp.DynSysGroup): def __init__(self, E=0.): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = DualExpSparseCOBA(self.pre, self.post, delay=None, prob=1., g_max=1., tau_decay=5., tau_rise=1., E=E) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` ## Biophysical synapse models ### Limitations of phenomenological models 打开的数量是有限的，而且有饱和期 1. Saturation of postsynaptic receptors by previously released transmitter. 2. Certain receptor types also exhibit desensitization that prevents them (re)opening for a period after transmitter-binding, like sodium channels underlying action potential. ![image-20230826111443117](/BrainPy-course-notes/master_content/Notes.assets/image-20230826111443117.png) ### Linetic/Markov models ![image-20230826111733654](/BrainPy-course-notes/master_content/Notes.assets/image-20230826111733654.png) - The simplest kinetic model is a two-state scheme in which receptors can be either closed, 𝐶, or open, 𝑂, and the transition between states depends on transmitter concentration, [𝑇], in the synaptic cleft: - 𝛼 and 𝛽 are voltage-independent forward and backward rate constants. - 𝐶 and 𝑂 can range from 0 to 1, and describe the fraction of receptors in the closed and open states, respectively. - The synaptic conductance is: $g_{syn}(t)=\bar{g}_{max}g(t)$ ### AMPA/GABA$_A$ synapse model $$ \begin{aligned}\frac{ds}{dt}&amp;=\alpha[T](1-s)-\beta s\\I&amp;=\tilde{g}s(V-E)\end{aligned} $$ - 𝛼[𝑇] denotes the transition probability from state (1−𝑠) to state (𝑠) - 𝛽 represents the transition probability of the other direction - 𝐸 is a reverse potential, which can determine whether the direction of 𝐼 is inhibition or excitation. - 𝐸 = 0 𝑚𝑚𝑉𝑉 =&amp;gt; Excitatory synapse [AMPA] - 𝐸 = −80 𝑚𝑚𝑉𝑉 =&amp;gt; Inhibitory synapse [GABA A ] ### Comparison ![image-20230826111950713](/BrainPy-course-notes/master_content/Notes.assets/image-20230826111950713.png) ### NMDA synapse model ![image-20230826112027689](/BrainPy-course-notes/master_content/Notes.assets/image-20230826112027689.png) ![image-20230826112034481](/BrainPy-course-notes/master_content/Notes.assets/image-20230826112034481.png) $$ \begin{aligned} &amp;\frac{ds}{dt} =\alpha[T](1-s)-\beta s \\ &amp;I=\tilde{g}sB(V)(V-E) \\ &amp;B(V )=\frac{1}{1+\exp(-0.062V)[Mg^{2+}]_{o}/3.57} \end{aligned} $$ The magnesium block of the NMDA receptor channel is an extremely fast process compared to the other kinetics of the receptor (Jahr and Stevens 1990a, 1990b). The block can therefore be accurately modeled as an instantaneous function of voltage(Jahr and Stevens 1990b). where $[Mg^{2+}]$ is the external magnesium concentration (1 to 2mM inphysiological conditions) ## Programming of biophysical synapse models ### AMPA synapse model ```python class AMPA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, E=0.): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.AMPA.desc(pre.num, alpha=0.98, beta=0.18, T=0.5, T_dur=0.5), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.COBA(E=E), post=post, ) ``` ```python class SimpleNet(bp.DynSysGroup): def __init__(self, syn_cls): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = syn_cls(self.pre, self.post, delay=None, prob=1., g_max=1.) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` ```python def run_a_net(net, duration=100): indices = np.arange(int(duration/bm.get_dt())) # duration ms conductances, currents, potentials = bm.for_loop(net.step_run, indices, progress_bar=True) ts = indices * bm.get_dt() # --- similar to: # runner = bp.DSRunner(net) # conductances, currents, potentials = runner.run(100.) fig, gs = bp.visualize.get_figure(1, 3, 3.5, 4) fig.add_subplot(gs[0, 0]) plt.plot(ts, conductances) plt.title(&apos;Syn conductance&apos;) fig.add_subplot(gs[0, 1]) plt.plot(ts, currents) plt.title(&apos;Syn current&apos;) fig.add_subplot(gs[0, 2]) plt.plot(ts, potentials) plt.title(&apos;Post V&apos;) plt.show() ``` ### $\text{GABA}_A$ synapse model ```python class GABAa(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, E=-80.): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.GABAa.desc(pre.num, alpha=0.53, beta=0.18, T=1.0, T_dur=1.0), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.COBA(E=E), post=post, ) ``` ```python run_a_net(SimpleNet(syn_cls=GABAa)) ``` ### NMDA synapse model ```python class NMDA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, E=0.0): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.NMDA.desc(pre.num, a=0.5, tau_decay=100., tau_rise=2.), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.MgBlock(E=E), post=post, ) ``` ```python run_a_net(SimpleNet(NMDA)) ``` ### Kinetic synapse models are more realistic ```python class SimpleNet5(bp.DynSysGroup): def __init__(self, freqs=10.): super().__init__() self.pre = bp.dyn.PoissonGroup(1, freqs=freqs) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = NMDA(self.pre, self.post, delay=None, prob=1., g_max=1., E=0.) def update(self): self.pre() self.syn() self.post() # monitor the following variables return self.syn.proj.refs[&apos;syn&apos;].g, self.post.V ``` ```python def compare_freqs(freqs): fig, _ = bp.visualize.get_figure(1, 1, 4.5, 6.) for freq in freqs: net = SimpleNet5(freqs=freq) indices = np.arange(1000) # 100 ms conductances, potentials = bm.for_loop(net.step_run, indices, progress_bar=True) ts = indices * bm.get_dt() plt.plot(ts, conductances, label=f&apos;{freq} Hz&apos;) plt.legend() plt.ylabel(&apos;g&apos;) plt.show() ``` ```python compare_freqs([10., 100., 1000., 10000.]) ``` ### How to customize a synapse #### Preparations `ProjAlignPostMg2` and `ProjAlignPreMg2` #### Exponential Model ```python class Exponen(bp.dyn.SynDyn, bp.mixin.AlignPost): def __init__(self, size, tau): super().__init__(size) # parameters self.tau = tau # variables self.g = bm.Variable(bm.zeros(self.num)) # integral self.integral = bp.odeint(lambda g, t: -g/tau, method=&apos;exp_auto&apos;) def update(self, pre_spike=None): self.g.value = self.integral(g=self.g.value, t=bp.share[&apos;t&apos;], dt=bp.share[&apos;dt&apos;]) if pre_spike is not None: self.add_current(pre_spike) return self.g.value def add_current(self, x): # specical for bp.mixin.AlignPost self.g += x def return_info(self): return self.g ``` #### AMPA Model ```python class AMPA(bp.dyn.SynDyn): def __init__(self, size, alpha= 0.98, beta=0.18, T=0.5, T_dur=0.5): super().__init__(size=size) # parameters self.alpha = alpha self.beta = beta self.T = T self.T_duration = T_dur # functions self.integral = bp.odeint(method=&apos;exp_auto&apos;, f=self.dg) # variables self.g = bm.Variable(bm.zeros(self.num)) self.spike_arrival_time = bm.Variable(bm.ones(self.num) * -1e7) def dg(self, g, t, TT): return self.alpha * TT * (1 - g) - self.beta * g def update(self, pre_spike): self.spike_arrival_time.value = bm.where(pre_spike, bp.share[&apos;t&apos;], self.spike_arrival_time) TT = ((bp.share[&apos;t&apos;] - self.spike_arrival_time) 做时间平均 STP based on firing rate $$ \begin{gathered} \frac{du(t)}{dt}=\frac{-u(t)}{\tau_{f}}+U_{sE}(1-u^{-})\delta\big(t-t_{sp}\big), \\ \frac{dx(t)}{dt}=\frac{1-x(t)}{\tau_{d}}-u^{+}x^{-}\delta\big(t-t_{sp}\big), \\ \frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{s}}+Au^{+}x^{-}\delta\big(t-t_{sp}\big), \\ u^{+}=\lim_{t-t_{sp\rightarrow0^{+}}}u(t), \end{gathered} $$ ![image-20230826155016657](/BrainPy-course-notes/master_content/Notes.assets/image-20230826155016657.png) 丢掉时间变化的具体细节，抓住了重要趋势 ### Theoretical analysis of the rate model Suppose the pre-synaptic firing rate keeps as constant, we can calculate the stationary response $$ u_{st}=\frac{U_{SE}R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}},\quad u_{st}^{+}=U_{SE}\frac{1+R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}},\quad x_{st}=\frac{1}{1+u_{st}^{+}\tau_{d}R_{0}}, $$ $$ EPSC_{st}=Au_{st}^{+}x_{st}=A\frac{u_{st}^{+}}{1+u_{st}^{+}\tau_{d}R_{0}},\quad PSV_{st}\propto g_{st}=\tau_{s}Au_{st}^{+}x_{st}R_{0}=A\frac{u_{st}^{+}R_{0}}{1+u_{st}^{+}\tau_{d}R_{0}}, $$ ![image-20230826155234134](/BrainPy-course-notes/master_content/Notes.assets/image-20230826155234134.png) ### Frequency-dependent Gain control of spike information $$ \begin{gathered} u_{st}^{+}=U_{SE}\frac{1+R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}}, \\ x_{st}=\frac{1}{1+u_{st}^{+}\tau_{d}R_{0}}, \\ EPSC_{st}=Au_{st}^{+}x_{st}=A\frac{u_{st}^{+}}{1+u_{st}^{+}\tau_{d}R_{0}}, \end{gathered} $$ Peak frequency: $\theta\sim\frac{1}{\sqrt{U\tau_{f}\tau_{d}}}$ ### Simulation of Frequency-dependent Gain control ![image-20230826155715445](/BrainPy-course-notes/master_content/Notes.assets/image-20230826155715445.png) ## Effects on network dynamics ### STP modeling Working memory ![image-20230826160102332](/BrainPy-course-notes/master_content/Notes.assets/image-20230826160102332.png) ![image-20230826160113456](/BrainPy-course-notes/master_content/Notes.assets/image-20230826160113456.png) # E-I Balanced Neural Network ## Irregular Spiking of Neurons ### Signal process of single neuron External Stimulus -&amp;gt; Single neuron model $$ \begin{aligned}\tau&amp;\frac{\mathrm{d}V}{\mathrm{d}t}=-(V-V_\text{rest })+RI(t)\\\\&amp;\text{if}V&amp;gt;V_\text{th},\quad V\leftarrow V_\text{reset }\text{last}t_\text{ref}\end{aligned} $$ -&amp;gt; ... -&amp;gt; Perception or action 真正的神经元并不是LIF model的输出 ![image-20230827100647851](/BrainPy-course-notes/master_content/Notes.assets/image-20230827100647851.png) Simulation ![image-20230827100706310](/BrainPy-course-notes/master_content/Notes.assets/image-20230827100706310.png) Neuron recorded in vivo ### Irregular Spiking of Neurons ![image-20230827092807270](/BrainPy-course-notes/master_content/Notes.assets/image-20230827092807270.png) #### Statistical Description of Spikes 用以下的变量来进行统计描述 - Firing Rate Rate = average over time(single neuron, single run) Spike count $v=\frac{n_{sp}}{T}$ - ISI(Interspike interval distributions) average ISI $\overline{\Delta t}=\frac{1}{n_{sp}-1}\sum_{i=1}^{n_{sp}-1}\Delta t_{i}$ standard deviation ISI: $\sigma_{\Delta t}^{2}=\frac{1}{n_{sp}-1}\sum_{i=1}^{n_{sp}-1}(\Delta t_{i}-\overline{\Delta t})^{2}$ - $C_V$(Coefficient of variation, Fano factor) **窄还是宽的分布** 信息表征有多强的不稳定性 $C_{V}=\sigma_{\Delta t}^{2}/\overline{\Delta t}$ #### Poisson Process In probability theory and statistics, the Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known **constant mean rate** and **independently** of the time since the last event。 $$ \begin{aligned} &amp;P(X=k\mathrm{~events~in~interval~}t)=e^{-rt}\frac{(rt)^{k}}{k!} \\ &amp;\mathrm{mean:}\quad\overline{X}=rt \\ &amp;\mathrm{variance}:\quad\sigma^{2}=rt\\ &amp;\mathrm{Fano factor:}\quad\frac{\sigma^{2}}{X}=1 \end{aligned} $$ Fano factor -&amp;gt; noise-to-signal ratio #### Irregular Spiking of Neurons LIF在单个神经元的情况下是基本没有太大问题的，在整个网络中会受网络信息调控 ![image-20230827093614607](/BrainPy-course-notes/master_content/Notes.assets/image-20230827093614607.png) #### Why Irregular? - 不完全是input影响的 - 不能简单来衡量 On average, a cortical neuron receives inputs from 1000~10000 connected neurons. -&amp;gt; averaged noise ~ 0 ## E-I Balanced Network $$ \begin{gathered} \tau\frac{du_{i}^{E}}{dt}=-u_{i}^{E}+\sum_{j=1}^{K_{E}}J_{EE}r_{j}^{E}+\sum_{j=1}^{K_{I}}J_{EI}r_{j}^{I}+I_{i}^{E} \\ \tau\frac{du_{i}^{I}}{dt}=-u_{i}^{I}+\sum_{j=1}^{K_{I}}J_{II}r_{j}^{I}+\sum_{j=1}^{K_{E}}J_{IE}r_{j}^{E}+I_{i}^{I} \end{gathered} $$ ![image-20230827093708220](/BrainPy-course-notes/master_content/Notes.assets/image-20230827093708220.png) Sparse &amp; random connections:$1\ll K_{\mathrm{E}},K_{1}\ll N_{\mathrm{E}},N_{\mathrm{I}}$ . Neurons fire largely independently to each other. $$ \begin{gathered} \text{Single neuron fires irregularly } r_j^E, r_j^{\prime} \text{with mean rate } \mu \text{and variance } \sigma^2.\\ \text{The mean of recurrent input received by E neuron:} \\ \sim K_{E}J_{EE}\mu-K_{I}J_{EI}\mu \\ \text{The variance of recurrent input received by E neuron:} \\ \sim K_{E}(J_{EE})^{2}\sigma^{2}+K_{I}(J_{EI})^{2}\sigma^{2} \\ \begin{gathered} \\ \text{The balanced condition:} \\ K_{E}J_{EE}-K_{l}J_{El}{\sim}0(1) \\ J_{EE}=\frac{1}{\sqrt{K_{E}}},J_{EI}=\frac{1}{\sqrt{K_{I}}},K_{E}(J_{EE})^{2}\sigma^{2}+K_{I}(J_{EI})^{2}\sigma^{2}\sim O(1) \end{gathered} \end{gathered} $$ $$ \begin{aligned}\frac{I_E}{I_I}&amp;&amp;gt;\frac{J_E}{J_I}&amp;&amp;gt;1\\\\J_E&amp;&amp;gt;1\\\\\text{r not too big}\end{aligned} $$ $$ \overline{I_a}=\overline{F_a}+\overline{R_a}=\sqrt{N}(f_a\mu_0+w_{aE}r_E+w_{aI}r_I),\quad a=E,I,\\ \begin{gathered} w_{ab}~=~p_{ab}j_{ab}q_{b} \\ J_{ij}^{ab}~=~j_{ab}/\sqrt{N}; \\ \frac{f_{E}}{f_{I}}&amp;gt;\frac{w_{EI}}{w_{II}}&amp;gt;\frac{w_{EE}}{w_{IE}}. \end{gathered} $$ ## BrainPy Simulation ### Simulation LIF neuron 4000 (E/I=4/1, P=0.02) 𝜏 = 20 ms 𝑉𝑟𝑒𝑠𝑡 = -60 mV Spiking threshold: -50 mV Refractory period: 5 ms $$ \begin{gathered} \tau\frac{dV}{dt}=(V_{\mathrm{rest}}-V)+I \\ I=g_{exc}(E_{exc}-V)+g_{inh}(E_{inh}-V)+I_{\mathrm{ext}} \end{gathered} \ \ \ \ \ \ \begin{aligned}\tau_{exc}&amp;\frac{dg_{exc}}{dt}=-g_{exc}\\\tau_{inh}&amp;\frac{dg_{inh}}{dt}=-g_{inh}\end{aligned} $$ $$ \begin{array}{l}E_\mathrm{exc}=0\text{mV}\mathrm{and}E_\mathrm{inh}=-80\text{mV},I_\mathrm{ext}=20.\\\tau_\mathrm{exc}=5\text{ ms},\tau_\mathrm{inh}=10\text{ ms},\Delta g_\mathrm{exc}=0.6\text{ and}\Delta g_\mathrm{inh}=6.7.\end{array} $$ ![image-20230827094502860](/BrainPy-course-notes/master_content/Notes.assets/image-20230827094502860.png) ### Synaptic Computation ```python # 基于 align post Exponential synaptic computation class Exponential(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E, label=None): super().__init__() self.pron = bp.dyn.ProjAlignPost2( pre=pre, delay=delay, comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), # 随机连接 syn=bp.dyn.Expon(size=post.num, tau=tau), # Exponential synapse out=bp.dyn.COBA(E=E), # COBA network post=post, out_label=label ) ``` ### E-I Balanced Network ```python # 构建 E-I Balanced Network class EINet(bp.DynamicalSystem): def __init__(self, ne=3200, ni=800): super().__init__() # bp.neurons.LIF() self.E = bp.dyn.LifRef(ne, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Normal(-55., 2.)) self.I = bp.dyn.LifRef(ni, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Normal(-55., 2.)) #### E2E, E2I, I2E, I2I Exponential synaptic computation # delay=0, prob=0.02, g_max_E=0.6, g_max_I=6.7, tau_E=5, tau_I=10, # reversal potentials E_E=0, E_E=-80, label=EE,EI,IE,II self.E2E = Exponential(self.E, self.E, 0., 0.02, 0.6, 5., 0., &apos;EE&apos;) self.E2I = Exponential(self.E, self.I, 0., 0.02, 0.6, 5., 0., &apos;EI&apos;) self.I2E = Exponential(self.I, self.E, 0., 0.02, 6.7, 5., -80., &apos;IE&apos;) self.I2I = Exponential(self.I, self.I, 0., 0.02, 6.7, 5., -80., &apos;II&apos;) ``` ```python def update(self, inp=0.): # 更新突触传入电流 self.E2E() self.E2I() self.I2E() self.I2I() # 更新神经元群体 self.E(inp) self.I(inp) # 记录需要 monitor的变量 E_E_inp = self.E.sum_inputs(self.E.V, label=&apos;EE&apos;) #E2E的输入 I_E_inp = self.E.sum_inputs(self.E.V, label=&apos;IE&apos;) # I2E的输入 return self.E.spike, self.I.spike, E_E_inp, I_E_inp ``` ![image-20230827110737553](/BrainPy-course-notes/master_content/Notes.assets/image-20230827110737553.png) ![image-20230827110746410](/BrainPy-course-notes/master_content/Notes.assets/image-20230827110746410.png) ## Properties of E-I Balanced Network - Linear encoding External input strength is “linearly” encoded by the mean firing rate of the neural population - Fast Response The network responds rapidly to abrupt changes of the input ### Noise speeds up computation 快速相应的原理，均匀分布在阈值下面的空间 - A neural ensemble jointly encodes stimulus information; - Noise randomizes the distribution of neuronal membrane potentials; - Those neurons (red circle) whose potentials are close to the threshold will fire rapidly; - If the noisy environment is proper, even for a small input, a certain number of neurons will fire instantly to report the presence of a stimulus. ![image-20230827113451626](/BrainPy-course-notes/master_content/Notes.assets/image-20230827113451626.png) # Continuous Attractor Neural Network ## Attractor Models ### The concept of attractor dynamics Different types of attractors: Point attractors, Line attractors, Ring attractors, Plane attractors, Cyclic attractors, Chaotic attractors ![image-20230827140250173](/BrainPy-course-notes/master_content/Notes.assets/image-20230827140250173.png) 稳态，能量梯度吸引到attractor ### Discrete attractor Network Model: Hopfield Model $S_i=\pm1$: the neuronal state $W_{ij}$ : the neuronal connection The network dynamics: $$ S_{i}=\mathrm{sign}\bigg(\sum_{j}w_{ij}S_{j}-\theta\bigg),\quad\mathrm{sign}(x)=1,\mathrm{for}x&amp;gt;0;-1,\mathrm{otherwise} $$ Updating rule: synchronous or asynchronous Consider the network stores $p$ pattern, $\xi_{i}^{\mu},\mathrm{for}\mu=1,\ldots p;i=1,\ldots N$ Setting $w_{ij}=\frac{1}{N}\sum_{\mu=1}^{p}\xi_{i}^{\mu}\xi_{j}^{\mu}$ ![image-20230827140827784](/BrainPy-course-notes/master_content/Notes.assets/image-20230827140827784.png) #### Energy space of Hopfield network $$ \begin{aligned} &amp;\text{Energy function: }E=-\frac{1}{2}\sum_{i,j}w_{ij}S_{i}S_{j}+\theta\sum_{i}S_{i} \\ &amp;\mathrm{Consider}S_{i}\mathrm{~is~updated},S_{i}(t+1)=sign[\sum_{j}w_{ij}S_{j}(t)-\theta] \\ &amp;\Delta E=E(t+1)-E(t)\\ &amp;=-[S_{i}(t+1)-S_{i}(t)]\sum_{j}w_{ij}S_{j}(t)+\theta\left[S_{i}(t+1)-S_{i}(t)\right] \\ &amp;=-[S_{i}(t+1)-S_{i}(t)][\sum_{j}w_{ij}S_{j}(t)-\theta] \\ &amp;\leq0 \end{aligned} $$ 同样激活同样pattern的神经元，~吸引子 #### Auto-associative memory in Hopfield Network A partial/noisy input can retrieve the related memory pattern ![image-20230827141253326](/BrainPy-course-notes/master_content/Notes.assets/image-20230827141253326.png) #### Persistent activity in working memory After the removal of external input, the neurons in the network encoding the stimulus continue to fire persistently. ![image-20230827141421796](/BrainPy-course-notes/master_content/Notes.assets/image-20230827141421796.png) ## Continuous Attractor Neural Network ### Neural coding #### Low-dimensional continuous feature ![image-20230827142520189](/BrainPy-course-notes/master_content/Notes.assets/image-20230827142520189.png) #### Continuous Attractor neural network ![image-20230827142606695](/BrainPy-course-notes/master_content/Notes.assets/image-20230827142606695.png) ### CANN: A rate-based recurrent circuit model $$ \begin{aligned}\tau\frac{\partial U(x,t)}{\partial t}&amp;=-U(x,t)+\rho\int f(x,x&apos;)r(x&apos;,t)dx&apos;+l^{ext}(1)\\r(x,t)&amp;=\frac{U^2(x,t)}{1+k\rho\int U^2(x,t)dx}\quad(2)\\J(x,x&apos;)&amp;=\frac{J_0}{\sqrt{2\pi}a}\exp\left[-\frac{(x-x&apos;)^2}{2a^2}\right](3)\end{aligned} $$ r频率，J强度，U decay #### A Continuous family of attractor states 做平移的改变，变化会被保留，line attractor，受到编码连续刺激 ![image-20230827143707784](/BrainPy-course-notes/master_content/Notes.assets/image-20230827143707784.png) #### Stability analysis derive continuous attractor dynamics 只需要看在原始状态加入一个小量项，再代入回 Consider small fluctuations around a stationary state at z: Projecting $\delta U$ on the $i$th right eigenvector of $F(\delta U)_i(t)=(\delta U)_i(0)e^{-(1-\lambda _i)t/\tau}$ Two cases: - If $\lambda _i ## Computation with CANN ### Persistent activity for working memory When the global inhibition is not too strong, the network spontaneously hold bump activity: $$ k\frac{\tau}{\tau _v}$, Travelling wave ![image-20230827150543244](/BrainPy-course-notes/master_content/Notes.assets/image-20230827150543244.png) #### Levy flights vs. Brownian motion ![image-20230827150851309](/BrainPy-course-notes/master_content/Notes.assets/image-20230827150851309.png) #### Lévy flights in ecology and human cogniDve behaviors 生物学大多运动服从levy flights ### Noisy adaptation generates Levy flight in CANN ![image-20230827151343126](/BrainPy-course-notes/master_content/Notes.assets/image-20230827151343126.png) ### Time Delay in Neural Signal Transmission ![image-20230827151622032](/BrainPy-course-notes/master_content/Notes.assets/image-20230827151622032.png) ### Anticipatory Head Direction Signals in Anterior Thalamus 有预测策略，实现抵消信息传递的delay CANN加入负反馈机制是可以实现预测的 ![image-20230827152731895](/BrainPy-course-notes/master_content/Notes.assets/image-20230827152731895.png) ### CANN with STP $$ \begin{gathered} \tau{\frac{\mathrm{d}U(x,t)}{\mathrm{d}t}} {\cal O}=-U(x,t)+\rho\int g^{+}(x)h(x^{\prime},t)J(x,x^{\prime})r(x^{\prime},t)dx^{\prime}+I^{ext}(x,t)(1) \\ \frac{dg(x,t)}{dt}=-\frac{g(x,t)}{\tau_{f}}+G(1-g^{-}(x))r(x^{\prime},t)\quad(2) \\ \frac{dh(x,t)}{dt}=\frac{1-h(x,t)}{\tau_{d}}-g^{+}(x)h(x,t)r(x^{\prime},t)\quad(3) \\ r(x,t)={\frac{U^{2}(x,t)}{1+k\rho\int U^{2}(x,t)dx}}\quad(4) \end{gathered} $$ ## Programming in BrainPy ### Customize a ring CANN in brainpy In simulations, we can not simulate a CANN encoding features ranging $(-\inf, \inf)$. Instead, we simulate a ring attractor network which encodes features ranging $(-\pi, \pi)$. Note that the distance on a ring should be: $$ dist_{ring}(x,x&apos;) = min(|x-x&apos;|,2\pi-|x-x&apos;|) $$ ![Image Name](https://cdn.kesci.com/upload/s01apgi89t.png?imageView2/0/w/320/h/320) ```python class CANN1D(bp.NeuGroupNS): def __init__(self, num, tau=1., k=8.1, a=0.5, A=10., J0=4., z_min=-bm.pi, z_max=bm.pi, **kwargs): super(CANN1D, self).__init__(size=num, **kwargs) # 初始化参数 self.tau = tau self.k = k self.a = a self.A = A self.J0 = J0 # 初始化特征空间相关参数 self.z_min = z_min self.z_max = z_max self.z_range = z_max - z_min self.x = bm.linspace(z_min, z_max, num) self.rho = num / self.z_range self.dx = self.z_range / num # 初始化变量 self.u = bm.Variable(bm.zeros(num)) self.input = bm.Variable(bm.zeros(num)) self.conn_mat = self.make_conn(self.x) # 连接矩阵 # 定义积分函数 self.integral = bp.odeint(self.derivative) # 微分方程 @property def derivative(self): du = lambda u, t, Irec, Iext: (-u + Irec + Iext) / self.tau return du # 将距离转换到[-z_range/2, z_range/2)之间 def dist(self, d): d = bm.remainder(d, self.z_range) d = bm.where(d &amp;gt; 0.5 * self.z_range, d - self.z_range, d) return d # 计算连接矩阵 def make_conn(self, x): assert bm.ndim(x) == 1 d = self.dist(x - x[:, None]) # 距离矩阵 Jxx = self.J0 * bm.exp( -0.5 * bm.square(d / self.a)) / (bm.sqrt(2 * bm.pi) * self.a) return Jxx # 获取各个神经元到pos处神经元的输入 def get_stimulus_by_pos(self, pos): return self.A * bm.exp(-0.25 * bm.square(self.dist(self.x - pos) / self.a)) def update(self, x=None): _t = bp.share[&apos;t&apos;] u2 = bm.square(self.u) r = u2 / (1.0 + self.k * bm.sum(u2)) Irec = bm.dot(self.conn_mat, r) self.u[:] = self.integral(self.u, _t,Irec, self.input) self.input[:] = 0. # 重置外部电流 ``` ### Simulate the persistent activity of CANN after the removal of external input ```python def Persistent_Activity(k=0.1,J0=1.): # 生成CANN cann = CANN1D(num=512, k=k,J0=J0) # 生成外部刺激，从第2到12ms，持续10ms dur1, dur2, dur3 = 2., 10., 10. I1 = cann.get_stimulus_by_pos(0.) Iext, duration = bp.inputs.section_input(values=[0., I1, 0.], durations=[dur1, dur2, dur3], return_length=True) noise_level = 0.1 noise = bm.random.normal(0., noise_level, (int(duration / bm.get_dt()), len(I1))) Iext += noise # 运行数值模拟 runner = bp.DSRunner(cann, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;]) runner.run(duration) # 可视化 def plot_response(t): fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann.x, I, label=&apos;Iext&apos;) ax.plot(cann.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() # plt.savefig(f&apos;CANN_t={t}.pdf&apos;, transparent=True, dpi=500) plot_response(t=10.) plot_response(t=20.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=1, frame_delay=40, show=True, ) plt.show() Persistent_Activity(k=0.1) ``` ### Simulate the tracking behavior of CANN ```python def smooth_tracking(): cann = CANN1D(num=512, k=8.1) # 定义随时间变化的外部刺激 v_ext = 1e-3 dur1, dur2, dur3 = 10., 10., 20 num1 = int(dur1 / bm.get_dt()) num2 = int(dur2 / bm.get_dt()) num3 = int(dur3 / bm.get_dt()) position = bm.zeros(num1 + num2 + num3) position[num1: num1 + num2] = bm.linspace(0., 1.5 * bm.pi, num2) position[num1 + num2: ] = 1.5 * bm.pi position = position.reshape((-1, 1)) Iext = cann.get_stimulus_by_pos(position) # 运行模拟 runner = bp.DSRunner(cann, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;]) runner.run(dur1 + dur2 + dur3) # 可视化 def plot_response(t, extra_fun=None): fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann.x, I, label=&apos;Iext&apos;) ax.plot(cann.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() if extra_fun: extra_fun() # plt.savefig(f&apos;CANN_tracking_t={t}.pdf&apos;, transparent=True, dpi=500) plot_response(t=10.) def f(): plt.annotate(&apos;&apos;, xy=(1.5, 10), xytext=(0.5, 10), arrowprops=dict(arrowstyle=&quot;-&amp;gt;&quot;)) plot_response(t=15., extra_fun=f) def f(): plt.annotate(&apos;&apos;, xy=(-2, 10), xytext=(-3, 10), arrowprops=dict(arrowstyle=&quot;-&amp;gt;&quot;)) plot_response(t=20., extra_fun=f) plot_response(t=30.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=5, frame_delay=50, show=True, ) plt.show() smooth_tracking() ``` ### Customize a CANN with SFASimulate the spontaneous traveling wave ```python class CANN1D_SFA(bp.NeuGroupNS): def __init__(self, num, m = 0.1, tau=1., tau_v=10., k=8.1, a=0.5, A=10., J0=4., z_min=-bm.pi, z_max=bm.pi, **kwargs): super(CANN1D_SFA, self).__init__(size=num, **kwargs) # 初始化参数 self.tau = tau self.tau_v = tau_v #time constant of SFA self.k = k self.a = a self.A = A self.J0 = J0 self.m = m #SFA strength # 初始化特征空间相关参数 self.z_min = z_min self.z_max = z_max self.z_range = z_max - z_min self.x = bm.linspace(z_min, z_max, num) self.rho = num / self.z_range self.dx = self.z_range / num # 初始化变量 self.u = bm.Variable(bm.zeros(num)) self.v = bm.Variable(bm.zeros(num)) #SFA current self.input = bm.Variable(bm.zeros(num)) self.conn_mat = self.make_conn(self.x) # 连接矩阵 # 定义积分函数 self.integral = bp.odeint(self.derivative) # 微分方程 @property def derivative(self): du = lambda u, t, v, Irec, Iext: (-u + Irec + Iext-v) / self.tau dv = lambda v, t, u: (-v + self.m*u) / self.tau_v return bp.JointEq([du, dv]) # 将距离转换到[-z_range/2, z_range/2)之间 def dist(self, d): d = bm.remainder(d, self.z_range) d = bm.where(d &amp;gt; 0.5 * self.z_range, d - self.z_range, d) return d # 计算连接矩阵 def make_conn(self, x): assert bm.ndim(x) == 1 d = self.dist(x - x[:, None]) # 距离矩阵 Jxx = self.J0 * bm.exp( -0.5 * bm.square(d / self.a)) / (bm.sqrt(2 * bm.pi) * self.a) return Jxx # 获取各个神经元到pos处神经元的输入 def get_stimulus_by_pos(self, pos): return self.A * bm.exp(-0.25 * bm.square(self.dist(self.x - pos) / self.a)) def update(self, x=None): u2 = bm.square(self.u) r = u2 / (1.0 + self.k * bm.sum(u2)) Irec = bm.dot(self.conn_mat, r) u, v = self.integral(self.u, self.v, bp.share[&apos;t&apos;],Irec, self.input) self.u[:] = bm.where(u&amp;gt;0,u,0) self.v[:] = v self.input[:] = 0. # 重置外部电流 ``` ### Simulate the spontaneous traveling wave ```python def traveling_wave(num=512,m=0.1,k=0.1): # 生成CANN cann_sfa = CANN1D_SFA(num=num, m=m,k=k) # 生成外部刺激 dur = 1000. noise_level = 0.1 Iext = bm.random.normal(0., noise_level, (int(dur / bm.get_dt()), num)) duration = dur # 运行数值模拟 runner = bp.DSRunner(cann_sfa, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;]) runner.run(duration) # 可视化 def plot_response(t): fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann_sfa.x, I, label=&apos;Iext&apos;) ax.plot(cann_sfa.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() # plt.savefig(f&apos;CANN_t={t}.pdf&apos;, transparent=True, dpi=500) plot_response(t=100.) plot_response(t=150.) plot_response(t=200.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann_sfa.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann_sfa.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=1, frame_delay=40, show=True, ) plt.show() traveling_wave(num=512,m=0.5,k=0.1) ``` ### Simulate the anticipative tracking ```python def anticipative_tracking(m=10,v_ext=6*1e-3): cann_sfa = CANN1D_SFA(num=512, m=m) # 定义随时间变化的外部刺激 v_ext = v_ext dur1, dur2, = 10., 1000. num1 = int(dur1 / bm.get_dt()) num2 = int(dur2 / bm.get_dt()) position = np.zeros(num1 + num2) for i in range(num2): pos = position[i+num1-1]+v_ext*bm.dt # the periodical boundary pos = np.where(pos&amp;gt;np.pi, pos-2*np.pi, pos) pos = np.where(pos 0.5 * self.z_range, d - self.z_range, d) return d # 计算连接矩阵 def make_conn(self, x): assert bm.ndim(x) == 1 d = self.dist(x - x[:, None]) # 距离矩阵 Jxx = self.J0 * bm.exp( -0.5 * bm.square(d / self.a)) / (bm.sqrt(2 * bm.pi) * self.a) return Jxx # 获取各个神经元到pos处神经元的输入 def get_stimulus_by_pos(self, pos): return self.A * bm.exp(-0.25 * bm.square(self.dist(self.x - pos) / self.a)) def update(self, x=None): u2 = bm.square(self.u) r = u2 / (1.0 + self.k * bm.sum(u2)) Irec = bm.dot(self.conn_mat, (self.g + self.G * (1 - self.g))*self.h*r) u, g, h = self.integral(u=self.u, g=self.g, h=self.h, t=bp.share[&apos;t&apos;], Irec=Irec, Iext=self.input, r=r, dt=bm.dt) self.u[:] = bm.where(u&amp;gt;0,u,0) self.g.value = g self.h.value = h self.input[:] = 0. # 重置外部电流 ``` ### Simulate traveling wave in CANN with STP ```python def traveling_wave_STP(num=512,k=0.1,J0=12.,tau_d=1000,tau_f=1.,G=0.9): # 生成CANN cann_stp = CANN1D_STP(num=num, k=k,tau_d=tau_d,tau_f=tau_f,G=G, J0=J0) # 生成外部刺激 dur = 1000. noise_level = 0.1 Iext = bm.random.normal(0., noise_level, (int(dur / bm.get_dt()), num)) duration = dur # 运行数值模拟 runner = bp.DSRunner(cann_stp, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;,&apos;g&apos;,&apos;h&apos;]) runner.run(duration) fig,ax = plt.subplots(figsize=(3,3)) u = bm.as_numpy(runner.mon.u) max_index = np.argmax(u[1000,:]) print(max_index) ax.plot(runner.mon.g[:,max_index],label=&apos;g&apos;) ax.plot(runner.mon.h[:,max_index],label=&apos;h&apos;) ax.legend() # 可视化 def plot_response(t): fig, gs = bp.visualize.get_figure(1, 1, 3, 3) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann_stp.x, I, label=&apos;Iext&apos;) ax.plot(cann_stp.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() plot_response(t=100.) plot_response(t=200.) plot_response(t=300.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann_stp.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann_stp.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=1, frame_delay=40, show=True, ) plt.show() traveling_wave_STP(G=0.5,tau_d=50) ``` # Decision-Making Network ## LIP -&amp;gt; Decision-Making ### Coherent motion task 判断随机点(大部分点)的运动朝向 ![image-20230828100425871](/BrainPy-course-notes/master_content/Notes.assets/image-20230828100425871.png) coherence影响任务的难度 0%难，100%简单 ![image-20230828100516123](/BrainPy-course-notes/master_content/Notes.assets/image-20230828100516123.png) 编码决策的响应，不是运动 ### Reaction Time vs. Fixed Duration coherence越高，反应时间越短 Fixed Duration多了Delay time ![image-20230828100658772](/BrainPy-course-notes/master_content/Notes.assets/image-20230828100658772.png) 实验设计纯粹把decision-making给提取出来 #### Effect of Difficulty coherence越大，反应时间是越短，single neuron很难做到这么短的decision-making，考虑要建模的因素 ![image-20230828101103008](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101103008.png) ![image-20230828101058059](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101058059.png) #### Response of MT Neurons 记录MT的神经元，对这种运动的朝向刺激进行编码 线性编码coherence运动强度的方向 做决策在它的下游 ![image-20230828101303674](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101303674.png) #### Response of LIP Neurons MT的下游找到LIP的神经元 爬升到一定高度再做选择 coherence与爬升的斜率也会有影响，任务越难，爬升斜率越小 ![image-20230828101609881](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101609881.png) ### Ramping-to-threshold(perfect integrator) Model $$ \begin{aligned}\frac{dR}{dt}=I_A-I_B+\text{noise},\quad R(t)&amp;=(I_A-I_B)t+\int_0^tdt\text{noise}.\\\tau_\text{network}&amp;=\infty!\end{aligned} $$ 两种选择积分求和做积累，等到阈值做决策 Accumulates information (evidence) -&amp;gt; Ramping 直接保存信息，没有特别好的生物对应 ## A Spiking Network of DM ### A cortical microcircuit model ![image-20230828103055151](Notes.assets/image-20230828103055151.png) A=Upward motion B=Downward motion 2-population excitatory neurons (integrate-and-fire neurons driven by Poisson input) Slow reverberatory excitation mediated by the NMDA receptors at recurrent synapses AMPA receptors ($\tau _{syn}=$1 - 3 ms) NMDA receptors ($\tau _{syn}=$ 50 - 100 ms). 两群神经元分别做不同的选择，与自己对方都有连接 NMDA 缓慢的信号使得有慢慢增长的ramping的过程 interneurons的backward有抑制作用 #### Coherence-Dependent Input 线性编码运动朝向的信息，coherence强度影响firing rate，一系列泊松过程，同时还有noise。 本身两种信息还是有差异 ![image-20230828104054275](/BrainPy-course-notes/master_content/Notes.assets/image-20230828104054275.png) #### Duality of this model 不同coherence的神经元响应 ![image-20230828104432061](/BrainPy-course-notes/master_content/Notes.assets/image-20230828104432061.png) 两个group会竞争，当有一个group达到20%，进入这个窗口，就会直接发放上去 Spontaneous symmetry breaking and stochastic decision making ![image-20230828104600840](/BrainPy-course-notes/master_content/Notes.assets/image-20230828104600840.png) ## Simulation of Spiking DM ### A Cortical Microcircuit Model 用两个coherence生成出来的序列 ![image-20230828110300576](/BrainPy-course-notes/master_content/Notes.assets/image-20230828110300576.png) $$ \begin{gathered}C_m\frac{dV(t)}{dt}=-g_L(V(t)-V_L)-I_{syn}(t)\\I_{syn}(t)=I_{\mathrm{ext},\mathrm{AMPA}}\left(t\right)+I_{\mathrm{rec},AMPA}(t)+I_{\mathrm{rec},NMDA}(t)+I_{\mathrm{rec},\mathrm{GABA}}(t)\end{gathered} $$ $$ \begin{gathered} I_{\mathrm{ext},\mathrm{AMPA}}\left(t\right)=g_{\mathrm{ext},\mathrm{AMPA}}\left(V(t)-V_{E}\right)s^{\mathrm{ext},\mathrm{AMPA}}\left(t\right) \\ I_{\mathrm{rec},\mathrm{AMP}\Lambda}\left(t\right)=g_{\mathrm{rec},\mathrm{AMP}\Lambda}\left(V(t)-V_{E}\right)\sum_{j=1}^{Ce}w_{j}s_{j}^{AMPA}(t) \\ I_{\mathrm{rec},\mathrm{NMDA}}\left(t\right)=\frac{g_{\mathrm{NMDA}}(V(t)-V_{E})}{\left(1+\left[\mathrm{Mg}^{2+}\right]\exp(-0.062V(t))/3.57\right)}\sum_{j=1}^{\mathrm{C_E}}w_{j}s_{j}^{\mathrm{NMDA}}\left(t\right) \\ I_\mathrm{rec,GABA}(t)=g_\mathrm{GABA}(V(t)-V_l)\sum_{j=1}^{C_1}s_j^\mathrm{GABA}(t) \end{gathered} $$ $$ w_j=\left\{\begin{matrix}w_+&amp;gt;1,\\w_-E/I conn self.I2B = AMPA(self.I, self.B, &apos;all2all&apos;, 0.5, g_I2E_GABAa, tau=5., E=-70.) self.I2A = AMPA(self.I, self.A, &apos;all2all&apos;, 0.5, g_I2E_GABAa, tau=5., E=-70.) self.I2N = AMPA(self.I, self.N, &apos;all2all&apos;, 0.5, g_I2E_GABAa, tau=5., E=-70.) self.I2I = AMPA(self.I, self.I, &apos;all2all&apos;, 0.5, g_I2I_GABAa, tau=5., E=-70.) # define external projections #### TO DO!!!! self.noise2B = AMPA(self.noise_B, self.B, &apos;one2one&apos;, None, g_ext2E_AMPA, tau=2., E=0.) self.noise2A = AMPA(self.noise_A, self.A, &apos;one2one&apos;, None, g_ext2E_AMPA, tau=2., E=0.) self.noise2N = AMPA(self.noise_N, self.N, &apos;one2one&apos;, None, g_ext2E_AMPA, tau=2., E=0.) self.noise2I = AMPA(self.noise_I, self.I, &apos;one2one&apos;, None, g_ext2I_AMPA, tau=2., E=0.) ``` ```python class Tool: def __init__(self, pre_stimulus_period=100., stimulus_period=1000., delay_period=500.): self.pre_stimulus_period = pre_stimulus_period self.stimulus_period = stimulus_period self.delay_period = delay_period self.freq_variance = 10. self.freq_interval = 50. self.total_period = pre_stimulus_period + stimulus_period + delay_period def generate_freqs(self, mean): # stimulus period n_stim = int(self.stimulus_period / self.freq_interval) n_interval = int(self.freq_interval / bm.get_dt()) freqs_stim = np.random.normal(mean, self.freq_variance, (n_stim, 1)) freqs_stim = np.tile(freqs_stim, (1, n_interval)).flatten() # pre stimulus period freqs_pre = np.zeros(int(self.pre_stimulus_period / bm.get_dt())) # post stimulus period freqs_delay = np.zeros(int(self.delay_period / bm.get_dt())) all_freqs = np.concatenate([freqs_pre, freqs_stim, freqs_delay], axis=0) return bm.asarray(all_freqs) def visualize_results(self, mon, IA_freqs, IB_freqs, t_start=0., title=None): fig, gs = bp.visualize.get_figure(4, 1, 3, 10) axes = [fig.add_subplot(gs[i, 0]) for i in range(4)] ax = axes[0] bp.visualize.raster_plot(mon[&apos;ts&apos;], mon[&apos;A.spike&apos;], markersize=1, ax=ax) if title: ax.set_title(title) ax.set_ylabel(&quot;Group A&quot;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax = axes[1] bp.visualize.raster_plot(mon[&apos;ts&apos;], mon[&apos;B.spike&apos;], markersize=1, ax=ax) ax.set_ylabel(&quot;Group B&quot;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax = axes[2] rateA = bp.measure.firing_rate(mon[&apos;A.spike&apos;], width=10.) rateB = bp.measure.firing_rate(mon[&apos;B.spike&apos;], width=10.) ax.plot(mon[&apos;ts&apos;], rateA, label=&quot;Group A&quot;) ax.plot(mon[&apos;ts&apos;], rateB, label=&quot;Group B&quot;) ax.set_ylabel(&apos;Population activity [Hz]&apos;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax.legend() ax = axes[3] ax.plot(mon[&apos;ts&apos;], IA_freqs, label=&quot;group A&quot;) ax.plot(mon[&apos;ts&apos;], IB_freqs, label=&quot;group B&quot;) ax.set_ylabel(&quot;Input activity [Hz]&quot;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax.legend() ax.set_xlabel(&quot;Time [ms]&quot;) plt.show() ``` ```python tool = Tool() net = DecisionMakingNet() mu0 = 40. coherence = 25.6 IA_freqs = tool.generate_freqs(mu0 + mu0 / 100. * coherence) IB_freqs = tool.generate_freqs(mu0 - mu0 / 100. * coherence) def give_input(): i = bp.share[&apos;i&apos;] net.IA.freqs[0] = IA_freqs[i] net.IB.freqs[0] = IB_freqs[i] runner = bp.DSRunner(net, inputs=give_input, monitors=[&apos;A.spike&apos;, &apos;B.spike&apos;]) runner.run(tool.total_period) tool.visualize_results(runner.mon, IA_freqs, IB_freqs) ``` ### Results ![image-20230828112245950](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112245950.png) #### Stochastic Decision Making ![image-20230828112253619](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112253619.png) ## A Rate Network of DM ### Reduced Model 化简到只有两群神经元，只接受外界输入信号，互相影响对方 ![image-20230828112326267](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112326267.png) Synaptic variables $$ \begin{gathered} \frac{dS_{1}}{dt} =F(x_1)\gamma(1-S_1)-S_1/\tau_s \\ \frac{dS_2}{dt} =F(x_2)\gamma(1-S_2)-S_2/\tau_s \end{gathered} $$ Input current to each population $$ \begin{gathered} x_{1} =J_{E}S_{1}+J_{I}S_{2}+I_{0}+I_{noise1}+J_{\text{ext }\mu_{1}} \\ x_{2} =J_{E}S_{2}+J_{I}S_{1}+I_{0}+I_{noise2}+J_{\mathrm{ext}}\mu_{2} \end{gathered} $$ Background input $$ I_0+I_{noise}\\ \begin{gathered} dI_{noise1} =-I_{noise1}\frac{dt}{\tau_{0}}+\sigma dW \\ dI_{noise2} =-I_{noise2}\frac{dt}{\tau_{0}}+\sigma dW \end{gathered} $$ Firing rates $$ r_i=F(x_i)=\frac{ax_i-b}{1-\exp(-d(ax_i-b))} $$ Coherence-dependent inputs $$ \begin{array}{l}\mu_1=\mu_0\big(1+c&apos;/100\big)\\\mu_2=\mu_0\big(1-c&apos;/100\big)\end{array} $$ $$ \begin{aligned}&amp;\gamma,a,b,d,J_E,J_I,J_{\mathrm{ext}},I_0,\mu_0,\tau_{\mathrm{AMPA}},\sigma_{\mathrm{noise}}\\&amp;\text{are fixed parameters.}\end{aligned} $$ ```python class DecisionMakingRateModel(bp.dyn.NeuGroup): def __init__(self, size, coherence, JE=0.2609, JI=0.0497, Jext=5.2e-4, I0=0.3255, gamma=6.41e-4, tau=100., tau_n=2., sigma_n=0.02, a=270., b=108., d=0.154, noise_freq=2400., method=&apos;exp_auto&apos;, **kwargs): super(DecisionMakingRateModel, self).__init__(size, **kwargs) # 初始化参数 self.coherence = coherence self.JE = JE self.JI = JI self.Jext = Jext self.I0 = I0 self.gamma = gamma self.tau = tau self.tau_n = tau_n self.sigma_n = sigma_n self.a = a self.b = b self.d = d # 初始化变量 self.s1 = bm.Variable(bm.zeros(self.num) + 0.15) self.s2 = bm.Variable(bm.zeros(self.num) + 0.15) self.r1 = bm.Variable(bm.zeros(self.num)) self.r2 = bm.Variable(bm.zeros(self.num)) self.mu0 = bm.Variable(bm.zeros(self.num)) self.I1_noise = bm.Variable(bm.zeros(self.num)) self.I2_noise = bm.Variable(bm.zeros(self.num)) # 噪声输入的神经元 self.noise1 = bp.dyn.PoissonGroup(self.num, freqs=noise_freq) self.noise2 = bp.dyn.PoissonGroup(self.num, freqs=noise_freq) # 定义积分函数 self.integral = bp.odeint(self.derivative, method=method) @property def derivative(self): return bp.JointEq([self.ds1, self.ds2, self.dI1noise, self.dI2noise]) def ds1(self, s1, t, s2, mu0): I1 = self.Jext * mu0 * (1. + self.coherence / 100.) x1 = self.JE * s1 - self.JI * s2 + self.I0 + I1 + self.I1_noise r1 = (self.a * x1 - self.b) / (1. - bm.exp(-self.d * (self.a * x1 - self.b))) return - s1 / self.tau + (1. - s1) * self.gamma * r1 def ds2(self, s2, t, s1, mu0): I2=self.Jext*mu0*(1.- self.coherence / 100.) x2 = self.JE * s2 - self.JI * s1 + self.I0 + I2 + self.I2_noise r2 = (self.a * x2 - self.b) / (1. - bm.exp(-self.d * (self.a * x2 - self.b))) return - s2 / self.tau + (1. - s2) * self.gamma * r2 def dI1noise(self, I1_noise, t, noise1): return (- I1_noise + noise1.spike * bm.sqrt(self.tau_n * self.sigma_n * self.sigma_n)) / self.tau_n def dI2noise(self, I2_noise, t, noise2): return (- I2_noise + noise2.spike * bm.sqrt(self.tau_n * self.sigma_n * self.sigma_n)) / self.tau_n def update(self, tdi): # 更新噪声神经元以产生新的随机发放 self.noise1.update(tdi) self.noise2.update(tdi) # 更新s1、s2、I1_noise、I2_noise integral = self.integral(self.s1, self.s2, self.I1_noise, self.I2_noise, tdi.t, mu0=self.mu0, noise1=self.noise1, noise2=self.noise2, dt=tdi.dt) self.s1.value, self.s2.value, self.I1_noise.value, self.I2_noise.value = integral # 用更新后的s1、s2计算r1、r2 I1 = self.Jext * self.mu0 * (1. + self.coherence / 100.) x1 = self.JE * self.s1 + self.JI * self.s2 + self.I0 + I1 + self.I1_noise self.r1.value = (self.a * x1 - self.b) / (1. - bm.exp(-self.d * (self.a * x1 - self.b))) I2 = self.Jext * self.mu0 * (1. - self.coherence / 100.) x2 = self.JE * self.s2 + self.JI * self.s1 + self.I0 + I2 + self.I2_noise self.r2.value = (self.a * x2 - self.b) / (1. - bm.exp(-self.d * (self.a * x2 - self.b))) # 重置外部输入 self.mu0[:] = 0. ``` ```python # 定义各个阶段的时长 pre_stimulus_period, stimulus_period, delay_period = 100., 2000., 500. # 生成模型 dmnet = DecisionMakingRateModel(1, coherence=25.6, noise_freq=2400.) # 定义电流随时间的变化 inputs, total_period = bp.inputs.constant_input([(0., pre_stimulus_period), (20., stimulus_period), (0., delay_period)]) # 运行数值模拟 runner = bp.DSRunner(dmnet, monitors=[&apos;s1&apos;, &apos;s2&apos;, &apos;r1&apos;, &apos;r2&apos;], inputs=(&apos;mu0&apos;, inputs, &apos;iter&apos;)) runner.run(total_period) # 可视化 fig, gs = plt.subplots(2, 1, figsize=(6, 6), sharex=&apos;all&apos;) gs[0].plot(runner.mon.ts, runner.mon.s1, label=&apos;s1&apos;) gs[0].plot(runner.mon.ts, runner.mon.s2, label=&apos;s2&apos;) gs[0].axvline(pre_stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[0].axvline(pre_stimulus_period + stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[0].set_ylabel(&apos;gating variable $s$&apos;) gs[0].legend() gs[1].plot(runner.mon.ts, runner.mon.r1, label=&apos;r1&apos;) gs[1].plot(runner.mon.ts, runner.mon.r2, label=&apos;r2&apos;) gs[1].axvline(pre_stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[1].axvline(pre_stimulus_period + stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[1].set_xlabel(&apos;t (ms)&apos;) gs[1].set_ylabel(&apos;firing rate $r$&apos;) gs[1].legend() plt.subplots_adjust(hspace=0.1) plt.show() ``` ### Results ![image-20230828112555018](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112555018.png) ## Phase Plane Analysis 因为只有两个variable ### Model implementation ```python @bp.odeint def int_s1(s1, t, s2, coh=0.5, mu=20.): x1 = JE * s1 + JI * s2 + Ib + JAext * mu * (1. + coh/100) r1 = (a * x1 - b) / (1. - bm.exp(-d * (a * x1 - b))) return - s1 / tau + (1. - s1) * gamma * r1 @bp.odeint def int_s2(s2, t, s1, coh=0.5, mu=20.): x2 = JE * s2 + JI * s1 + Ib + JAext * mu * (1. - coh/100) r2 = (a * x2 - b) / (1. - bm.exp(-d * (a * x2 - b))) return - s2 / tau + (1. - s2) * gamma * r2 ``` ### Without / with input ![image-20230828112709355](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112709355.png) 只受扰动影响，有input后中间变得不稳定，但如果已经选择，网络仍维持之前选择的结果 ![image-20230828112811394](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112811394.png) ### Coherence 稳定点对网络的拉伸更强 ![image-20230828113031946](/BrainPy-course-notes/master_content/Notes.assets/image-20230828113031946.png) ![image-20230828113009219](/BrainPy-course-notes/master_content/Notes.assets/image-20230828113009219.png) # Reservoir Computing 引入训练 倾向于使用RNN ![image-20230828140305956](/BrainPy-course-notes/master_content/Notes.assets/image-20230828140305956.png) Connecting different units $$ \begin{aligned} &amp;\textsf{Input to unit i from unit j:} \\ &amp;&amp;&amp;I_{j\rightarrow i}=J_{ij}r_{j}(t) \\ &amp;\textsf{Total input to unit i:} \\ &amp;&amp;&amp;I_{i}^{(tot)}=\sum_{j=1}^{N}J_{ij}r_{j}(t)+I_{i}^{(ext)} \end{aligned} $$ $$ \textsf{Activation of unit i:} \\ \tau\frac{dx_{i}}{dt}=-x_{i}+\sum_{j=1}^{N}J_{ij}\frac{\phi(x_{j})}{1}+I_{i}^{(ext)}(t) $$ 训练范式 ![image-20230828140707998](/BrainPy-course-notes/master_content/Notes.assets/image-20230828140707998.png) ## Echo state machine ### Echo state machine 类似人工神经网络RNN，可以处理temporal信息 ![image-20230828140937455](/BrainPy-course-notes/master_content/Notes.assets/image-20230828140937455.png) $$ \begin{aligned} &amp;\mathbf{x}(n+1) =f(\mathbf{W}^{\mathrm{in}}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n)+\mathbf{W}^{\mathrm{back}}\mathbf{y}(n)) \\ &amp;\mathbf{y}(n+1) =\mathbf{W}^{\mathrm{out}}(\mathbf{u}(n+1),\mathbf{x}(n+1),\mathbf{y}(n)) \end{aligned} $$ For an RNN, the state of its internal neurons reflects the historical information of the external inputs. 反映的echo的历史信息，唯一依赖历史信息 Assuming that the updates of the network are discrete, the external input at the 𝑛th moment is u(𝑛) and the neuron state is x(𝑛), then x(𝑛) should be determined by u(𝑛), u(𝑛 - 1), ... uniquely determined. At this point, x(𝑛) can be regarded as an &quot;echo&quot; of the historical input signals. 不需要训练connection ### Echo state machine with leaky integrator 有一个leaky项，引入decay ### $$ \begin{aligned}\hat{h}(n)=\tanh(W^{in}x(n)+W^{rec}h(n-1)+W^{fb}y(n-1)+b^{rec})\\h(n)=(1-\alpha)x(n-1)+\alpha\hat{h}(n)\end{aligned} $$ where $h(n)$ is a vector of reservoir neuron activations, $W^{in}$ and $W^{rec}$ are the input and recurrent weight matrices respectively, and $\alpha\in(0,1]$ is the leaking rate. The model is also sometimes used without the leaky integration, which is a special case of $\alpha=1$ The linear readout layer is defined as $$ y(n)=W^{out}h(n)+b^{out} $$ where $y(n)$ is network output, $W^{out}$ the output weight matrix, and $b^out$ is the output bias ## Constraints of echo state machine ### Echo state property #### Theorem 1 For the echo state network defined above, the network will be echoey as long as the maximum singular value $\sigma_{max} Provement: &amp;gt; $$ &amp;gt; \begin{aligned} &amp;gt; d(\mathbf{x}(n+1),\mathbf{x}^{\prime}(n+1))&amp; =d(T(\mathbf{x}(n),\mathbf{u}(n+1)),T(\mathbf{x}&apos;(n),\mathbf{u}(n+1))) \\ &amp;gt; &amp;=d(f(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n)),f(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}&apos;(n))) \\ &amp;gt; &amp;\leq d(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n),\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}^{\prime}(n)) \\ &amp;gt; &amp;=d(\mathbf{W}\mathbf{x}(n),\mathbf{W}\mathbf{x}&apos;(n)) \\ &amp;gt; &amp;=||\mathbf{W}(\mathbf{x}(n)-\mathbf{x}^{\prime}(n))|| \\ &amp;gt; &amp;\leq\sigma_{\max}(\mathbf{W})d(\mathbf{x}(n),\mathbf{x}&apos;(n)) &amp;gt; \end{aligned} &amp;gt; $$ #### Theorem 2 For the echo state network defined above, as long as the spectral radius $|\lambda_{max}|$ of the recurrent connection matrix W &amp;gt; 1, then the network must not be echogenic. The spectral radius of the matrix is the absolute value of the largest eigenvalue $\lambda_{max}$. #### How to initialize Using these two theorems, how should we initialize W so that the network has an echo property? If we scale W, i.e., multiply it by a scaling factor $\alpha$, then $\sigma_{max}\alpha_{max}\text{,the network will not have the echo state.}\\\bullet&amp;\text{if}\alpha_{min}\le\alpha\le\alpha_{max}\text{,the network may have the echo state.}\end{array} $$ **$\alpha$设的略小于1** ![image-20230828142516052](/BrainPy-course-notes/master_content/Notes.assets/image-20230828142516052.png) ### Global parameters of reservoir 这些超参会影响reservoir network的性能，需要手动调参，很难自动去调整 - The size $N_x$ - General wisdom: the bigger the reservoir, the better the obtainable performance - Select global parameters with smaller reservoirs, then scale to bigger ones. - Sparsity - Distribution of nonzero elements: - Normal distribution - Uniform distribution - The width of the distributions does not matter - spectral radius of $W$ - scales the width of the distribution of its nonzero elements - determines how fast the influence of an input dies out in a reservoir with time, and how stable the reservoir activations are - The spectral radius should be larger in tasks requiring longer memory of the input - Scaling(-s) to $W^{in}$: - For uniform distributed $W^{in}$, $\alpha$ in the range of the interval $[-a;a]$. - For normal distributed $W^{in}$, one may take the standard deviation as a scaling measure. The leaking rate $\alpha$ ## Training of echo state machine ### Offline learning The advantage of the echo state network is that it does not train recurrent connections within the reservoir, but only the readout layer from the reservoir to the output. 线性层的优化方法是简单的 **Ridge regression** $$ \begin{aligned}\epsilon_{\mathrm{train}}(n)&amp;=\mathbf{y}(n)-\mathbf{\hat{y}}(n) \\&amp;=\mathbf{y}(n)-\mathbf{W}^{\mathrm{out}}\mathbf{x}(n) \\&amp;L_{\mathrm{ridge}}=\frac{1}{N}\sum_{i=1}^{N}\epsilon_{\mathrm{train}}^{2}(i)+\alpha||\mathbf{W^{out}}||^{2} \\\\W^{out}&amp;=Y^{target}X^T(XX^T+\beta I)^{-1}\end{aligned} $$ ```python trainer = bp.OfflineTrainer(model, fit_method=bp.algorithms.RidgeRegression(1e-7), dt=dt) ``` ### Online learning 来一个sample，进行一次training，对训练资源可以避免瓶颈 The training data is passed to the trainer in a certain sequence (e.g., time series), and the trainer continuously learns based on the new incoming data. **Recursive Least Squares (RLS) algorithm** $$ E(\mathbf{y},\mathbf{y}^\mathrm{target},n)=\frac{1}{N_\mathrm{y}}\sum_{i=1}^{N_\mathrm{y}}\sum_{j=1}^{n}\lambda^{n-j}\left(y_i(j)-y_i^\mathrm{target}(j)\right)^2, $$ ```python trainer = bp.OnlineTrainer(model, fit_method=bp.algorithms.RLS(), dt=dt) ``` ### Dataset 给定time sequence，可以让网络去预测regression ![image-20230828144309742](/BrainPy-course-notes/master_content/Notes.assets/image-20230828144309742.png) 用到BrainPy集成的`Neuromorphic and Cognitive Datasets` ### Other tasks `MNIST dataset` or `Fashion MNIST` Two aspect: - Running time - Memory Usage ## Echo state machine programming ```python import brainpy as bp import brainpy.math as bm import brainpy_datasets as bd import matplotlib.pyplot as plt # enable x64 computation bm.set_environment(x64=True, mode=bm.batching_mode) bm.set_platform(&apos;cpu&apos;) ``` ### Dataset ```python def plot_mackey_glass_series(ts, x_series, x_tau_series, num_sample): plt.figure(figsize=(13, 5)) plt.subplot(121) plt.title(f&quot;Timeserie - {num_sample} timesteps&quot;) plt.plot(ts[:num_sample], x_series[:num_sample], lw=2, color=&quot;lightgrey&quot;, zorder=0) plt.scatter(ts[:num_sample], x_series[:num_sample], c=ts[:num_sample], cmap=&quot;viridis&quot;, s=6) plt.xlabel(&quot;$t$&quot;) plt.ylabel(&quot;$P(t)$&quot;) ax = plt.subplot(122) ax.margins(0.05) plt.title(f&quot;Phase diagram: $P(t) = f(P(t-\\tau))$&quot;) plt.plot(x_tau_series[: num_sample], x_series[: num_sample], lw=1, color=&quot;lightgrey&quot;, zorder=0) plt.scatter(x_tau_series[:num_sample], x_series[: num_sample], lw=0.5, c=ts[:num_sample], cmap=&quot;viridis&quot;, s=6) plt.xlabel(&quot;$P(t-\\tau)$&quot;) plt.ylabel(&quot;$P(t)$&quot;) cbar = plt.colorbar() cbar.ax.set_ylabel(&apos;$t$&apos;) plt.tight_layout() plt.show() ``` ```python dt = 0.1 mg_data = bd.chaos.MackeyGlassEq(25000, dt=dt, tau=17, beta=0.2, gamma=0.1, n=10) ts = mg_data.ts xs = mg_data.xs ys = mg_data.ys plot_mackey_glass_series(ts, xs, ys, num_sample=int(1000 / dt)) ``` ![image-20230828151451523](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151451523.png) ### Prediction of Mackey-Glass timeseries #### Prepare the data ```python def get_data(t_warm, t_forcast, t_train, sample_rate=1): warmup = int(t_warm / dt) # warmup the reservoir forecast = int(t_forcast / dt) # predict 10 ms ahead train_length = int(t_train / dt) X_warm = xs[:warmup:sample_rate] X_warm = bm.expand_dims(X_warm, 0) X_train = xs[warmup: warmup+train_length: sample_rate] X_train = bm.expand_dims(X_train, 0) Y_train = xs[warmup+forecast: warmup+train_length+forecast: sample_rate] Y_train = bm.expand_dims(Y_train, 0) X_test = xs[warmup + train_length: -forecast: sample_rate] X_test = bm.expand_dims(X_test, 0) Y_test = xs[warmup + train_length + forecast::sample_rate] Y_test = bm.expand_dims(Y_test, 0) return X_warm, X_train, Y_train, X_test, Y_test ``` ```python # First warmup the reservoir using the first 100 ms # Then, train the network in 20000 ms to predict 1 ms chaotic series ahead x_warm, x_train, y_train, x_test, y_test = get_data(100, 1, 20000) ``` ```python sample = 3000 fig = plt.figure(figsize=(15, 5)) plt.plot(x_train[0, :sample], label=&quot;Training data&quot;) plt.plot(y_train[0, :sample], label=&quot;True prediction&quot;) plt.legend() plt.show() ``` ![image-20230828151606545](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151606545.png) #### Prepare the ESN ```python class ESN(bp.DynamicalSystemNS): def __init__(self, num_in, num_hidden, num_out, sr=1., leaky_rate=0.3, Win_initializer=bp.init.Uniform(0, 0.2)): super(ESN, self).__init__() self.r = bp.layers.Reservoir( num_in, num_hidden, Win_initializer=Win_initializer, spectral_radius=sr, leaky_rate=leaky_rate, ) self.o = bp.layers.Dense(num_hidden, num_out, mode=bm.training_mode) def update(self, x): return x &amp;gt;&amp;gt; self.r &amp;gt;&amp;gt; self.o ``` #### Train and test ```python model = ESN(1, 100, 1) model.reset_state(1) trainer = bp.RidgeTrainer(model, alpha=1e-6) ``` ```python # warmup _ = trainer.predict(x_warm) ``` ```python # train _ = trainer.fit([x_train, y_train]) ``` #### Test the training data ```python ys_predict = trainer.predict(x_train) ``` ```python start, end = 1000, 6000 plt.figure(figsize=(15, 7)) plt.subplot(211) plt.plot(bm.as_numpy(ys_predict)[0, start:end, 0], lw=3, label=&quot;ESN prediction&quot;) plt.plot(bm.as_numpy(y_train)[0, start:end, 0], linestyle=&quot;--&quot;, lw=2, label=&quot;True value&quot;) plt.title(f&apos;Mean Square Error: {bp.losses.mean_squared_error(ys_predict, y_train)}&apos;) plt.legend() plt.show() ``` ![image-20230828151747954](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151747954.png) #### Test the testing data ```python ys_predict = trainer.predict(x_test) start, end = 1000, 6000 plt.figure(figsize=(15, 7)) plt.subplot(211) plt.plot(bm.as_numpy(ys_predict)[0, start:end, 0], lw=3, label=&quot;ESN prediction&quot;) plt.plot(bm.as_numpy(y_test)[0,start:end, 0], linestyle=&quot;--&quot;, lw=2, label=&quot;True value&quot;) plt.title(f&apos;Mean Square Error: {bp.losses.mean_squared_error(ys_predict, y_test)}&apos;) plt.legend() plt.show() ``` ![image-20230828151824907](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151824907.png) ### JIT connection operators - Just-in-time randomly generated matrix. - Support for Mat@Vec and Mat@Mat. - Support different random generation methods.(homogenous, uniform, normal) ```python import math, random def jitconn_prob_homo(events, prob, weight, seed, outs): random.seed(seed) max_cdist= math.ceil(2/prob -1) for event in events: if event: post_i = random.randint(1, max_cdist) outs[post_i] += weight ``` ![image-20230828153353131](/BrainPy-course-notes/master_content/Notes.assets/image-20230828153353131.png) ## Applications ### From the perspective of kernel methods 维度扩张思想 Non-linear SVMs: Kernel Mapping ![image-20230828153621843](/BrainPy-course-notes/master_content/Notes.assets/image-20230828153621843.png) Kernel methods in neural system? **与维度扩张的思想相似** ![image-20230828153801285](/BrainPy-course-notes/master_content/Notes.assets/image-20230828153801285.png) ### Subcortical pathway for rapid motion processing The first two stages of subcortical visual pathway: Retina -&amp;gt; superior colliculus The first two stages of primary auditory pathway: Inner Ear -&amp;gt; Cochlear Nuclei 维度扩张在subcortical pathway中体现，reservoir 能够高维处理的更简单 ### Spatial-temporal tasks ![image-20230828154155803](/BrainPy-course-notes/master_content/Notes.assets/image-20230828154155803.png) 既有时间信息，又有空间信息的dataset，使用reservoir来处理高维信息，坐位 Dimension expansion ### Gait recognition input来了再做计算 ![image-20230828154352087](/BrainPy-course-notes/master_content/Notes.assets/image-20230828154352087.png) ### Spatial-temporal tasks large-scale，随size增大，accuracy增大 ![image-20230828154428762](/BrainPy-course-notes/master_content/Notes.assets/image-20230828154428762.png) ### Liquid state machine A liquid state machine (LSM) is a type of reservoir computer that uses a spiking neural network. 与ESN一样的范式，都是去做dimension expansion 很难去分析怎么work的">
<meta name="keywords" content="Jekyll, NexT">
<meta property="og:type" content="website">
<meta property="og:title" content="BrainPy course notes">
<meta property="og:url" content="routhleck.github.io/BrainPy-course-notes/master_content/Notes/">
<meta property="og:site_name" content="BrainPy course notes">
<meta property="og:description" content="[TOC] # 神经计算建模简介 ## 计算神经科学的背景与使命 计算神经科学是**脑科学**对**类脑智能**的**桥梁** ### 两大目标 - 用计算建模的方法来阐明大脑功能的计算原理 - 发展类脑智能的模型和算法 ### Prehistory - 1907 LIF model 神经计算的本质 - 1950s HH model 电位定量化模型 最fundamental的 - 1960s Roll&apos;s cable equation 描述信号在轴突和树突怎么传递 - 1970s Amari, Wilson, Cowan et al. 现今建模的基础 - 1982 Hopfield model(Amari-Hopfield model) 引入物理学技术，吸引子模型 - 1988 Sejnowski et al. &quot;Computational Neuroscience&quot;(science) 提出计算神经科学概念 **现在的计算神经科学对应于物理学的第谷-伽利略时代，对大脑工作原理还缺乏清晰的理论** ### Three levels of Brain Science ![image-20230823105226568](/BrainPy-course-notes/master_content/Notes.assets/image-20230823105226568.png) - 大脑做什么 Computational theory -&amp;gt; Psychology &amp; Cognitive Science -&amp;gt; Human-like Cognitive function - 大脑怎么做 Representation &amp; Algorithm -&amp;gt; Computational Neuroscience -&amp;gt; Brain-inspired model &amp; algorithm - 大脑怎么实现 Implementation -&amp;gt; Neuroscience -&amp;gt; Neuromorphic computing ### Mission of Computational Neuroscience &amp;gt; What I can not build a computational model, I do not understand ## 神经计算建模的目标与挑战 ### Limitation of Deep Learning - 不擅长对抗样本 - 对图像的理解有限 ![image-20230823105836259](/BrainPy-course-notes/master_content/Notes.assets/image-20230823105836259.png) ### Brain is for Processing Dynamical Information **We never &quot;see&quot; a static image** ![image-20230823105918336](/BrainPy-course-notes/master_content/Notes.assets/image-20230823105918336.png) ### The missing link a computational model of higher cognitive functior ![image-20230823110617639](/BrainPy-course-notes/master_content/Notes.assets/image-20230823110617639.png) 现在只是做的**局部**的网络，没有一个成功的模型，能**从神经元出发构建网络，到系统层面上** **原因**: 因为神经科学底层数据的缺失，可以考虑数据驱动、大数据的方式来加快发展 ## 神经计算建模的工具 &amp;gt; 工欲行其事，必先利其器 &amp;gt; We need &quot;PyTorch/TensorFlow&quot; in Computational Neuroscience! ### Challenges in neural modelling 有不同的尺度 - Mutiple-scale - Large-scale - Multiple purposes ![image-20230823111212460](/BrainPy-course-notes/master_content/Notes.assets/image-20230823111212460.png) &amp;gt; The modeling targets and methods are extremely complex, and we need a general framework. ### Limitations of Existing Brain Simulators 现今的框架不能满足以上 ![image-20230823111509523](/BrainPy-course-notes/master_content/Notes.assets/image-20230823111509523.png) ### What are needed for a brain simulator 1. Efficiency High-speed simulation on parallel computing devices, etc. 2. Integration Integrated modeling of simulation, training, and analysis 3. Flexibility New models at all scales can be accommodated 4. Extensibility Extensible to new modeling methods(machine learning) 需要新的范式 ### Our solution: BrainPy 4 levels ![image-20230823111903456](/BrainPy-course-notes/master_content/Notes.assets/image-20230823111903456.png) ## 神经计算建模举例 ### Image understanding: an ill-posed problem Image Understanding = image segmentation + image object recognition &amp;gt; Chicken vs. Egg dilemma &amp;gt; &amp;gt; - Without segmentation, how to recognize &amp;gt; - Without recognition, how to segment **The solution of brain:** Analysis-by-synthesis 猜测与验证方法 ### Reverse Hierarchy Theory 人的感知是整体到局部 ### Two pathways for visual information processing ![image-20230823114517888](/BrainPy-course-notes/master_content/Notes.assets/image-20230823114517888.png) ### Key Computational Issues for Global-to-local Neural Information Processing - What are global and local features - How to rapidly extract global features - How to generate global hypotheses - How to implement from global to local processing - The interplay between global and local features - Others #### How to extract global features **Global first = Topology first**(大范围首先，陈霖) 视觉系统更敏感于拓扑性质的差异 &amp;gt; DNNs has difficulty to recognize topology **A retina-SC network for topology detection** 视网膜到上丘的检测，Gap junction coupling ... ### A Model for Motion Pattern Recognition Reservoir Module Decision-making Module ### How to generate &quot;global&quot; hypotheses in the representation space Attractor neural network ![image-20230823115853980](/BrainPy-course-notes/master_content/Notes.assets/image-20230823115853980.png) Levy Flight in Animal Behaviors ![image-20230823120000911](/BrainPy-course-notes/master_content/Notes.assets/image-20230823120000911.png) ### How to process information from global to local Push-pull Feedback A hierarchical Hopfield Model ### Interplay between global and local features A two-pathway model for object recognition ![image-20230823120750349](/BrainPy-course-notes/master_content/Notes.assets/image-20230823120750349.png) Modeling visual masking 可以用two-pathway很好解释 # Programming basics ## Python Basics ### Values - Boolean - String - Integer - Float - ... ### Keywords Not allowed to use keywords, they define structure and rules of a language. ```python help(&quot;keywords&quot;) ``` ### Operators 数据之间的操作 #### For Integers and Floats ```python a=5 b=3 # addition + print(&quot;a+b=&quot;,atb) # subtraction - print(&quot;a-b=&quot;,a-b) # multiplication * print(&quot;axb=&quot;a*b) # division / print(&quot;a/b=&quot;,a/b) # power ** print(&quot;a**b=&quot;,a**b) ``` #### Booleans ```python #Boolean experssions # equals: == print(&quot;5==5&quot;,5==5) # do not equal: != print(&quot;5!-5&quot;,5!=5) # greater than: &amp;gt; print(&quot;5&amp;gt;5&quot;,5&amp;gt;5) # greater than or equal: &amp;gt;= print(&quot;5&amp;gt;=5”5&amp;gt;=5) ``` ```python # logica operators print(&quot;True and False:&quot;, True and False) print(&quot;True or False:&quot;, True or False) print(&quot;not False:&quot;, not False) ``` ### Modules Not all functionality available comes automatically when starting python. ```python import match import numpy as np print(math.pi) print(np.pi) from numpy import pi print(pi) from numpy import * print(pi) ``` ### Control statements #### If ```python a = 5 # In Python, blocks of code are defined using indentation. if a == 5: print(&quot;ok&quot;) ``` &amp;gt; ok #### For ```python # range(5) means a list with integers, 0, 1, 2, 3, 4 for i in range(5): print(i) ``` &amp;gt; 0 &amp;gt; 1 &amp;gt; 2 &amp;gt; 3 &amp;gt; 4 #### While ```python i = 1 while i 1 &amp;gt; 8 &amp;gt; 1000 ### Functions - Functions are used to abstract components of a program. - Much like a mathematical function, they take some input and then find the result. start a function definition with a keyword def - Then comes the function name, with arguments in braces, and then a colon. ```python def func(args1, args2): pass ``` ### Data types #### List - Group variables together - Specific order - Access item with brankets: [ ] - List can be sliced - List can be multiplied - List can be added - Lists are mutable - Copying a list ```python myList = [0, 1, 2, 0,&quot;name&quot;] print(&quot;myList[0]:&quot;, myList[0]) print(&quot;myList[1]:&quot;, myList[1]) print(&quot;myList[3]:&quot;, myList[3]) print(&quot;myList[-1]:&quot;, myList[-1]) print(&quot;myList[-2]:&quot;, myList[-2]) ``` &amp;gt; myList[0]: 0 &amp;gt; myList[1]: 1 &amp;gt; myList[3]: name &amp;gt; myList[-1]: name &amp;gt; myList[-2]: 2.0 ```python myList = [0, 1.0, &quot;hello&quot;] print(&quot;myList[0:2]:&quot;, mylist[0:2]) print(&quot;myList*2:&quot;, myList*2) myList2 = [2,&quot;yes&quot;] print(&quot;myList+myList2:&quot;, myList+myList2) ``` &amp;gt; myList[0:2]: [0，1.0] &amp;gt; myList*2: [0，1.0， hello&apos;，0，1.0， hello&apos;] &amp;gt; myList+myList2: [0，1.0，&apos;hello&apos;，2，yes&apos;] #### tuple Tuples are immutable. #### dictionary A dictionary is a collection of key-value pairs ```python d = {} d[1] = 2 d[&quot;a&quot;] = 3 print(&quot;d: &quot;, d) c = {1:2, &quot;a&quot;:3} print(&quot;c: &quot;, c) print(&quot;c[1]: &quot;, c[1]) ``` &amp;gt; d: {1: 2, &apos;a&apos;: 3} &amp;gt; c: {1: 2, &apos;a&apos;: 3} &amp;gt; c[1]: 2 ### Class In Python, everything is an object. Classes are objects, instances of classes are objects, modules are objects, and functions are objects. 1. a **type** 2. an internal **data representation** (primitive or composite) 3. a set of procedures for **interaction** with the object **a simple example** ```python # define class class Linear(): pass # instantiate object layer1 = Linear() print(layer1) ``` &amp;gt; `` #### Initializing an object ```python # define class class Linear(): # It refers to the object (instance) itself def __init__(self, n_input): self.n_input = n_input layer1 = Linear(100) layer2 = Linear(1000) print(&quot;layer1 : &quot;, layer1.n_input) print(&quot;layer2 : &quot;, layer2.n_input) ``` &amp;gt; layer1 : 100 &amp;gt; layer2 : 1000 #### Class has methods (similar to functions) ```python # define class class Linear(): ### It refers to the the object (instance) itself def __init__(self, n_input, n_output): self.n_input = n_input self.n_output = n_output def compute n params(self): num_params = self.n_input * self.n_output return num_params layerl = Linear(10,100) print(layerl.compute_n_params()) ``` &amp;gt; 1000 ## NumPy Basic ### Numpy Introduction - Fundamental package for scientific computing with Python - N-dimensional array object - Linear algebra, frontier transform, random number capacities - Building block for other packages (e.g. Scipy) ### Array - Arrays are mutable - Arrays attributes - ... ```python A = np.zeros((2, 2)) print(A) ``` &amp;gt; [[0. 0.] &amp;gt; [0. 0.]] ```python a.ndim # 2 dimension a.shape # (2, 5) shape of array a.size # 10 $ of elements a.T # transpose a.dtype # data type ``` #### Array broadcasting When operating on two arrays, numpy compares shapes. Two dimensions are compatible when 1. They are of equal size 2. One of them is 1 ![image-20230823143622229](/BrainPy-course-notes/master_content/Notes.assets/image-20230823143622229.png) ### Vector operations - Inner product - Outer product - Dot product (matrix multiplication) ```python u = [1, 2, 3] v = [1, 1, 1] np.inner(u, v) np.outer(u, v) np.dot(u, v) ``` &amp;gt; 6 &amp;gt; array([[1, 1, 1], &amp;gt; [2, 2, 2], &amp;gt; [3, 3, 3]]) &amp;gt; 6 ### Matrix operations - `np.ones` - `.T` - `np.dot` - `np.eye` - `np.trace` - `np.row_stack` - `np.column_stack` ### Operations along axes ```python a = np.ones((2, 3)) print(a) a.sum() a.sum(axis=0) a.cumsum() a.cumsum(axis=0) ``` ### Slicing arrays ```python a = np.random.random((2, 3)) print(a) a[0,:] # first row, all columns a[0:2] # first and second rows, al columns a[:,1:3]# all rows, second and third columns ``` ### Reshape ```python a = np.ones((10,1)) a.reshape(2,5) ``` ### Linear algebra ```python qr # Computes the QR decomposition cholesky # Computes the Cholesky decomposition inv(A) # Inverse solve(A,b) # Solves Ax = b for A full rank lstsq(A,b) # Solves arg minx //Ax - b//2 eig(A) # Eigenvalue decomposition eigvals(A) # Computes eigenvalues svd(A，full) # Sinqular value decomposition pinv(A) # Computes pseudo-inverse of A ``` ### Fourier transform ```python import numpy.fft fft # 1-dimensional DFT fft2 # 2-dimensional DFT fftn # N-dimensional DFT ifft # 1-dimensional inverse DFT (etc.) rfft # Real DFT (1-dim) ``` ### Random sampling ```python import numpy.random rand(d0, d1, ..., dn) # Random values in a given shape randn(d0, d1, ..., dn) # Random standard normal randint(lo, hi, size) # Random integers [lo hi) choice(a, size, repl, p) # Sample from a shuffle(a) # Permutation (in-place) permutation(a) # Permutation (new array) ``` ### Distributions in random ```python import numpy.random beta binomial chisquare exponential dirichlet gamma laplace lognormal ... ``` ### Scipy - `SciPy` is a library of algorithms and mathematical tools built to work with `NumPy ` arrays. - `scipy.linalg linear algebra` - `scipy.stats statistics` - `scipy.optimize optimization` - `scipy.sparse sparse matrices` - `scipy.signal signal processing` - etc. ## BrainPy introduction ### Modeling demands - Large-scale - Multi-scale - Methods ### BrainPy Architecture - Infrastructure - Functions - Just-in-time compilation - Devices ![image-20230823145349681](/BrainPy-course-notes/master_content/Notes.assets/image-20230823145349681.png) ### Main features #### Dense operators - Compatible with `NumPy`, `TensorFlow`, `PyTorch` and other dense matrix operator syntax. - Users do not need to learn and get started programming directly. #### Dedicated operatorsq - Applies brain dynamics sparse connectivity properties with event-driven computational features. - Reduce the complexity of brain dynamics simulations by several orders of magnitude. #### Numerical Integrators - Ordinary differential equations: brainpy.odeint - Stochastic differential equations: brainpy.sdeint - Fractional differential equations: brainpy.fdeint - Delayed differential equations #### Modular and composable 从微观到宏观 **brainpy.DynamicalSystem** ![image-20230823151159786](/BrainPy-course-notes/master_content/Notes.assets/image-20230823151159786.png) #### JIT of object-oriented BrainPy provides object-oriented transformations: - `brainpy.math.jit` - `brainpy.math.grad` - `brainpy.math.for_loop` - `brainpy.math.ifelse` ## BrainPy Programming Basics ### Just-in-Time compilation Just In Time Compilation (JIT, or Dynamic Translation), is compilation that is being done during the execution of a program. JIT compilation attempts to use **the benefits of both**. While the interpreted program is being run, the JIT compiler determines the most frequently used code and compiles it to machine code. The advantages of a JIT are due to the fact that since the compilation takes place in run time, a JIT compiler has access to dynamic runtime information enabling it to make better optimizations (such as inlining functions). ```python def gelu(x): sqrt = bm.sqrt(2 / bm.pi) cdf = 0.5 * (1.0 + bm.tanh(sqrt * (x + 0.044715 * (x ** 3)))) y = x *cdf return y &amp;gt;&amp;gt;&amp;gt; gelu_jit = bm.jit(gelu) # 使用JIT ``` ### Object-oriented JIT compilation - The class object must be inherited from brainpy.BrainPyObject, the base class of BrainPy, whose methods will be automatically JIT compiled. - All time-dependent variables must be defined as brainpy.math.Variable. ```python class LogisticRegression(bp.BrainPyObject): def __init__(self, dimension): super(LogisticRegression, self).__init__() # parameters self.dimension = dimension # variables self.w = bm.Variable(2.0 * bm.ones(dimension) - 1.3) def __call__(self, X, Y): u = bm.dot(((1.0 / (1.0 + bm.exp(-Y * bm.dot(X, self.w))) - 1.0) * Y), X) self.w.value = self.w - u # in-place update ``` **ExampleL Run a neuron model** ```python model = bp.neurons.HH(1000) #一共1000个神经元 runner = bp.DSRunner(target=model, inputs=(&apos;input&apos;, 10.)) # jit默认为True runner(duration=1000, eval_time=True) #模拟 1000ms ``` 禁用JIT来debug ### Data operations #### Array 等价于`numpy`的`array` #### BrainPy arrays &amp; JAX arrays ```python t1 = bm.arange(3) print(t1) print(t1.value) ``` &amp;gt; JaxArray([0, 1, 2], dtype=int32) &amp;gt; DeviceArray([0, 1, 2], dtype=int32) #### Variables Arrays that are not marked as dynamic variables will be JIT-compiled as static arrays, and modifications to static arrays will not be valid in the JIT compilation environment. ```python t = bm.arange(4) v = bm.Variable(t) print(v) print(v.value) ``` &amp;gt; Variable([0, 1, 2, 3], dtype=int32) &amp;gt; DeviceArray([0, 1, 2, 3], dtype=int32) ### Variables **In-place updating** 就地更新 #### Indexing and slicing - Indexing: `v[i] = a` or `v[(1, 3)] = c` - Slicing: `v[i:j] = b` - Slicing all values `v[:] = d`, `v[...] = e` #### Augmented assignment - add - subtract - divide - multiply - floor divide - modulo - power - and - or - xor - left shift - right shift #### Value assignment ```python v.value = bm.arange(10) check_no_change(v) ``` #### Update assignment ```python v.update(bm.random.randint(0, 20, size=10)) ``` ### Control flows #### If-else `brainpy.math.where` ```python a = 1. bm.where(a DeviceArray(1., dtype=float32, weak_type=True) `brainpy.math.ifelse` ```python def ifelse(condition, branches, operands): true_fun, false_fun = branches if condition: return true_fun(operands) else: return false_fun(operands) ``` #### For loop ```python import brainpy.math hist_of_out_vars = brainpy.math.for_loop(body_fun, operands) ``` #### While loop ```python i = bm.Variable(bm.zeros(1)) counter = bm.Variable(bm.zeros(1)) def cond_f(): return i[0] $$ (2\pi a\Delta x)c_{\mathrm{M}}\frac{\partial V(x,t)}{\partial t}+(2\pi a\Delta x)i_{\mathrm{ion}}=\frac{\pi a^{2}}{\rho_{\mathrm{L}}}\frac{\partial V(x+\Delta x,t)}{\partial x}-\frac{\pi a^{2}}{\rho_{\mathrm{L}}}\frac{\partial V(x,t)}{\partial x} $$ **Cable Equation** $$ c_\mathrm{M}\frac{\partial V(x,t)}{\partial t}=\frac{a}{2\rho_\mathrm{L}}\frac{\partial^2V(x,t)}{\partial x^2}-i_\mathrm{ion} $$ 电流在通过长直导体时会泄露电流，如何记录膜电位，可以使用此方程来描述 **Passive conduction:** ion currents are caused by leaky channels exclusively $$ i_{\mathrm{ion}}=V(x,t)/r_{\mathrm{M}} $$ -&amp;gt; $$ \begin{aligned}c_\mathrm{M}\frac{\partial V(x,t)}{\partial t}&amp;=\frac{a}{2\rho_\mathrm{L}}\frac{\partial^2V(x,t)}{\partial x^2}-\frac{V(x,t)}{r_\mathrm{M}}\\\\\tau\frac{\partial V(x,t)}{\partial t}&amp;=\lambda^2\frac{\partial^2V(x,t)}{\partial x^2}-V(x,t)\quad\lambda=\sqrt{0.5ar_\mathrm{M}/\rho_\mathrm{L}}\end{aligned} $$ 没有动作电位，单纯通过电缆传输 ![image-20230824102932665](/BrainPy-course-notes/master_content/Notes.assets/image-20230824102932665.png) If a constant external current is applied to 𝑥 = 0 the steady-state membrane potential $𝑉_{ss}(𝑥)$ is $$ \lambda^2\frac{\mathrm{d}^2V_{\mathrm{ss}}(x)}{\mathrm{d}x^2}-V_{\mathrm{ss}}(x)=0\longrightarrow V_{\mathrm{ss}}(x)=\frac{\lambda\rho_{\mathrm{L}}}{\pi a^2}I_0e^{-x/\lambda} $$ 电信号无衰减传播: 动作电位 ## Action potential &amp; active transport Steps of an action potential: - Depolarization - Repolarization - Hyperpolarization - Resting Characteristics: - All-or-none - Fixed shape - Active electrical property ![image-20230824103322522](/BrainPy-course-notes/master_content/Notes.assets/image-20230824103322522.png) How to simulate an action potential? $$ \begin{aligned} \frac{I(t)}{A}&amp; =c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+i_{\mathrm{ion}} \\ \Rightarrow\quad c_{\mathrm{M}}\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}&amp; =-g_{\mathrm{Cl}}(V_{\mathrm{M}}-E_{\mathrm{Cl}})-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}})-g_{\mathrm{Na}}(V_{\mathrm{M}}-E_{\mathrm{Na}})+\frac{I(t)}{A} \end{aligned} $$ 离子通道的开闭会随着电压而变化，电导也随着电压而变化 Mechanism: voltage-gated ion channels **HH建模思路：通过电导** ### Nodes of Ranvier Saltatory conduction with a much higher speed and less energy consumption 两个郎飞结之间会有离子通道，既有被动传导，也有主动的防止衰减 ![image-20230824104220106](/BrainPy-course-notes/master_content/Notes.assets/image-20230824104220106.png) ## The Hodgkin-Huxley Model ### Modeling of each ion channel Modeling of each ion channel: $$ g_m=\bar{g}_mm^x $$ Modeling of each ion gate: $$ \mathcal{C}\underset{}{\operatorname*{\overset{\alpha(\mathrm{V})}{\underset{\beta(\mathrm{V})}{\operatorname*{\longrightarrow}}}}\mathcal{O}} \\ \Rightarrow \begin{aligned} \frac{\mathrm{d}m}{\mathrm{d}t}&amp; =\alpha(V)(1-m)-\beta(V)m \\ &amp;=\frac{m_{\infty}(V)-m}{\tau_{m}(V)} \end{aligned} \\ \\ \begin{aligned}m_\infty(V)&amp;=\frac{\alpha(V)}{\alpha(V)+\beta(V)}.\\\tau_m(V)&amp;=\frac{1}{\alpha(V)+\beta(V)}\end{aligned} $$ $$ \text{If}\ V\text{ is constant:}m(t)=m_\infty(V)+(m_0-m_\infty(V))\mathrm{e}^{-t/\tau_m(V)} $$ ### Voltage clamp $$ \begin{aligned} \frac{I(t)}{A}&amp; =c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+i_{\mathrm{ion}} \\ \Rightarrow\quad c_{\mathrm{M}}\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}&amp; =-g_{\mathrm{Cl}}(V_{\mathrm{M}}-E_{\mathrm{Cl}})-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}})-g_{\mathrm{Na}}(V_{\mathrm{M}}-E_{\mathrm{Na}})+\frac{I(t)}{A} \end{aligned} $$ - The membrane potential is kept constant - The current from capacitors is excluded - Currents must come from leaky/voltage-gated ion channels $$ \begin{aligned}I_{\mathrm{cap}}&amp;=c\frac{dV}{dt}=0\\I_{\mathrm{fb}}&amp;=\quad i_{\mathrm{ion}}=g_{\mathrm{Na}}(V-E_{\mathrm{Na}})+g_{\mathrm{K}}(V-E_{\mathrm{K}})+g_{\mathrm{L}}(V-E_{\mathrm{L}})\end{aligned} $$ 只测量一个离子通道就可以很容易得到电导 ![image-20230824111620056](/BrainPy-course-notes/master_content/Notes.assets/image-20230824111620056.png) ### Leaky channel Hyperpolarization → the sodium and potassium channels are closed $$ I_{\mathrm{fb}}=g_{\mathrm{Na}}(V-E_{\mathrm{Na}})+g_{\mathrm{K}}(V-E_{\mathrm{K}})+g_{\mathrm{L}}(V-E_{\mathrm{L}}) $$ $$ \Rightarrow I_{\mathrm{fb}}=g_L(V-E_L) $$ $$ g_\mathrm{L}=0.3\mathrm{mS/cm}^2,E_\mathrm{L}=-54.4\mathrm{mV} $$ #### Potassium and sodium channels Potassium channels: Use choline to eliminate the inward current of Na + Na + current: $I_{fb} - I_{K}$ ![image-20230824112328953](/BrainPy-course-notes/master_content/Notes.assets/image-20230824112328953.png) ![image-20230824112333144](/BrainPy-course-notes/master_content/Notes.assets/image-20230824112333144.png) 转化速率和电导率两个因素 Potassium channels - Resting state (gate closed) - Activated state (gate open) → Activation gate: $g_{\mathrm{K}}=\bar{g}_{K}n^{x}$ Sodium channels - Resting state (gate closed) - Activated state (gate open) - Inactivated state (gate blocked) → Activation gate + inactivation gate: $g_{\mathrm{Na}}=\bar{g}_\text{Na}m^3h$ ![image-20230824113116329](/BrainPy-course-notes/master_content/Notes.assets/image-20230824113116329.png) The gates of sodium channels Modeling of each ion gate: $$ \begin{aligned} &amp;\text{gk}&amp;&amp; =\bar{g}_{K}n^{x} \\ &amp;\text{gNa}&amp;&amp; =\bar{g}_{\mathrm{Na}}m^{3}h \\ &amp;\frac{\mathrm{d}n}{\mathrm{d}t}&amp;&amp; =\alpha_{n}(V)(1-n)-\beta_{n}(V)n \\ &amp;\frac{\mathrm{d}m}{\mathrm{d}t}&amp;&amp; =\alpha_{m}(V)(1-m)-\beta_{m}(V)m \\ &amp;\frac{\mathrm{d}h}{\mathrm{d}t}&amp;&amp; =\alpha_{h}(V)(1-h)-\beta_{h}(V)h \end{aligned} $$ $$ \begin{aligned} \frac{\mathrm{d}m}{\mathrm{d}t}&amp; =\alpha(V)(1-m)-\beta(V)m \\ &amp;=\frac{m_{\infty}(V)-m}{\tau_{m}(V)} \end{aligned} $$ $$ \begin{aligned}m_\infty(V)&amp;=\frac{\alpha(V)}{\alpha(V)+\beta(V)}\\\tau_m(V)&amp;=\frac{1}{\alpha(V)+\beta(V)}\end{aligned}. $$ $$ m(t)=m_\infty(V)+(m_0-m_\infty(V))\mathrm{e}^{-t/\tau_m(V)} $$ ### The Hodgkin-Huxley(HH) Model $$ c_\mathrm{M}\frac{\mathrm{d}V_\mathrm{M}}{\mathrm{d}t}=-g_\mathrm{Cl}(V_\mathrm{M}-E_\mathrm{Cl})-g_\mathrm{K}(V_\mathrm{M}-E_\mathrm{K})-g_\mathrm{Na}(V_\mathrm{M}-E_\mathrm{Na})+\frac{I(t)}{A} $$ 本质是4个微分方程联立在一起 $$ \left\{\begin{aligned}&amp;c\frac{\mathrm{d}V}{\mathrm{d}t}=-\bar{g}_\text{Na}m^3h(V-E_\text{Na})-\bar{g}_\text{K}n^4(V-E_\text{K})-\bar{g}_\text{L}(V-E_\text{L})+I_\text{ext},\\&amp;\frac{\mathrm{d}n}{\mathrm{d}t}=\phi\left[\alpha_n(V)(1-n)-\beta_n(V)n\right]\\&amp;\frac{\mathrm{d}m}{\mathrm{d}t}=\phi\left[\alpha_m(V)(1-m)-\beta_m(V)m\right],\\&amp;\frac{\mathrm{d}h}{\mathrm{d}t}=\phi\left[\alpha_h(V)(1-h)-\beta_h(V)h\right],\end{aligned}\right. $$ $$ \begin{aligned}\alpha_n(V)&amp;=\frac{0.01(V+55)}{1-\exp\left(-\frac{V+55}{10}\right)},\quad\beta_n(V)&amp;=0.125\exp\left(-\frac{V+65}{80}\right),\\\alpha_h(V)&amp;=0.07\exp\left(-\frac{V+65}{20}\right),\quad\beta_n(V)&amp;=\frac{1}{\left(\exp\left(-\frac{V+55}{10}\right)+1\right)},\\\alpha_m(V)&amp;=\frac{0.1(V+40)}{1-\exp\left(-(V+40)/10\right)},\quad\beta_m(V)&amp;=4\exp\left(-(V+65)/18\right).\end{aligned} $$ $$ \phi=Q_{10}^{(T-T_{\mathrm{base}})/10} $$ 每一步符合生物学 ![image-20230824113714178](/BrainPy-course-notes/master_content/Notes.assets/image-20230824113714178.png) #### How to fit each gating variable? **Fitting n:** $g_{\mathbf{K}}=\bar{g}_{K}n^{x}\quad m(t)=m_{\infty}(V)+(m_{0}-\color{red}{\boxed{m_{\infty}(V)}})\mathrm{e}^{-t/\pi_{m}(V)}$ → $g_\mathrm{K}(V,t)=\bar{g}_\mathrm{K}\left[n_\infty(V)-(n_\infty(V)-n_0(V))\mathrm{e}^{-\frac{t}{\tau_n(V)}}\right]^x$ by $g_{\mathrm{K}\infty}=\bar{g}_{\mathrm{K}}n_{\infty}^{x},g_{\mathrm{K}0}=\bar{g}_{\mathrm{K}}n_{0}^{x}$ → $g_{\mathrm{K}}(V,t)=\left[g_{\mathrm{K}\infty}^{1/x}-(g_{\mathrm{K}\infty}^{1/x}-g_{\mathrm{K}0}^{1/x})\mathrm{e}^{-\frac{t}{\tau_{n}(V)}}\right]^{x}$ ![image-20230824114623467](/BrainPy-course-notes/master_content/Notes.assets/image-20230824114623467.png) # Hodgkin-Huxley brain dynamics programming ## Dynamics Programming Basics ### Integrators 微分器 ![image-20230824140806650](/BrainPy-course-notes/master_content/Notes.assets/image-20230824140806650.png) **example** FitzHugh-Nagumo equation $$ \begin{aligned}\tau\dot{w}&amp;=v+a-bw,\\\dot{v}&amp;=v-\frac{ u^3}{3}-w+I_{\mathrm{ext}}.\end{aligned} $$ ```python @bp.odeint(method=&apos;Euler&apos;, dt=0.01) def integral(V, w, t, Iext, a, b, tau): dw = (V + a - b * w) / tau dV = V - V * V * V / 3 - w + Iext return dV, dw ``` **JointEq** In a dynamical system, there may be multiple variables that change dynamically over time. Sometimes these variables are interrelated, and updating one variable requires other variables as inputs. For better integration accuracy, we recommend that you use `brainpy.JointEq` to jointly solve interrelated differential equations. ```python a, b = 0.02, 0.20 dV = lambda V, t, w, Iext: 0.04 * V * V + 5 * V + 140 - w + Iext # 第一个方程 dw = lambda w, t, V: a * (b * V - w) # 第二个方程 joint_eq = bp.JointEq(dV, dw) # 联合微分方程 integral2 = bp.odeint(joint_eq, method=&apos;rk2&apos;) # 定义该联合微分方程的数值积分方法 ``` ```python # 声明积分运行器 runner = bp.integrators.IntegratorRunner( integral, monitors=[&apos;V&apos;] inits=dict(V=0., w=0.) args=dict(a=a, b=b, tau=tau, Iext=Iext), dt=0.01 ) # 使用积分运行器来进行模拟100ms，结合步长dt=0.01 runner.run(100.) plt.plot(runner.mon.ts, runner.mon.V) plt.show() ``` ![image-20230824142019832](/BrainPy-course-notes/master_content/Notes.assets/image-20230824142019832.png) ### `DynamicalSystem` BrainPy provides a generic `SynamicalSystem` class to define various types of dynamical models. BrainPy supports modelings in brain simulation and brain-inspired computing. All these supports are based on one common concept: **Dynamical System** via `brainpy.DynamicalSystem`. #### What is `DynamicalSystem` A `DynamicalSystem` defines the updating rule of the model at single time step. 1. For models with state, `DynamicalSystem` defines the state transition from $t$ to $t + dt$, i.e., $S(t+dt)=F(S(t),x,t,dt)$, where $S$ is the state, $x$ is input, $t$ is the time, and $dt$ is the time step. This is the case for recurrent neural networks (like GRU, LSTM), neuron models (like HH, LIF), or synapse models which are widely used in brain simulation. 2. However, for models in deep learning, like convolution and fully-connected linear layers, `DynamicalSystem` defines the input-to-output mapping, i.e., $y=F(x,t)$. ![img](https://brainpy.readthedocs.io/en/latest/_images/dynamical_system.png) #### How to define `DynamicalSystem` ```python class YourDynamicalSystem(bp.DynamicalSystem): def update(self, x): ... ``` Instead of input x, there are shared arguments across all nodes/layers in the network: - the current time `t`, or - the current running index `i`, or - the current time step `dt`, or - the current phase of training or testing `fit=True/False`. Here, it is necessary to explain the usage of `bp.share`. - `bp.share.save( )`: The function saves shared arguments in the global context. User can save shared arguments in tow ways, for example, if user want to set the current time `t=100`, the current time step `dt=0.1`,the user can use `bp.share.save(&quot;t&quot;,100,&quot;dt&quot;,0.1)` or `bp.share.save(t=100,dt=0.1)`. - `bp.share.load( )`: The function gets the shared data by the `key`, for example, `bp.share.load(&quot;t&quot;)`. - `bp.share.clear_shargs( )`: The function clears the specific shared arguments in the global context, for example, `bp.share.clear_shargs(&quot;t&quot;)`. - `bp.share.clear( )`: The function clears all shared arguments in the global context. #### How to run `DynamicalSystem` As we have stated above that `DynamicalSystem` only defines the updating rule at single time step, to run a `DynamicalSystem` instance over time, we need a for loop mechanism. ![img](https://brainpy.readthedocs.io/en/latest/_images/dynamical_system_and_dsrunner.png) ##### `brainpy.math.for_loop` `for_loop` is a structural control flow API which runs a function with the looping over the inputs. Moreover, this API just-in-time compile the looping process into the machine code. ```python inputs = bp.inputs.section_input([0., 6.0, 0.], [100., 200., 100.]) indices = np.arange(inputs.size) def run(i, x): neu.step_run(i, x) return neu.V.value vs = bm.for_loop(run, (indices, inputs), progress_bar=True) ``` ##### `brainpy.LoopOverTime` Different from `for_loop`, `brainpy.LoopOverTime` is used for constructing a dynamical system that automatically loops the model over time when receiving an input. `for_loop` runs the model over time. While `brainpy.LoopOverTime` creates a model which will run the model over time when calling it. ```python net2.reset_state(batch_size=10) looper = bp.LoopOverTime(net2) out = looper(currents) ``` ##### `brainpy.DSRunner` **Initializing a `DSRunner`** Generally, we can initialize a runner for dynamical systems with the format of: ``` runner = DSRunner(target=instance_of_dynamical_system, inputs=inputs_for_target_DynamicalSystem, monitors=interested_variables_to_monitor, dyn_vars=dynamical_changed_variables, jit=enable_jit_or_not, progress_bar=report_the_running_progress, numpy_mon_after_run=transform_into_numpy_ndarray ) ``` - `target` specifies the model to be simulated. It must an instance of brainpy.DynamicalSystem. - `inputs` is used to define the input operations for specific variables. - It should be the format of `[(target, value, [type, operation])]`, where `target` is the input target, `value` is the input value, `type` is the input type (such as “fix”, “iter”, “func”), `operation` is the operation for inputs (such as “+”, “-”, “*”, “/”, “=”). Also, if you want to specify multiple inputs, just give multiple `(target, value, [type, operation])`, such as `[(target1, value1), (target2, value2)]`. - It can also be a function, which is used to manually specify the inputs for the target variables. This input function should receive one argument `tdi` which contains the shared arguments like time `t`, time step `dt`, and index `i`. - `monitors` is used to define target variables in the model. During the simulation, the history values of the monitored variables will be recorded. It can also to monitor variables by callable functions and it should be a `dict`. The `key` should be a string for later retrieval by `runner.mon[key]`. The `value` should be a callable function which receives an argument: `tdt`. - `dyn_vars` is used to specify all the dynamically changed [variables](https://brainpy.readthedocs.io/en/latest/tutorial_math/variables.html) used in the `target` model. - `jit` determines whether to use JIT compilation during the simulation. - `progress_bar` determines whether to use progress bar to report the running progress or not. - `numpy_mon_after_run` determines whether to transform the JAX arrays into numpy ndarray or not when the network finishes running. **Running a `DSRunner`** After initialization of the runner, users can call `.run()` function to run the simulation. The format of function `.run()` is showed as follows: ```python runner.run(duration=simulation_time_length, inputs=input_data, reset_state=whether_reset_the_model_states, shared_args=shared_arguments_across_different_layers, progress_bar=report_the_running_progress, eval_time=evaluate_the_running_time ) ``` - `duration` is the simulation time length. - `inputs` is the input data. If `inputs_are_batching=True`, `inputs` must be a PyTree of data with two dimensions: `(num_sample, num_time, ...)`. Otherwise, the `inputs` should be a PyTree of data with one dimension: `(num_time, ...)`. - `reset_state` determines whether to reset the model states. - `shared_args` is shared arguments across different layers. All the layers can access the elements in `shared_args`. - `progress_bar` determines whether to use progress bar to report the running progress or not. - `eval_time` determines whether to evaluate the running time. ### Monitors ```python # initialize monitor through a list of strings runner1 = bp.DSRunner(target=net, monitors=[&apos;E.spike&apos;, &apos;E.V&apos;, &apos;I.spike&apos;, &apos;I.V&apos;], # 4 elements in monitors inputs=[(&apos;E.input&apos;, 20.), (&apos;I.input&apos;, 20.)], jit=True) ``` Once we call the runner with a given time duration, the monitor will automatically record the variable evolutions in the corresponding models. Afterwards, users can access these variable trajectories by using .mon.[variable_name]. The default history times .mon.ts will also be generated after the model finishes its running. Let’s see an example. ```python runner1.run(100.) bp.visualize.raster_plot(runner1.mon.ts, runner1.mon[&apos;E.spike&apos;], show=True) ``` **Initialization with index specification** ```python monitors=[(&apos;E.spike&apos;, [1, 2, 3]), # monitor values of Variable at index of [1, 2, 3] &apos;E.V&apos;], # monitor all values of Variable &apos;V&apos; ``` &amp;gt; The monitor shape of &quot;E.V&quot; is (run length, variable size) = (1000, 3200) &amp;gt; The monitor shape of &quot;E.spike&quot; is (run length, index size) = (1000, 3) **Explicit monitor target** ```python monitors={&apos;spike&apos;: net.E.spike, &apos;V&apos;: net.E.V}, ``` &amp;gt; The monitor shape of &quot;V&quot; is = (1000, 3200) &amp;gt; The monitor shape of &quot;spike&quot; is = (1000, 3200) **Explicit monitor target with index specification** ```python monitors={&apos;E.spike&apos;: (net.E.spike, [1, 2]), # monitor values of Variable at index of [1, 2] &apos;E.V&apos;: net.E.V}, # monitor all values of Variable &apos;V&apos; ``` &amp;gt; The monitor shape of &quot;E.V&quot; is = (1000, 3200) &amp;gt; The monitor shape of &quot;E.spike&quot; is = (1000, 2) ### Inputs In brain dynamics simulation, various inputs are usually given to different units of the dynamical system. In BrainPy, `inputs` can be specified to runners for dynamical systems. The aim of `inputs` is to mimic the input operations in experiments like Transcranial Magnetic Stimulation (TMS) and patch clamp recording. `inputs` should have the format like `(target, value, [type, operation])`, where - `target` is the target variable to inject the input. - `value` is the input value. It can be a scalar, a tensor, or a iterable object/function. - `type` is the type of the input value. It support two types of input: `fix` and `iter`. The first one means that the data is static; the second one denotes the data can be iterable, no matter whether the input value is a tensor or a function. The `iter` type must be explicitly stated. - `operation` is the input operation on the target variable. It should be set as one of `{ + , - , * , / , = }`, and if users do not provide this item explicitly, it will be set to ‘+’ by default, which means that the target variable will be updated as `val = val + input`. #### Static inputs ```python runner6 = bp.DSRunner(target=net, monitors=[&apos;E.spike&apos;], inputs=[(&apos;E.input&apos;, 20.), (&apos;I.input&apos;, 20.)], # static inputs jit=True) runner6.run(100.) bp.visualize.raster_plot(runner6.mon.ts, runner6.mon[&apos;E.spike&apos;]) ``` #### Iterable inputs ```python I, length = bp.inputs.section_input(values=[0, 20., 0], durations=[100, 1000, 100], return_length=True, dt=0.1) runner7 = bp.DSRunner(target=net, monitors=[&apos;E.spike&apos;], inputs=[(&apos;E.input&apos;, I, &apos;iter&apos;), (&apos;I.input&apos;, I, &apos;iter&apos;)], # iterable inputs jit=True) runner7.run(length) bp.visualize.raster_plot(runner7.mon.ts, runner7.mon[&apos;E.spike&apos;]) ``` ## Run a built-in HH model [Using Built-in Models — BrainPy documentation](https://brainpy.readthedocs.io/en/latest/tutorial_building/overview_of_dynamic_model.html) ```python import brainpy as bp import brainpy.math as bm current, length = bp.inputs.section_input(values=[0., bm.asarray([1., 2., 4., 8., 10., 15.]), 0.], durations=[10, 2, 25], return_length=True) hh_neurons = bp.neurons.HH(current.shape[1]) runner = bp.DSRunner(hh_neurons, monitors=[&apos;V&apos;, &apos;m&apos;, &apos;h&apos;, &apos;n&apos;], inputs=(&apos;input&apos;, current, &apos;iter&apos;)) runner.run(length) ``` ## Run a HH model from scratch The mathematic expression of the HH model $$ \left\{\begin{aligned}&amp;c\frac{\mathrm{d}V}{\mathrm{d}t}=-\bar{g}_\text{Na}m^3h(V-E_\text{Na})-\bar{g}_\text{K}n^4(V-E_\text{K})-\bar{g}_\text{L}(V-E_\text{L})+I_\text{ext},\\&amp;\frac{\mathrm{d}n}{\mathrm{d}t}=\phi\left[\alpha_n(V)(1-n)-\beta_n(V)n\right]\\&amp;\frac{\mathrm{d}m}{\mathrm{d}t}=\phi\left[\alpha_m(V)(1-m)-\beta_m(V)m\right],\\&amp;\frac{\mathrm{d}h}{\mathrm{d}t}=\phi\left[\alpha_h(V)(1-h)-\beta_h(V)h\right],\end{aligned}\right. $$ $$ \begin{aligned}\alpha_n(V)&amp;=\frac{0.01(V+55)}{1-\exp\left(-\frac{V+55}{10}\right)},\quad\beta_n(V)&amp;=0.125\exp\left(-\frac{V+65}{80}\right),\\\alpha_h(V)&amp;=0.07\exp\left(-\frac{V+65}{20}\right),\quad\beta_n(V)&amp;=\frac{1}{\left(\exp\left(-\frac{V+55}{10}\right)+1\right)},\\\alpha_m(V)&amp;=\frac{0.1(V+40)}{1-\exp\left(-(V+40)/10\right)},\quad\beta_m(V)&amp;=4\exp\left(-(V+65)/18\right).\end{aligned} $$ $$ \phi=Q_{10}^{(T-T_{\mathrm{base}})/10} $$ V: the membrane potential n: activation variable of the Kt channel m: activation variable of the Nat channel h; inactivation variable of the Nat channe ### Define HH model `class` - Inherit `bp.dyn.NeuDyn` ```python import brainpy as bp import brainpy.math as bm class HH(bp.dyn.NeuDyn): def __init__(self, size, ENa=50., gNa=120., Ek=-77., gK=36., EL=-54.387, gL=0.03, V_th=0., C=1.0, T=6.3): super(HH, self).__init__(size=size) ``` ### Initialization ```python import brainpy as bp import brainpy.math as bm class HH(bp.dyn.NeuDyn): def __init__(self, size, ENa=50., gNa=120., Ek=-77., gK=36., EL=-54.387, gL=0.03, V_th=0., C=1.0, T=6.3): super(HH, self).__init__(size=size) # parameters self.ENa = ENa self.EK = EK self.EL = EL self.gNA = gNa self.gK = gK self.gL = gL self.C = C self.V_th = V_th self.T_base = 6.3 self.phi = 3.0 ** ((T - self.T_base) / 10.0) # variable self.V = bm.Variable(-70.68 * bm.ones(self.num)) self.m = bm.Variable(0.0266 * bm.ones(self.num)) self.h = bm.Variable(0.772 * bm.ones(self.num)) self.n = bm.Variable(0.235 * bm.ones(self.num)) self.input = bm.Variable(bm.zeros(self.num)) self.spike = bm.Variable(bm.zeros(self.num, dtype=bool)) self.t_last_spike = bm.Variable(bm.ones(self.num) * -1e7) # 定义积分函数 self.integral = bp.odeint(f=self.derivative, method=&apos;exp_auto&apos;) ``` ### Define the derivative function ```python @property def derivative(self): return bp.JointEq(self.dV, self.dm, self.dh, self.dn) def dV(self, V, t, m, h, n, Iext): I_Na = (self.gNa * m ** 3.0 * h) * (V - self.ENa) I_K = (self.gK * n ** 4.0) * (V - self.EK) I_leak = self.gL * (V - self.EL) dVdt = (- I_Na - I_K - I_leak + Iext) / self.C return dVdt def dm(self, m, t, V): alpha = 0.1 * (V + 40) / (1 - bm.exp(-(V + 40) / 10)) beta = 4.0 * bm.exp(-(V + 65) / 18) dmdt = alpha * (1 - m) - beta * m return self.phi * dmdt def dh(self, h, t, V): alpha = 0.07 * bm.exp(-(V + 65) / 20.) beta = 1 / (1 + bm.exp(-(V + 35) / 10)) dhdt = alpha * (1 - h) - beta * h return self.phi * dhdt def dn(self, n, t, V): alpha = 0.01 * (V + 55) / (1 - bm.exp(-(V + 55) / 10)) beta = 0.125 * bm.exp(-(V + 65) / 80) dndt = alpha * (1 - n) - beta * n return self.phi * dndt ``` ### Complete the `update()` function ```python def update(self, x=None): t = bp.share.load(&apos;t&apos;) dt = bp.share.load(&apos;dt&apos;) # TODO: 更新变量V, m, h, n, 暂存在V, m, h, n中 V, m, h, n = self.integral(self.V, self.m, self.h, self.n, t, self.input, dt=dt) #判断是否发生动作电位 self.spike.value = bm.logical_and(self.V = self.V_th) # 更新最后一次脉冲发放时间 self.t_last_spike.value = bm.where(self.spike, t, self.t_last_spike) # TODO: 更新变量V, m, h, n的值 self.V.value = V self.m.value = m self.h.value = h self.n.value = n #重置输入 self.input[:] = 0 ``` ### Simulation ```python current, length = bp.inputs.section_input(values=[0., bm.asarray([1., 2., 4., 8., 10., 15.]), 0.], durations=[10, 2, 25], return_length=True) hh_neurons = HH(current.shape[1]) runner = bp.DSRunner(hh_neurons, monitors=[&apos;V&apos;, &apos;m&apos;, &apos;h&apos;, &apos;n&apos;], inputs=(&apos;input&apos;, current, &apos;iter&apos;)) runner.run(length) ``` ### Visualization ```python import numpy as np import matplotlib.pyplot as plt bp.visualize.line_plot(runner.mon.ts, runner.mon.V, ylabel=&apos;V (mV)&apos;, plot_ids=np.arange(current.shape[1])) plt.plot(runner.mon.ts, bm.where(current[:, -1]&amp;gt;0, 10, 0) - 90.) plt.figure() plt.plot(runner.mon.ts, runner.mon.m[:, -1]) plt.plot(runner.mon.ts, runner.mon.h[:, -1]) plt.plot(runner.mon.ts, runner.mon.n[:, -1]) plt.legend([&apos;m&apos;, &apos;h&apos;, &apos;n&apos;]) plt.xlabel(&apos;Time (ms)&apos;) ``` ## Customize a conductance-based model 电路模拟，写成电导形式 ![image-20230824180831033](/BrainPy-course-notes/master_content/Notes.assets/image-20230824180831033.png) $$ \begin{aligned} \text{gK}&amp; =\bar{g}_\text{K}n^4, \\ \frac{\mathrm{d}n}{\mathrm{d}t}&amp; =\phi[\alpha_n(V)(1-n)-\beta_n(V)n], \end{aligned} $$ 动力学形式描述，引入门框变量$n$ $$ \begin{aligned} &amp;\alpha_{n}(V) =\frac{0.01(V+55)}{1-\exp(-\frac{V+55}{10})}, \\ &amp;\beta_{n}(V) =0.125\exp\left(-\frac{V+65}{80}\right). \end{aligned} $$ 由此式来建模钾离子通道 ### Programming an ion channel #### Three ion channel ```python import brainpy as bp import brainpy.math as bm class IK(bp.dyn.IonChannel): def __init__(self, size, E=-77., g_max=36., phi=1., method=&apos;exp_auto&apos;): super(IK, self).__init__(size) self.g_max = g_max self.E = E self.phi = phi self.n = bm.Variable(bm.zeros(size)) # variables should be packed with bm.Variable self.integral = bp.odeint(self.dn, method=method) def dn(self, n, t, V): alpha_n = 0.01 * (V + 55) / (1 - bm.exp(-(V + 55) / 10)) beta_n = 0.125 * bm.exp(-(V + 65) / 80) return self.phi * (alpha_n * (1. - n) - beta_n * n) def update(self, V): t = bp.share.load(&apos;t&apos;) dt = bp.share.load(&apos;dt&apos;) self.n.value = self.integral(self.n, t, V, dt=dt) def current(self, V): return self.g_max * self.n ** 4 * (self.E - V) ``` ```python class INa(bp.dyn.IonChannel): def __init__(self, size, E= 50., g_max=120., phi=1., method=&apos;exp_auto&apos;): super(INa, self).__init__(size) self.g_max = g_max self.E = E self.phi = phi self.m = bm.Variable(bm.zeros(size)) # variables should be packed with bm.Variable self.h = bm.Variable(bm.zeros(size)) self.integral_m = bp.odeint(self.dm, method=method) self.integral_h = bp.odeint(self.dh, method=method) def dm(self, m, t, V): # TODO: 计算dm/dt alpha_m = 0.11 * (V + 40) / (1 - bm.exp(-(V + 40) / 10)) beta_m = 4 * bm.exp(-(V + 65) / 18) return self.phi * (alpha_m * (1. - m) - beta_m * m) def dh(self, h, t, V): # TODO: 计算dh/dt alpha_h = 0.07 * bm.exp(-(V + 65) / 20) beta_h = 1. / (1 + bm.exp(-(V + 35) / 10)) return self.phi * (alpha_h * (1. - h) - beta_h * h) def update(self, V): t = bp.share.load(&apos;t&apos;) dt = bp.share.load(&apos;dt&apos;) # TODO: 更新self.m, self.h self.m.value = self.integral_m(self.m, t, V, dt=dt) self.h.value = self.integral_h(self.h, t, V, dt=dt) def current(self, V): return self.g_max * self.m ** 3 * self.h * (self.E - V) ``` ```python class IL(bp.dyn.IonChannel): def __init__(self, size, E=-54.39, g_max=0.03): super(IL, self).__init__(size) self.g_max = g_max self.E = E def current(self, V): return self.g_max * (self.E - V) def update(self, V): pass ``` #### Build a HH model with ion channels **Using customized ion channels** ```python class HH(bp.dyn.CondNeuGroup): def __init__(self, size): super(HH, self).__init__(size, V_initializer=bp.init.Uniform(-80, -60.)) # TODO: 初始化三个离子通道 self.IK = IK(size, E=-77., g_max=36.) self.INa = INa(size, E=50., g_max=120.) self.IL = IL(size, E=-54.39, g_max=0.03) ``` **Using built-in ion channels** ```python class HH(bp.dyn.CondNeuGroup): def __init__(self, size): super().__init__(size) self.INa = bp.channels.INa_HH1952(size) self.IK = bp.channels.IK_HH1952(size) self.IL = bp.cahnnels.IL(size, E=-54.387, g_max=0.03) ``` #### Simulation ```python neu = HH(1) runner = bp.DSRunner( neu, monitors=[&apos;V&apos;, &apos;IK.n&apos;, &apos;INa.m&apos;, &apos;INa.h&apos;], inputs=(&apos;input&apos;, 1.698) # near the threshold current ) runner.run(200) # the running time is 200 ms import matplotlib.pyplot as plt plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;V&apos;]) plt.xlabel(&apos;t (ms)&apos;) plt.ylabel(&apos;V (mV)&apos;) plt.savefig(&quot;HH.jpg&quot;) plt.show() plt.figure(figsize=(6, 2)) plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;IK.n&apos;], label=&apos;n&apos;) plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;INa.m&apos;], label=&apos;m&apos;) plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;INa.h&apos;], label=&apos;h&apos;) plt.xlabel(&apos;t (ms)&apos;) plt.legend() plt.savefig(&quot;HH_channels.jpg&quot;) plt.show() ``` ![image-20230824184016011](/BrainPy-course-notes/master_content/Notes.assets/image-20230824184016011.png) # Simple Neuron Modeling: Simplified Models ## The Leaky Integrate-and-Fire(LIF) Neuron Model ### The LIF neuron model $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-(V-V_{\mathrm{rest}})+RI(t)\\\\\mathrm{if}V&amp;&amp;gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\text{last}\ {t_{ref}}\end{aligned} $$ 只有一个微分方程，要加一个不应期(**t refractory period**)，膜电位不发生任何改变，认为离子通道只有泄露通道 ![image-20230825101057570](/BrainPy-course-notes/master_content/Notes.assets/image-20230825101057570.png) Given a constant current input: ![image-20230825101410745](/BrainPy-course-notes/master_content/Notes.assets/image-20230825101410745.png) 没有建模准确变化，只提供什么时候膜电位的变化 ### The dynamic features of the LIF model **General solution (constant input):**$V(t)=V_{\text{reset}}+RI_{\text{c}}(1-\mathrm{e}^{-\frac{t-t_0}{\tau}})$ **Firing frequency:** $$ \begin{aligned}T&amp;=-\tau\ln\left(1-\frac{V_{\phi h}-V_{\mathrm{rest}}}{RI_{\varsigma}}\right)\\f&amp;=\frac{1}{T+t_{\mathrm{ref}}}=\frac{1}{t_{\mathrm{ref}}-\tau\ln\left(1-\frac{V_{0}-V_{\mathrm{rest}}}{RI_{\varsigma}}\right)}\end{aligned} $$ **Rheobase current (minimal current):** $$ I_{\theta}=\frac{V_{\mathrm{th}}-V_{\mathrm{reset}}}{R} $$ 基强电流，如果小于它将无法发放 ### Strengths &amp; weaknesses of the LIF model #### Strengths - Simple, high simulation efficiency - Intuitive - Fits well the subthreshold membrane potential #### Weaknesses - The shape of action potentials is over-simplified - Has no memory of the spiking history - Cannot reproduce diverse firing patterns ### Other Univariate neuron models #### The Quadratic Integrate-and-Fire (QOF) model: $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=a_{0}(V-V_{\mathrm{re}t})(V-V_{\mathrm{c}})+RI(t)\\&amp;\text{if }V&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{re}set}\quad\text{last}\quad t_{\mathrm{ref}}\end{aligned} $$ ![image-20230825103243039](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103243039.png) 膜电位仍需要手动重置 #### The Theta neuron model $$ \frac{\mathrm{d}\theta}{\mathrm{d}t}=1-\cos\theta+(1+\cos\theta)(\beta+I(t)) $$ ![image-20230825103331170](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103331170.png) 隐式表达，不具有物理意义，但也会进行整合发放 #### The Exponential Integrate-and-Fire (ExpIF) model $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-\left(V-V_{\mathrm{res}t}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{3T}}+RI(t)\\\mathrm{if~}V&amp;&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{res}t}\mathrm{last}t_{\mathrm{ref}}\end{aligned} $$ ![image-20230825103501912](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103501912.png) 仍需要手动重置膜电位 ## The Adaptive Exponential Integrate-and-Fire(AdEx) Neuron Model ### The AdEx neuron model Two variables: - 𝑉: membrane potential - 𝑤: adaptation variable $$ \begin{aligned} \tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{S_{T}}}-Rw+RI(t) \\ \tau_{w}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{\mathrm{w}}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ \mathrm{if}V&amp; &amp;gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} \end{aligned} $$ 不为零，就会衰减到$-w$ ![image-20230825103840880](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103840880.png) - A larger 𝑤 suppresses 𝑉 from increasing - 𝑤 decays exponentially while having a sudden increase when the neuron fires **Firing patterns of the AdEx model** ![image-20230825104254936](/BrainPy-course-notes/master_content/Notes.assets/image-20230825104254936.png) **Categorization of firing patterns** According to the steady-state firing time intervals: - Tonic/regular spiking - Adapting - Bursting - Irregular spiking According to the initial-state features: - Tonic/classic spiking - Initial burst - Delayed spiking ### Other multivariate neuron models #### The Izhikevich model $$ \begin{aligned} &amp;\frac{dV}{dt} =0.04V^{2}+5V+140-u+I \\ &amp;\frac{\mathrm{d}u}{\mathrm{d}t} =a\left(bV-u\right) \\ &amp;\operatorname{if}V &amp;gt;\theta,\quad V\leftarrow c,u\leftarrow u+d\text{ last }t_{\mathrm{ref}} \end{aligned} $$ 二次整合发放多加了一个$u$ ![image-20230825104832770](/BrainPy-course-notes/master_content/Notes.assets/image-20230825104832770.png) #### The FitzHugh–Nagumo (FHN) model $$ \begin{aligned}\dot{v}&amp;=v-\frac{v^3}3-w+RI_{\mathrm{ext}}\\\tau\dot{w}&amp;=v+a-bw.\end{aligned} $$ 没有对膜电位进行人为的重置，可以更好的进行动力学分析，没有打破微分方程的连续性 ![image-20230825104922636](/BrainPy-course-notes/master_content/Notes.assets/image-20230825104922636.png) #### The Generalized Integrate-and-Fire (GIF) model n+2个变量 $$ \begin{aligned} &amp;\tau{\frac{\mathrm{d}V}{\mathrm{d}t}} =-\left(V-V_{\mathrm{rest}}\right)+R\sum_{j}I_{j}+RI \\ &amp;\frac{\mathrm{d}\Theta}{\mathrm{d}t} =a\left(V-V_{\mathrm{rest}}\right)-b\left(\Theta-\Theta_{\infty}\right) \\ &amp;\frac{\mathrm{d}l_{j}}{\mathrm{d}t} =-k_{j}I_{j},\quad j=1,2,...,n \\ &amp;\operatorname{if}V &amp;gt;\Theta,\quad I_{j}\leftarrow R_{j}I_{j}+A_{j},V\leftarrow V_{\mathrm{reset}},\Theta\leftarrow max(\Theta_{\mathrm{reset}},\Theta) \end{aligned} $$ 每个变量都是线性的，泛化性体现在重置条件上 ![image-20230825105035349](/BrainPy-course-notes/master_content/Notes.assets/image-20230825105035349.png) ## Dynamic analysis: phase-plane analysis ### Phase plane analysis 对动力学系统的行为来分析，普遍对两个变量来进行分析 Analyzes the behavior of a dynamical system with (usually two) variables described by ordinary differential equations $$ \begin{aligned} &amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp;&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{S_{T}}}-Rw+RI(t) \\ &amp;\tau_{W}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp;&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ &amp;\mathrm{if}V&amp;&amp; &amp;gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} \end{aligned} $$ **Elements:** - Nullclines: $\mathrm{d}V/\mathrm{d}t=0;\mathrm{d}w/\mathrm{d}t=0$ - Fixed points: $\mathrm{d}V/\mathrm{d}t=0\mathrm{~and~}\mathrm{d}w/\mathrm{d}t=0$ - The vector field - The trajectory of variables 假设外部电流恒定 ![image-20230825110708994](/BrainPy-course-notes/master_content/Notes.assets/image-20230825110708994.png) ### Phase plane analysis for the AdEx neuron model $$ \begin{aligned} &amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp;&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{\Lambda_{T}}}-Rw+RI(t) \\ &amp;\tau_{w}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp;&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ &amp;\text{ifV}&amp;&amp; &amp;gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} \end{aligned} $$ ![image-20230825110811399](/BrainPy-course-notes/master_content/Notes.assets/image-20230825110811399.png) #### Tonic ![image-20230825112857175](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112857175.png) #### Adaptation ![image-20230825112918815](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112918815.png) #### Bursting ![image-20230825112933938](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112933938.png) #### Transient spiking ![image-20230825112950297](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112950297.png) ## Dynamic analysis: bifurcation analysis ### Bifurcation analysis Quantitative analysis of the existence and the properties of fixed points in a dynamical system with a changing parameter 某个外界条件变化时，固定点的变化 Elements: - Lines of fixed points - Stability properties of fixed points ![image-20230825114510710](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114510710.png) ### Bifurcation analysis for the AdEx Neuron model bifurcation analysis for 2 variables Variables: 𝑉 and 𝑤 Parameters: $I_{ext}$ $$ \begin{aligned} &amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}=-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{{\frac{V-V_{T}}{ST}}}-Rw+RI(t) \\ &amp;\text{-} {\frac{\mathrm{d}w}{\mathrm{d}t}}=a(V-V_{\mathrm{rest}})-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ &amp;\mathrm{if}V&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{reset}}\ \mathrm{last}\ t_{\mathrm{ref}} \end{aligned} $$ ![image-20230825114801456](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114801456.png) ![image-20230825114742740](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114742740.png) **Subjects: two variables (𝑉 and 𝑤)** ![image-20230825114856403](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114856403.png) ### Extended: The limit cycle The FitzHugh–Nagumo (FHN) model $$ \begin{aligned}\dot{v}&amp;=v-\frac{v^3}3-w+RI_\mathrm{ext}\\\tau\dot{w}&amp;=v+a-bw.\end{aligned} $$ This dynamical system, in certain conditions, exhibits a cyclic pattern of variable changes which can be visualized as a closed trajectory in the phase plane. 变化锁定到环中 ![image-20230825115348008](/BrainPy-course-notes/master_content/Notes.assets/image-20230825115348008.png) ![image-20230825115354146](/BrainPy-course-notes/master_content/Notes.assets/image-20230825115354146.png) # Reduced Models - brain dynamics programming ## LIF neuron models programming ### Define LIF `class` $$ \begin{aligned}&amp;\tau\frac{\mathrm{d}V}{\mathrm{d}t}=-(V-V_{\mathrm{rest}})+RI(t)\\&amp;\text{if }V&amp;gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\text{last}t_{\mathrm{ref}}\end{aligned} $$ ```python class LIF(bp.dyn.NeuDyn): def __init__(self, size, V_rest=0, V_reset=-5, V_th=20, R=1, tau=10, t_ref=5., **kwargs): # 初始化父类 super(LIF, self).__init__(size=size, **kwargs) ``` ### Initialization ```python class LIF(bp.dyn.NeuDyn): def __init__(self, size, V_rest=0, V_reset=-5, V_th=20, R=1, tau=10, t_ref=5., **kwargs): # 初始化父类 super(LIF, self).__init__(size=size, **kwargs) # 初始化参数 self.V_rest = V_rest self.V_reset = V_reset self.V_th = V_th self.R = R self.tau = tau self.t_ref = t_ref # 不应期时长 # 初始化变量 self.V = bm.Variable(bm.random.randn(self.num) + V_reset) self.input = bm.Variable(bm.zeros(self.num)) self.t_last_spike = bm.Variable(bm.ones(self.num) * -1e7) # 上一次脉冲发放时间 self.refractory = bm.Variable(bm.zeros(self.num, dtype=bool)) # 是否处于不应期 self.spike = bm.Variable(bm.zeros(self.num, dtype=bool)) # 脉冲发放状态 # 使用指数欧拉方法进行积分 self.integral = bp.odeint(f=self.derivative, method=&apos;exponential_euler&apos;) ``` ### Define the derivative function ```python # 定义膜电位关于时间变化的微分方程 def derivative(self, V, t, Iext): dVdt = (-V + self.V_rest + self.R * Iext) / self.tau return dVdt ``` ### Complete the `update()` function ```python def update(self): t, dt = bp.share[&apos;t&apos;], bp.share[&apos;dt&apos;] # 以数组的方式对神经元进行更新 refractory = (t - self.t_last_spike) self.V_th # 将大于阈值的神经元标记为发放了脉冲 self.spike[:] = spike # 更新神经元脉冲发放状态 self.t_last_spike[:] = bm.where(spike, t, self.t_last_spike) # 更新最后一次脉冲发放时间 self.V[:] = bm.where(spike, self.V_reset, V) # 将发放了脉冲的神经元膜电位置为V_reset，其余不变 self.refractory[:] = bm.logical_or(refractory, spike) # 更新神经元是否处于不应期 self.input[:] = 0. # 重置外界输入 ``` ### Simulation ```python def run_LIF(): # 运行LIF模型 group = LIF(1) runner = bp.DSRunner(group, monitors=[&apos;V&apos;], inputs=(&apos;input&apos;, 22.)) runner(200) # 运行时长为200ms # 结果可视化 fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) plt.plot(runner.mon.ts, runner.mon.V) plt.xlabel(r&apos;$t$ (ms)&apos;) plt.ylabel(r&apos;$V$ (mV)&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) plt.show() ``` ![image-20230825141201825](/BrainPy-course-notes/master_content/Notes.assets/image-20230825141201825.png) ### Input current &amp; firing frequency $$ \begin{gathered} V(t)=V_{\mathrm{reset}}+RI_{\mathrm{c}}(1-\mathrm{e}^{-\frac{t-t_{0}}{\tau}}). \\ T=-\tau\ln\left[1-\frac{V_{\mathrm{th}}-V_{\mathrm{rest}}}{RI_{\mathrm{c}}}\right] \\ f={\frac{1}{T+t_{\mathrm{ref}}}}={\frac{1}{t_{\mathrm{ref}}-\tau\ln\left[1-{\frac{V_{\mathrm{th}}-V_{\mathrm{rest}}}{RI_{c}}}\right]}} \end{gathered} $$ ```python # 输入与频率的关系 current = bm.arange(0, 600, 2) duration = 1000 LIF_neuron = LIF(current.shape[0]) runner_2 = bp.dyn.DSRunner(LIF_neurons, monitors=[&apos;spike&apos;], inputs={&apos;input&apos;, current}, dt=0.01) runner_2.run(duration) freqs = runner_2.mon.spike.sum(axis=0) / (duration/1000) plt.figure() plt.plot(current, freqs) plt.xlabel(&apos;inputs&apos;) plt.ylabel(&apos;frequencies&apos;) ``` ![image-20230825143405952](/BrainPy-course-notes/master_content/Notes.assets/image-20230825143405952.png) ### Other Univariate neuron models **The Quadratic Integrate-and-Fire (QIF) model** $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=a_{0}(V-V_{\mathrm{res}t})(V-V_{c})+RI(t)\\\mathrm{if~}V&amp;&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{reset~last~}t_{\mathrm{ref}}}\end{aligned} $$ ```python def derivative(self, V, t, I): dVdt = (self.c * (V - self.V_reset) * (V - self.V_c) + self.R * I) / self.tau return dVdt ``` **The Exponential Integrate-and-Fire (ExpIF) model** $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{\delta_{T}}}+RI(t)\\&amp;\mathrm{if~}V&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{reset}}\mathrm{last}t_{\mathrm{ref}}\end{aligned} $$ ```python def derivative(self, V, t, I): exp_v = self.delta_T * bm.exp((V - self.V_T) / self.delta_T) dvdt = (- (V - self.V_rest) + exp_v + self.R * I) / self.tau return dvdt ``` ## AdEx neuron models programming $$ \begin{gathered} \tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}=-(V-V_{\mathrm{rest}})+\Delta_{T}\mathrm{e}^{{\frac{V-V_{T}}{\Delta T}}}-Rw+RI(t), \\ \tau_{w}\frac{\mathrm{d}w}{\mathrm{d}t}=a(V-V_{\mathrm{rest}})-w+b\tau_{w}\sum_{t^{(f)}}\delta(t-t^{(f)})), \\ \mathrm{if~}V&amp;gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\mathrm{last}t_{\mathrm{ref}}. \end{gathered} $$ ### Define AdEx `class` ```python class AdEx(bp.dyn.NeuDyn): def __init__(self, size, V_rest=-65, V_reset=-68, V_th=-30, V_T=-59.9, delta_T=3.48 a=1., b=1., R=1., tau=10., tau_w=30., tau_ref=0., **kwargs): # 初始化父类 super(AdEx, self).__init__(size=size, **kwargs) ``` ### Initialization ```python class AdEx(bp.dyn.NeuDyn): def __init__(self, size, V_rest=-65, V_reset=-68, V_th=-30, V_T=-59.9, delta_T=3.48 a=1., b=1., R=1., tau=10., tau_w=30., tau_ref=0., **kwargs): # 初始化父类 super(AdEx, self).__init__(size=size, **kwargs) # 初始化参数 self.V_rest = V_rest self.V_reset = V_reset self.V_th = V_th self.V_T = V_T self.delta_T = delta_T self.a = a self.b = b self.R = R self.tau = tau self.tau_w = tau_w self.tau_ref = tau_ref # 初始化变量 self.V = bm.Variable(bm.random.randn(self.num) - 65.) self.w = bm.Variable(bm.zeros(self.num)) self.input = bm.Variable(bm.zeros(self.num)) self.t_last_spike = bm.Variable(bm.ones(self.num) * -1e7) # 上一次脉冲发放时间 self.refractory = bm.Variable(bm.zeros(self.num, dtype=bool)) # 是否处于不应期 self.spike = bm.Variable(bm.zeros(self.num, dtype=bool)) # 脉冲发放状态 # 定义积分器 self.integral = bp.odeint(f=self.derivative, method=&apos;exp_auto&apos;) ``` ### Define the derivative function ```python def dV(self, V, t, w, I): exp = self.delta_T * bm.exp((V - self.V_T) / self.delta_T) dVdt = (-V + self.V_rest + exp - self.R * w + self.R * I) / self.tau return dVdt def dw(self, w, t, V): dwdt = (self.a * (V - self.V_rest) - w) / self.tau_w return dwdt @property def derivative(self): return bp.JointEq([self.dV, self.dw]) ``` ### Complete the `update()` function ```python def update(self): t, dt = bp.share[&apos;t&apos;], bp.share[&apos;dt&apos;] V, w = self.integral(self.V.value, self.w.value, t, self.input, dt=dt) # 以数组的方式对神经元进行更新 refractory = (t - self.t_last_spike) self.V_th # 将大于阈值的神经元标记为发放了脉冲 self.spike[:] = spike # 更新神经元脉冲发放状态 self.t_last_spike[:] = bm.where(spike, t, self.t_last_spike) # 更新最后一次脉冲发放时间 self.V[:] = bm.where(spike, self.V_reset, V) # 将发放了脉冲的神经元膜电位置为V_reset，其余不变 self.w[:] = bm.where(spike, w + self.b, w) #更新自适应电流 self.refractory[:] = bm.logical_or(refractory, spike) # 更新神经元是否处于不应期 self.input[:] = 0. # 重置外界输入 ``` ### Simulation ![image-20230825145518709](/BrainPy-course-notes/master_content/Notes.assets/image-20230825145518709.png) ### Other multivariate neuron models **The Izhikevich model** $$ \begin{aligned} &amp;\frac{dV}{dt} =0.04V^{2}+5V+140-u+I \\ &amp;\frac{\mathrm{d}u}{\mathrm{d}t} =a\left(bV-u\right) \\ &amp;\operatorname{if}V &amp;gt;\theta,\quad V\leftarrow c,u\leftarrow u+d\mathrm{last}t_{\mathrm{ref}} \end{aligned} $$ ```python def dV(self, V, t, u, I): dVdt = 0.04 * V * V + 5 * V + 140 - u + I return dVdt def du(self, u, t, V): dudt = self.a * (self.b * V - u) return dudt ``` **The Generalized Integrate-and-Fire (GIF) model** $$ \begin{aligned} &amp;\tau{\frac{\mathrm{d}V}{\mathrm{d}t}} =-\left(V-V_{\mathrm{rest}}\right)+R\sum_{j}I_{j}+RI \\ &amp;\frac{\mathrm{d}\Theta}{\mathrm{d}t} =a\left(V-V_{\mathrm{est}}\right)-b\left(\Theta-\Theta_{\infty}\right) \\ &amp;\frac{\mathrm{d}I_j}{\mathrm{d}r} =-k_jI_j,\quad j=1,2,\ldots,n \\ &amp;\text{if V} &amp;gt;\Theta,\quad I_{j}\leftarrow R_{j}I_{j}+A_{j},V\leftarrow V_{\mathrm{reset}},\Theta\leftarrow max\left(\Theta_{\mathrm{reset}},\Theta\right) \end{aligned} $$ ```python def dI1(self, I1, t): return - self.k1 * I1 def dI2(self, I2, t): return - self.k2 * I2 def dVth(self, V_th, t, V): return self.a * (V - self.v_rest) - self.b * (V_th - self.V_th_inf) def dV(self, V, t, I1, I2, I): return (- (V - self.V_rest) + self.R * (I + I1 + I2)) / self.tau ``` **Built-in reduced neuron models** ![image-20230825145947800](/BrainPy-course-notes/master_content/Notes.assets/image-20230825145947800.png) ## Dynamic analysis: phase-plane analysis ### Simple case $$ \frac{dx}{dt}=\sin(x)+I, $$ ```python @bp.odeint def int_x(x, t, Iext): return bp.math.sin(x) + Iext ``` ```python pp = bp.analysis.PhasePlane1D( model=int_x, target_vars={&apos;x&apos;: [-10, 10]}, pars_update={&apos;Iext&apos;: 0.}, resolutions={&apos;x&apos;: 0.01} ) pp.plot_vector_field() pp.plot_fixed_point(show=True) ``` ![image-20230825152003373](/BrainPy-course-notes/master_content/Notes.assets/image-20230825152003373.png) - Nullcline: The zero-growth isoclines, such as $f(x,y) = 0$ and $g(x,y) = 0$ - Fixed points: The equilibrium points of the system, which are located at all the nullclines intersect. - Vector field: The vector field of the system. - Limit cycles: The limit cycles. - Trajectories: A simulation trajectory with the given initial values ### Phase plane analysis for AdEx ```python def ppa_AdEx(group): bm.enable_x64() v_range = [-70., -40.] w_range = [-10., 50.] phase_plane_analyzer = bp.analysis.PhasePlane2D( model=group, target_vars={&apos;V&apos;: v_range, &apos;w&apos;: w_range, }, # 待分析变量 pars_update={&apos;I&apos;: Iext}, # 需要更新的变量 resolutions=0.05 ) # 画出V, w的零增长曲线 phase_plane_analyzer.plot_nullcline() # 画出奇点 phase_plane_analyzer.plot_fixed_point() # 画出向量场 phase_plane_analyzer.plot_vector_field() # 分段画出V, w的变化轨迹 group.V[:], group.w[:] = group.V_reset, 0 runner = bp.DSRunner(group, monitors=[&apos;V&apos;, &apos;w&apos;, &apos;spike&apos;], inputs=(&apos;input&apos;, Iext)) runner(500) spike = runner.mon.spike.squeeze() s_idx = np.where(spike)[0] # 找到所有发放动作电位对应的index s_idx = np.concatenate(([0], s_idx, [len(spike) - 1])) # 加上起始点和终止点的index for i in range(len(s_idx) - 1): vs = runner.mon.V[s_idx[i]: s_idx[i + 1]] ws = runner.mon.w[s_idx[i]: s_idx[i + 1]] plt.plot(vs, ws, color=&apos;darkslateblue&apos;) # 画出虚线 x = V_reset plt.plot([group.V_reset, group.V_reset], w_range, &apos;--&apos;, color=&apos;grey&apos;, zorder=-1) plt.show() ``` ![image-20230825152925463](/BrainPy-course-notes/master_content/Notes.assets/image-20230825152925463.png) ## Dynamic analysis: bifurcation analysis ### Simple case $$ \frac{dx}{dt}=\sin(x)+I, $$ ```python bif = bp.analysis.Bifurcation1D( model=int_x, target_vars={&apos;x&apos;: [-10, 10]}, target_pars={&apos;Iext&apos;: [0., 1.5]}, resolutions={&apos;Iext&apos;: 0.005, &apos;x&apos;: 0.05} ) bif.plot_bifurcation(show=True) ``` ![image-20230825154227567](/BrainPy-course-notes/master_content/Notes.assets/image-20230825154227567.png) # Synapse models and their programming ## The biology of synapses ### Neurotransmitter &amp; Synapse When the action potential invades the axon terminals, it causes voltage-gated 𝐶𝐶𝑎𝑎 2+ channels to open (1), which triggers vesicles to bind to the presynaptic membrane (2). Neurotransmitter is released into the synaptic cleft by exocytosis and diffuses across the cleft (3). Binding of the neurotransmitter to receptor molecules in the postsynaptic membrane completes the process of transmission (4). 去极化时钙离子内流，与囊泡相结合，...，与受体结合，打开离子通道，超极化、去极化现象 ![image-20230826100321307](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100321307.png) ![image-20230826100418911](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100418911.png) **Neurotransmitter leading to postsynaptic potential.** The binding of neurotransmitter to the postsynaptic membrane receptors changes the membrane potential ($V_m$). These postsynaptic potentials can be either excitatory (depolarizing the membrane), as shown here, or inhibitory (hyperpolarizing the membrane). ![image-20230826100531535](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100531535.png) ### Neurotransmitters 兴奋性神经递质： - 乙酰胆碱 (ACh) - 儿茶酚胺 (catecholamines) - 谷氨酸 (glutamate) - 组胺 (histamine) - 5-羟色胺 (serotonin) - 某些神经肽类 (some of neuropeptides) 抑制性神经递质： - GABA - 甘氨酸 (glycine) - 某些神经肽类 (some of peptides) ![image-20230826100609904](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100609904.png) ### The postsynaptic response The aim of a synapse model is to describe accurately the postsynaptic response generated by the arrival of an action potential at a presynaptic terminal. 1. The fundamental quantity to be modelled is the time course of the postsynaptic receptor conductance 2. The models: - Simple phenomenological waveforms - More complex kinetic schemes that are analogous to the models of membrane- bound ion channels ![image-20230826100701580](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100701580.png) 建模这种响应模式，打开关闭的概率... ## Phenomenological synapse models ### Exponential Model ![image-20230826100738460](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100738460.png) **Assumption**: - The release of neurotransmitter, its diffusion across the cleft, the receptor binding, and channel opening all happen very quickly, so that the channels instantaneously jump from the closed to the open state. channel会瞬间增加然后逐渐关闭 $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}e^{-(t-t_{0})/\tau} \\ \begin{matrix}\bullet&amp;\tau \ \text{is the time constant}\\\bullet&amp;t_0 \ \text{is the time of the pre-synaptic spike}\\\bullet&amp;\bar{g_{syn}}\ \text{is the maximal conductance}\end{matrix} $$ -&amp;gt; corresponding differential equation $$ \tau\frac{dg_{\mathrm{syn}}(t)}{dt}=-g_{\mathrm{syn}}(t)+\bar{g}_{\mathrm{syn}}\delta\left(t_{0}-t\right) $$ - Can fit with experimental data. - A good approximation for GABA A and AMPA, because the rising phase is much shorter than their decay phase. ### Dual Exponential Model ![image-20230826101203059](/BrainPy-course-notes/master_content/Notes.assets/image-20230826101203059.png) exponential model上升的太快，不太符合某些synapse Dual exponential synapse provides a general way to describe the synaptic conductance with different rising and decay time constants. $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}\frac{\tau_{1}\tau_{2}}{\tau_{1}-\tau_{2}}\left(\exp\left(-\frac{t-t_{0}}{\tau_{1}}\right)-\exp\left(-\frac{t-t_{0}}{\tau_{2}}\right)\right) \\ \begin{matrix} \bullet &amp;t_1\ \text{is the decay synaptic time constant} \\ \bullet &amp;\tau_2\ \text{is the rise synaptic time constant} \\ \bullet &amp;t_0\ \text{is the time of the pre-synaptic spike} \\ \bullet &amp;\bar{g}_{syn}\ \text{is the maximal conductance} \end{matrix} $$ -&amp;gt;corresponding differential equation $$ \begin{aligned} &amp;g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}g \\ &amp;\frac{dg}{dt}=-\frac{g}{\tau_{\mathrm{decay}}}+h \\ &amp;\frac{dh}{dt}&amp; =-\frac{h}{\tau_{\mathrm{rise}}}+\delta\left(t_{0}-t\right), \end{aligned} $$ The time course of most synaptic conductance can be well described by this sum of two exponentials. ### Synaptic time constants ![image-20230826101544786](/BrainPy-course-notes/master_content/Notes.assets/image-20230826101544786.png) http://compneuro.uwaterloo.ca/research/constants-constraints/neurotransmitter-time-constants-pscs.html #### AMPA synapse - $t_{decay}$ = 0.18 ms in the auditory system of the chick nucleus magnocellularis (Trussell, 1999). - $t_{rise}$ 25 ms and $\tau_{decay}$ =0.77 ms in dentate gyrus basket cells (Geiger et al., 1997). - $t_{rise}$ = 0.2 ms and $\tau_{decay}$ =1.7 ms in in neocortical layer 5 pyramidal neurons (Hausser and Roth, 1997b). - Reversal potential is nearly 0 mV. #### NMDA synapse - The decay time constants (at near-physiological temperature): - 19 ms in dentate gyrus basket cells (Geiger et al., 1997), - 26 ms in neocortical layer 2/3 pyramidal neurons (Feldmeyer et al., 2002), - 89 ms in CA1 pyramidal cells (Diamond, 2001). - The rise time constants are about 2 ms (Feldmeyer et al., 2002). - Reversal potential is nearly 0 mV. #### GABA$_A$ synapse - GABAergic synapses from dentate gyrus basket cells onto other basket cells are faster: $t_{rise}$ = 0.3 ms and $t_{decay}$ = 2.5 ms (Bartos et al., 2001) than synapses from basket cells to granule cells: $t_{rise}$ = 0.26 ms and $t_{decay}$ = 6.5 ms (Kraushaar and Jonas, 2000). - Reversal potential is nearly -80 mV. #### GABA$_B$ synapse - Common models use models with a rise time of about 25-50 ms, a fast decay time in the range of 100-300ms and a slow decay time of 500-1000 ms. - Reversal potential is nearly -90 mV. ### General property of synaptic time constants - The time constants of synaptic conductance vary widely among synapse types. - The synaptic kinetics tends to accelerate during development (T. Takahashi, Neuroscience Research, 2005) . - The synaptic kinetics becomes substantially faster with increasing temperature. ![image-20230826102033433](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102033433.png) ### Current- and Conductance-based Response ![image-20230826102042614](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102042614.png) #### Conductance-based Response Most synaptic ion channels, such as AMPA and GABA, display an approximately linear current-voltage relationship when they open. ![image-20230826102113670](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102113670.png) **For example**: The synapse is located on a thin dendrite, because the local membrane potential V changes considerably when the synapse is activated. #### Current-based Response In some case, we can also approximate the synapses as sources of current and not a conductance. ![image-20230826102150487](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102150487.png) **For example**: The excitatory synapse on a large compartment, because the depolarization of the membrane is small. ## Programming of phenomenological synapse models ### `ProjAlignPostMg2` ![Image Name](https://cdn.kesci.com/upload/rzz4o4uyar.png?imageView2/0/w/960/h/960) ```python brainpy.dyn.ProjAlignPostMg2( pre, delay, comm, syn, out, post ) ``` - ``pre (JointType[DynamicalSystem, AutoDelaySupp])``: The pre-synaptic neuron group. - ``delay (Union[None, int, float])``: The synaptic delay. - ``comm (DynamicalSystem)``: The synaptic communication. - ``syn (ParamDescInit)``: The synaptic dynamics. - ``out (ParamDescInit)``: The synaptic output. - ``post (DynamicalSystem)`` The post-synaptic neuron group. 只需要建模所有post的neurons ### CSR matrix ![Image Name](https://cdn.kesci.com/upload/rzz4on32hr.png?imageView2/0/w/960/h/960) ### Exponential Model The single exponential decay synapse model assumes the release of neurotransmitter, its diffusion across the cleft, the receptor binding, and channel opening all happen very quickly, so that the channels instantaneously jump from the closed to the open state. Therefore, its expression is given by $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} e^{-\left(t-t_{0}\right) / \tau} $$ where $\tau$ is the time constant, $t_0$ is the time of the pre-synaptic spike, $\bar{g}_{\mathrm{syn}}$ is the maximal conductance. The corresponding differential equation: $$ \frac{d g}{d t} = -\frac{g}{\tau_{decay}}+\sum_{k} \delta(t-t_{j}^{k}). $$ #### COBA Given the synaptic conductance, the COBA model outputs the post-synaptic current with $$ I_{syn}(t) = g_{\mathrm{syn}}(t) (E - V(t)) $$ ```python class ExponSparseCOBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.COBA.desc(E=E), post=post, ) ``` ```python class SimpleNet(bp.DynSysGroup): def __init__(self, E=0.): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = ExponSparseCOBA(self.pre, self.post, delay=None, prob=1., g_max=1., tau=5., E=E) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` ```python def run_a_net(net): indices = np.arange(1000) # 100 ms conductances, currents, potentials = bm.for_loop(net.step_run, indices, progress_bar=True) ts = indices * bm.get_dt() # --- similar to: # runner = bp.DSRunner(net) # conductances, currents, potentials = runner.run(100.) fig, gs = bp.visualize.get_figure(1, 3, 3.5, 4) fig.add_subplot(gs[0, 0]) plt.plot(ts, conductances) plt.title(&apos;Syn conductance&apos;) fig.add_subplot(gs[0, 1]) plt.plot(ts, currents) plt.title(&apos;Syn current&apos;) fig.add_subplot(gs[0, 2]) plt.plot(ts, potentials) plt.title(&apos;Post V&apos;) plt.show() ``` #### CUBA Given the conductance, this model outputs the post-synaptic current with a identity function: $$ I_{\mathrm{syn}}(t) = g_{\mathrm{syn}}(t) $$ ```python class ExponSparseCUBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.CUBA.desc(), post=post, ) ``` ```python class SimpleNet2(bp.DynSysGroup): def __init__(self, g_max=1.): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = ExponSparseCUBA(self.pre, self.post, delay=None, prob=1., g_max=g_max, tau=5.) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` #### Dense connections Exponential synapse model with the conductance-based (COBA) output current and dense connections. ```python class ExponDenseCOBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.MaskedLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.COBA.desc(E=E), post=post, ) ``` ![Image Name](https://cdn.kesci.com/upload/rzz4p7x6dl.png?imageView2/0/w/960/h/960) Exponential synapse model with the current-based (COBA) output current and dense connections. ```python class ExponDenseCUBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.MaskedLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.CUBA.desc(), post=post, ) ``` ### `ProjAlignPreMg2` Synaptic projection which defines the synaptic computation with the dimension of presynaptic neuron group. ![Image Name](https://cdn.kesci.com/upload/rzz4pj1qmk.png?imageView2/0/w/960/h/960) ```python brainpy.dyn.ProjAlignPreMg2( pre, delay, syn, comm, out, post ) ``` - ``pre (JointType[DynamicalSystem, AutoDelaySupp])``: The pre-synaptic neuron group. - ``delay (Union[None, int, float])``: The synaptic delay. - ``syn (ParamDescInit)``: The synaptic dynamics. - ``comm (DynamicalSystem)``: The synaptic communication. - ``out (ParamDescInit)``: The synaptic output. - ``post (DynamicalSystem)`` The post-synaptic neuron group. ### Dual Exponential Model The dual exponential synapse model, also named as **difference of two exponentials model**, is given by: $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} \frac{\tau_{1} \tau_{2}}{\tau_{1}-\tau_{2}}\left(\exp \left(-\frac{t-t_{0}}{\tau_{1}}\right)-\exp \left(-\frac{t-t_{0}}{\tau_{2}}\right)\right) $$ where $\tau_1$ is the time constant of the decay phase, $\tau_2$ is the time constant of the rise phase, $t_0$ is the time of the pre-synaptic spike, $\bar{g}_{\mathrm{syn}}$ is the maximal conductance. The corresponding differential equation: $$ \begin{aligned} &amp;g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} g \\ &amp;\frac{d g}{d t}=-\frac{g}{\tau_{\mathrm{decay}}}+h \\ &amp;\frac{d h}{d t}=-\frac{h}{\tau_{\text {rise }}}+ \delta\left(t_{0}-t\right), \end{aligned} $$ The alpha function is retrieved in the limit when both time constants are equal. ```python class DualExpSparseCOBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau_decay, tau_rise, E): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.DualExpon.desc(pre.num, tau_decay=tau_decay, tau_rise=tau_rise), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.COBA(E=E), post=post, ) ``` ```python class SimpleNet4(bp.DynSysGroup): def __init__(self, E=0.): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = DualExpSparseCOBA(self.pre, self.post, delay=None, prob=1., g_max=1., tau_decay=5., tau_rise=1., E=E) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` ## Biophysical synapse models ### Limitations of phenomenological models 打开的数量是有限的，而且有饱和期 1. Saturation of postsynaptic receptors by previously released transmitter. 2. Certain receptor types also exhibit desensitization that prevents them (re)opening for a period after transmitter-binding, like sodium channels underlying action potential. ![image-20230826111443117](/BrainPy-course-notes/master_content/Notes.assets/image-20230826111443117.png) ### Linetic/Markov models ![image-20230826111733654](/BrainPy-course-notes/master_content/Notes.assets/image-20230826111733654.png) - The simplest kinetic model is a two-state scheme in which receptors can be either closed, 𝐶, or open, 𝑂, and the transition between states depends on transmitter concentration, [𝑇], in the synaptic cleft: - 𝛼 and 𝛽 are voltage-independent forward and backward rate constants. - 𝐶 and 𝑂 can range from 0 to 1, and describe the fraction of receptors in the closed and open states, respectively. - The synaptic conductance is: $g_{syn}(t)=\bar{g}_{max}g(t)$ ### AMPA/GABA$_A$ synapse model $$ \begin{aligned}\frac{ds}{dt}&amp;=\alpha[T](1-s)-\beta s\\I&amp;=\tilde{g}s(V-E)\end{aligned} $$ - 𝛼[𝑇] denotes the transition probability from state (1−𝑠) to state (𝑠) - 𝛽 represents the transition probability of the other direction - 𝐸 is a reverse potential, which can determine whether the direction of 𝐼 is inhibition or excitation. - 𝐸 = 0 𝑚𝑚𝑉𝑉 =&amp;gt; Excitatory synapse [AMPA] - 𝐸 = −80 𝑚𝑚𝑉𝑉 =&amp;gt; Inhibitory synapse [GABA A ] ### Comparison ![image-20230826111950713](/BrainPy-course-notes/master_content/Notes.assets/image-20230826111950713.png) ### NMDA synapse model ![image-20230826112027689](/BrainPy-course-notes/master_content/Notes.assets/image-20230826112027689.png) ![image-20230826112034481](/BrainPy-course-notes/master_content/Notes.assets/image-20230826112034481.png) $$ \begin{aligned} &amp;\frac{ds}{dt} =\alpha[T](1-s)-\beta s \\ &amp;I=\tilde{g}sB(V)(V-E) \\ &amp;B(V )=\frac{1}{1+\exp(-0.062V)[Mg^{2+}]_{o}/3.57} \end{aligned} $$ The magnesium block of the NMDA receptor channel is an extremely fast process compared to the other kinetics of the receptor (Jahr and Stevens 1990a, 1990b). The block can therefore be accurately modeled as an instantaneous function of voltage(Jahr and Stevens 1990b). where $[Mg^{2+}]$ is the external magnesium concentration (1 to 2mM inphysiological conditions) ## Programming of biophysical synapse models ### AMPA synapse model ```python class AMPA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, E=0.): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.AMPA.desc(pre.num, alpha=0.98, beta=0.18, T=0.5, T_dur=0.5), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.COBA(E=E), post=post, ) ``` ```python class SimpleNet(bp.DynSysGroup): def __init__(self, syn_cls): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = syn_cls(self.pre, self.post, delay=None, prob=1., g_max=1.) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` ```python def run_a_net(net, duration=100): indices = np.arange(int(duration/bm.get_dt())) # duration ms conductances, currents, potentials = bm.for_loop(net.step_run, indices, progress_bar=True) ts = indices * bm.get_dt() # --- similar to: # runner = bp.DSRunner(net) # conductances, currents, potentials = runner.run(100.) fig, gs = bp.visualize.get_figure(1, 3, 3.5, 4) fig.add_subplot(gs[0, 0]) plt.plot(ts, conductances) plt.title(&apos;Syn conductance&apos;) fig.add_subplot(gs[0, 1]) plt.plot(ts, currents) plt.title(&apos;Syn current&apos;) fig.add_subplot(gs[0, 2]) plt.plot(ts, potentials) plt.title(&apos;Post V&apos;) plt.show() ``` ### $\text{GABA}_A$ synapse model ```python class GABAa(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, E=-80.): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.GABAa.desc(pre.num, alpha=0.53, beta=0.18, T=1.0, T_dur=1.0), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.COBA(E=E), post=post, ) ``` ```python run_a_net(SimpleNet(syn_cls=GABAa)) ``` ### NMDA synapse model ```python class NMDA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, E=0.0): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.NMDA.desc(pre.num, a=0.5, tau_decay=100., tau_rise=2.), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.MgBlock(E=E), post=post, ) ``` ```python run_a_net(SimpleNet(NMDA)) ``` ### Kinetic synapse models are more realistic ```python class SimpleNet5(bp.DynSysGroup): def __init__(self, freqs=10.): super().__init__() self.pre = bp.dyn.PoissonGroup(1, freqs=freqs) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = NMDA(self.pre, self.post, delay=None, prob=1., g_max=1., E=0.) def update(self): self.pre() self.syn() self.post() # monitor the following variables return self.syn.proj.refs[&apos;syn&apos;].g, self.post.V ``` ```python def compare_freqs(freqs): fig, _ = bp.visualize.get_figure(1, 1, 4.5, 6.) for freq in freqs: net = SimpleNet5(freqs=freq) indices = np.arange(1000) # 100 ms conductances, potentials = bm.for_loop(net.step_run, indices, progress_bar=True) ts = indices * bm.get_dt() plt.plot(ts, conductances, label=f&apos;{freq} Hz&apos;) plt.legend() plt.ylabel(&apos;g&apos;) plt.show() ``` ```python compare_freqs([10., 100., 1000., 10000.]) ``` ### How to customize a synapse #### Preparations `ProjAlignPostMg2` and `ProjAlignPreMg2` #### Exponential Model ```python class Exponen(bp.dyn.SynDyn, bp.mixin.AlignPost): def __init__(self, size, tau): super().__init__(size) # parameters self.tau = tau # variables self.g = bm.Variable(bm.zeros(self.num)) # integral self.integral = bp.odeint(lambda g, t: -g/tau, method=&apos;exp_auto&apos;) def update(self, pre_spike=None): self.g.value = self.integral(g=self.g.value, t=bp.share[&apos;t&apos;], dt=bp.share[&apos;dt&apos;]) if pre_spike is not None: self.add_current(pre_spike) return self.g.value def add_current(self, x): # specical for bp.mixin.AlignPost self.g += x def return_info(self): return self.g ``` #### AMPA Model ```python class AMPA(bp.dyn.SynDyn): def __init__(self, size, alpha= 0.98, beta=0.18, T=0.5, T_dur=0.5): super().__init__(size=size) # parameters self.alpha = alpha self.beta = beta self.T = T self.T_duration = T_dur # functions self.integral = bp.odeint(method=&apos;exp_auto&apos;, f=self.dg) # variables self.g = bm.Variable(bm.zeros(self.num)) self.spike_arrival_time = bm.Variable(bm.ones(self.num) * -1e7) def dg(self, g, t, TT): return self.alpha * TT * (1 - g) - self.beta * g def update(self, pre_spike): self.spike_arrival_time.value = bm.where(pre_spike, bp.share[&apos;t&apos;], self.spike_arrival_time) TT = ((bp.share[&apos;t&apos;] - self.spike_arrival_time) 做时间平均 STP based on firing rate $$ \begin{gathered} \frac{du(t)}{dt}=\frac{-u(t)}{\tau_{f}}+U_{sE}(1-u^{-})\delta\big(t-t_{sp}\big), \\ \frac{dx(t)}{dt}=\frac{1-x(t)}{\tau_{d}}-u^{+}x^{-}\delta\big(t-t_{sp}\big), \\ \frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{s}}+Au^{+}x^{-}\delta\big(t-t_{sp}\big), \\ u^{+}=\lim_{t-t_{sp\rightarrow0^{+}}}u(t), \end{gathered} $$ ![image-20230826155016657](/BrainPy-course-notes/master_content/Notes.assets/image-20230826155016657.png) 丢掉时间变化的具体细节，抓住了重要趋势 ### Theoretical analysis of the rate model Suppose the pre-synaptic firing rate keeps as constant, we can calculate the stationary response $$ u_{st}=\frac{U_{SE}R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}},\quad u_{st}^{+}=U_{SE}\frac{1+R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}},\quad x_{st}=\frac{1}{1+u_{st}^{+}\tau_{d}R_{0}}, $$ $$ EPSC_{st}=Au_{st}^{+}x_{st}=A\frac{u_{st}^{+}}{1+u_{st}^{+}\tau_{d}R_{0}},\quad PSV_{st}\propto g_{st}=\tau_{s}Au_{st}^{+}x_{st}R_{0}=A\frac{u_{st}^{+}R_{0}}{1+u_{st}^{+}\tau_{d}R_{0}}, $$ ![image-20230826155234134](/BrainPy-course-notes/master_content/Notes.assets/image-20230826155234134.png) ### Frequency-dependent Gain control of spike information $$ \begin{gathered} u_{st}^{+}=U_{SE}\frac{1+R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}}, \\ x_{st}=\frac{1}{1+u_{st}^{+}\tau_{d}R_{0}}, \\ EPSC_{st}=Au_{st}^{+}x_{st}=A\frac{u_{st}^{+}}{1+u_{st}^{+}\tau_{d}R_{0}}, \end{gathered} $$ Peak frequency: $\theta\sim\frac{1}{\sqrt{U\tau_{f}\tau_{d}}}$ ### Simulation of Frequency-dependent Gain control ![image-20230826155715445](/BrainPy-course-notes/master_content/Notes.assets/image-20230826155715445.png) ## Effects on network dynamics ### STP modeling Working memory ![image-20230826160102332](/BrainPy-course-notes/master_content/Notes.assets/image-20230826160102332.png) ![image-20230826160113456](/BrainPy-course-notes/master_content/Notes.assets/image-20230826160113456.png) # E-I Balanced Neural Network ## Irregular Spiking of Neurons ### Signal process of single neuron External Stimulus -&amp;gt; Single neuron model $$ \begin{aligned}\tau&amp;\frac{\mathrm{d}V}{\mathrm{d}t}=-(V-V_\text{rest })+RI(t)\\\\&amp;\text{if}V&amp;gt;V_\text{th},\quad V\leftarrow V_\text{reset }\text{last}t_\text{ref}\end{aligned} $$ -&amp;gt; ... -&amp;gt; Perception or action 真正的神经元并不是LIF model的输出 ![image-20230827100647851](/BrainPy-course-notes/master_content/Notes.assets/image-20230827100647851.png) Simulation ![image-20230827100706310](/BrainPy-course-notes/master_content/Notes.assets/image-20230827100706310.png) Neuron recorded in vivo ### Irregular Spiking of Neurons ![image-20230827092807270](/BrainPy-course-notes/master_content/Notes.assets/image-20230827092807270.png) #### Statistical Description of Spikes 用以下的变量来进行统计描述 - Firing Rate Rate = average over time(single neuron, single run) Spike count $v=\frac{n_{sp}}{T}$ - ISI(Interspike interval distributions) average ISI $\overline{\Delta t}=\frac{1}{n_{sp}-1}\sum_{i=1}^{n_{sp}-1}\Delta t_{i}$ standard deviation ISI: $\sigma_{\Delta t}^{2}=\frac{1}{n_{sp}-1}\sum_{i=1}^{n_{sp}-1}(\Delta t_{i}-\overline{\Delta t})^{2}$ - $C_V$(Coefficient of variation, Fano factor) **窄还是宽的分布** 信息表征有多强的不稳定性 $C_{V}=\sigma_{\Delta t}^{2}/\overline{\Delta t}$ #### Poisson Process In probability theory and statistics, the Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known **constant mean rate** and **independently** of the time since the last event。 $$ \begin{aligned} &amp;P(X=k\mathrm{~events~in~interval~}t)=e^{-rt}\frac{(rt)^{k}}{k!} \\ &amp;\mathrm{mean:}\quad\overline{X}=rt \\ &amp;\mathrm{variance}:\quad\sigma^{2}=rt\\ &amp;\mathrm{Fano factor:}\quad\frac{\sigma^{2}}{X}=1 \end{aligned} $$ Fano factor -&amp;gt; noise-to-signal ratio #### Irregular Spiking of Neurons LIF在单个神经元的情况下是基本没有太大问题的，在整个网络中会受网络信息调控 ![image-20230827093614607](/BrainPy-course-notes/master_content/Notes.assets/image-20230827093614607.png) #### Why Irregular? - 不完全是input影响的 - 不能简单来衡量 On average, a cortical neuron receives inputs from 1000~10000 connected neurons. -&amp;gt; averaged noise ~ 0 ## E-I Balanced Network $$ \begin{gathered} \tau\frac{du_{i}^{E}}{dt}=-u_{i}^{E}+\sum_{j=1}^{K_{E}}J_{EE}r_{j}^{E}+\sum_{j=1}^{K_{I}}J_{EI}r_{j}^{I}+I_{i}^{E} \\ \tau\frac{du_{i}^{I}}{dt}=-u_{i}^{I}+\sum_{j=1}^{K_{I}}J_{II}r_{j}^{I}+\sum_{j=1}^{K_{E}}J_{IE}r_{j}^{E}+I_{i}^{I} \end{gathered} $$ ![image-20230827093708220](/BrainPy-course-notes/master_content/Notes.assets/image-20230827093708220.png) Sparse &amp; random connections:$1\ll K_{\mathrm{E}},K_{1}\ll N_{\mathrm{E}},N_{\mathrm{I}}$ . Neurons fire largely independently to each other. $$ \begin{gathered} \text{Single neuron fires irregularly } r_j^E, r_j^{\prime} \text{with mean rate } \mu \text{and variance } \sigma^2.\\ \text{The mean of recurrent input received by E neuron:} \\ \sim K_{E}J_{EE}\mu-K_{I}J_{EI}\mu \\ \text{The variance of recurrent input received by E neuron:} \\ \sim K_{E}(J_{EE})^{2}\sigma^{2}+K_{I}(J_{EI})^{2}\sigma^{2} \\ \begin{gathered} \\ \text{The balanced condition:} \\ K_{E}J_{EE}-K_{l}J_{El}{\sim}0(1) \\ J_{EE}=\frac{1}{\sqrt{K_{E}}},J_{EI}=\frac{1}{\sqrt{K_{I}}},K_{E}(J_{EE})^{2}\sigma^{2}+K_{I}(J_{EI})^{2}\sigma^{2}\sim O(1) \end{gathered} \end{gathered} $$ $$ \begin{aligned}\frac{I_E}{I_I}&amp;&amp;gt;\frac{J_E}{J_I}&amp;&amp;gt;1\\\\J_E&amp;&amp;gt;1\\\\\text{r not too big}\end{aligned} $$ $$ \overline{I_a}=\overline{F_a}+\overline{R_a}=\sqrt{N}(f_a\mu_0+w_{aE}r_E+w_{aI}r_I),\quad a=E,I,\\ \begin{gathered} w_{ab}~=~p_{ab}j_{ab}q_{b} \\ J_{ij}^{ab}~=~j_{ab}/\sqrt{N}; \\ \frac{f_{E}}{f_{I}}&amp;gt;\frac{w_{EI}}{w_{II}}&amp;gt;\frac{w_{EE}}{w_{IE}}. \end{gathered} $$ ## BrainPy Simulation ### Simulation LIF neuron 4000 (E/I=4/1, P=0.02) 𝜏 = 20 ms 𝑉𝑟𝑒𝑠𝑡 = -60 mV Spiking threshold: -50 mV Refractory period: 5 ms $$ \begin{gathered} \tau\frac{dV}{dt}=(V_{\mathrm{rest}}-V)+I \\ I=g_{exc}(E_{exc}-V)+g_{inh}(E_{inh}-V)+I_{\mathrm{ext}} \end{gathered} \ \ \ \ \ \ \begin{aligned}\tau_{exc}&amp;\frac{dg_{exc}}{dt}=-g_{exc}\\\tau_{inh}&amp;\frac{dg_{inh}}{dt}=-g_{inh}\end{aligned} $$ $$ \begin{array}{l}E_\mathrm{exc}=0\text{mV}\mathrm{and}E_\mathrm{inh}=-80\text{mV},I_\mathrm{ext}=20.\\\tau_\mathrm{exc}=5\text{ ms},\tau_\mathrm{inh}=10\text{ ms},\Delta g_\mathrm{exc}=0.6\text{ and}\Delta g_\mathrm{inh}=6.7.\end{array} $$ ![image-20230827094502860](/BrainPy-course-notes/master_content/Notes.assets/image-20230827094502860.png) ### Synaptic Computation ```python # 基于 align post Exponential synaptic computation class Exponential(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E, label=None): super().__init__() self.pron = bp.dyn.ProjAlignPost2( pre=pre, delay=delay, comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), # 随机连接 syn=bp.dyn.Expon(size=post.num, tau=tau), # Exponential synapse out=bp.dyn.COBA(E=E), # COBA network post=post, out_label=label ) ``` ### E-I Balanced Network ```python # 构建 E-I Balanced Network class EINet(bp.DynamicalSystem): def __init__(self, ne=3200, ni=800): super().__init__() # bp.neurons.LIF() self.E = bp.dyn.LifRef(ne, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Normal(-55., 2.)) self.I = bp.dyn.LifRef(ni, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Normal(-55., 2.)) #### E2E, E2I, I2E, I2I Exponential synaptic computation # delay=0, prob=0.02, g_max_E=0.6, g_max_I=6.7, tau_E=5, tau_I=10, # reversal potentials E_E=0, E_E=-80, label=EE,EI,IE,II self.E2E = Exponential(self.E, self.E, 0., 0.02, 0.6, 5., 0., &apos;EE&apos;) self.E2I = Exponential(self.E, self.I, 0., 0.02, 0.6, 5., 0., &apos;EI&apos;) self.I2E = Exponential(self.I, self.E, 0., 0.02, 6.7, 5., -80., &apos;IE&apos;) self.I2I = Exponential(self.I, self.I, 0., 0.02, 6.7, 5., -80., &apos;II&apos;) ``` ```python def update(self, inp=0.): # 更新突触传入电流 self.E2E() self.E2I() self.I2E() self.I2I() # 更新神经元群体 self.E(inp) self.I(inp) # 记录需要 monitor的变量 E_E_inp = self.E.sum_inputs(self.E.V, label=&apos;EE&apos;) #E2E的输入 I_E_inp = self.E.sum_inputs(self.E.V, label=&apos;IE&apos;) # I2E的输入 return self.E.spike, self.I.spike, E_E_inp, I_E_inp ``` ![image-20230827110737553](/BrainPy-course-notes/master_content/Notes.assets/image-20230827110737553.png) ![image-20230827110746410](/BrainPy-course-notes/master_content/Notes.assets/image-20230827110746410.png) ## Properties of E-I Balanced Network - Linear encoding External input strength is “linearly” encoded by the mean firing rate of the neural population - Fast Response The network responds rapidly to abrupt changes of the input ### Noise speeds up computation 快速相应的原理，均匀分布在阈值下面的空间 - A neural ensemble jointly encodes stimulus information; - Noise randomizes the distribution of neuronal membrane potentials; - Those neurons (red circle) whose potentials are close to the threshold will fire rapidly; - If the noisy environment is proper, even for a small input, a certain number of neurons will fire instantly to report the presence of a stimulus. ![image-20230827113451626](/BrainPy-course-notes/master_content/Notes.assets/image-20230827113451626.png) # Continuous Attractor Neural Network ## Attractor Models ### The concept of attractor dynamics Different types of attractors: Point attractors, Line attractors, Ring attractors, Plane attractors, Cyclic attractors, Chaotic attractors ![image-20230827140250173](/BrainPy-course-notes/master_content/Notes.assets/image-20230827140250173.png) 稳态，能量梯度吸引到attractor ### Discrete attractor Network Model: Hopfield Model $S_i=\pm1$: the neuronal state $W_{ij}$ : the neuronal connection The network dynamics: $$ S_{i}=\mathrm{sign}\bigg(\sum_{j}w_{ij}S_{j}-\theta\bigg),\quad\mathrm{sign}(x)=1,\mathrm{for}x&amp;gt;0;-1,\mathrm{otherwise} $$ Updating rule: synchronous or asynchronous Consider the network stores $p$ pattern, $\xi_{i}^{\mu},\mathrm{for}\mu=1,\ldots p;i=1,\ldots N$ Setting $w_{ij}=\frac{1}{N}\sum_{\mu=1}^{p}\xi_{i}^{\mu}\xi_{j}^{\mu}$ ![image-20230827140827784](/BrainPy-course-notes/master_content/Notes.assets/image-20230827140827784.png) #### Energy space of Hopfield network $$ \begin{aligned} &amp;\text{Energy function: }E=-\frac{1}{2}\sum_{i,j}w_{ij}S_{i}S_{j}+\theta\sum_{i}S_{i} \\ &amp;\mathrm{Consider}S_{i}\mathrm{~is~updated},S_{i}(t+1)=sign[\sum_{j}w_{ij}S_{j}(t)-\theta] \\ &amp;\Delta E=E(t+1)-E(t)\\ &amp;=-[S_{i}(t+1)-S_{i}(t)]\sum_{j}w_{ij}S_{j}(t)+\theta\left[S_{i}(t+1)-S_{i}(t)\right] \\ &amp;=-[S_{i}(t+1)-S_{i}(t)][\sum_{j}w_{ij}S_{j}(t)-\theta] \\ &amp;\leq0 \end{aligned} $$ 同样激活同样pattern的神经元，~吸引子 #### Auto-associative memory in Hopfield Network A partial/noisy input can retrieve the related memory pattern ![image-20230827141253326](/BrainPy-course-notes/master_content/Notes.assets/image-20230827141253326.png) #### Persistent activity in working memory After the removal of external input, the neurons in the network encoding the stimulus continue to fire persistently. ![image-20230827141421796](/BrainPy-course-notes/master_content/Notes.assets/image-20230827141421796.png) ## Continuous Attractor Neural Network ### Neural coding #### Low-dimensional continuous feature ![image-20230827142520189](/BrainPy-course-notes/master_content/Notes.assets/image-20230827142520189.png) #### Continuous Attractor neural network ![image-20230827142606695](/BrainPy-course-notes/master_content/Notes.assets/image-20230827142606695.png) ### CANN: A rate-based recurrent circuit model $$ \begin{aligned}\tau\frac{\partial U(x,t)}{\partial t}&amp;=-U(x,t)+\rho\int f(x,x&apos;)r(x&apos;,t)dx&apos;+l^{ext}(1)\\r(x,t)&amp;=\frac{U^2(x,t)}{1+k\rho\int U^2(x,t)dx}\quad(2)\\J(x,x&apos;)&amp;=\frac{J_0}{\sqrt{2\pi}a}\exp\left[-\frac{(x-x&apos;)^2}{2a^2}\right](3)\end{aligned} $$ r频率，J强度，U decay #### A Continuous family of attractor states 做平移的改变，变化会被保留，line attractor，受到编码连续刺激 ![image-20230827143707784](/BrainPy-course-notes/master_content/Notes.assets/image-20230827143707784.png) #### Stability analysis derive continuous attractor dynamics 只需要看在原始状态加入一个小量项，再代入回 Consider small fluctuations around a stationary state at z: Projecting $\delta U$ on the $i$th right eigenvector of $F(\delta U)_i(t)=(\delta U)_i(0)e^{-(1-\lambda _i)t/\tau}$ Two cases: - If $\lambda _i ## Computation with CANN ### Persistent activity for working memory When the global inhibition is not too strong, the network spontaneously hold bump activity: $$ k\frac{\tau}{\tau _v}$, Travelling wave ![image-20230827150543244](/BrainPy-course-notes/master_content/Notes.assets/image-20230827150543244.png) #### Levy flights vs. Brownian motion ![image-20230827150851309](/BrainPy-course-notes/master_content/Notes.assets/image-20230827150851309.png) #### Lévy flights in ecology and human cogniDve behaviors 生物学大多运动服从levy flights ### Noisy adaptation generates Levy flight in CANN ![image-20230827151343126](/BrainPy-course-notes/master_content/Notes.assets/image-20230827151343126.png) ### Time Delay in Neural Signal Transmission ![image-20230827151622032](/BrainPy-course-notes/master_content/Notes.assets/image-20230827151622032.png) ### Anticipatory Head Direction Signals in Anterior Thalamus 有预测策略，实现抵消信息传递的delay CANN加入负反馈机制是可以实现预测的 ![image-20230827152731895](/BrainPy-course-notes/master_content/Notes.assets/image-20230827152731895.png) ### CANN with STP $$ \begin{gathered} \tau{\frac{\mathrm{d}U(x,t)}{\mathrm{d}t}} {\cal O}=-U(x,t)+\rho\int g^{+}(x)h(x^{\prime},t)J(x,x^{\prime})r(x^{\prime},t)dx^{\prime}+I^{ext}(x,t)(1) \\ \frac{dg(x,t)}{dt}=-\frac{g(x,t)}{\tau_{f}}+G(1-g^{-}(x))r(x^{\prime},t)\quad(2) \\ \frac{dh(x,t)}{dt}=\frac{1-h(x,t)}{\tau_{d}}-g^{+}(x)h(x,t)r(x^{\prime},t)\quad(3) \\ r(x,t)={\frac{U^{2}(x,t)}{1+k\rho\int U^{2}(x,t)dx}}\quad(4) \end{gathered} $$ ## Programming in BrainPy ### Customize a ring CANN in brainpy In simulations, we can not simulate a CANN encoding features ranging $(-\inf, \inf)$. Instead, we simulate a ring attractor network which encodes features ranging $(-\pi, \pi)$. Note that the distance on a ring should be: $$ dist_{ring}(x,x&apos;) = min(|x-x&apos;|,2\pi-|x-x&apos;|) $$ ![Image Name](https://cdn.kesci.com/upload/s01apgi89t.png?imageView2/0/w/320/h/320) ```python class CANN1D(bp.NeuGroupNS): def __init__(self, num, tau=1., k=8.1, a=0.5, A=10., J0=4., z_min=-bm.pi, z_max=bm.pi, **kwargs): super(CANN1D, self).__init__(size=num, **kwargs) # 初始化参数 self.tau = tau self.k = k self.a = a self.A = A self.J0 = J0 # 初始化特征空间相关参数 self.z_min = z_min self.z_max = z_max self.z_range = z_max - z_min self.x = bm.linspace(z_min, z_max, num) self.rho = num / self.z_range self.dx = self.z_range / num # 初始化变量 self.u = bm.Variable(bm.zeros(num)) self.input = bm.Variable(bm.zeros(num)) self.conn_mat = self.make_conn(self.x) # 连接矩阵 # 定义积分函数 self.integral = bp.odeint(self.derivative) # 微分方程 @property def derivative(self): du = lambda u, t, Irec, Iext: (-u + Irec + Iext) / self.tau return du # 将距离转换到[-z_range/2, z_range/2)之间 def dist(self, d): d = bm.remainder(d, self.z_range) d = bm.where(d &amp;gt; 0.5 * self.z_range, d - self.z_range, d) return d # 计算连接矩阵 def make_conn(self, x): assert bm.ndim(x) == 1 d = self.dist(x - x[:, None]) # 距离矩阵 Jxx = self.J0 * bm.exp( -0.5 * bm.square(d / self.a)) / (bm.sqrt(2 * bm.pi) * self.a) return Jxx # 获取各个神经元到pos处神经元的输入 def get_stimulus_by_pos(self, pos): return self.A * bm.exp(-0.25 * bm.square(self.dist(self.x - pos) / self.a)) def update(self, x=None): _t = bp.share[&apos;t&apos;] u2 = bm.square(self.u) r = u2 / (1.0 + self.k * bm.sum(u2)) Irec = bm.dot(self.conn_mat, r) self.u[:] = self.integral(self.u, _t,Irec, self.input) self.input[:] = 0. # 重置外部电流 ``` ### Simulate the persistent activity of CANN after the removal of external input ```python def Persistent_Activity(k=0.1,J0=1.): # 生成CANN cann = CANN1D(num=512, k=k,J0=J0) # 生成外部刺激，从第2到12ms，持续10ms dur1, dur2, dur3 = 2., 10., 10. I1 = cann.get_stimulus_by_pos(0.) Iext, duration = bp.inputs.section_input(values=[0., I1, 0.], durations=[dur1, dur2, dur3], return_length=True) noise_level = 0.1 noise = bm.random.normal(0., noise_level, (int(duration / bm.get_dt()), len(I1))) Iext += noise # 运行数值模拟 runner = bp.DSRunner(cann, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;]) runner.run(duration) # 可视化 def plot_response(t): fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann.x, I, label=&apos;Iext&apos;) ax.plot(cann.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() # plt.savefig(f&apos;CANN_t={t}.pdf&apos;, transparent=True, dpi=500) plot_response(t=10.) plot_response(t=20.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=1, frame_delay=40, show=True, ) plt.show() Persistent_Activity(k=0.1) ``` ### Simulate the tracking behavior of CANN ```python def smooth_tracking(): cann = CANN1D(num=512, k=8.1) # 定义随时间变化的外部刺激 v_ext = 1e-3 dur1, dur2, dur3 = 10., 10., 20 num1 = int(dur1 / bm.get_dt()) num2 = int(dur2 / bm.get_dt()) num3 = int(dur3 / bm.get_dt()) position = bm.zeros(num1 + num2 + num3) position[num1: num1 + num2] = bm.linspace(0., 1.5 * bm.pi, num2) position[num1 + num2: ] = 1.5 * bm.pi position = position.reshape((-1, 1)) Iext = cann.get_stimulus_by_pos(position) # 运行模拟 runner = bp.DSRunner(cann, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;]) runner.run(dur1 + dur2 + dur3) # 可视化 def plot_response(t, extra_fun=None): fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann.x, I, label=&apos;Iext&apos;) ax.plot(cann.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() if extra_fun: extra_fun() # plt.savefig(f&apos;CANN_tracking_t={t}.pdf&apos;, transparent=True, dpi=500) plot_response(t=10.) def f(): plt.annotate(&apos;&apos;, xy=(1.5, 10), xytext=(0.5, 10), arrowprops=dict(arrowstyle=&quot;-&amp;gt;&quot;)) plot_response(t=15., extra_fun=f) def f(): plt.annotate(&apos;&apos;, xy=(-2, 10), xytext=(-3, 10), arrowprops=dict(arrowstyle=&quot;-&amp;gt;&quot;)) plot_response(t=20., extra_fun=f) plot_response(t=30.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=5, frame_delay=50, show=True, ) plt.show() smooth_tracking() ``` ### Customize a CANN with SFASimulate the spontaneous traveling wave ```python class CANN1D_SFA(bp.NeuGroupNS): def __init__(self, num, m = 0.1, tau=1., tau_v=10., k=8.1, a=0.5, A=10., J0=4., z_min=-bm.pi, z_max=bm.pi, **kwargs): super(CANN1D_SFA, self).__init__(size=num, **kwargs) # 初始化参数 self.tau = tau self.tau_v = tau_v #time constant of SFA self.k = k self.a = a self.A = A self.J0 = J0 self.m = m #SFA strength # 初始化特征空间相关参数 self.z_min = z_min self.z_max = z_max self.z_range = z_max - z_min self.x = bm.linspace(z_min, z_max, num) self.rho = num / self.z_range self.dx = self.z_range / num # 初始化变量 self.u = bm.Variable(bm.zeros(num)) self.v = bm.Variable(bm.zeros(num)) #SFA current self.input = bm.Variable(bm.zeros(num)) self.conn_mat = self.make_conn(self.x) # 连接矩阵 # 定义积分函数 self.integral = bp.odeint(self.derivative) # 微分方程 @property def derivative(self): du = lambda u, t, v, Irec, Iext: (-u + Irec + Iext-v) / self.tau dv = lambda v, t, u: (-v + self.m*u) / self.tau_v return bp.JointEq([du, dv]) # 将距离转换到[-z_range/2, z_range/2)之间 def dist(self, d): d = bm.remainder(d, self.z_range) d = bm.where(d &amp;gt; 0.5 * self.z_range, d - self.z_range, d) return d # 计算连接矩阵 def make_conn(self, x): assert bm.ndim(x) == 1 d = self.dist(x - x[:, None]) # 距离矩阵 Jxx = self.J0 * bm.exp( -0.5 * bm.square(d / self.a)) / (bm.sqrt(2 * bm.pi) * self.a) return Jxx # 获取各个神经元到pos处神经元的输入 def get_stimulus_by_pos(self, pos): return self.A * bm.exp(-0.25 * bm.square(self.dist(self.x - pos) / self.a)) def update(self, x=None): u2 = bm.square(self.u) r = u2 / (1.0 + self.k * bm.sum(u2)) Irec = bm.dot(self.conn_mat, r) u, v = self.integral(self.u, self.v, bp.share[&apos;t&apos;],Irec, self.input) self.u[:] = bm.where(u&amp;gt;0,u,0) self.v[:] = v self.input[:] = 0. # 重置外部电流 ``` ### Simulate the spontaneous traveling wave ```python def traveling_wave(num=512,m=0.1,k=0.1): # 生成CANN cann_sfa = CANN1D_SFA(num=num, m=m,k=k) # 生成外部刺激 dur = 1000. noise_level = 0.1 Iext = bm.random.normal(0., noise_level, (int(dur / bm.get_dt()), num)) duration = dur # 运行数值模拟 runner = bp.DSRunner(cann_sfa, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;]) runner.run(duration) # 可视化 def plot_response(t): fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann_sfa.x, I, label=&apos;Iext&apos;) ax.plot(cann_sfa.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() # plt.savefig(f&apos;CANN_t={t}.pdf&apos;, transparent=True, dpi=500) plot_response(t=100.) plot_response(t=150.) plot_response(t=200.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann_sfa.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann_sfa.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=1, frame_delay=40, show=True, ) plt.show() traveling_wave(num=512,m=0.5,k=0.1) ``` ### Simulate the anticipative tracking ```python def anticipative_tracking(m=10,v_ext=6*1e-3): cann_sfa = CANN1D_SFA(num=512, m=m) # 定义随时间变化的外部刺激 v_ext = v_ext dur1, dur2, = 10., 1000. num1 = int(dur1 / bm.get_dt()) num2 = int(dur2 / bm.get_dt()) position = np.zeros(num1 + num2) for i in range(num2): pos = position[i+num1-1]+v_ext*bm.dt # the periodical boundary pos = np.where(pos&amp;gt;np.pi, pos-2*np.pi, pos) pos = np.where(pos 0.5 * self.z_range, d - self.z_range, d) return d # 计算连接矩阵 def make_conn(self, x): assert bm.ndim(x) == 1 d = self.dist(x - x[:, None]) # 距离矩阵 Jxx = self.J0 * bm.exp( -0.5 * bm.square(d / self.a)) / (bm.sqrt(2 * bm.pi) * self.a) return Jxx # 获取各个神经元到pos处神经元的输入 def get_stimulus_by_pos(self, pos): return self.A * bm.exp(-0.25 * bm.square(self.dist(self.x - pos) / self.a)) def update(self, x=None): u2 = bm.square(self.u) r = u2 / (1.0 + self.k * bm.sum(u2)) Irec = bm.dot(self.conn_mat, (self.g + self.G * (1 - self.g))*self.h*r) u, g, h = self.integral(u=self.u, g=self.g, h=self.h, t=bp.share[&apos;t&apos;], Irec=Irec, Iext=self.input, r=r, dt=bm.dt) self.u[:] = bm.where(u&amp;gt;0,u,0) self.g.value = g self.h.value = h self.input[:] = 0. # 重置外部电流 ``` ### Simulate traveling wave in CANN with STP ```python def traveling_wave_STP(num=512,k=0.1,J0=12.,tau_d=1000,tau_f=1.,G=0.9): # 生成CANN cann_stp = CANN1D_STP(num=num, k=k,tau_d=tau_d,tau_f=tau_f,G=G, J0=J0) # 生成外部刺激 dur = 1000. noise_level = 0.1 Iext = bm.random.normal(0., noise_level, (int(dur / bm.get_dt()), num)) duration = dur # 运行数值模拟 runner = bp.DSRunner(cann_stp, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;,&apos;g&apos;,&apos;h&apos;]) runner.run(duration) fig,ax = plt.subplots(figsize=(3,3)) u = bm.as_numpy(runner.mon.u) max_index = np.argmax(u[1000,:]) print(max_index) ax.plot(runner.mon.g[:,max_index],label=&apos;g&apos;) ax.plot(runner.mon.h[:,max_index],label=&apos;h&apos;) ax.legend() # 可视化 def plot_response(t): fig, gs = bp.visualize.get_figure(1, 1, 3, 3) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann_stp.x, I, label=&apos;Iext&apos;) ax.plot(cann_stp.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() plot_response(t=100.) plot_response(t=200.) plot_response(t=300.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann_stp.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann_stp.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=1, frame_delay=40, show=True, ) plt.show() traveling_wave_STP(G=0.5,tau_d=50) ``` # Decision-Making Network ## LIP -&amp;gt; Decision-Making ### Coherent motion task 判断随机点(大部分点)的运动朝向 ![image-20230828100425871](/BrainPy-course-notes/master_content/Notes.assets/image-20230828100425871.png) coherence影响任务的难度 0%难，100%简单 ![image-20230828100516123](/BrainPy-course-notes/master_content/Notes.assets/image-20230828100516123.png) 编码决策的响应，不是运动 ### Reaction Time vs. Fixed Duration coherence越高，反应时间越短 Fixed Duration多了Delay time ![image-20230828100658772](/BrainPy-course-notes/master_content/Notes.assets/image-20230828100658772.png) 实验设计纯粹把decision-making给提取出来 #### Effect of Difficulty coherence越大，反应时间是越短，single neuron很难做到这么短的decision-making，考虑要建模的因素 ![image-20230828101103008](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101103008.png) ![image-20230828101058059](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101058059.png) #### Response of MT Neurons 记录MT的神经元，对这种运动的朝向刺激进行编码 线性编码coherence运动强度的方向 做决策在它的下游 ![image-20230828101303674](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101303674.png) #### Response of LIP Neurons MT的下游找到LIP的神经元 爬升到一定高度再做选择 coherence与爬升的斜率也会有影响，任务越难，爬升斜率越小 ![image-20230828101609881](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101609881.png) ### Ramping-to-threshold(perfect integrator) Model $$ \begin{aligned}\frac{dR}{dt}=I_A-I_B+\text{noise},\quad R(t)&amp;=(I_A-I_B)t+\int_0^tdt\text{noise}.\\\tau_\text{network}&amp;=\infty!\end{aligned} $$ 两种选择积分求和做积累，等到阈值做决策 Accumulates information (evidence) -&amp;gt; Ramping 直接保存信息，没有特别好的生物对应 ## A Spiking Network of DM ### A cortical microcircuit model ![image-20230828103055151](Notes.assets/image-20230828103055151.png) A=Upward motion B=Downward motion 2-population excitatory neurons (integrate-and-fire neurons driven by Poisson input) Slow reverberatory excitation mediated by the NMDA receptors at recurrent synapses AMPA receptors ($\tau _{syn}=$1 - 3 ms) NMDA receptors ($\tau _{syn}=$ 50 - 100 ms). 两群神经元分别做不同的选择，与自己对方都有连接 NMDA 缓慢的信号使得有慢慢增长的ramping的过程 interneurons的backward有抑制作用 #### Coherence-Dependent Input 线性编码运动朝向的信息，coherence强度影响firing rate，一系列泊松过程，同时还有noise。 本身两种信息还是有差异 ![image-20230828104054275](/BrainPy-course-notes/master_content/Notes.assets/image-20230828104054275.png) #### Duality of this model 不同coherence的神经元响应 ![image-20230828104432061](/BrainPy-course-notes/master_content/Notes.assets/image-20230828104432061.png) 两个group会竞争，当有一个group达到20%，进入这个窗口，就会直接发放上去 Spontaneous symmetry breaking and stochastic decision making ![image-20230828104600840](/BrainPy-course-notes/master_content/Notes.assets/image-20230828104600840.png) ## Simulation of Spiking DM ### A Cortical Microcircuit Model 用两个coherence生成出来的序列 ![image-20230828110300576](/BrainPy-course-notes/master_content/Notes.assets/image-20230828110300576.png) $$ \begin{gathered}C_m\frac{dV(t)}{dt}=-g_L(V(t)-V_L)-I_{syn}(t)\\I_{syn}(t)=I_{\mathrm{ext},\mathrm{AMPA}}\left(t\right)+I_{\mathrm{rec},AMPA}(t)+I_{\mathrm{rec},NMDA}(t)+I_{\mathrm{rec},\mathrm{GABA}}(t)\end{gathered} $$ $$ \begin{gathered} I_{\mathrm{ext},\mathrm{AMPA}}\left(t\right)=g_{\mathrm{ext},\mathrm{AMPA}}\left(V(t)-V_{E}\right)s^{\mathrm{ext},\mathrm{AMPA}}\left(t\right) \\ I_{\mathrm{rec},\mathrm{AMP}\Lambda}\left(t\right)=g_{\mathrm{rec},\mathrm{AMP}\Lambda}\left(V(t)-V_{E}\right)\sum_{j=1}^{Ce}w_{j}s_{j}^{AMPA}(t) \\ I_{\mathrm{rec},\mathrm{NMDA}}\left(t\right)=\frac{g_{\mathrm{NMDA}}(V(t)-V_{E})}{\left(1+\left[\mathrm{Mg}^{2+}\right]\exp(-0.062V(t))/3.57\right)}\sum_{j=1}^{\mathrm{C_E}}w_{j}s_{j}^{\mathrm{NMDA}}\left(t\right) \\ I_\mathrm{rec,GABA}(t)=g_\mathrm{GABA}(V(t)-V_l)\sum_{j=1}^{C_1}s_j^\mathrm{GABA}(t) \end{gathered} $$ $$ w_j=\left\{\begin{matrix}w_+&amp;gt;1,\\w_-E/I conn self.I2B = AMPA(self.I, self.B, &apos;all2all&apos;, 0.5, g_I2E_GABAa, tau=5., E=-70.) self.I2A = AMPA(self.I, self.A, &apos;all2all&apos;, 0.5, g_I2E_GABAa, tau=5., E=-70.) self.I2N = AMPA(self.I, self.N, &apos;all2all&apos;, 0.5, g_I2E_GABAa, tau=5., E=-70.) self.I2I = AMPA(self.I, self.I, &apos;all2all&apos;, 0.5, g_I2I_GABAa, tau=5., E=-70.) # define external projections #### TO DO!!!! self.noise2B = AMPA(self.noise_B, self.B, &apos;one2one&apos;, None, g_ext2E_AMPA, tau=2., E=0.) self.noise2A = AMPA(self.noise_A, self.A, &apos;one2one&apos;, None, g_ext2E_AMPA, tau=2., E=0.) self.noise2N = AMPA(self.noise_N, self.N, &apos;one2one&apos;, None, g_ext2E_AMPA, tau=2., E=0.) self.noise2I = AMPA(self.noise_I, self.I, &apos;one2one&apos;, None, g_ext2I_AMPA, tau=2., E=0.) ``` ```python class Tool: def __init__(self, pre_stimulus_period=100., stimulus_period=1000., delay_period=500.): self.pre_stimulus_period = pre_stimulus_period self.stimulus_period = stimulus_period self.delay_period = delay_period self.freq_variance = 10. self.freq_interval = 50. self.total_period = pre_stimulus_period + stimulus_period + delay_period def generate_freqs(self, mean): # stimulus period n_stim = int(self.stimulus_period / self.freq_interval) n_interval = int(self.freq_interval / bm.get_dt()) freqs_stim = np.random.normal(mean, self.freq_variance, (n_stim, 1)) freqs_stim = np.tile(freqs_stim, (1, n_interval)).flatten() # pre stimulus period freqs_pre = np.zeros(int(self.pre_stimulus_period / bm.get_dt())) # post stimulus period freqs_delay = np.zeros(int(self.delay_period / bm.get_dt())) all_freqs = np.concatenate([freqs_pre, freqs_stim, freqs_delay], axis=0) return bm.asarray(all_freqs) def visualize_results(self, mon, IA_freqs, IB_freqs, t_start=0., title=None): fig, gs = bp.visualize.get_figure(4, 1, 3, 10) axes = [fig.add_subplot(gs[i, 0]) for i in range(4)] ax = axes[0] bp.visualize.raster_plot(mon[&apos;ts&apos;], mon[&apos;A.spike&apos;], markersize=1, ax=ax) if title: ax.set_title(title) ax.set_ylabel(&quot;Group A&quot;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax = axes[1] bp.visualize.raster_plot(mon[&apos;ts&apos;], mon[&apos;B.spike&apos;], markersize=1, ax=ax) ax.set_ylabel(&quot;Group B&quot;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax = axes[2] rateA = bp.measure.firing_rate(mon[&apos;A.spike&apos;], width=10.) rateB = bp.measure.firing_rate(mon[&apos;B.spike&apos;], width=10.) ax.plot(mon[&apos;ts&apos;], rateA, label=&quot;Group A&quot;) ax.plot(mon[&apos;ts&apos;], rateB, label=&quot;Group B&quot;) ax.set_ylabel(&apos;Population activity [Hz]&apos;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax.legend() ax = axes[3] ax.plot(mon[&apos;ts&apos;], IA_freqs, label=&quot;group A&quot;) ax.plot(mon[&apos;ts&apos;], IB_freqs, label=&quot;group B&quot;) ax.set_ylabel(&quot;Input activity [Hz]&quot;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax.legend() ax.set_xlabel(&quot;Time [ms]&quot;) plt.show() ``` ```python tool = Tool() net = DecisionMakingNet() mu0 = 40. coherence = 25.6 IA_freqs = tool.generate_freqs(mu0 + mu0 / 100. * coherence) IB_freqs = tool.generate_freqs(mu0 - mu0 / 100. * coherence) def give_input(): i = bp.share[&apos;i&apos;] net.IA.freqs[0] = IA_freqs[i] net.IB.freqs[0] = IB_freqs[i] runner = bp.DSRunner(net, inputs=give_input, monitors=[&apos;A.spike&apos;, &apos;B.spike&apos;]) runner.run(tool.total_period) tool.visualize_results(runner.mon, IA_freqs, IB_freqs) ``` ### Results ![image-20230828112245950](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112245950.png) #### Stochastic Decision Making ![image-20230828112253619](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112253619.png) ## A Rate Network of DM ### Reduced Model 化简到只有两群神经元，只接受外界输入信号，互相影响对方 ![image-20230828112326267](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112326267.png) Synaptic variables $$ \begin{gathered} \frac{dS_{1}}{dt} =F(x_1)\gamma(1-S_1)-S_1/\tau_s \\ \frac{dS_2}{dt} =F(x_2)\gamma(1-S_2)-S_2/\tau_s \end{gathered} $$ Input current to each population $$ \begin{gathered} x_{1} =J_{E}S_{1}+J_{I}S_{2}+I_{0}+I_{noise1}+J_{\text{ext }\mu_{1}} \\ x_{2} =J_{E}S_{2}+J_{I}S_{1}+I_{0}+I_{noise2}+J_{\mathrm{ext}}\mu_{2} \end{gathered} $$ Background input $$ I_0+I_{noise}\\ \begin{gathered} dI_{noise1} =-I_{noise1}\frac{dt}{\tau_{0}}+\sigma dW \\ dI_{noise2} =-I_{noise2}\frac{dt}{\tau_{0}}+\sigma dW \end{gathered} $$ Firing rates $$ r_i=F(x_i)=\frac{ax_i-b}{1-\exp(-d(ax_i-b))} $$ Coherence-dependent inputs $$ \begin{array}{l}\mu_1=\mu_0\big(1+c&apos;/100\big)\\\mu_2=\mu_0\big(1-c&apos;/100\big)\end{array} $$ $$ \begin{aligned}&amp;\gamma,a,b,d,J_E,J_I,J_{\mathrm{ext}},I_0,\mu_0,\tau_{\mathrm{AMPA}},\sigma_{\mathrm{noise}}\\&amp;\text{are fixed parameters.}\end{aligned} $$ ```python class DecisionMakingRateModel(bp.dyn.NeuGroup): def __init__(self, size, coherence, JE=0.2609, JI=0.0497, Jext=5.2e-4, I0=0.3255, gamma=6.41e-4, tau=100., tau_n=2., sigma_n=0.02, a=270., b=108., d=0.154, noise_freq=2400., method=&apos;exp_auto&apos;, **kwargs): super(DecisionMakingRateModel, self).__init__(size, **kwargs) # 初始化参数 self.coherence = coherence self.JE = JE self.JI = JI self.Jext = Jext self.I0 = I0 self.gamma = gamma self.tau = tau self.tau_n = tau_n self.sigma_n = sigma_n self.a = a self.b = b self.d = d # 初始化变量 self.s1 = bm.Variable(bm.zeros(self.num) + 0.15) self.s2 = bm.Variable(bm.zeros(self.num) + 0.15) self.r1 = bm.Variable(bm.zeros(self.num)) self.r2 = bm.Variable(bm.zeros(self.num)) self.mu0 = bm.Variable(bm.zeros(self.num)) self.I1_noise = bm.Variable(bm.zeros(self.num)) self.I2_noise = bm.Variable(bm.zeros(self.num)) # 噪声输入的神经元 self.noise1 = bp.dyn.PoissonGroup(self.num, freqs=noise_freq) self.noise2 = bp.dyn.PoissonGroup(self.num, freqs=noise_freq) # 定义积分函数 self.integral = bp.odeint(self.derivative, method=method) @property def derivative(self): return bp.JointEq([self.ds1, self.ds2, self.dI1noise, self.dI2noise]) def ds1(self, s1, t, s2, mu0): I1 = self.Jext * mu0 * (1. + self.coherence / 100.) x1 = self.JE * s1 - self.JI * s2 + self.I0 + I1 + self.I1_noise r1 = (self.a * x1 - self.b) / (1. - bm.exp(-self.d * (self.a * x1 - self.b))) return - s1 / self.tau + (1. - s1) * self.gamma * r1 def ds2(self, s2, t, s1, mu0): I2=self.Jext*mu0*(1.- self.coherence / 100.) x2 = self.JE * s2 - self.JI * s1 + self.I0 + I2 + self.I2_noise r2 = (self.a * x2 - self.b) / (1. - bm.exp(-self.d * (self.a * x2 - self.b))) return - s2 / self.tau + (1. - s2) * self.gamma * r2 def dI1noise(self, I1_noise, t, noise1): return (- I1_noise + noise1.spike * bm.sqrt(self.tau_n * self.sigma_n * self.sigma_n)) / self.tau_n def dI2noise(self, I2_noise, t, noise2): return (- I2_noise + noise2.spike * bm.sqrt(self.tau_n * self.sigma_n * self.sigma_n)) / self.tau_n def update(self, tdi): # 更新噪声神经元以产生新的随机发放 self.noise1.update(tdi) self.noise2.update(tdi) # 更新s1、s2、I1_noise、I2_noise integral = self.integral(self.s1, self.s2, self.I1_noise, self.I2_noise, tdi.t, mu0=self.mu0, noise1=self.noise1, noise2=self.noise2, dt=tdi.dt) self.s1.value, self.s2.value, self.I1_noise.value, self.I2_noise.value = integral # 用更新后的s1、s2计算r1、r2 I1 = self.Jext * self.mu0 * (1. + self.coherence / 100.) x1 = self.JE * self.s1 + self.JI * self.s2 + self.I0 + I1 + self.I1_noise self.r1.value = (self.a * x1 - self.b) / (1. - bm.exp(-self.d * (self.a * x1 - self.b))) I2 = self.Jext * self.mu0 * (1. - self.coherence / 100.) x2 = self.JE * self.s2 + self.JI * self.s1 + self.I0 + I2 + self.I2_noise self.r2.value = (self.a * x2 - self.b) / (1. - bm.exp(-self.d * (self.a * x2 - self.b))) # 重置外部输入 self.mu0[:] = 0. ``` ```python # 定义各个阶段的时长 pre_stimulus_period, stimulus_period, delay_period = 100., 2000., 500. # 生成模型 dmnet = DecisionMakingRateModel(1, coherence=25.6, noise_freq=2400.) # 定义电流随时间的变化 inputs, total_period = bp.inputs.constant_input([(0., pre_stimulus_period), (20., stimulus_period), (0., delay_period)]) # 运行数值模拟 runner = bp.DSRunner(dmnet, monitors=[&apos;s1&apos;, &apos;s2&apos;, &apos;r1&apos;, &apos;r2&apos;], inputs=(&apos;mu0&apos;, inputs, &apos;iter&apos;)) runner.run(total_period) # 可视化 fig, gs = plt.subplots(2, 1, figsize=(6, 6), sharex=&apos;all&apos;) gs[0].plot(runner.mon.ts, runner.mon.s1, label=&apos;s1&apos;) gs[0].plot(runner.mon.ts, runner.mon.s2, label=&apos;s2&apos;) gs[0].axvline(pre_stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[0].axvline(pre_stimulus_period + stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[0].set_ylabel(&apos;gating variable $s$&apos;) gs[0].legend() gs[1].plot(runner.mon.ts, runner.mon.r1, label=&apos;r1&apos;) gs[1].plot(runner.mon.ts, runner.mon.r2, label=&apos;r2&apos;) gs[1].axvline(pre_stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[1].axvline(pre_stimulus_period + stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[1].set_xlabel(&apos;t (ms)&apos;) gs[1].set_ylabel(&apos;firing rate $r$&apos;) gs[1].legend() plt.subplots_adjust(hspace=0.1) plt.show() ``` ### Results ![image-20230828112555018](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112555018.png) ## Phase Plane Analysis 因为只有两个variable ### Model implementation ```python @bp.odeint def int_s1(s1, t, s2, coh=0.5, mu=20.): x1 = JE * s1 + JI * s2 + Ib + JAext * mu * (1. + coh/100) r1 = (a * x1 - b) / (1. - bm.exp(-d * (a * x1 - b))) return - s1 / tau + (1. - s1) * gamma * r1 @bp.odeint def int_s2(s2, t, s1, coh=0.5, mu=20.): x2 = JE * s2 + JI * s1 + Ib + JAext * mu * (1. - coh/100) r2 = (a * x2 - b) / (1. - bm.exp(-d * (a * x2 - b))) return - s2 / tau + (1. - s2) * gamma * r2 ``` ### Without / with input ![image-20230828112709355](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112709355.png) 只受扰动影响，有input后中间变得不稳定，但如果已经选择，网络仍维持之前选择的结果 ![image-20230828112811394](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112811394.png) ### Coherence 稳定点对网络的拉伸更强 ![image-20230828113031946](/BrainPy-course-notes/master_content/Notes.assets/image-20230828113031946.png) ![image-20230828113009219](/BrainPy-course-notes/master_content/Notes.assets/image-20230828113009219.png) # Reservoir Computing 引入训练 倾向于使用RNN ![image-20230828140305956](/BrainPy-course-notes/master_content/Notes.assets/image-20230828140305956.png) Connecting different units $$ \begin{aligned} &amp;\textsf{Input to unit i from unit j:} \\ &amp;&amp;&amp;I_{j\rightarrow i}=J_{ij}r_{j}(t) \\ &amp;\textsf{Total input to unit i:} \\ &amp;&amp;&amp;I_{i}^{(tot)}=\sum_{j=1}^{N}J_{ij}r_{j}(t)+I_{i}^{(ext)} \end{aligned} $$ $$ \textsf{Activation of unit i:} \\ \tau\frac{dx_{i}}{dt}=-x_{i}+\sum_{j=1}^{N}J_{ij}\frac{\phi(x_{j})}{1}+I_{i}^{(ext)}(t) $$ 训练范式 ![image-20230828140707998](/BrainPy-course-notes/master_content/Notes.assets/image-20230828140707998.png) ## Echo state machine ### Echo state machine 类似人工神经网络RNN，可以处理temporal信息 ![image-20230828140937455](/BrainPy-course-notes/master_content/Notes.assets/image-20230828140937455.png) $$ \begin{aligned} &amp;\mathbf{x}(n+1) =f(\mathbf{W}^{\mathrm{in}}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n)+\mathbf{W}^{\mathrm{back}}\mathbf{y}(n)) \\ &amp;\mathbf{y}(n+1) =\mathbf{W}^{\mathrm{out}}(\mathbf{u}(n+1),\mathbf{x}(n+1),\mathbf{y}(n)) \end{aligned} $$ For an RNN, the state of its internal neurons reflects the historical information of the external inputs. 反映的echo的历史信息，唯一依赖历史信息 Assuming that the updates of the network are discrete, the external input at the 𝑛th moment is u(𝑛) and the neuron state is x(𝑛), then x(𝑛) should be determined by u(𝑛), u(𝑛 - 1), ... uniquely determined. At this point, x(𝑛) can be regarded as an &quot;echo&quot; of the historical input signals. 不需要训练connection ### Echo state machine with leaky integrator 有一个leaky项，引入decay ### $$ \begin{aligned}\hat{h}(n)=\tanh(W^{in}x(n)+W^{rec}h(n-1)+W^{fb}y(n-1)+b^{rec})\\h(n)=(1-\alpha)x(n-1)+\alpha\hat{h}(n)\end{aligned} $$ where $h(n)$ is a vector of reservoir neuron activations, $W^{in}$ and $W^{rec}$ are the input and recurrent weight matrices respectively, and $\alpha\in(0,1]$ is the leaking rate. The model is also sometimes used without the leaky integration, which is a special case of $\alpha=1$ The linear readout layer is defined as $$ y(n)=W^{out}h(n)+b^{out} $$ where $y(n)$ is network output, $W^{out}$ the output weight matrix, and $b^out$ is the output bias ## Constraints of echo state machine ### Echo state property #### Theorem 1 For the echo state network defined above, the network will be echoey as long as the maximum singular value $\sigma_{max} Provement: &amp;gt; $$ &amp;gt; \begin{aligned} &amp;gt; d(\mathbf{x}(n+1),\mathbf{x}^{\prime}(n+1))&amp; =d(T(\mathbf{x}(n),\mathbf{u}(n+1)),T(\mathbf{x}&apos;(n),\mathbf{u}(n+1))) \\ &amp;gt; &amp;=d(f(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n)),f(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}&apos;(n))) \\ &amp;gt; &amp;\leq d(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n),\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}^{\prime}(n)) \\ &amp;gt; &amp;=d(\mathbf{W}\mathbf{x}(n),\mathbf{W}\mathbf{x}&apos;(n)) \\ &amp;gt; &amp;=||\mathbf{W}(\mathbf{x}(n)-\mathbf{x}^{\prime}(n))|| \\ &amp;gt; &amp;\leq\sigma_{\max}(\mathbf{W})d(\mathbf{x}(n),\mathbf{x}&apos;(n)) &amp;gt; \end{aligned} &amp;gt; $$ #### Theorem 2 For the echo state network defined above, as long as the spectral radius $|\lambda_{max}|$ of the recurrent connection matrix W &amp;gt; 1, then the network must not be echogenic. The spectral radius of the matrix is the absolute value of the largest eigenvalue $\lambda_{max}$. #### How to initialize Using these two theorems, how should we initialize W so that the network has an echo property? If we scale W, i.e., multiply it by a scaling factor $\alpha$, then $\sigma_{max}\alpha_{max}\text{,the network will not have the echo state.}\\\bullet&amp;\text{if}\alpha_{min}\le\alpha\le\alpha_{max}\text{,the network may have the echo state.}\end{array} $$ **$\alpha$设的略小于1** ![image-20230828142516052](/BrainPy-course-notes/master_content/Notes.assets/image-20230828142516052.png) ### Global parameters of reservoir 这些超参会影响reservoir network的性能，需要手动调参，很难自动去调整 - The size $N_x$ - General wisdom: the bigger the reservoir, the better the obtainable performance - Select global parameters with smaller reservoirs, then scale to bigger ones. - Sparsity - Distribution of nonzero elements: - Normal distribution - Uniform distribution - The width of the distributions does not matter - spectral radius of $W$ - scales the width of the distribution of its nonzero elements - determines how fast the influence of an input dies out in a reservoir with time, and how stable the reservoir activations are - The spectral radius should be larger in tasks requiring longer memory of the input - Scaling(-s) to $W^{in}$: - For uniform distributed $W^{in}$, $\alpha$ in the range of the interval $[-a;a]$. - For normal distributed $W^{in}$, one may take the standard deviation as a scaling measure. The leaking rate $\alpha$ ## Training of echo state machine ### Offline learning The advantage of the echo state network is that it does not train recurrent connections within the reservoir, but only the readout layer from the reservoir to the output. 线性层的优化方法是简单的 **Ridge regression** $$ \begin{aligned}\epsilon_{\mathrm{train}}(n)&amp;=\mathbf{y}(n)-\mathbf{\hat{y}}(n) \\&amp;=\mathbf{y}(n)-\mathbf{W}^{\mathrm{out}}\mathbf{x}(n) \\&amp;L_{\mathrm{ridge}}=\frac{1}{N}\sum_{i=1}^{N}\epsilon_{\mathrm{train}}^{2}(i)+\alpha||\mathbf{W^{out}}||^{2} \\\\W^{out}&amp;=Y^{target}X^T(XX^T+\beta I)^{-1}\end{aligned} $$ ```python trainer = bp.OfflineTrainer(model, fit_method=bp.algorithms.RidgeRegression(1e-7), dt=dt) ``` ### Online learning 来一个sample，进行一次training，对训练资源可以避免瓶颈 The training data is passed to the trainer in a certain sequence (e.g., time series), and the trainer continuously learns based on the new incoming data. **Recursive Least Squares (RLS) algorithm** $$ E(\mathbf{y},\mathbf{y}^\mathrm{target},n)=\frac{1}{N_\mathrm{y}}\sum_{i=1}^{N_\mathrm{y}}\sum_{j=1}^{n}\lambda^{n-j}\left(y_i(j)-y_i^\mathrm{target}(j)\right)^2, $$ ```python trainer = bp.OnlineTrainer(model, fit_method=bp.algorithms.RLS(), dt=dt) ``` ### Dataset 给定time sequence，可以让网络去预测regression ![image-20230828144309742](/BrainPy-course-notes/master_content/Notes.assets/image-20230828144309742.png) 用到BrainPy集成的`Neuromorphic and Cognitive Datasets` ### Other tasks `MNIST dataset` or `Fashion MNIST` Two aspect: - Running time - Memory Usage ## Echo state machine programming ```python import brainpy as bp import brainpy.math as bm import brainpy_datasets as bd import matplotlib.pyplot as plt # enable x64 computation bm.set_environment(x64=True, mode=bm.batching_mode) bm.set_platform(&apos;cpu&apos;) ``` ### Dataset ```python def plot_mackey_glass_series(ts, x_series, x_tau_series, num_sample): plt.figure(figsize=(13, 5)) plt.subplot(121) plt.title(f&quot;Timeserie - {num_sample} timesteps&quot;) plt.plot(ts[:num_sample], x_series[:num_sample], lw=2, color=&quot;lightgrey&quot;, zorder=0) plt.scatter(ts[:num_sample], x_series[:num_sample], c=ts[:num_sample], cmap=&quot;viridis&quot;, s=6) plt.xlabel(&quot;$t$&quot;) plt.ylabel(&quot;$P(t)$&quot;) ax = plt.subplot(122) ax.margins(0.05) plt.title(f&quot;Phase diagram: $P(t) = f(P(t-\\tau))$&quot;) plt.plot(x_tau_series[: num_sample], x_series[: num_sample], lw=1, color=&quot;lightgrey&quot;, zorder=0) plt.scatter(x_tau_series[:num_sample], x_series[: num_sample], lw=0.5, c=ts[:num_sample], cmap=&quot;viridis&quot;, s=6) plt.xlabel(&quot;$P(t-\\tau)$&quot;) plt.ylabel(&quot;$P(t)$&quot;) cbar = plt.colorbar() cbar.ax.set_ylabel(&apos;$t$&apos;) plt.tight_layout() plt.show() ``` ```python dt = 0.1 mg_data = bd.chaos.MackeyGlassEq(25000, dt=dt, tau=17, beta=0.2, gamma=0.1, n=10) ts = mg_data.ts xs = mg_data.xs ys = mg_data.ys plot_mackey_glass_series(ts, xs, ys, num_sample=int(1000 / dt)) ``` ![image-20230828151451523](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151451523.png) ### Prediction of Mackey-Glass timeseries #### Prepare the data ```python def get_data(t_warm, t_forcast, t_train, sample_rate=1): warmup = int(t_warm / dt) # warmup the reservoir forecast = int(t_forcast / dt) # predict 10 ms ahead train_length = int(t_train / dt) X_warm = xs[:warmup:sample_rate] X_warm = bm.expand_dims(X_warm, 0) X_train = xs[warmup: warmup+train_length: sample_rate] X_train = bm.expand_dims(X_train, 0) Y_train = xs[warmup+forecast: warmup+train_length+forecast: sample_rate] Y_train = bm.expand_dims(Y_train, 0) X_test = xs[warmup + train_length: -forecast: sample_rate] X_test = bm.expand_dims(X_test, 0) Y_test = xs[warmup + train_length + forecast::sample_rate] Y_test = bm.expand_dims(Y_test, 0) return X_warm, X_train, Y_train, X_test, Y_test ``` ```python # First warmup the reservoir using the first 100 ms # Then, train the network in 20000 ms to predict 1 ms chaotic series ahead x_warm, x_train, y_train, x_test, y_test = get_data(100, 1, 20000) ``` ```python sample = 3000 fig = plt.figure(figsize=(15, 5)) plt.plot(x_train[0, :sample], label=&quot;Training data&quot;) plt.plot(y_train[0, :sample], label=&quot;True prediction&quot;) plt.legend() plt.show() ``` ![image-20230828151606545](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151606545.png) #### Prepare the ESN ```python class ESN(bp.DynamicalSystemNS): def __init__(self, num_in, num_hidden, num_out, sr=1., leaky_rate=0.3, Win_initializer=bp.init.Uniform(0, 0.2)): super(ESN, self).__init__() self.r = bp.layers.Reservoir( num_in, num_hidden, Win_initializer=Win_initializer, spectral_radius=sr, leaky_rate=leaky_rate, ) self.o = bp.layers.Dense(num_hidden, num_out, mode=bm.training_mode) def update(self, x): return x &amp;gt;&amp;gt; self.r &amp;gt;&amp;gt; self.o ``` #### Train and test ```python model = ESN(1, 100, 1) model.reset_state(1) trainer = bp.RidgeTrainer(model, alpha=1e-6) ``` ```python # warmup _ = trainer.predict(x_warm) ``` ```python # train _ = trainer.fit([x_train, y_train]) ``` #### Test the training data ```python ys_predict = trainer.predict(x_train) ``` ```python start, end = 1000, 6000 plt.figure(figsize=(15, 7)) plt.subplot(211) plt.plot(bm.as_numpy(ys_predict)[0, start:end, 0], lw=3, label=&quot;ESN prediction&quot;) plt.plot(bm.as_numpy(y_train)[0, start:end, 0], linestyle=&quot;--&quot;, lw=2, label=&quot;True value&quot;) plt.title(f&apos;Mean Square Error: {bp.losses.mean_squared_error(ys_predict, y_train)}&apos;) plt.legend() plt.show() ``` ![image-20230828151747954](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151747954.png) #### Test the testing data ```python ys_predict = trainer.predict(x_test) start, end = 1000, 6000 plt.figure(figsize=(15, 7)) plt.subplot(211) plt.plot(bm.as_numpy(ys_predict)[0, start:end, 0], lw=3, label=&quot;ESN prediction&quot;) plt.plot(bm.as_numpy(y_test)[0,start:end, 0], linestyle=&quot;--&quot;, lw=2, label=&quot;True value&quot;) plt.title(f&apos;Mean Square Error: {bp.losses.mean_squared_error(ys_predict, y_test)}&apos;) plt.legend() plt.show() ``` ![image-20230828151824907](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151824907.png) ### JIT connection operators - Just-in-time randomly generated matrix. - Support for Mat@Vec and Mat@Mat. - Support different random generation methods.(homogenous, uniform, normal) ```python import math, random def jitconn_prob_homo(events, prob, weight, seed, outs): random.seed(seed) max_cdist= math.ceil(2/prob -1) for event in events: if event: post_i = random.randint(1, max_cdist) outs[post_i] += weight ``` ![image-20230828153353131](/BrainPy-course-notes/master_content/Notes.assets/image-20230828153353131.png) ## Applications ### From the perspective of kernel methods 维度扩张思想 Non-linear SVMs: Kernel Mapping ![image-20230828153621843](/BrainPy-course-notes/master_content/Notes.assets/image-20230828153621843.png) Kernel methods in neural system? **与维度扩张的思想相似** ![image-20230828153801285](/BrainPy-course-notes/master_content/Notes.assets/image-20230828153801285.png) ### Subcortical pathway for rapid motion processing The first two stages of subcortical visual pathway: Retina -&amp;gt; superior colliculus The first two stages of primary auditory pathway: Inner Ear -&amp;gt; Cochlear Nuclei 维度扩张在subcortical pathway中体现，reservoir 能够高维处理的更简单 ### Spatial-temporal tasks ![image-20230828154155803](/BrainPy-course-notes/master_content/Notes.assets/image-20230828154155803.png) 既有时间信息，又有空间信息的dataset，使用reservoir来处理高维信息，坐位 Dimension expansion ### Gait recognition input来了再做计算 ![image-20230828154352087](/BrainPy-course-notes/master_content/Notes.assets/image-20230828154352087.png) ### Spatial-temporal tasks large-scale，随size增大，accuracy增大 ![image-20230828154428762](/BrainPy-course-notes/master_content/Notes.assets/image-20230828154428762.png) ### Liquid state machine A liquid state machine (LSM) is a type of reservoir computer that uses a spiking neural network. 与ESN一样的范式，都是去做dimension expansion 很难去分析怎么work的">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="Notes.assets/image-20230826111831637.png">
<meta property="og:image" content="Notes.assets/image-20230826111841073.png">
<meta property="og:image" content="Notes.assets/image-20230827142918529.png">
<meta property="og:image" content="Notes.assets/image-20230827143435002.png">
<meta property="og:image" content="Notes.assets/image-20230827144557359.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="BrainPy course notes">
<meta name="twitter:description" content="[TOC] # 神经计算建模简介 ## 计算神经科学的背景与使命 计算神经科学是**脑科学**对**类脑智能**的**桥梁** ### 两大目标 - 用计算建模的方法来阐明大脑功能的计算原理 - 发展类脑智能的模型和算法 ### Prehistory - 1907 LIF model 神经计算的本质 - 1950s HH model 电位定量化模型 最fundamental的 - 1960s Roll&apos;s cable equation 描述信号在轴突和树突怎么传递 - 1970s Amari, Wilson, Cowan et al. 现今建模的基础 - 1982 Hopfield model(Amari-Hopfield model) 引入物理学技术，吸引子模型 - 1988 Sejnowski et al. &quot;Computational Neuroscience&quot;(science) 提出计算神经科学概念 **现在的计算神经科学对应于物理学的第谷-伽利略时代，对大脑工作原理还缺乏清晰的理论** ### Three levels of Brain Science ![image-20230823105226568](/BrainPy-course-notes/master_content/Notes.assets/image-20230823105226568.png) - 大脑做什么 Computational theory -&amp;gt; Psychology &amp; Cognitive Science -&amp;gt; Human-like Cognitive function - 大脑怎么做 Representation &amp; Algorithm -&amp;gt; Computational Neuroscience -&amp;gt; Brain-inspired model &amp; algorithm - 大脑怎么实现 Implementation -&amp;gt; Neuroscience -&amp;gt; Neuromorphic computing ### Mission of Computational Neuroscience &amp;gt; What I can not build a computational model, I do not understand ## 神经计算建模的目标与挑战 ### Limitation of Deep Learning - 不擅长对抗样本 - 对图像的理解有限 ![image-20230823105836259](/BrainPy-course-notes/master_content/Notes.assets/image-20230823105836259.png) ### Brain is for Processing Dynamical Information **We never &quot;see&quot; a static image** ![image-20230823105918336](/BrainPy-course-notes/master_content/Notes.assets/image-20230823105918336.png) ### The missing link a computational model of higher cognitive functior ![image-20230823110617639](/BrainPy-course-notes/master_content/Notes.assets/image-20230823110617639.png) 现在只是做的**局部**的网络，没有一个成功的模型，能**从神经元出发构建网络，到系统层面上** **原因**: 因为神经科学底层数据的缺失，可以考虑数据驱动、大数据的方式来加快发展 ## 神经计算建模的工具 &amp;gt; 工欲行其事，必先利其器 &amp;gt; We need &quot;PyTorch/TensorFlow&quot; in Computational Neuroscience! ### Challenges in neural modelling 有不同的尺度 - Mutiple-scale - Large-scale - Multiple purposes ![image-20230823111212460](/BrainPy-course-notes/master_content/Notes.assets/image-20230823111212460.png) &amp;gt; The modeling targets and methods are extremely complex, and we need a general framework. ### Limitations of Existing Brain Simulators 现今的框架不能满足以上 ![image-20230823111509523](/BrainPy-course-notes/master_content/Notes.assets/image-20230823111509523.png) ### What are needed for a brain simulator 1. Efficiency High-speed simulation on parallel computing devices, etc. 2. Integration Integrated modeling of simulation, training, and analysis 3. Flexibility New models at all scales can be accommodated 4. Extensibility Extensible to new modeling methods(machine learning) 需要新的范式 ### Our solution: BrainPy 4 levels ![image-20230823111903456](/BrainPy-course-notes/master_content/Notes.assets/image-20230823111903456.png) ## 神经计算建模举例 ### Image understanding: an ill-posed problem Image Understanding = image segmentation + image object recognition &amp;gt; Chicken vs. Egg dilemma &amp;gt; &amp;gt; - Without segmentation, how to recognize &amp;gt; - Without recognition, how to segment **The solution of brain:** Analysis-by-synthesis 猜测与验证方法 ### Reverse Hierarchy Theory 人的感知是整体到局部 ### Two pathways for visual information processing ![image-20230823114517888](/BrainPy-course-notes/master_content/Notes.assets/image-20230823114517888.png) ### Key Computational Issues for Global-to-local Neural Information Processing - What are global and local features - How to rapidly extract global features - How to generate global hypotheses - How to implement from global to local processing - The interplay between global and local features - Others #### How to extract global features **Global first = Topology first**(大范围首先，陈霖) 视觉系统更敏感于拓扑性质的差异 &amp;gt; DNNs has difficulty to recognize topology **A retina-SC network for topology detection** 视网膜到上丘的检测，Gap junction coupling ... ### A Model for Motion Pattern Recognition Reservoir Module Decision-making Module ### How to generate &quot;global&quot; hypotheses in the representation space Attractor neural network ![image-20230823115853980](/BrainPy-course-notes/master_content/Notes.assets/image-20230823115853980.png) Levy Flight in Animal Behaviors ![image-20230823120000911](/BrainPy-course-notes/master_content/Notes.assets/image-20230823120000911.png) ### How to process information from global to local Push-pull Feedback A hierarchical Hopfield Model ### Interplay between global and local features A two-pathway model for object recognition ![image-20230823120750349](/BrainPy-course-notes/master_content/Notes.assets/image-20230823120750349.png) Modeling visual masking 可以用two-pathway很好解释 # Programming basics ## Python Basics ### Values - Boolean - String - Integer - Float - ... ### Keywords Not allowed to use keywords, they define structure and rules of a language. ```python help(&quot;keywords&quot;) ``` ### Operators 数据之间的操作 #### For Integers and Floats ```python a=5 b=3 # addition + print(&quot;a+b=&quot;,atb) # subtraction - print(&quot;a-b=&quot;,a-b) # multiplication * print(&quot;axb=&quot;a*b) # division / print(&quot;a/b=&quot;,a/b) # power ** print(&quot;a**b=&quot;,a**b) ``` #### Booleans ```python #Boolean experssions # equals: == print(&quot;5==5&quot;,5==5) # do not equal: != print(&quot;5!-5&quot;,5!=5) # greater than: &amp;gt; print(&quot;5&amp;gt;5&quot;,5&amp;gt;5) # greater than or equal: &amp;gt;= print(&quot;5&amp;gt;=5”5&amp;gt;=5) ``` ```python # logica operators print(&quot;True and False:&quot;, True and False) print(&quot;True or False:&quot;, True or False) print(&quot;not False:&quot;, not False) ``` ### Modules Not all functionality available comes automatically when starting python. ```python import match import numpy as np print(math.pi) print(np.pi) from numpy import pi print(pi) from numpy import * print(pi) ``` ### Control statements #### If ```python a = 5 # In Python, blocks of code are defined using indentation. if a == 5: print(&quot;ok&quot;) ``` &amp;gt; ok #### For ```python # range(5) means a list with integers, 0, 1, 2, 3, 4 for i in range(5): print(i) ``` &amp;gt; 0 &amp;gt; 1 &amp;gt; 2 &amp;gt; 3 &amp;gt; 4 #### While ```python i = 1 while i 1 &amp;gt; 8 &amp;gt; 1000 ### Functions - Functions are used to abstract components of a program. - Much like a mathematical function, they take some input and then find the result. start a function definition with a keyword def - Then comes the function name, with arguments in braces, and then a colon. ```python def func(args1, args2): pass ``` ### Data types #### List - Group variables together - Specific order - Access item with brankets: [ ] - List can be sliced - List can be multiplied - List can be added - Lists are mutable - Copying a list ```python myList = [0, 1, 2, 0,&quot;name&quot;] print(&quot;myList[0]:&quot;, myList[0]) print(&quot;myList[1]:&quot;, myList[1]) print(&quot;myList[3]:&quot;, myList[3]) print(&quot;myList[-1]:&quot;, myList[-1]) print(&quot;myList[-2]:&quot;, myList[-2]) ``` &amp;gt; myList[0]: 0 &amp;gt; myList[1]: 1 &amp;gt; myList[3]: name &amp;gt; myList[-1]: name &amp;gt; myList[-2]: 2.0 ```python myList = [0, 1.0, &quot;hello&quot;] print(&quot;myList[0:2]:&quot;, mylist[0:2]) print(&quot;myList*2:&quot;, myList*2) myList2 = [2,&quot;yes&quot;] print(&quot;myList+myList2:&quot;, myList+myList2) ``` &amp;gt; myList[0:2]: [0，1.0] &amp;gt; myList*2: [0，1.0， hello&apos;，0，1.0， hello&apos;] &amp;gt; myList+myList2: [0，1.0，&apos;hello&apos;，2，yes&apos;] #### tuple Tuples are immutable. #### dictionary A dictionary is a collection of key-value pairs ```python d = {} d[1] = 2 d[&quot;a&quot;] = 3 print(&quot;d: &quot;, d) c = {1:2, &quot;a&quot;:3} print(&quot;c: &quot;, c) print(&quot;c[1]: &quot;, c[1]) ``` &amp;gt; d: {1: 2, &apos;a&apos;: 3} &amp;gt; c: {1: 2, &apos;a&apos;: 3} &amp;gt; c[1]: 2 ### Class In Python, everything is an object. Classes are objects, instances of classes are objects, modules are objects, and functions are objects. 1. a **type** 2. an internal **data representation** (primitive or composite) 3. a set of procedures for **interaction** with the object **a simple example** ```python # define class class Linear(): pass # instantiate object layer1 = Linear() print(layer1) ``` &amp;gt; `` #### Initializing an object ```python # define class class Linear(): # It refers to the object (instance) itself def __init__(self, n_input): self.n_input = n_input layer1 = Linear(100) layer2 = Linear(1000) print(&quot;layer1 : &quot;, layer1.n_input) print(&quot;layer2 : &quot;, layer2.n_input) ``` &amp;gt; layer1 : 100 &amp;gt; layer2 : 1000 #### Class has methods (similar to functions) ```python # define class class Linear(): ### It refers to the the object (instance) itself def __init__(self, n_input, n_output): self.n_input = n_input self.n_output = n_output def compute n params(self): num_params = self.n_input * self.n_output return num_params layerl = Linear(10,100) print(layerl.compute_n_params()) ``` &amp;gt; 1000 ## NumPy Basic ### Numpy Introduction - Fundamental package for scientific computing with Python - N-dimensional array object - Linear algebra, frontier transform, random number capacities - Building block for other packages (e.g. Scipy) ### Array - Arrays are mutable - Arrays attributes - ... ```python A = np.zeros((2, 2)) print(A) ``` &amp;gt; [[0. 0.] &amp;gt; [0. 0.]] ```python a.ndim # 2 dimension a.shape # (2, 5) shape of array a.size # 10 $ of elements a.T # transpose a.dtype # data type ``` #### Array broadcasting When operating on two arrays, numpy compares shapes. Two dimensions are compatible when 1. They are of equal size 2. One of them is 1 ![image-20230823143622229](/BrainPy-course-notes/master_content/Notes.assets/image-20230823143622229.png) ### Vector operations - Inner product - Outer product - Dot product (matrix multiplication) ```python u = [1, 2, 3] v = [1, 1, 1] np.inner(u, v) np.outer(u, v) np.dot(u, v) ``` &amp;gt; 6 &amp;gt; array([[1, 1, 1], &amp;gt; [2, 2, 2], &amp;gt; [3, 3, 3]]) &amp;gt; 6 ### Matrix operations - `np.ones` - `.T` - `np.dot` - `np.eye` - `np.trace` - `np.row_stack` - `np.column_stack` ### Operations along axes ```python a = np.ones((2, 3)) print(a) a.sum() a.sum(axis=0) a.cumsum() a.cumsum(axis=0) ``` ### Slicing arrays ```python a = np.random.random((2, 3)) print(a) a[0,:] # first row, all columns a[0:2] # first and second rows, al columns a[:,1:3]# all rows, second and third columns ``` ### Reshape ```python a = np.ones((10,1)) a.reshape(2,5) ``` ### Linear algebra ```python qr # Computes the QR decomposition cholesky # Computes the Cholesky decomposition inv(A) # Inverse solve(A,b) # Solves Ax = b for A full rank lstsq(A,b) # Solves arg minx //Ax - b//2 eig(A) # Eigenvalue decomposition eigvals(A) # Computes eigenvalues svd(A，full) # Sinqular value decomposition pinv(A) # Computes pseudo-inverse of A ``` ### Fourier transform ```python import numpy.fft fft # 1-dimensional DFT fft2 # 2-dimensional DFT fftn # N-dimensional DFT ifft # 1-dimensional inverse DFT (etc.) rfft # Real DFT (1-dim) ``` ### Random sampling ```python import numpy.random rand(d0, d1, ..., dn) # Random values in a given shape randn(d0, d1, ..., dn) # Random standard normal randint(lo, hi, size) # Random integers [lo hi) choice(a, size, repl, p) # Sample from a shuffle(a) # Permutation (in-place) permutation(a) # Permutation (new array) ``` ### Distributions in random ```python import numpy.random beta binomial chisquare exponential dirichlet gamma laplace lognormal ... ``` ### Scipy - `SciPy` is a library of algorithms and mathematical tools built to work with `NumPy ` arrays. - `scipy.linalg linear algebra` - `scipy.stats statistics` - `scipy.optimize optimization` - `scipy.sparse sparse matrices` - `scipy.signal signal processing` - etc. ## BrainPy introduction ### Modeling demands - Large-scale - Multi-scale - Methods ### BrainPy Architecture - Infrastructure - Functions - Just-in-time compilation - Devices ![image-20230823145349681](/BrainPy-course-notes/master_content/Notes.assets/image-20230823145349681.png) ### Main features #### Dense operators - Compatible with `NumPy`, `TensorFlow`, `PyTorch` and other dense matrix operator syntax. - Users do not need to learn and get started programming directly. #### Dedicated operatorsq - Applies brain dynamics sparse connectivity properties with event-driven computational features. - Reduce the complexity of brain dynamics simulations by several orders of magnitude. #### Numerical Integrators - Ordinary differential equations: brainpy.odeint - Stochastic differential equations: brainpy.sdeint - Fractional differential equations: brainpy.fdeint - Delayed differential equations #### Modular and composable 从微观到宏观 **brainpy.DynamicalSystem** ![image-20230823151159786](/BrainPy-course-notes/master_content/Notes.assets/image-20230823151159786.png) #### JIT of object-oriented BrainPy provides object-oriented transformations: - `brainpy.math.jit` - `brainpy.math.grad` - `brainpy.math.for_loop` - `brainpy.math.ifelse` ## BrainPy Programming Basics ### Just-in-Time compilation Just In Time Compilation (JIT, or Dynamic Translation), is compilation that is being done during the execution of a program. JIT compilation attempts to use **the benefits of both**. While the interpreted program is being run, the JIT compiler determines the most frequently used code and compiles it to machine code. The advantages of a JIT are due to the fact that since the compilation takes place in run time, a JIT compiler has access to dynamic runtime information enabling it to make better optimizations (such as inlining functions). ```python def gelu(x): sqrt = bm.sqrt(2 / bm.pi) cdf = 0.5 * (1.0 + bm.tanh(sqrt * (x + 0.044715 * (x ** 3)))) y = x *cdf return y &amp;gt;&amp;gt;&amp;gt; gelu_jit = bm.jit(gelu) # 使用JIT ``` ### Object-oriented JIT compilation - The class object must be inherited from brainpy.BrainPyObject, the base class of BrainPy, whose methods will be automatically JIT compiled. - All time-dependent variables must be defined as brainpy.math.Variable. ```python class LogisticRegression(bp.BrainPyObject): def __init__(self, dimension): super(LogisticRegression, self).__init__() # parameters self.dimension = dimension # variables self.w = bm.Variable(2.0 * bm.ones(dimension) - 1.3) def __call__(self, X, Y): u = bm.dot(((1.0 / (1.0 + bm.exp(-Y * bm.dot(X, self.w))) - 1.0) * Y), X) self.w.value = self.w - u # in-place update ``` **ExampleL Run a neuron model** ```python model = bp.neurons.HH(1000) #一共1000个神经元 runner = bp.DSRunner(target=model, inputs=(&apos;input&apos;, 10.)) # jit默认为True runner(duration=1000, eval_time=True) #模拟 1000ms ``` 禁用JIT来debug ### Data operations #### Array 等价于`numpy`的`array` #### BrainPy arrays &amp; JAX arrays ```python t1 = bm.arange(3) print(t1) print(t1.value) ``` &amp;gt; JaxArray([0, 1, 2], dtype=int32) &amp;gt; DeviceArray([0, 1, 2], dtype=int32) #### Variables Arrays that are not marked as dynamic variables will be JIT-compiled as static arrays, and modifications to static arrays will not be valid in the JIT compilation environment. ```python t = bm.arange(4) v = bm.Variable(t) print(v) print(v.value) ``` &amp;gt; Variable([0, 1, 2, 3], dtype=int32) &amp;gt; DeviceArray([0, 1, 2, 3], dtype=int32) ### Variables **In-place updating** 就地更新 #### Indexing and slicing - Indexing: `v[i] = a` or `v[(1, 3)] = c` - Slicing: `v[i:j] = b` - Slicing all values `v[:] = d`, `v[...] = e` #### Augmented assignment - add - subtract - divide - multiply - floor divide - modulo - power - and - or - xor - left shift - right shift #### Value assignment ```python v.value = bm.arange(10) check_no_change(v) ``` #### Update assignment ```python v.update(bm.random.randint(0, 20, size=10)) ``` ### Control flows #### If-else `brainpy.math.where` ```python a = 1. bm.where(a DeviceArray(1., dtype=float32, weak_type=True) `brainpy.math.ifelse` ```python def ifelse(condition, branches, operands): true_fun, false_fun = branches if condition: return true_fun(operands) else: return false_fun(operands) ``` #### For loop ```python import brainpy.math hist_of_out_vars = brainpy.math.for_loop(body_fun, operands) ``` #### While loop ```python i = bm.Variable(bm.zeros(1)) counter = bm.Variable(bm.zeros(1)) def cond_f(): return i[0] $$ (2\pi a\Delta x)c_{\mathrm{M}}\frac{\partial V(x,t)}{\partial t}+(2\pi a\Delta x)i_{\mathrm{ion}}=\frac{\pi a^{2}}{\rho_{\mathrm{L}}}\frac{\partial V(x+\Delta x,t)}{\partial x}-\frac{\pi a^{2}}{\rho_{\mathrm{L}}}\frac{\partial V(x,t)}{\partial x} $$ **Cable Equation** $$ c_\mathrm{M}\frac{\partial V(x,t)}{\partial t}=\frac{a}{2\rho_\mathrm{L}}\frac{\partial^2V(x,t)}{\partial x^2}-i_\mathrm{ion} $$ 电流在通过长直导体时会泄露电流，如何记录膜电位，可以使用此方程来描述 **Passive conduction:** ion currents are caused by leaky channels exclusively $$ i_{\mathrm{ion}}=V(x,t)/r_{\mathrm{M}} $$ -&amp;gt; $$ \begin{aligned}c_\mathrm{M}\frac{\partial V(x,t)}{\partial t}&amp;=\frac{a}{2\rho_\mathrm{L}}\frac{\partial^2V(x,t)}{\partial x^2}-\frac{V(x,t)}{r_\mathrm{M}}\\\\\tau\frac{\partial V(x,t)}{\partial t}&amp;=\lambda^2\frac{\partial^2V(x,t)}{\partial x^2}-V(x,t)\quad\lambda=\sqrt{0.5ar_\mathrm{M}/\rho_\mathrm{L}}\end{aligned} $$ 没有动作电位，单纯通过电缆传输 ![image-20230824102932665](/BrainPy-course-notes/master_content/Notes.assets/image-20230824102932665.png) If a constant external current is applied to 𝑥 = 0 the steady-state membrane potential $𝑉_{ss}(𝑥)$ is $$ \lambda^2\frac{\mathrm{d}^2V_{\mathrm{ss}}(x)}{\mathrm{d}x^2}-V_{\mathrm{ss}}(x)=0\longrightarrow V_{\mathrm{ss}}(x)=\frac{\lambda\rho_{\mathrm{L}}}{\pi a^2}I_0e^{-x/\lambda} $$ 电信号无衰减传播: 动作电位 ## Action potential &amp; active transport Steps of an action potential: - Depolarization - Repolarization - Hyperpolarization - Resting Characteristics: - All-or-none - Fixed shape - Active electrical property ![image-20230824103322522](/BrainPy-course-notes/master_content/Notes.assets/image-20230824103322522.png) How to simulate an action potential? $$ \begin{aligned} \frac{I(t)}{A}&amp; =c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+i_{\mathrm{ion}} \\ \Rightarrow\quad c_{\mathrm{M}}\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}&amp; =-g_{\mathrm{Cl}}(V_{\mathrm{M}}-E_{\mathrm{Cl}})-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}})-g_{\mathrm{Na}}(V_{\mathrm{M}}-E_{\mathrm{Na}})+\frac{I(t)}{A} \end{aligned} $$ 离子通道的开闭会随着电压而变化，电导也随着电压而变化 Mechanism: voltage-gated ion channels **HH建模思路：通过电导** ### Nodes of Ranvier Saltatory conduction with a much higher speed and less energy consumption 两个郎飞结之间会有离子通道，既有被动传导，也有主动的防止衰减 ![image-20230824104220106](/BrainPy-course-notes/master_content/Notes.assets/image-20230824104220106.png) ## The Hodgkin-Huxley Model ### Modeling of each ion channel Modeling of each ion channel: $$ g_m=\bar{g}_mm^x $$ Modeling of each ion gate: $$ \mathcal{C}\underset{}{\operatorname*{\overset{\alpha(\mathrm{V})}{\underset{\beta(\mathrm{V})}{\operatorname*{\longrightarrow}}}}\mathcal{O}} \\ \Rightarrow \begin{aligned} \frac{\mathrm{d}m}{\mathrm{d}t}&amp; =\alpha(V)(1-m)-\beta(V)m \\ &amp;=\frac{m_{\infty}(V)-m}{\tau_{m}(V)} \end{aligned} \\ \\ \begin{aligned}m_\infty(V)&amp;=\frac{\alpha(V)}{\alpha(V)+\beta(V)}.\\\tau_m(V)&amp;=\frac{1}{\alpha(V)+\beta(V)}\end{aligned} $$ $$ \text{If}\ V\text{ is constant:}m(t)=m_\infty(V)+(m_0-m_\infty(V))\mathrm{e}^{-t/\tau_m(V)} $$ ### Voltage clamp $$ \begin{aligned} \frac{I(t)}{A}&amp; =c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+i_{\mathrm{ion}} \\ \Rightarrow\quad c_{\mathrm{M}}\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}&amp; =-g_{\mathrm{Cl}}(V_{\mathrm{M}}-E_{\mathrm{Cl}})-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}})-g_{\mathrm{Na}}(V_{\mathrm{M}}-E_{\mathrm{Na}})+\frac{I(t)}{A} \end{aligned} $$ - The membrane potential is kept constant - The current from capacitors is excluded - Currents must come from leaky/voltage-gated ion channels $$ \begin{aligned}I_{\mathrm{cap}}&amp;=c\frac{dV}{dt}=0\\I_{\mathrm{fb}}&amp;=\quad i_{\mathrm{ion}}=g_{\mathrm{Na}}(V-E_{\mathrm{Na}})+g_{\mathrm{K}}(V-E_{\mathrm{K}})+g_{\mathrm{L}}(V-E_{\mathrm{L}})\end{aligned} $$ 只测量一个离子通道就可以很容易得到电导 ![image-20230824111620056](/BrainPy-course-notes/master_content/Notes.assets/image-20230824111620056.png) ### Leaky channel Hyperpolarization → the sodium and potassium channels are closed $$ I_{\mathrm{fb}}=g_{\mathrm{Na}}(V-E_{\mathrm{Na}})+g_{\mathrm{K}}(V-E_{\mathrm{K}})+g_{\mathrm{L}}(V-E_{\mathrm{L}}) $$ $$ \Rightarrow I_{\mathrm{fb}}=g_L(V-E_L) $$ $$ g_\mathrm{L}=0.3\mathrm{mS/cm}^2,E_\mathrm{L}=-54.4\mathrm{mV} $$ #### Potassium and sodium channels Potassium channels: Use choline to eliminate the inward current of Na + Na + current: $I_{fb} - I_{K}$ ![image-20230824112328953](/BrainPy-course-notes/master_content/Notes.assets/image-20230824112328953.png) ![image-20230824112333144](/BrainPy-course-notes/master_content/Notes.assets/image-20230824112333144.png) 转化速率和电导率两个因素 Potassium channels - Resting state (gate closed) - Activated state (gate open) → Activation gate: $g_{\mathrm{K}}=\bar{g}_{K}n^{x}$ Sodium channels - Resting state (gate closed) - Activated state (gate open) - Inactivated state (gate blocked) → Activation gate + inactivation gate: $g_{\mathrm{Na}}=\bar{g}_\text{Na}m^3h$ ![image-20230824113116329](/BrainPy-course-notes/master_content/Notes.assets/image-20230824113116329.png) The gates of sodium channels Modeling of each ion gate: $$ \begin{aligned} &amp;\text{gk}&amp;&amp; =\bar{g}_{K}n^{x} \\ &amp;\text{gNa}&amp;&amp; =\bar{g}_{\mathrm{Na}}m^{3}h \\ &amp;\frac{\mathrm{d}n}{\mathrm{d}t}&amp;&amp; =\alpha_{n}(V)(1-n)-\beta_{n}(V)n \\ &amp;\frac{\mathrm{d}m}{\mathrm{d}t}&amp;&amp; =\alpha_{m}(V)(1-m)-\beta_{m}(V)m \\ &amp;\frac{\mathrm{d}h}{\mathrm{d}t}&amp;&amp; =\alpha_{h}(V)(1-h)-\beta_{h}(V)h \end{aligned} $$ $$ \begin{aligned} \frac{\mathrm{d}m}{\mathrm{d}t}&amp; =\alpha(V)(1-m)-\beta(V)m \\ &amp;=\frac{m_{\infty}(V)-m}{\tau_{m}(V)} \end{aligned} $$ $$ \begin{aligned}m_\infty(V)&amp;=\frac{\alpha(V)}{\alpha(V)+\beta(V)}\\\tau_m(V)&amp;=\frac{1}{\alpha(V)+\beta(V)}\end{aligned}. $$ $$ m(t)=m_\infty(V)+(m_0-m_\infty(V))\mathrm{e}^{-t/\tau_m(V)} $$ ### The Hodgkin-Huxley(HH) Model $$ c_\mathrm{M}\frac{\mathrm{d}V_\mathrm{M}}{\mathrm{d}t}=-g_\mathrm{Cl}(V_\mathrm{M}-E_\mathrm{Cl})-g_\mathrm{K}(V_\mathrm{M}-E_\mathrm{K})-g_\mathrm{Na}(V_\mathrm{M}-E_\mathrm{Na})+\frac{I(t)}{A} $$ 本质是4个微分方程联立在一起 $$ \left\{\begin{aligned}&amp;c\frac{\mathrm{d}V}{\mathrm{d}t}=-\bar{g}_\text{Na}m^3h(V-E_\text{Na})-\bar{g}_\text{K}n^4(V-E_\text{K})-\bar{g}_\text{L}(V-E_\text{L})+I_\text{ext},\\&amp;\frac{\mathrm{d}n}{\mathrm{d}t}=\phi\left[\alpha_n(V)(1-n)-\beta_n(V)n\right]\\&amp;\frac{\mathrm{d}m}{\mathrm{d}t}=\phi\left[\alpha_m(V)(1-m)-\beta_m(V)m\right],\\&amp;\frac{\mathrm{d}h}{\mathrm{d}t}=\phi\left[\alpha_h(V)(1-h)-\beta_h(V)h\right],\end{aligned}\right. $$ $$ \begin{aligned}\alpha_n(V)&amp;=\frac{0.01(V+55)}{1-\exp\left(-\frac{V+55}{10}\right)},\quad\beta_n(V)&amp;=0.125\exp\left(-\frac{V+65}{80}\right),\\\alpha_h(V)&amp;=0.07\exp\left(-\frac{V+65}{20}\right),\quad\beta_n(V)&amp;=\frac{1}{\left(\exp\left(-\frac{V+55}{10}\right)+1\right)},\\\alpha_m(V)&amp;=\frac{0.1(V+40)}{1-\exp\left(-(V+40)/10\right)},\quad\beta_m(V)&amp;=4\exp\left(-(V+65)/18\right).\end{aligned} $$ $$ \phi=Q_{10}^{(T-T_{\mathrm{base}})/10} $$ 每一步符合生物学 ![image-20230824113714178](/BrainPy-course-notes/master_content/Notes.assets/image-20230824113714178.png) #### How to fit each gating variable? **Fitting n:** $g_{\mathbf{K}}=\bar{g}_{K}n^{x}\quad m(t)=m_{\infty}(V)+(m_{0}-\color{red}{\boxed{m_{\infty}(V)}})\mathrm{e}^{-t/\pi_{m}(V)}$ → $g_\mathrm{K}(V,t)=\bar{g}_\mathrm{K}\left[n_\infty(V)-(n_\infty(V)-n_0(V))\mathrm{e}^{-\frac{t}{\tau_n(V)}}\right]^x$ by $g_{\mathrm{K}\infty}=\bar{g}_{\mathrm{K}}n_{\infty}^{x},g_{\mathrm{K}0}=\bar{g}_{\mathrm{K}}n_{0}^{x}$ → $g_{\mathrm{K}}(V,t)=\left[g_{\mathrm{K}\infty}^{1/x}-(g_{\mathrm{K}\infty}^{1/x}-g_{\mathrm{K}0}^{1/x})\mathrm{e}^{-\frac{t}{\tau_{n}(V)}}\right]^{x}$ ![image-20230824114623467](/BrainPy-course-notes/master_content/Notes.assets/image-20230824114623467.png) # Hodgkin-Huxley brain dynamics programming ## Dynamics Programming Basics ### Integrators 微分器 ![image-20230824140806650](/BrainPy-course-notes/master_content/Notes.assets/image-20230824140806650.png) **example** FitzHugh-Nagumo equation $$ \begin{aligned}\tau\dot{w}&amp;=v+a-bw,\\\dot{v}&amp;=v-\frac{ u^3}{3}-w+I_{\mathrm{ext}}.\end{aligned} $$ ```python @bp.odeint(method=&apos;Euler&apos;, dt=0.01) def integral(V, w, t, Iext, a, b, tau): dw = (V + a - b * w) / tau dV = V - V * V * V / 3 - w + Iext return dV, dw ``` **JointEq** In a dynamical system, there may be multiple variables that change dynamically over time. Sometimes these variables are interrelated, and updating one variable requires other variables as inputs. For better integration accuracy, we recommend that you use `brainpy.JointEq` to jointly solve interrelated differential equations. ```python a, b = 0.02, 0.20 dV = lambda V, t, w, Iext: 0.04 * V * V + 5 * V + 140 - w + Iext # 第一个方程 dw = lambda w, t, V: a * (b * V - w) # 第二个方程 joint_eq = bp.JointEq(dV, dw) # 联合微分方程 integral2 = bp.odeint(joint_eq, method=&apos;rk2&apos;) # 定义该联合微分方程的数值积分方法 ``` ```python # 声明积分运行器 runner = bp.integrators.IntegratorRunner( integral, monitors=[&apos;V&apos;] inits=dict(V=0., w=0.) args=dict(a=a, b=b, tau=tau, Iext=Iext), dt=0.01 ) # 使用积分运行器来进行模拟100ms，结合步长dt=0.01 runner.run(100.) plt.plot(runner.mon.ts, runner.mon.V) plt.show() ``` ![image-20230824142019832](/BrainPy-course-notes/master_content/Notes.assets/image-20230824142019832.png) ### `DynamicalSystem` BrainPy provides a generic `SynamicalSystem` class to define various types of dynamical models. BrainPy supports modelings in brain simulation and brain-inspired computing. All these supports are based on one common concept: **Dynamical System** via `brainpy.DynamicalSystem`. #### What is `DynamicalSystem` A `DynamicalSystem` defines the updating rule of the model at single time step. 1. For models with state, `DynamicalSystem` defines the state transition from $t$ to $t + dt$, i.e., $S(t+dt)=F(S(t),x,t,dt)$, where $S$ is the state, $x$ is input, $t$ is the time, and $dt$ is the time step. This is the case for recurrent neural networks (like GRU, LSTM), neuron models (like HH, LIF), or synapse models which are widely used in brain simulation. 2. However, for models in deep learning, like convolution and fully-connected linear layers, `DynamicalSystem` defines the input-to-output mapping, i.e., $y=F(x,t)$. ![img](https://brainpy.readthedocs.io/en/latest/_images/dynamical_system.png) #### How to define `DynamicalSystem` ```python class YourDynamicalSystem(bp.DynamicalSystem): def update(self, x): ... ``` Instead of input x, there are shared arguments across all nodes/layers in the network: - the current time `t`, or - the current running index `i`, or - the current time step `dt`, or - the current phase of training or testing `fit=True/False`. Here, it is necessary to explain the usage of `bp.share`. - `bp.share.save( )`: The function saves shared arguments in the global context. User can save shared arguments in tow ways, for example, if user want to set the current time `t=100`, the current time step `dt=0.1`,the user can use `bp.share.save(&quot;t&quot;,100,&quot;dt&quot;,0.1)` or `bp.share.save(t=100,dt=0.1)`. - `bp.share.load( )`: The function gets the shared data by the `key`, for example, `bp.share.load(&quot;t&quot;)`. - `bp.share.clear_shargs( )`: The function clears the specific shared arguments in the global context, for example, `bp.share.clear_shargs(&quot;t&quot;)`. - `bp.share.clear( )`: The function clears all shared arguments in the global context. #### How to run `DynamicalSystem` As we have stated above that `DynamicalSystem` only defines the updating rule at single time step, to run a `DynamicalSystem` instance over time, we need a for loop mechanism. ![img](https://brainpy.readthedocs.io/en/latest/_images/dynamical_system_and_dsrunner.png) ##### `brainpy.math.for_loop` `for_loop` is a structural control flow API which runs a function with the looping over the inputs. Moreover, this API just-in-time compile the looping process into the machine code. ```python inputs = bp.inputs.section_input([0., 6.0, 0.], [100., 200., 100.]) indices = np.arange(inputs.size) def run(i, x): neu.step_run(i, x) return neu.V.value vs = bm.for_loop(run, (indices, inputs), progress_bar=True) ``` ##### `brainpy.LoopOverTime` Different from `for_loop`, `brainpy.LoopOverTime` is used for constructing a dynamical system that automatically loops the model over time when receiving an input. `for_loop` runs the model over time. While `brainpy.LoopOverTime` creates a model which will run the model over time when calling it. ```python net2.reset_state(batch_size=10) looper = bp.LoopOverTime(net2) out = looper(currents) ``` ##### `brainpy.DSRunner` **Initializing a `DSRunner`** Generally, we can initialize a runner for dynamical systems with the format of: ``` runner = DSRunner(target=instance_of_dynamical_system, inputs=inputs_for_target_DynamicalSystem, monitors=interested_variables_to_monitor, dyn_vars=dynamical_changed_variables, jit=enable_jit_or_not, progress_bar=report_the_running_progress, numpy_mon_after_run=transform_into_numpy_ndarray ) ``` - `target` specifies the model to be simulated. It must an instance of brainpy.DynamicalSystem. - `inputs` is used to define the input operations for specific variables. - It should be the format of `[(target, value, [type, operation])]`, where `target` is the input target, `value` is the input value, `type` is the input type (such as “fix”, “iter”, “func”), `operation` is the operation for inputs (such as “+”, “-”, “*”, “/”, “=”). Also, if you want to specify multiple inputs, just give multiple `(target, value, [type, operation])`, such as `[(target1, value1), (target2, value2)]`. - It can also be a function, which is used to manually specify the inputs for the target variables. This input function should receive one argument `tdi` which contains the shared arguments like time `t`, time step `dt`, and index `i`. - `monitors` is used to define target variables in the model. During the simulation, the history values of the monitored variables will be recorded. It can also to monitor variables by callable functions and it should be a `dict`. The `key` should be a string for later retrieval by `runner.mon[key]`. The `value` should be a callable function which receives an argument: `tdt`. - `dyn_vars` is used to specify all the dynamically changed [variables](https://brainpy.readthedocs.io/en/latest/tutorial_math/variables.html) used in the `target` model. - `jit` determines whether to use JIT compilation during the simulation. - `progress_bar` determines whether to use progress bar to report the running progress or not. - `numpy_mon_after_run` determines whether to transform the JAX arrays into numpy ndarray or not when the network finishes running. **Running a `DSRunner`** After initialization of the runner, users can call `.run()` function to run the simulation. The format of function `.run()` is showed as follows: ```python runner.run(duration=simulation_time_length, inputs=input_data, reset_state=whether_reset_the_model_states, shared_args=shared_arguments_across_different_layers, progress_bar=report_the_running_progress, eval_time=evaluate_the_running_time ) ``` - `duration` is the simulation time length. - `inputs` is the input data. If `inputs_are_batching=True`, `inputs` must be a PyTree of data with two dimensions: `(num_sample, num_time, ...)`. Otherwise, the `inputs` should be a PyTree of data with one dimension: `(num_time, ...)`. - `reset_state` determines whether to reset the model states. - `shared_args` is shared arguments across different layers. All the layers can access the elements in `shared_args`. - `progress_bar` determines whether to use progress bar to report the running progress or not. - `eval_time` determines whether to evaluate the running time. ### Monitors ```python # initialize monitor through a list of strings runner1 = bp.DSRunner(target=net, monitors=[&apos;E.spike&apos;, &apos;E.V&apos;, &apos;I.spike&apos;, &apos;I.V&apos;], # 4 elements in monitors inputs=[(&apos;E.input&apos;, 20.), (&apos;I.input&apos;, 20.)], jit=True) ``` Once we call the runner with a given time duration, the monitor will automatically record the variable evolutions in the corresponding models. Afterwards, users can access these variable trajectories by using .mon.[variable_name]. The default history times .mon.ts will also be generated after the model finishes its running. Let’s see an example. ```python runner1.run(100.) bp.visualize.raster_plot(runner1.mon.ts, runner1.mon[&apos;E.spike&apos;], show=True) ``` **Initialization with index specification** ```python monitors=[(&apos;E.spike&apos;, [1, 2, 3]), # monitor values of Variable at index of [1, 2, 3] &apos;E.V&apos;], # monitor all values of Variable &apos;V&apos; ``` &amp;gt; The monitor shape of &quot;E.V&quot; is (run length, variable size) = (1000, 3200) &amp;gt; The monitor shape of &quot;E.spike&quot; is (run length, index size) = (1000, 3) **Explicit monitor target** ```python monitors={&apos;spike&apos;: net.E.spike, &apos;V&apos;: net.E.V}, ``` &amp;gt; The monitor shape of &quot;V&quot; is = (1000, 3200) &amp;gt; The monitor shape of &quot;spike&quot; is = (1000, 3200) **Explicit monitor target with index specification** ```python monitors={&apos;E.spike&apos;: (net.E.spike, [1, 2]), # monitor values of Variable at index of [1, 2] &apos;E.V&apos;: net.E.V}, # monitor all values of Variable &apos;V&apos; ``` &amp;gt; The monitor shape of &quot;E.V&quot; is = (1000, 3200) &amp;gt; The monitor shape of &quot;E.spike&quot; is = (1000, 2) ### Inputs In brain dynamics simulation, various inputs are usually given to different units of the dynamical system. In BrainPy, `inputs` can be specified to runners for dynamical systems. The aim of `inputs` is to mimic the input operations in experiments like Transcranial Magnetic Stimulation (TMS) and patch clamp recording. `inputs` should have the format like `(target, value, [type, operation])`, where - `target` is the target variable to inject the input. - `value` is the input value. It can be a scalar, a tensor, or a iterable object/function. - `type` is the type of the input value. It support two types of input: `fix` and `iter`. The first one means that the data is static; the second one denotes the data can be iterable, no matter whether the input value is a tensor or a function. The `iter` type must be explicitly stated. - `operation` is the input operation on the target variable. It should be set as one of `{ + , - , * , / , = }`, and if users do not provide this item explicitly, it will be set to ‘+’ by default, which means that the target variable will be updated as `val = val + input`. #### Static inputs ```python runner6 = bp.DSRunner(target=net, monitors=[&apos;E.spike&apos;], inputs=[(&apos;E.input&apos;, 20.), (&apos;I.input&apos;, 20.)], # static inputs jit=True) runner6.run(100.) bp.visualize.raster_plot(runner6.mon.ts, runner6.mon[&apos;E.spike&apos;]) ``` #### Iterable inputs ```python I, length = bp.inputs.section_input(values=[0, 20., 0], durations=[100, 1000, 100], return_length=True, dt=0.1) runner7 = bp.DSRunner(target=net, monitors=[&apos;E.spike&apos;], inputs=[(&apos;E.input&apos;, I, &apos;iter&apos;), (&apos;I.input&apos;, I, &apos;iter&apos;)], # iterable inputs jit=True) runner7.run(length) bp.visualize.raster_plot(runner7.mon.ts, runner7.mon[&apos;E.spike&apos;]) ``` ## Run a built-in HH model [Using Built-in Models — BrainPy documentation](https://brainpy.readthedocs.io/en/latest/tutorial_building/overview_of_dynamic_model.html) ```python import brainpy as bp import brainpy.math as bm current, length = bp.inputs.section_input(values=[0., bm.asarray([1., 2., 4., 8., 10., 15.]), 0.], durations=[10, 2, 25], return_length=True) hh_neurons = bp.neurons.HH(current.shape[1]) runner = bp.DSRunner(hh_neurons, monitors=[&apos;V&apos;, &apos;m&apos;, &apos;h&apos;, &apos;n&apos;], inputs=(&apos;input&apos;, current, &apos;iter&apos;)) runner.run(length) ``` ## Run a HH model from scratch The mathematic expression of the HH model $$ \left\{\begin{aligned}&amp;c\frac{\mathrm{d}V}{\mathrm{d}t}=-\bar{g}_\text{Na}m^3h(V-E_\text{Na})-\bar{g}_\text{K}n^4(V-E_\text{K})-\bar{g}_\text{L}(V-E_\text{L})+I_\text{ext},\\&amp;\frac{\mathrm{d}n}{\mathrm{d}t}=\phi\left[\alpha_n(V)(1-n)-\beta_n(V)n\right]\\&amp;\frac{\mathrm{d}m}{\mathrm{d}t}=\phi\left[\alpha_m(V)(1-m)-\beta_m(V)m\right],\\&amp;\frac{\mathrm{d}h}{\mathrm{d}t}=\phi\left[\alpha_h(V)(1-h)-\beta_h(V)h\right],\end{aligned}\right. $$ $$ \begin{aligned}\alpha_n(V)&amp;=\frac{0.01(V+55)}{1-\exp\left(-\frac{V+55}{10}\right)},\quad\beta_n(V)&amp;=0.125\exp\left(-\frac{V+65}{80}\right),\\\alpha_h(V)&amp;=0.07\exp\left(-\frac{V+65}{20}\right),\quad\beta_n(V)&amp;=\frac{1}{\left(\exp\left(-\frac{V+55}{10}\right)+1\right)},\\\alpha_m(V)&amp;=\frac{0.1(V+40)}{1-\exp\left(-(V+40)/10\right)},\quad\beta_m(V)&amp;=4\exp\left(-(V+65)/18\right).\end{aligned} $$ $$ \phi=Q_{10}^{(T-T_{\mathrm{base}})/10} $$ V: the membrane potential n: activation variable of the Kt channel m: activation variable of the Nat channel h; inactivation variable of the Nat channe ### Define HH model `class` - Inherit `bp.dyn.NeuDyn` ```python import brainpy as bp import brainpy.math as bm class HH(bp.dyn.NeuDyn): def __init__(self, size, ENa=50., gNa=120., Ek=-77., gK=36., EL=-54.387, gL=0.03, V_th=0., C=1.0, T=6.3): super(HH, self).__init__(size=size) ``` ### Initialization ```python import brainpy as bp import brainpy.math as bm class HH(bp.dyn.NeuDyn): def __init__(self, size, ENa=50., gNa=120., Ek=-77., gK=36., EL=-54.387, gL=0.03, V_th=0., C=1.0, T=6.3): super(HH, self).__init__(size=size) # parameters self.ENa = ENa self.EK = EK self.EL = EL self.gNA = gNa self.gK = gK self.gL = gL self.C = C self.V_th = V_th self.T_base = 6.3 self.phi = 3.0 ** ((T - self.T_base) / 10.0) # variable self.V = bm.Variable(-70.68 * bm.ones(self.num)) self.m = bm.Variable(0.0266 * bm.ones(self.num)) self.h = bm.Variable(0.772 * bm.ones(self.num)) self.n = bm.Variable(0.235 * bm.ones(self.num)) self.input = bm.Variable(bm.zeros(self.num)) self.spike = bm.Variable(bm.zeros(self.num, dtype=bool)) self.t_last_spike = bm.Variable(bm.ones(self.num) * -1e7) # 定义积分函数 self.integral = bp.odeint(f=self.derivative, method=&apos;exp_auto&apos;) ``` ### Define the derivative function ```python @property def derivative(self): return bp.JointEq(self.dV, self.dm, self.dh, self.dn) def dV(self, V, t, m, h, n, Iext): I_Na = (self.gNa * m ** 3.0 * h) * (V - self.ENa) I_K = (self.gK * n ** 4.0) * (V - self.EK) I_leak = self.gL * (V - self.EL) dVdt = (- I_Na - I_K - I_leak + Iext) / self.C return dVdt def dm(self, m, t, V): alpha = 0.1 * (V + 40) / (1 - bm.exp(-(V + 40) / 10)) beta = 4.0 * bm.exp(-(V + 65) / 18) dmdt = alpha * (1 - m) - beta * m return self.phi * dmdt def dh(self, h, t, V): alpha = 0.07 * bm.exp(-(V + 65) / 20.) beta = 1 / (1 + bm.exp(-(V + 35) / 10)) dhdt = alpha * (1 - h) - beta * h return self.phi * dhdt def dn(self, n, t, V): alpha = 0.01 * (V + 55) / (1 - bm.exp(-(V + 55) / 10)) beta = 0.125 * bm.exp(-(V + 65) / 80) dndt = alpha * (1 - n) - beta * n return self.phi * dndt ``` ### Complete the `update()` function ```python def update(self, x=None): t = bp.share.load(&apos;t&apos;) dt = bp.share.load(&apos;dt&apos;) # TODO: 更新变量V, m, h, n, 暂存在V, m, h, n中 V, m, h, n = self.integral(self.V, self.m, self.h, self.n, t, self.input, dt=dt) #判断是否发生动作电位 self.spike.value = bm.logical_and(self.V = self.V_th) # 更新最后一次脉冲发放时间 self.t_last_spike.value = bm.where(self.spike, t, self.t_last_spike) # TODO: 更新变量V, m, h, n的值 self.V.value = V self.m.value = m self.h.value = h self.n.value = n #重置输入 self.input[:] = 0 ``` ### Simulation ```python current, length = bp.inputs.section_input(values=[0., bm.asarray([1., 2., 4., 8., 10., 15.]), 0.], durations=[10, 2, 25], return_length=True) hh_neurons = HH(current.shape[1]) runner = bp.DSRunner(hh_neurons, monitors=[&apos;V&apos;, &apos;m&apos;, &apos;h&apos;, &apos;n&apos;], inputs=(&apos;input&apos;, current, &apos;iter&apos;)) runner.run(length) ``` ### Visualization ```python import numpy as np import matplotlib.pyplot as plt bp.visualize.line_plot(runner.mon.ts, runner.mon.V, ylabel=&apos;V (mV)&apos;, plot_ids=np.arange(current.shape[1])) plt.plot(runner.mon.ts, bm.where(current[:, -1]&amp;gt;0, 10, 0) - 90.) plt.figure() plt.plot(runner.mon.ts, runner.mon.m[:, -1]) plt.plot(runner.mon.ts, runner.mon.h[:, -1]) plt.plot(runner.mon.ts, runner.mon.n[:, -1]) plt.legend([&apos;m&apos;, &apos;h&apos;, &apos;n&apos;]) plt.xlabel(&apos;Time (ms)&apos;) ``` ## Customize a conductance-based model 电路模拟，写成电导形式 ![image-20230824180831033](/BrainPy-course-notes/master_content/Notes.assets/image-20230824180831033.png) $$ \begin{aligned} \text{gK}&amp; =\bar{g}_\text{K}n^4, \\ \frac{\mathrm{d}n}{\mathrm{d}t}&amp; =\phi[\alpha_n(V)(1-n)-\beta_n(V)n], \end{aligned} $$ 动力学形式描述，引入门框变量$n$ $$ \begin{aligned} &amp;\alpha_{n}(V) =\frac{0.01(V+55)}{1-\exp(-\frac{V+55}{10})}, \\ &amp;\beta_{n}(V) =0.125\exp\left(-\frac{V+65}{80}\right). \end{aligned} $$ 由此式来建模钾离子通道 ### Programming an ion channel #### Three ion channel ```python import brainpy as bp import brainpy.math as bm class IK(bp.dyn.IonChannel): def __init__(self, size, E=-77., g_max=36., phi=1., method=&apos;exp_auto&apos;): super(IK, self).__init__(size) self.g_max = g_max self.E = E self.phi = phi self.n = bm.Variable(bm.zeros(size)) # variables should be packed with bm.Variable self.integral = bp.odeint(self.dn, method=method) def dn(self, n, t, V): alpha_n = 0.01 * (V + 55) / (1 - bm.exp(-(V + 55) / 10)) beta_n = 0.125 * bm.exp(-(V + 65) / 80) return self.phi * (alpha_n * (1. - n) - beta_n * n) def update(self, V): t = bp.share.load(&apos;t&apos;) dt = bp.share.load(&apos;dt&apos;) self.n.value = self.integral(self.n, t, V, dt=dt) def current(self, V): return self.g_max * self.n ** 4 * (self.E - V) ``` ```python class INa(bp.dyn.IonChannel): def __init__(self, size, E= 50., g_max=120., phi=1., method=&apos;exp_auto&apos;): super(INa, self).__init__(size) self.g_max = g_max self.E = E self.phi = phi self.m = bm.Variable(bm.zeros(size)) # variables should be packed with bm.Variable self.h = bm.Variable(bm.zeros(size)) self.integral_m = bp.odeint(self.dm, method=method) self.integral_h = bp.odeint(self.dh, method=method) def dm(self, m, t, V): # TODO: 计算dm/dt alpha_m = 0.11 * (V + 40) / (1 - bm.exp(-(V + 40) / 10)) beta_m = 4 * bm.exp(-(V + 65) / 18) return self.phi * (alpha_m * (1. - m) - beta_m * m) def dh(self, h, t, V): # TODO: 计算dh/dt alpha_h = 0.07 * bm.exp(-(V + 65) / 20) beta_h = 1. / (1 + bm.exp(-(V + 35) / 10)) return self.phi * (alpha_h * (1. - h) - beta_h * h) def update(self, V): t = bp.share.load(&apos;t&apos;) dt = bp.share.load(&apos;dt&apos;) # TODO: 更新self.m, self.h self.m.value = self.integral_m(self.m, t, V, dt=dt) self.h.value = self.integral_h(self.h, t, V, dt=dt) def current(self, V): return self.g_max * self.m ** 3 * self.h * (self.E - V) ``` ```python class IL(bp.dyn.IonChannel): def __init__(self, size, E=-54.39, g_max=0.03): super(IL, self).__init__(size) self.g_max = g_max self.E = E def current(self, V): return self.g_max * (self.E - V) def update(self, V): pass ``` #### Build a HH model with ion channels **Using customized ion channels** ```python class HH(bp.dyn.CondNeuGroup): def __init__(self, size): super(HH, self).__init__(size, V_initializer=bp.init.Uniform(-80, -60.)) # TODO: 初始化三个离子通道 self.IK = IK(size, E=-77., g_max=36.) self.INa = INa(size, E=50., g_max=120.) self.IL = IL(size, E=-54.39, g_max=0.03) ``` **Using built-in ion channels** ```python class HH(bp.dyn.CondNeuGroup): def __init__(self, size): super().__init__(size) self.INa = bp.channels.INa_HH1952(size) self.IK = bp.channels.IK_HH1952(size) self.IL = bp.cahnnels.IL(size, E=-54.387, g_max=0.03) ``` #### Simulation ```python neu = HH(1) runner = bp.DSRunner( neu, monitors=[&apos;V&apos;, &apos;IK.n&apos;, &apos;INa.m&apos;, &apos;INa.h&apos;], inputs=(&apos;input&apos;, 1.698) # near the threshold current ) runner.run(200) # the running time is 200 ms import matplotlib.pyplot as plt plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;V&apos;]) plt.xlabel(&apos;t (ms)&apos;) plt.ylabel(&apos;V (mV)&apos;) plt.savefig(&quot;HH.jpg&quot;) plt.show() plt.figure(figsize=(6, 2)) plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;IK.n&apos;], label=&apos;n&apos;) plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;INa.m&apos;], label=&apos;m&apos;) plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;INa.h&apos;], label=&apos;h&apos;) plt.xlabel(&apos;t (ms)&apos;) plt.legend() plt.savefig(&quot;HH_channels.jpg&quot;) plt.show() ``` ![image-20230824184016011](/BrainPy-course-notes/master_content/Notes.assets/image-20230824184016011.png) # Simple Neuron Modeling: Simplified Models ## The Leaky Integrate-and-Fire(LIF) Neuron Model ### The LIF neuron model $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-(V-V_{\mathrm{rest}})+RI(t)\\\\\mathrm{if}V&amp;&amp;gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\text{last}\ {t_{ref}}\end{aligned} $$ 只有一个微分方程，要加一个不应期(**t refractory period**)，膜电位不发生任何改变，认为离子通道只有泄露通道 ![image-20230825101057570](/BrainPy-course-notes/master_content/Notes.assets/image-20230825101057570.png) Given a constant current input: ![image-20230825101410745](/BrainPy-course-notes/master_content/Notes.assets/image-20230825101410745.png) 没有建模准确变化，只提供什么时候膜电位的变化 ### The dynamic features of the LIF model **General solution (constant input):**$V(t)=V_{\text{reset}}+RI_{\text{c}}(1-\mathrm{e}^{-\frac{t-t_0}{\tau}})$ **Firing frequency:** $$ \begin{aligned}T&amp;=-\tau\ln\left(1-\frac{V_{\phi h}-V_{\mathrm{rest}}}{RI_{\varsigma}}\right)\\f&amp;=\frac{1}{T+t_{\mathrm{ref}}}=\frac{1}{t_{\mathrm{ref}}-\tau\ln\left(1-\frac{V_{0}-V_{\mathrm{rest}}}{RI_{\varsigma}}\right)}\end{aligned} $$ **Rheobase current (minimal current):** $$ I_{\theta}=\frac{V_{\mathrm{th}}-V_{\mathrm{reset}}}{R} $$ 基强电流，如果小于它将无法发放 ### Strengths &amp; weaknesses of the LIF model #### Strengths - Simple, high simulation efficiency - Intuitive - Fits well the subthreshold membrane potential #### Weaknesses - The shape of action potentials is over-simplified - Has no memory of the spiking history - Cannot reproduce diverse firing patterns ### Other Univariate neuron models #### The Quadratic Integrate-and-Fire (QOF) model: $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=a_{0}(V-V_{\mathrm{re}t})(V-V_{\mathrm{c}})+RI(t)\\&amp;\text{if }V&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{re}set}\quad\text{last}\quad t_{\mathrm{ref}}\end{aligned} $$ ![image-20230825103243039](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103243039.png) 膜电位仍需要手动重置 #### The Theta neuron model $$ \frac{\mathrm{d}\theta}{\mathrm{d}t}=1-\cos\theta+(1+\cos\theta)(\beta+I(t)) $$ ![image-20230825103331170](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103331170.png) 隐式表达，不具有物理意义，但也会进行整合发放 #### The Exponential Integrate-and-Fire (ExpIF) model $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-\left(V-V_{\mathrm{res}t}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{3T}}+RI(t)\\\mathrm{if~}V&amp;&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{res}t}\mathrm{last}t_{\mathrm{ref}}\end{aligned} $$ ![image-20230825103501912](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103501912.png) 仍需要手动重置膜电位 ## The Adaptive Exponential Integrate-and-Fire(AdEx) Neuron Model ### The AdEx neuron model Two variables: - 𝑉: membrane potential - 𝑤: adaptation variable $$ \begin{aligned} \tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{S_{T}}}-Rw+RI(t) \\ \tau_{w}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{\mathrm{w}}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ \mathrm{if}V&amp; &amp;gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} \end{aligned} $$ 不为零，就会衰减到$-w$ ![image-20230825103840880](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103840880.png) - A larger 𝑤 suppresses 𝑉 from increasing - 𝑤 decays exponentially while having a sudden increase when the neuron fires **Firing patterns of the AdEx model** ![image-20230825104254936](/BrainPy-course-notes/master_content/Notes.assets/image-20230825104254936.png) **Categorization of firing patterns** According to the steady-state firing time intervals: - Tonic/regular spiking - Adapting - Bursting - Irregular spiking According to the initial-state features: - Tonic/classic spiking - Initial burst - Delayed spiking ### Other multivariate neuron models #### The Izhikevich model $$ \begin{aligned} &amp;\frac{dV}{dt} =0.04V^{2}+5V+140-u+I \\ &amp;\frac{\mathrm{d}u}{\mathrm{d}t} =a\left(bV-u\right) \\ &amp;\operatorname{if}V &amp;gt;\theta,\quad V\leftarrow c,u\leftarrow u+d\text{ last }t_{\mathrm{ref}} \end{aligned} $$ 二次整合发放多加了一个$u$ ![image-20230825104832770](/BrainPy-course-notes/master_content/Notes.assets/image-20230825104832770.png) #### The FitzHugh–Nagumo (FHN) model $$ \begin{aligned}\dot{v}&amp;=v-\frac{v^3}3-w+RI_{\mathrm{ext}}\\\tau\dot{w}&amp;=v+a-bw.\end{aligned} $$ 没有对膜电位进行人为的重置，可以更好的进行动力学分析，没有打破微分方程的连续性 ![image-20230825104922636](/BrainPy-course-notes/master_content/Notes.assets/image-20230825104922636.png) #### The Generalized Integrate-and-Fire (GIF) model n+2个变量 $$ \begin{aligned} &amp;\tau{\frac{\mathrm{d}V}{\mathrm{d}t}} =-\left(V-V_{\mathrm{rest}}\right)+R\sum_{j}I_{j}+RI \\ &amp;\frac{\mathrm{d}\Theta}{\mathrm{d}t} =a\left(V-V_{\mathrm{rest}}\right)-b\left(\Theta-\Theta_{\infty}\right) \\ &amp;\frac{\mathrm{d}l_{j}}{\mathrm{d}t} =-k_{j}I_{j},\quad j=1,2,...,n \\ &amp;\operatorname{if}V &amp;gt;\Theta,\quad I_{j}\leftarrow R_{j}I_{j}+A_{j},V\leftarrow V_{\mathrm{reset}},\Theta\leftarrow max(\Theta_{\mathrm{reset}},\Theta) \end{aligned} $$ 每个变量都是线性的，泛化性体现在重置条件上 ![image-20230825105035349](/BrainPy-course-notes/master_content/Notes.assets/image-20230825105035349.png) ## Dynamic analysis: phase-plane analysis ### Phase plane analysis 对动力学系统的行为来分析，普遍对两个变量来进行分析 Analyzes the behavior of a dynamical system with (usually two) variables described by ordinary differential equations $$ \begin{aligned} &amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp;&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{S_{T}}}-Rw+RI(t) \\ &amp;\tau_{W}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp;&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ &amp;\mathrm{if}V&amp;&amp; &amp;gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} \end{aligned} $$ **Elements:** - Nullclines: $\mathrm{d}V/\mathrm{d}t=0;\mathrm{d}w/\mathrm{d}t=0$ - Fixed points: $\mathrm{d}V/\mathrm{d}t=0\mathrm{~and~}\mathrm{d}w/\mathrm{d}t=0$ - The vector field - The trajectory of variables 假设外部电流恒定 ![image-20230825110708994](/BrainPy-course-notes/master_content/Notes.assets/image-20230825110708994.png) ### Phase plane analysis for the AdEx neuron model $$ \begin{aligned} &amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp;&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{\Lambda_{T}}}-Rw+RI(t) \\ &amp;\tau_{w}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp;&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ &amp;\text{ifV}&amp;&amp; &amp;gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} \end{aligned} $$ ![image-20230825110811399](/BrainPy-course-notes/master_content/Notes.assets/image-20230825110811399.png) #### Tonic ![image-20230825112857175](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112857175.png) #### Adaptation ![image-20230825112918815](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112918815.png) #### Bursting ![image-20230825112933938](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112933938.png) #### Transient spiking ![image-20230825112950297](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112950297.png) ## Dynamic analysis: bifurcation analysis ### Bifurcation analysis Quantitative analysis of the existence and the properties of fixed points in a dynamical system with a changing parameter 某个外界条件变化时，固定点的变化 Elements: - Lines of fixed points - Stability properties of fixed points ![image-20230825114510710](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114510710.png) ### Bifurcation analysis for the AdEx Neuron model bifurcation analysis for 2 variables Variables: 𝑉 and 𝑤 Parameters: $I_{ext}$ $$ \begin{aligned} &amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}=-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{{\frac{V-V_{T}}{ST}}}-Rw+RI(t) \\ &amp;\text{-} {\frac{\mathrm{d}w}{\mathrm{d}t}}=a(V-V_{\mathrm{rest}})-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ &amp;\mathrm{if}V&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{reset}}\ \mathrm{last}\ t_{\mathrm{ref}} \end{aligned} $$ ![image-20230825114801456](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114801456.png) ![image-20230825114742740](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114742740.png) **Subjects: two variables (𝑉 and 𝑤)** ![image-20230825114856403](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114856403.png) ### Extended: The limit cycle The FitzHugh–Nagumo (FHN) model $$ \begin{aligned}\dot{v}&amp;=v-\frac{v^3}3-w+RI_\mathrm{ext}\\\tau\dot{w}&amp;=v+a-bw.\end{aligned} $$ This dynamical system, in certain conditions, exhibits a cyclic pattern of variable changes which can be visualized as a closed trajectory in the phase plane. 变化锁定到环中 ![image-20230825115348008](/BrainPy-course-notes/master_content/Notes.assets/image-20230825115348008.png) ![image-20230825115354146](/BrainPy-course-notes/master_content/Notes.assets/image-20230825115354146.png) # Reduced Models - brain dynamics programming ## LIF neuron models programming ### Define LIF `class` $$ \begin{aligned}&amp;\tau\frac{\mathrm{d}V}{\mathrm{d}t}=-(V-V_{\mathrm{rest}})+RI(t)\\&amp;\text{if }V&amp;gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\text{last}t_{\mathrm{ref}}\end{aligned} $$ ```python class LIF(bp.dyn.NeuDyn): def __init__(self, size, V_rest=0, V_reset=-5, V_th=20, R=1, tau=10, t_ref=5., **kwargs): # 初始化父类 super(LIF, self).__init__(size=size, **kwargs) ``` ### Initialization ```python class LIF(bp.dyn.NeuDyn): def __init__(self, size, V_rest=0, V_reset=-5, V_th=20, R=1, tau=10, t_ref=5., **kwargs): # 初始化父类 super(LIF, self).__init__(size=size, **kwargs) # 初始化参数 self.V_rest = V_rest self.V_reset = V_reset self.V_th = V_th self.R = R self.tau = tau self.t_ref = t_ref # 不应期时长 # 初始化变量 self.V = bm.Variable(bm.random.randn(self.num) + V_reset) self.input = bm.Variable(bm.zeros(self.num)) self.t_last_spike = bm.Variable(bm.ones(self.num) * -1e7) # 上一次脉冲发放时间 self.refractory = bm.Variable(bm.zeros(self.num, dtype=bool)) # 是否处于不应期 self.spike = bm.Variable(bm.zeros(self.num, dtype=bool)) # 脉冲发放状态 # 使用指数欧拉方法进行积分 self.integral = bp.odeint(f=self.derivative, method=&apos;exponential_euler&apos;) ``` ### Define the derivative function ```python # 定义膜电位关于时间变化的微分方程 def derivative(self, V, t, Iext): dVdt = (-V + self.V_rest + self.R * Iext) / self.tau return dVdt ``` ### Complete the `update()` function ```python def update(self): t, dt = bp.share[&apos;t&apos;], bp.share[&apos;dt&apos;] # 以数组的方式对神经元进行更新 refractory = (t - self.t_last_spike) self.V_th # 将大于阈值的神经元标记为发放了脉冲 self.spike[:] = spike # 更新神经元脉冲发放状态 self.t_last_spike[:] = bm.where(spike, t, self.t_last_spike) # 更新最后一次脉冲发放时间 self.V[:] = bm.where(spike, self.V_reset, V) # 将发放了脉冲的神经元膜电位置为V_reset，其余不变 self.refractory[:] = bm.logical_or(refractory, spike) # 更新神经元是否处于不应期 self.input[:] = 0. # 重置外界输入 ``` ### Simulation ```python def run_LIF(): # 运行LIF模型 group = LIF(1) runner = bp.DSRunner(group, monitors=[&apos;V&apos;], inputs=(&apos;input&apos;, 22.)) runner(200) # 运行时长为200ms # 结果可视化 fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) plt.plot(runner.mon.ts, runner.mon.V) plt.xlabel(r&apos;$t$ (ms)&apos;) plt.ylabel(r&apos;$V$ (mV)&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) plt.show() ``` ![image-20230825141201825](/BrainPy-course-notes/master_content/Notes.assets/image-20230825141201825.png) ### Input current &amp; firing frequency $$ \begin{gathered} V(t)=V_{\mathrm{reset}}+RI_{\mathrm{c}}(1-\mathrm{e}^{-\frac{t-t_{0}}{\tau}}). \\ T=-\tau\ln\left[1-\frac{V_{\mathrm{th}}-V_{\mathrm{rest}}}{RI_{\mathrm{c}}}\right] \\ f={\frac{1}{T+t_{\mathrm{ref}}}}={\frac{1}{t_{\mathrm{ref}}-\tau\ln\left[1-{\frac{V_{\mathrm{th}}-V_{\mathrm{rest}}}{RI_{c}}}\right]}} \end{gathered} $$ ```python # 输入与频率的关系 current = bm.arange(0, 600, 2) duration = 1000 LIF_neuron = LIF(current.shape[0]) runner_2 = bp.dyn.DSRunner(LIF_neurons, monitors=[&apos;spike&apos;], inputs={&apos;input&apos;, current}, dt=0.01) runner_2.run(duration) freqs = runner_2.mon.spike.sum(axis=0) / (duration/1000) plt.figure() plt.plot(current, freqs) plt.xlabel(&apos;inputs&apos;) plt.ylabel(&apos;frequencies&apos;) ``` ![image-20230825143405952](/BrainPy-course-notes/master_content/Notes.assets/image-20230825143405952.png) ### Other Univariate neuron models **The Quadratic Integrate-and-Fire (QIF) model** $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=a_{0}(V-V_{\mathrm{res}t})(V-V_{c})+RI(t)\\\mathrm{if~}V&amp;&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{reset~last~}t_{\mathrm{ref}}}\end{aligned} $$ ```python def derivative(self, V, t, I): dVdt = (self.c * (V - self.V_reset) * (V - self.V_c) + self.R * I) / self.tau return dVdt ``` **The Exponential Integrate-and-Fire (ExpIF) model** $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{\delta_{T}}}+RI(t)\\&amp;\mathrm{if~}V&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{reset}}\mathrm{last}t_{\mathrm{ref}}\end{aligned} $$ ```python def derivative(self, V, t, I): exp_v = self.delta_T * bm.exp((V - self.V_T) / self.delta_T) dvdt = (- (V - self.V_rest) + exp_v + self.R * I) / self.tau return dvdt ``` ## AdEx neuron models programming $$ \begin{gathered} \tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}=-(V-V_{\mathrm{rest}})+\Delta_{T}\mathrm{e}^{{\frac{V-V_{T}}{\Delta T}}}-Rw+RI(t), \\ \tau_{w}\frac{\mathrm{d}w}{\mathrm{d}t}=a(V-V_{\mathrm{rest}})-w+b\tau_{w}\sum_{t^{(f)}}\delta(t-t^{(f)})), \\ \mathrm{if~}V&amp;gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\mathrm{last}t_{\mathrm{ref}}. \end{gathered} $$ ### Define AdEx `class` ```python class AdEx(bp.dyn.NeuDyn): def __init__(self, size, V_rest=-65, V_reset=-68, V_th=-30, V_T=-59.9, delta_T=3.48 a=1., b=1., R=1., tau=10., tau_w=30., tau_ref=0., **kwargs): # 初始化父类 super(AdEx, self).__init__(size=size, **kwargs) ``` ### Initialization ```python class AdEx(bp.dyn.NeuDyn): def __init__(self, size, V_rest=-65, V_reset=-68, V_th=-30, V_T=-59.9, delta_T=3.48 a=1., b=1., R=1., tau=10., tau_w=30., tau_ref=0., **kwargs): # 初始化父类 super(AdEx, self).__init__(size=size, **kwargs) # 初始化参数 self.V_rest = V_rest self.V_reset = V_reset self.V_th = V_th self.V_T = V_T self.delta_T = delta_T self.a = a self.b = b self.R = R self.tau = tau self.tau_w = tau_w self.tau_ref = tau_ref # 初始化变量 self.V = bm.Variable(bm.random.randn(self.num) - 65.) self.w = bm.Variable(bm.zeros(self.num)) self.input = bm.Variable(bm.zeros(self.num)) self.t_last_spike = bm.Variable(bm.ones(self.num) * -1e7) # 上一次脉冲发放时间 self.refractory = bm.Variable(bm.zeros(self.num, dtype=bool)) # 是否处于不应期 self.spike = bm.Variable(bm.zeros(self.num, dtype=bool)) # 脉冲发放状态 # 定义积分器 self.integral = bp.odeint(f=self.derivative, method=&apos;exp_auto&apos;) ``` ### Define the derivative function ```python def dV(self, V, t, w, I): exp = self.delta_T * bm.exp((V - self.V_T) / self.delta_T) dVdt = (-V + self.V_rest + exp - self.R * w + self.R * I) / self.tau return dVdt def dw(self, w, t, V): dwdt = (self.a * (V - self.V_rest) - w) / self.tau_w return dwdt @property def derivative(self): return bp.JointEq([self.dV, self.dw]) ``` ### Complete the `update()` function ```python def update(self): t, dt = bp.share[&apos;t&apos;], bp.share[&apos;dt&apos;] V, w = self.integral(self.V.value, self.w.value, t, self.input, dt=dt) # 以数组的方式对神经元进行更新 refractory = (t - self.t_last_spike) self.V_th # 将大于阈值的神经元标记为发放了脉冲 self.spike[:] = spike # 更新神经元脉冲发放状态 self.t_last_spike[:] = bm.where(spike, t, self.t_last_spike) # 更新最后一次脉冲发放时间 self.V[:] = bm.where(spike, self.V_reset, V) # 将发放了脉冲的神经元膜电位置为V_reset，其余不变 self.w[:] = bm.where(spike, w + self.b, w) #更新自适应电流 self.refractory[:] = bm.logical_or(refractory, spike) # 更新神经元是否处于不应期 self.input[:] = 0. # 重置外界输入 ``` ### Simulation ![image-20230825145518709](/BrainPy-course-notes/master_content/Notes.assets/image-20230825145518709.png) ### Other multivariate neuron models **The Izhikevich model** $$ \begin{aligned} &amp;\frac{dV}{dt} =0.04V^{2}+5V+140-u+I \\ &amp;\frac{\mathrm{d}u}{\mathrm{d}t} =a\left(bV-u\right) \\ &amp;\operatorname{if}V &amp;gt;\theta,\quad V\leftarrow c,u\leftarrow u+d\mathrm{last}t_{\mathrm{ref}} \end{aligned} $$ ```python def dV(self, V, t, u, I): dVdt = 0.04 * V * V + 5 * V + 140 - u + I return dVdt def du(self, u, t, V): dudt = self.a * (self.b * V - u) return dudt ``` **The Generalized Integrate-and-Fire (GIF) model** $$ \begin{aligned} &amp;\tau{\frac{\mathrm{d}V}{\mathrm{d}t}} =-\left(V-V_{\mathrm{rest}}\right)+R\sum_{j}I_{j}+RI \\ &amp;\frac{\mathrm{d}\Theta}{\mathrm{d}t} =a\left(V-V_{\mathrm{est}}\right)-b\left(\Theta-\Theta_{\infty}\right) \\ &amp;\frac{\mathrm{d}I_j}{\mathrm{d}r} =-k_jI_j,\quad j=1,2,\ldots,n \\ &amp;\text{if V} &amp;gt;\Theta,\quad I_{j}\leftarrow R_{j}I_{j}+A_{j},V\leftarrow V_{\mathrm{reset}},\Theta\leftarrow max\left(\Theta_{\mathrm{reset}},\Theta\right) \end{aligned} $$ ```python def dI1(self, I1, t): return - self.k1 * I1 def dI2(self, I2, t): return - self.k2 * I2 def dVth(self, V_th, t, V): return self.a * (V - self.v_rest) - self.b * (V_th - self.V_th_inf) def dV(self, V, t, I1, I2, I): return (- (V - self.V_rest) + self.R * (I + I1 + I2)) / self.tau ``` **Built-in reduced neuron models** ![image-20230825145947800](/BrainPy-course-notes/master_content/Notes.assets/image-20230825145947800.png) ## Dynamic analysis: phase-plane analysis ### Simple case $$ \frac{dx}{dt}=\sin(x)+I, $$ ```python @bp.odeint def int_x(x, t, Iext): return bp.math.sin(x) + Iext ``` ```python pp = bp.analysis.PhasePlane1D( model=int_x, target_vars={&apos;x&apos;: [-10, 10]}, pars_update={&apos;Iext&apos;: 0.}, resolutions={&apos;x&apos;: 0.01} ) pp.plot_vector_field() pp.plot_fixed_point(show=True) ``` ![image-20230825152003373](/BrainPy-course-notes/master_content/Notes.assets/image-20230825152003373.png) - Nullcline: The zero-growth isoclines, such as $f(x,y) = 0$ and $g(x,y) = 0$ - Fixed points: The equilibrium points of the system, which are located at all the nullclines intersect. - Vector field: The vector field of the system. - Limit cycles: The limit cycles. - Trajectories: A simulation trajectory with the given initial values ### Phase plane analysis for AdEx ```python def ppa_AdEx(group): bm.enable_x64() v_range = [-70., -40.] w_range = [-10., 50.] phase_plane_analyzer = bp.analysis.PhasePlane2D( model=group, target_vars={&apos;V&apos;: v_range, &apos;w&apos;: w_range, }, # 待分析变量 pars_update={&apos;I&apos;: Iext}, # 需要更新的变量 resolutions=0.05 ) # 画出V, w的零增长曲线 phase_plane_analyzer.plot_nullcline() # 画出奇点 phase_plane_analyzer.plot_fixed_point() # 画出向量场 phase_plane_analyzer.plot_vector_field() # 分段画出V, w的变化轨迹 group.V[:], group.w[:] = group.V_reset, 0 runner = bp.DSRunner(group, monitors=[&apos;V&apos;, &apos;w&apos;, &apos;spike&apos;], inputs=(&apos;input&apos;, Iext)) runner(500) spike = runner.mon.spike.squeeze() s_idx = np.where(spike)[0] # 找到所有发放动作电位对应的index s_idx = np.concatenate(([0], s_idx, [len(spike) - 1])) # 加上起始点和终止点的index for i in range(len(s_idx) - 1): vs = runner.mon.V[s_idx[i]: s_idx[i + 1]] ws = runner.mon.w[s_idx[i]: s_idx[i + 1]] plt.plot(vs, ws, color=&apos;darkslateblue&apos;) # 画出虚线 x = V_reset plt.plot([group.V_reset, group.V_reset], w_range, &apos;--&apos;, color=&apos;grey&apos;, zorder=-1) plt.show() ``` ![image-20230825152925463](/BrainPy-course-notes/master_content/Notes.assets/image-20230825152925463.png) ## Dynamic analysis: bifurcation analysis ### Simple case $$ \frac{dx}{dt}=\sin(x)+I, $$ ```python bif = bp.analysis.Bifurcation1D( model=int_x, target_vars={&apos;x&apos;: [-10, 10]}, target_pars={&apos;Iext&apos;: [0., 1.5]}, resolutions={&apos;Iext&apos;: 0.005, &apos;x&apos;: 0.05} ) bif.plot_bifurcation(show=True) ``` ![image-20230825154227567](/BrainPy-course-notes/master_content/Notes.assets/image-20230825154227567.png) # Synapse models and their programming ## The biology of synapses ### Neurotransmitter &amp; Synapse When the action potential invades the axon terminals, it causes voltage-gated 𝐶𝐶𝑎𝑎 2+ channels to open (1), which triggers vesicles to bind to the presynaptic membrane (2). Neurotransmitter is released into the synaptic cleft by exocytosis and diffuses across the cleft (3). Binding of the neurotransmitter to receptor molecules in the postsynaptic membrane completes the process of transmission (4). 去极化时钙离子内流，与囊泡相结合，...，与受体结合，打开离子通道，超极化、去极化现象 ![image-20230826100321307](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100321307.png) ![image-20230826100418911](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100418911.png) **Neurotransmitter leading to postsynaptic potential.** The binding of neurotransmitter to the postsynaptic membrane receptors changes the membrane potential ($V_m$). These postsynaptic potentials can be either excitatory (depolarizing the membrane), as shown here, or inhibitory (hyperpolarizing the membrane). ![image-20230826100531535](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100531535.png) ### Neurotransmitters 兴奋性神经递质： - 乙酰胆碱 (ACh) - 儿茶酚胺 (catecholamines) - 谷氨酸 (glutamate) - 组胺 (histamine) - 5-羟色胺 (serotonin) - 某些神经肽类 (some of neuropeptides) 抑制性神经递质： - GABA - 甘氨酸 (glycine) - 某些神经肽类 (some of peptides) ![image-20230826100609904](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100609904.png) ### The postsynaptic response The aim of a synapse model is to describe accurately the postsynaptic response generated by the arrival of an action potential at a presynaptic terminal. 1. The fundamental quantity to be modelled is the time course of the postsynaptic receptor conductance 2. The models: - Simple phenomenological waveforms - More complex kinetic schemes that are analogous to the models of membrane- bound ion channels ![image-20230826100701580](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100701580.png) 建模这种响应模式，打开关闭的概率... ## Phenomenological synapse models ### Exponential Model ![image-20230826100738460](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100738460.png) **Assumption**: - The release of neurotransmitter, its diffusion across the cleft, the receptor binding, and channel opening all happen very quickly, so that the channels instantaneously jump from the closed to the open state. channel会瞬间增加然后逐渐关闭 $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}e^{-(t-t_{0})/\tau} \\ \begin{matrix}\bullet&amp;\tau \ \text{is the time constant}\\\bullet&amp;t_0 \ \text{is the time of the pre-synaptic spike}\\\bullet&amp;\bar{g_{syn}}\ \text{is the maximal conductance}\end{matrix} $$ -&amp;gt; corresponding differential equation $$ \tau\frac{dg_{\mathrm{syn}}(t)}{dt}=-g_{\mathrm{syn}}(t)+\bar{g}_{\mathrm{syn}}\delta\left(t_{0}-t\right) $$ - Can fit with experimental data. - A good approximation for GABA A and AMPA, because the rising phase is much shorter than their decay phase. ### Dual Exponential Model ![image-20230826101203059](/BrainPy-course-notes/master_content/Notes.assets/image-20230826101203059.png) exponential model上升的太快，不太符合某些synapse Dual exponential synapse provides a general way to describe the synaptic conductance with different rising and decay time constants. $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}\frac{\tau_{1}\tau_{2}}{\tau_{1}-\tau_{2}}\left(\exp\left(-\frac{t-t_{0}}{\tau_{1}}\right)-\exp\left(-\frac{t-t_{0}}{\tau_{2}}\right)\right) \\ \begin{matrix} \bullet &amp;t_1\ \text{is the decay synaptic time constant} \\ \bullet &amp;\tau_2\ \text{is the rise synaptic time constant} \\ \bullet &amp;t_0\ \text{is the time of the pre-synaptic spike} \\ \bullet &amp;\bar{g}_{syn}\ \text{is the maximal conductance} \end{matrix} $$ -&amp;gt;corresponding differential equation $$ \begin{aligned} &amp;g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}g \\ &amp;\frac{dg}{dt}=-\frac{g}{\tau_{\mathrm{decay}}}+h \\ &amp;\frac{dh}{dt}&amp; =-\frac{h}{\tau_{\mathrm{rise}}}+\delta\left(t_{0}-t\right), \end{aligned} $$ The time course of most synaptic conductance can be well described by this sum of two exponentials. ### Synaptic time constants ![image-20230826101544786](/BrainPy-course-notes/master_content/Notes.assets/image-20230826101544786.png) http://compneuro.uwaterloo.ca/research/constants-constraints/neurotransmitter-time-constants-pscs.html #### AMPA synapse - $t_{decay}$ = 0.18 ms in the auditory system of the chick nucleus magnocellularis (Trussell, 1999). - $t_{rise}$ 25 ms and $\tau_{decay}$ =0.77 ms in dentate gyrus basket cells (Geiger et al., 1997). - $t_{rise}$ = 0.2 ms and $\tau_{decay}$ =1.7 ms in in neocortical layer 5 pyramidal neurons (Hausser and Roth, 1997b). - Reversal potential is nearly 0 mV. #### NMDA synapse - The decay time constants (at near-physiological temperature): - 19 ms in dentate gyrus basket cells (Geiger et al., 1997), - 26 ms in neocortical layer 2/3 pyramidal neurons (Feldmeyer et al., 2002), - 89 ms in CA1 pyramidal cells (Diamond, 2001). - The rise time constants are about 2 ms (Feldmeyer et al., 2002). - Reversal potential is nearly 0 mV. #### GABA$_A$ synapse - GABAergic synapses from dentate gyrus basket cells onto other basket cells are faster: $t_{rise}$ = 0.3 ms and $t_{decay}$ = 2.5 ms (Bartos et al., 2001) than synapses from basket cells to granule cells: $t_{rise}$ = 0.26 ms and $t_{decay}$ = 6.5 ms (Kraushaar and Jonas, 2000). - Reversal potential is nearly -80 mV. #### GABA$_B$ synapse - Common models use models with a rise time of about 25-50 ms, a fast decay time in the range of 100-300ms and a slow decay time of 500-1000 ms. - Reversal potential is nearly -90 mV. ### General property of synaptic time constants - The time constants of synaptic conductance vary widely among synapse types. - The synaptic kinetics tends to accelerate during development (T. Takahashi, Neuroscience Research, 2005) . - The synaptic kinetics becomes substantially faster with increasing temperature. ![image-20230826102033433](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102033433.png) ### Current- and Conductance-based Response ![image-20230826102042614](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102042614.png) #### Conductance-based Response Most synaptic ion channels, such as AMPA and GABA, display an approximately linear current-voltage relationship when they open. ![image-20230826102113670](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102113670.png) **For example**: The synapse is located on a thin dendrite, because the local membrane potential V changes considerably when the synapse is activated. #### Current-based Response In some case, we can also approximate the synapses as sources of current and not a conductance. ![image-20230826102150487](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102150487.png) **For example**: The excitatory synapse on a large compartment, because the depolarization of the membrane is small. ## Programming of phenomenological synapse models ### `ProjAlignPostMg2` ![Image Name](https://cdn.kesci.com/upload/rzz4o4uyar.png?imageView2/0/w/960/h/960) ```python brainpy.dyn.ProjAlignPostMg2( pre, delay, comm, syn, out, post ) ``` - ``pre (JointType[DynamicalSystem, AutoDelaySupp])``: The pre-synaptic neuron group. - ``delay (Union[None, int, float])``: The synaptic delay. - ``comm (DynamicalSystem)``: The synaptic communication. - ``syn (ParamDescInit)``: The synaptic dynamics. - ``out (ParamDescInit)``: The synaptic output. - ``post (DynamicalSystem)`` The post-synaptic neuron group. 只需要建模所有post的neurons ### CSR matrix ![Image Name](https://cdn.kesci.com/upload/rzz4on32hr.png?imageView2/0/w/960/h/960) ### Exponential Model The single exponential decay synapse model assumes the release of neurotransmitter, its diffusion across the cleft, the receptor binding, and channel opening all happen very quickly, so that the channels instantaneously jump from the closed to the open state. Therefore, its expression is given by $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} e^{-\left(t-t_{0}\right) / \tau} $$ where $\tau$ is the time constant, $t_0$ is the time of the pre-synaptic spike, $\bar{g}_{\mathrm{syn}}$ is the maximal conductance. The corresponding differential equation: $$ \frac{d g}{d t} = -\frac{g}{\tau_{decay}}+\sum_{k} \delta(t-t_{j}^{k}). $$ #### COBA Given the synaptic conductance, the COBA model outputs the post-synaptic current with $$ I_{syn}(t) = g_{\mathrm{syn}}(t) (E - V(t)) $$ ```python class ExponSparseCOBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.COBA.desc(E=E), post=post, ) ``` ```python class SimpleNet(bp.DynSysGroup): def __init__(self, E=0.): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = ExponSparseCOBA(self.pre, self.post, delay=None, prob=1., g_max=1., tau=5., E=E) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` ```python def run_a_net(net): indices = np.arange(1000) # 100 ms conductances, currents, potentials = bm.for_loop(net.step_run, indices, progress_bar=True) ts = indices * bm.get_dt() # --- similar to: # runner = bp.DSRunner(net) # conductances, currents, potentials = runner.run(100.) fig, gs = bp.visualize.get_figure(1, 3, 3.5, 4) fig.add_subplot(gs[0, 0]) plt.plot(ts, conductances) plt.title(&apos;Syn conductance&apos;) fig.add_subplot(gs[0, 1]) plt.plot(ts, currents) plt.title(&apos;Syn current&apos;) fig.add_subplot(gs[0, 2]) plt.plot(ts, potentials) plt.title(&apos;Post V&apos;) plt.show() ``` #### CUBA Given the conductance, this model outputs the post-synaptic current with a identity function: $$ I_{\mathrm{syn}}(t) = g_{\mathrm{syn}}(t) $$ ```python class ExponSparseCUBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.CUBA.desc(), post=post, ) ``` ```python class SimpleNet2(bp.DynSysGroup): def __init__(self, g_max=1.): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = ExponSparseCUBA(self.pre, self.post, delay=None, prob=1., g_max=g_max, tau=5.) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` #### Dense connections Exponential synapse model with the conductance-based (COBA) output current and dense connections. ```python class ExponDenseCOBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.MaskedLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.COBA.desc(E=E), post=post, ) ``` ![Image Name](https://cdn.kesci.com/upload/rzz4p7x6dl.png?imageView2/0/w/960/h/960) Exponential synapse model with the current-based (COBA) output current and dense connections. ```python class ExponDenseCUBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.MaskedLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.CUBA.desc(), post=post, ) ``` ### `ProjAlignPreMg2` Synaptic projection which defines the synaptic computation with the dimension of presynaptic neuron group. ![Image Name](https://cdn.kesci.com/upload/rzz4pj1qmk.png?imageView2/0/w/960/h/960) ```python brainpy.dyn.ProjAlignPreMg2( pre, delay, syn, comm, out, post ) ``` - ``pre (JointType[DynamicalSystem, AutoDelaySupp])``: The pre-synaptic neuron group. - ``delay (Union[None, int, float])``: The synaptic delay. - ``syn (ParamDescInit)``: The synaptic dynamics. - ``comm (DynamicalSystem)``: The synaptic communication. - ``out (ParamDescInit)``: The synaptic output. - ``post (DynamicalSystem)`` The post-synaptic neuron group. ### Dual Exponential Model The dual exponential synapse model, also named as **difference of two exponentials model**, is given by: $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} \frac{\tau_{1} \tau_{2}}{\tau_{1}-\tau_{2}}\left(\exp \left(-\frac{t-t_{0}}{\tau_{1}}\right)-\exp \left(-\frac{t-t_{0}}{\tau_{2}}\right)\right) $$ where $\tau_1$ is the time constant of the decay phase, $\tau_2$ is the time constant of the rise phase, $t_0$ is the time of the pre-synaptic spike, $\bar{g}_{\mathrm{syn}}$ is the maximal conductance. The corresponding differential equation: $$ \begin{aligned} &amp;g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} g \\ &amp;\frac{d g}{d t}=-\frac{g}{\tau_{\mathrm{decay}}}+h \\ &amp;\frac{d h}{d t}=-\frac{h}{\tau_{\text {rise }}}+ \delta\left(t_{0}-t\right), \end{aligned} $$ The alpha function is retrieved in the limit when both time constants are equal. ```python class DualExpSparseCOBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau_decay, tau_rise, E): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.DualExpon.desc(pre.num, tau_decay=tau_decay, tau_rise=tau_rise), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.COBA(E=E), post=post, ) ``` ```python class SimpleNet4(bp.DynSysGroup): def __init__(self, E=0.): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = DualExpSparseCOBA(self.pre, self.post, delay=None, prob=1., g_max=1., tau_decay=5., tau_rise=1., E=E) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` ## Biophysical synapse models ### Limitations of phenomenological models 打开的数量是有限的，而且有饱和期 1. Saturation of postsynaptic receptors by previously released transmitter. 2. Certain receptor types also exhibit desensitization that prevents them (re)opening for a period after transmitter-binding, like sodium channels underlying action potential. ![image-20230826111443117](/BrainPy-course-notes/master_content/Notes.assets/image-20230826111443117.png) ### Linetic/Markov models ![image-20230826111733654](/BrainPy-course-notes/master_content/Notes.assets/image-20230826111733654.png) - The simplest kinetic model is a two-state scheme in which receptors can be either closed, 𝐶, or open, 𝑂, and the transition between states depends on transmitter concentration, [𝑇], in the synaptic cleft: - 𝛼 and 𝛽 are voltage-independent forward and backward rate constants. - 𝐶 and 𝑂 can range from 0 to 1, and describe the fraction of receptors in the closed and open states, respectively. - The synaptic conductance is: $g_{syn}(t)=\bar{g}_{max}g(t)$ ### AMPA/GABA$_A$ synapse model $$ \begin{aligned}\frac{ds}{dt}&amp;=\alpha[T](1-s)-\beta s\\I&amp;=\tilde{g}s(V-E)\end{aligned} $$ - 𝛼[𝑇] denotes the transition probability from state (1−𝑠) to state (𝑠) - 𝛽 represents the transition probability of the other direction - 𝐸 is a reverse potential, which can determine whether the direction of 𝐼 is inhibition or excitation. - 𝐸 = 0 𝑚𝑚𝑉𝑉 =&amp;gt; Excitatory synapse [AMPA] - 𝐸 = −80 𝑚𝑚𝑉𝑉 =&amp;gt; Inhibitory synapse [GABA A ] ### Comparison ![image-20230826111950713](/BrainPy-course-notes/master_content/Notes.assets/image-20230826111950713.png) ### NMDA synapse model ![image-20230826112027689](/BrainPy-course-notes/master_content/Notes.assets/image-20230826112027689.png) ![image-20230826112034481](/BrainPy-course-notes/master_content/Notes.assets/image-20230826112034481.png) $$ \begin{aligned} &amp;\frac{ds}{dt} =\alpha[T](1-s)-\beta s \\ &amp;I=\tilde{g}sB(V)(V-E) \\ &amp;B(V )=\frac{1}{1+\exp(-0.062V)[Mg^{2+}]_{o}/3.57} \end{aligned} $$ The magnesium block of the NMDA receptor channel is an extremely fast process compared to the other kinetics of the receptor (Jahr and Stevens 1990a, 1990b). The block can therefore be accurately modeled as an instantaneous function of voltage(Jahr and Stevens 1990b). where $[Mg^{2+}]$ is the external magnesium concentration (1 to 2mM inphysiological conditions) ## Programming of biophysical synapse models ### AMPA synapse model ```python class AMPA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, E=0.): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.AMPA.desc(pre.num, alpha=0.98, beta=0.18, T=0.5, T_dur=0.5), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.COBA(E=E), post=post, ) ``` ```python class SimpleNet(bp.DynSysGroup): def __init__(self, syn_cls): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = syn_cls(self.pre, self.post, delay=None, prob=1., g_max=1.) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` ```python def run_a_net(net, duration=100): indices = np.arange(int(duration/bm.get_dt())) # duration ms conductances, currents, potentials = bm.for_loop(net.step_run, indices, progress_bar=True) ts = indices * bm.get_dt() # --- similar to: # runner = bp.DSRunner(net) # conductances, currents, potentials = runner.run(100.) fig, gs = bp.visualize.get_figure(1, 3, 3.5, 4) fig.add_subplot(gs[0, 0]) plt.plot(ts, conductances) plt.title(&apos;Syn conductance&apos;) fig.add_subplot(gs[0, 1]) plt.plot(ts, currents) plt.title(&apos;Syn current&apos;) fig.add_subplot(gs[0, 2]) plt.plot(ts, potentials) plt.title(&apos;Post V&apos;) plt.show() ``` ### $\text{GABA}_A$ synapse model ```python class GABAa(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, E=-80.): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.GABAa.desc(pre.num, alpha=0.53, beta=0.18, T=1.0, T_dur=1.0), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.COBA(E=E), post=post, ) ``` ```python run_a_net(SimpleNet(syn_cls=GABAa)) ``` ### NMDA synapse model ```python class NMDA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, E=0.0): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.NMDA.desc(pre.num, a=0.5, tau_decay=100., tau_rise=2.), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.MgBlock(E=E), post=post, ) ``` ```python run_a_net(SimpleNet(NMDA)) ``` ### Kinetic synapse models are more realistic ```python class SimpleNet5(bp.DynSysGroup): def __init__(self, freqs=10.): super().__init__() self.pre = bp.dyn.PoissonGroup(1, freqs=freqs) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = NMDA(self.pre, self.post, delay=None, prob=1., g_max=1., E=0.) def update(self): self.pre() self.syn() self.post() # monitor the following variables return self.syn.proj.refs[&apos;syn&apos;].g, self.post.V ``` ```python def compare_freqs(freqs): fig, _ = bp.visualize.get_figure(1, 1, 4.5, 6.) for freq in freqs: net = SimpleNet5(freqs=freq) indices = np.arange(1000) # 100 ms conductances, potentials = bm.for_loop(net.step_run, indices, progress_bar=True) ts = indices * bm.get_dt() plt.plot(ts, conductances, label=f&apos;{freq} Hz&apos;) plt.legend() plt.ylabel(&apos;g&apos;) plt.show() ``` ```python compare_freqs([10., 100., 1000., 10000.]) ``` ### How to customize a synapse #### Preparations `ProjAlignPostMg2` and `ProjAlignPreMg2` #### Exponential Model ```python class Exponen(bp.dyn.SynDyn, bp.mixin.AlignPost): def __init__(self, size, tau): super().__init__(size) # parameters self.tau = tau # variables self.g = bm.Variable(bm.zeros(self.num)) # integral self.integral = bp.odeint(lambda g, t: -g/tau, method=&apos;exp_auto&apos;) def update(self, pre_spike=None): self.g.value = self.integral(g=self.g.value, t=bp.share[&apos;t&apos;], dt=bp.share[&apos;dt&apos;]) if pre_spike is not None: self.add_current(pre_spike) return self.g.value def add_current(self, x): # specical for bp.mixin.AlignPost self.g += x def return_info(self): return self.g ``` #### AMPA Model ```python class AMPA(bp.dyn.SynDyn): def __init__(self, size, alpha= 0.98, beta=0.18, T=0.5, T_dur=0.5): super().__init__(size=size) # parameters self.alpha = alpha self.beta = beta self.T = T self.T_duration = T_dur # functions self.integral = bp.odeint(method=&apos;exp_auto&apos;, f=self.dg) # variables self.g = bm.Variable(bm.zeros(self.num)) self.spike_arrival_time = bm.Variable(bm.ones(self.num) * -1e7) def dg(self, g, t, TT): return self.alpha * TT * (1 - g) - self.beta * g def update(self, pre_spike): self.spike_arrival_time.value = bm.where(pre_spike, bp.share[&apos;t&apos;], self.spike_arrival_time) TT = ((bp.share[&apos;t&apos;] - self.spike_arrival_time) 做时间平均 STP based on firing rate $$ \begin{gathered} \frac{du(t)}{dt}=\frac{-u(t)}{\tau_{f}}+U_{sE}(1-u^{-})\delta\big(t-t_{sp}\big), \\ \frac{dx(t)}{dt}=\frac{1-x(t)}{\tau_{d}}-u^{+}x^{-}\delta\big(t-t_{sp}\big), \\ \frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{s}}+Au^{+}x^{-}\delta\big(t-t_{sp}\big), \\ u^{+}=\lim_{t-t_{sp\rightarrow0^{+}}}u(t), \end{gathered} $$ ![image-20230826155016657](/BrainPy-course-notes/master_content/Notes.assets/image-20230826155016657.png) 丢掉时间变化的具体细节，抓住了重要趋势 ### Theoretical analysis of the rate model Suppose the pre-synaptic firing rate keeps as constant, we can calculate the stationary response $$ u_{st}=\frac{U_{SE}R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}},\quad u_{st}^{+}=U_{SE}\frac{1+R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}},\quad x_{st}=\frac{1}{1+u_{st}^{+}\tau_{d}R_{0}}, $$ $$ EPSC_{st}=Au_{st}^{+}x_{st}=A\frac{u_{st}^{+}}{1+u_{st}^{+}\tau_{d}R_{0}},\quad PSV_{st}\propto g_{st}=\tau_{s}Au_{st}^{+}x_{st}R_{0}=A\frac{u_{st}^{+}R_{0}}{1+u_{st}^{+}\tau_{d}R_{0}}, $$ ![image-20230826155234134](/BrainPy-course-notes/master_content/Notes.assets/image-20230826155234134.png) ### Frequency-dependent Gain control of spike information $$ \begin{gathered} u_{st}^{+}=U_{SE}\frac{1+R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}}, \\ x_{st}=\frac{1}{1+u_{st}^{+}\tau_{d}R_{0}}, \\ EPSC_{st}=Au_{st}^{+}x_{st}=A\frac{u_{st}^{+}}{1+u_{st}^{+}\tau_{d}R_{0}}, \end{gathered} $$ Peak frequency: $\theta\sim\frac{1}{\sqrt{U\tau_{f}\tau_{d}}}$ ### Simulation of Frequency-dependent Gain control ![image-20230826155715445](/BrainPy-course-notes/master_content/Notes.assets/image-20230826155715445.png) ## Effects on network dynamics ### STP modeling Working memory ![image-20230826160102332](/BrainPy-course-notes/master_content/Notes.assets/image-20230826160102332.png) ![image-20230826160113456](/BrainPy-course-notes/master_content/Notes.assets/image-20230826160113456.png) # E-I Balanced Neural Network ## Irregular Spiking of Neurons ### Signal process of single neuron External Stimulus -&amp;gt; Single neuron model $$ \begin{aligned}\tau&amp;\frac{\mathrm{d}V}{\mathrm{d}t}=-(V-V_\text{rest })+RI(t)\\\\&amp;\text{if}V&amp;gt;V_\text{th},\quad V\leftarrow V_\text{reset }\text{last}t_\text{ref}\end{aligned} $$ -&amp;gt; ... -&amp;gt; Perception or action 真正的神经元并不是LIF model的输出 ![image-20230827100647851](/BrainPy-course-notes/master_content/Notes.assets/image-20230827100647851.png) Simulation ![image-20230827100706310](/BrainPy-course-notes/master_content/Notes.assets/image-20230827100706310.png) Neuron recorded in vivo ### Irregular Spiking of Neurons ![image-20230827092807270](/BrainPy-course-notes/master_content/Notes.assets/image-20230827092807270.png) #### Statistical Description of Spikes 用以下的变量来进行统计描述 - Firing Rate Rate = average over time(single neuron, single run) Spike count $v=\frac{n_{sp}}{T}$ - ISI(Interspike interval distributions) average ISI $\overline{\Delta t}=\frac{1}{n_{sp}-1}\sum_{i=1}^{n_{sp}-1}\Delta t_{i}$ standard deviation ISI: $\sigma_{\Delta t}^{2}=\frac{1}{n_{sp}-1}\sum_{i=1}^{n_{sp}-1}(\Delta t_{i}-\overline{\Delta t})^{2}$ - $C_V$(Coefficient of variation, Fano factor) **窄还是宽的分布** 信息表征有多强的不稳定性 $C_{V}=\sigma_{\Delta t}^{2}/\overline{\Delta t}$ #### Poisson Process In probability theory and statistics, the Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known **constant mean rate** and **independently** of the time since the last event。 $$ \begin{aligned} &amp;P(X=k\mathrm{~events~in~interval~}t)=e^{-rt}\frac{(rt)^{k}}{k!} \\ &amp;\mathrm{mean:}\quad\overline{X}=rt \\ &amp;\mathrm{variance}:\quad\sigma^{2}=rt\\ &amp;\mathrm{Fano factor:}\quad\frac{\sigma^{2}}{X}=1 \end{aligned} $$ Fano factor -&amp;gt; noise-to-signal ratio #### Irregular Spiking of Neurons LIF在单个神经元的情况下是基本没有太大问题的，在整个网络中会受网络信息调控 ![image-20230827093614607](/BrainPy-course-notes/master_content/Notes.assets/image-20230827093614607.png) #### Why Irregular? - 不完全是input影响的 - 不能简单来衡量 On average, a cortical neuron receives inputs from 1000~10000 connected neurons. -&amp;gt; averaged noise ~ 0 ## E-I Balanced Network $$ \begin{gathered} \tau\frac{du_{i}^{E}}{dt}=-u_{i}^{E}+\sum_{j=1}^{K_{E}}J_{EE}r_{j}^{E}+\sum_{j=1}^{K_{I}}J_{EI}r_{j}^{I}+I_{i}^{E} \\ \tau\frac{du_{i}^{I}}{dt}=-u_{i}^{I}+\sum_{j=1}^{K_{I}}J_{II}r_{j}^{I}+\sum_{j=1}^{K_{E}}J_{IE}r_{j}^{E}+I_{i}^{I} \end{gathered} $$ ![image-20230827093708220](/BrainPy-course-notes/master_content/Notes.assets/image-20230827093708220.png) Sparse &amp; random connections:$1\ll K_{\mathrm{E}},K_{1}\ll N_{\mathrm{E}},N_{\mathrm{I}}$ . Neurons fire largely independently to each other. $$ \begin{gathered} \text{Single neuron fires irregularly } r_j^E, r_j^{\prime} \text{with mean rate } \mu \text{and variance } \sigma^2.\\ \text{The mean of recurrent input received by E neuron:} \\ \sim K_{E}J_{EE}\mu-K_{I}J_{EI}\mu \\ \text{The variance of recurrent input received by E neuron:} \\ \sim K_{E}(J_{EE})^{2}\sigma^{2}+K_{I}(J_{EI})^{2}\sigma^{2} \\ \begin{gathered} \\ \text{The balanced condition:} \\ K_{E}J_{EE}-K_{l}J_{El}{\sim}0(1) \\ J_{EE}=\frac{1}{\sqrt{K_{E}}},J_{EI}=\frac{1}{\sqrt{K_{I}}},K_{E}(J_{EE})^{2}\sigma^{2}+K_{I}(J_{EI})^{2}\sigma^{2}\sim O(1) \end{gathered} \end{gathered} $$ $$ \begin{aligned}\frac{I_E}{I_I}&amp;&amp;gt;\frac{J_E}{J_I}&amp;&amp;gt;1\\\\J_E&amp;&amp;gt;1\\\\\text{r not too big}\end{aligned} $$ $$ \overline{I_a}=\overline{F_a}+\overline{R_a}=\sqrt{N}(f_a\mu_0+w_{aE}r_E+w_{aI}r_I),\quad a=E,I,\\ \begin{gathered} w_{ab}~=~p_{ab}j_{ab}q_{b} \\ J_{ij}^{ab}~=~j_{ab}/\sqrt{N}; \\ \frac{f_{E}}{f_{I}}&amp;gt;\frac{w_{EI}}{w_{II}}&amp;gt;\frac{w_{EE}}{w_{IE}}. \end{gathered} $$ ## BrainPy Simulation ### Simulation LIF neuron 4000 (E/I=4/1, P=0.02) 𝜏 = 20 ms 𝑉𝑟𝑒𝑠𝑡 = -60 mV Spiking threshold: -50 mV Refractory period: 5 ms $$ \begin{gathered} \tau\frac{dV}{dt}=(V_{\mathrm{rest}}-V)+I \\ I=g_{exc}(E_{exc}-V)+g_{inh}(E_{inh}-V)+I_{\mathrm{ext}} \end{gathered} \ \ \ \ \ \ \begin{aligned}\tau_{exc}&amp;\frac{dg_{exc}}{dt}=-g_{exc}\\\tau_{inh}&amp;\frac{dg_{inh}}{dt}=-g_{inh}\end{aligned} $$ $$ \begin{array}{l}E_\mathrm{exc}=0\text{mV}\mathrm{and}E_\mathrm{inh}=-80\text{mV},I_\mathrm{ext}=20.\\\tau_\mathrm{exc}=5\text{ ms},\tau_\mathrm{inh}=10\text{ ms},\Delta g_\mathrm{exc}=0.6\text{ and}\Delta g_\mathrm{inh}=6.7.\end{array} $$ ![image-20230827094502860](/BrainPy-course-notes/master_content/Notes.assets/image-20230827094502860.png) ### Synaptic Computation ```python # 基于 align post Exponential synaptic computation class Exponential(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E, label=None): super().__init__() self.pron = bp.dyn.ProjAlignPost2( pre=pre, delay=delay, comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), # 随机连接 syn=bp.dyn.Expon(size=post.num, tau=tau), # Exponential synapse out=bp.dyn.COBA(E=E), # COBA network post=post, out_label=label ) ``` ### E-I Balanced Network ```python # 构建 E-I Balanced Network class EINet(bp.DynamicalSystem): def __init__(self, ne=3200, ni=800): super().__init__() # bp.neurons.LIF() self.E = bp.dyn.LifRef(ne, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Normal(-55., 2.)) self.I = bp.dyn.LifRef(ni, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Normal(-55., 2.)) #### E2E, E2I, I2E, I2I Exponential synaptic computation # delay=0, prob=0.02, g_max_E=0.6, g_max_I=6.7, tau_E=5, tau_I=10, # reversal potentials E_E=0, E_E=-80, label=EE,EI,IE,II self.E2E = Exponential(self.E, self.E, 0., 0.02, 0.6, 5., 0., &apos;EE&apos;) self.E2I = Exponential(self.E, self.I, 0., 0.02, 0.6, 5., 0., &apos;EI&apos;) self.I2E = Exponential(self.I, self.E, 0., 0.02, 6.7, 5., -80., &apos;IE&apos;) self.I2I = Exponential(self.I, self.I, 0., 0.02, 6.7, 5., -80., &apos;II&apos;) ``` ```python def update(self, inp=0.): # 更新突触传入电流 self.E2E() self.E2I() self.I2E() self.I2I() # 更新神经元群体 self.E(inp) self.I(inp) # 记录需要 monitor的变量 E_E_inp = self.E.sum_inputs(self.E.V, label=&apos;EE&apos;) #E2E的输入 I_E_inp = self.E.sum_inputs(self.E.V, label=&apos;IE&apos;) # I2E的输入 return self.E.spike, self.I.spike, E_E_inp, I_E_inp ``` ![image-20230827110737553](/BrainPy-course-notes/master_content/Notes.assets/image-20230827110737553.png) ![image-20230827110746410](/BrainPy-course-notes/master_content/Notes.assets/image-20230827110746410.png) ## Properties of E-I Balanced Network - Linear encoding External input strength is “linearly” encoded by the mean firing rate of the neural population - Fast Response The network responds rapidly to abrupt changes of the input ### Noise speeds up computation 快速相应的原理，均匀分布在阈值下面的空间 - A neural ensemble jointly encodes stimulus information; - Noise randomizes the distribution of neuronal membrane potentials; - Those neurons (red circle) whose potentials are close to the threshold will fire rapidly; - If the noisy environment is proper, even for a small input, a certain number of neurons will fire instantly to report the presence of a stimulus. ![image-20230827113451626](/BrainPy-course-notes/master_content/Notes.assets/image-20230827113451626.png) # Continuous Attractor Neural Network ## Attractor Models ### The concept of attractor dynamics Different types of attractors: Point attractors, Line attractors, Ring attractors, Plane attractors, Cyclic attractors, Chaotic attractors ![image-20230827140250173](/BrainPy-course-notes/master_content/Notes.assets/image-20230827140250173.png) 稳态，能量梯度吸引到attractor ### Discrete attractor Network Model: Hopfield Model $S_i=\pm1$: the neuronal state $W_{ij}$ : the neuronal connection The network dynamics: $$ S_{i}=\mathrm{sign}\bigg(\sum_{j}w_{ij}S_{j}-\theta\bigg),\quad\mathrm{sign}(x)=1,\mathrm{for}x&amp;gt;0;-1,\mathrm{otherwise} $$ Updating rule: synchronous or asynchronous Consider the network stores $p$ pattern, $\xi_{i}^{\mu},\mathrm{for}\mu=1,\ldots p;i=1,\ldots N$ Setting $w_{ij}=\frac{1}{N}\sum_{\mu=1}^{p}\xi_{i}^{\mu}\xi_{j}^{\mu}$ ![image-20230827140827784](/BrainPy-course-notes/master_content/Notes.assets/image-20230827140827784.png) #### Energy space of Hopfield network $$ \begin{aligned} &amp;\text{Energy function: }E=-\frac{1}{2}\sum_{i,j}w_{ij}S_{i}S_{j}+\theta\sum_{i}S_{i} \\ &amp;\mathrm{Consider}S_{i}\mathrm{~is~updated},S_{i}(t+1)=sign[\sum_{j}w_{ij}S_{j}(t)-\theta] \\ &amp;\Delta E=E(t+1)-E(t)\\ &amp;=-[S_{i}(t+1)-S_{i}(t)]\sum_{j}w_{ij}S_{j}(t)+\theta\left[S_{i}(t+1)-S_{i}(t)\right] \\ &amp;=-[S_{i}(t+1)-S_{i}(t)][\sum_{j}w_{ij}S_{j}(t)-\theta] \\ &amp;\leq0 \end{aligned} $$ 同样激活同样pattern的神经元，~吸引子 #### Auto-associative memory in Hopfield Network A partial/noisy input can retrieve the related memory pattern ![image-20230827141253326](/BrainPy-course-notes/master_content/Notes.assets/image-20230827141253326.png) #### Persistent activity in working memory After the removal of external input, the neurons in the network encoding the stimulus continue to fire persistently. ![image-20230827141421796](/BrainPy-course-notes/master_content/Notes.assets/image-20230827141421796.png) ## Continuous Attractor Neural Network ### Neural coding #### Low-dimensional continuous feature ![image-20230827142520189](/BrainPy-course-notes/master_content/Notes.assets/image-20230827142520189.png) #### Continuous Attractor neural network ![image-20230827142606695](/BrainPy-course-notes/master_content/Notes.assets/image-20230827142606695.png) ### CANN: A rate-based recurrent circuit model $$ \begin{aligned}\tau\frac{\partial U(x,t)}{\partial t}&amp;=-U(x,t)+\rho\int f(x,x&apos;)r(x&apos;,t)dx&apos;+l^{ext}(1)\\r(x,t)&amp;=\frac{U^2(x,t)}{1+k\rho\int U^2(x,t)dx}\quad(2)\\J(x,x&apos;)&amp;=\frac{J_0}{\sqrt{2\pi}a}\exp\left[-\frac{(x-x&apos;)^2}{2a^2}\right](3)\end{aligned} $$ r频率，J强度，U decay #### A Continuous family of attractor states 做平移的改变，变化会被保留，line attractor，受到编码连续刺激 ![image-20230827143707784](/BrainPy-course-notes/master_content/Notes.assets/image-20230827143707784.png) #### Stability analysis derive continuous attractor dynamics 只需要看在原始状态加入一个小量项，再代入回 Consider small fluctuations around a stationary state at z: Projecting $\delta U$ on the $i$th right eigenvector of $F(\delta U)_i(t)=(\delta U)_i(0)e^{-(1-\lambda _i)t/\tau}$ Two cases: - If $\lambda _i ## Computation with CANN ### Persistent activity for working memory When the global inhibition is not too strong, the network spontaneously hold bump activity: $$ k\frac{\tau}{\tau _v}$, Travelling wave ![image-20230827150543244](/BrainPy-course-notes/master_content/Notes.assets/image-20230827150543244.png) #### Levy flights vs. Brownian motion ![image-20230827150851309](/BrainPy-course-notes/master_content/Notes.assets/image-20230827150851309.png) #### Lévy flights in ecology and human cogniDve behaviors 生物学大多运动服从levy flights ### Noisy adaptation generates Levy flight in CANN ![image-20230827151343126](/BrainPy-course-notes/master_content/Notes.assets/image-20230827151343126.png) ### Time Delay in Neural Signal Transmission ![image-20230827151622032](/BrainPy-course-notes/master_content/Notes.assets/image-20230827151622032.png) ### Anticipatory Head Direction Signals in Anterior Thalamus 有预测策略，实现抵消信息传递的delay CANN加入负反馈机制是可以实现预测的 ![image-20230827152731895](/BrainPy-course-notes/master_content/Notes.assets/image-20230827152731895.png) ### CANN with STP $$ \begin{gathered} \tau{\frac{\mathrm{d}U(x,t)}{\mathrm{d}t}} {\cal O}=-U(x,t)+\rho\int g^{+}(x)h(x^{\prime},t)J(x,x^{\prime})r(x^{\prime},t)dx^{\prime}+I^{ext}(x,t)(1) \\ \frac{dg(x,t)}{dt}=-\frac{g(x,t)}{\tau_{f}}+G(1-g^{-}(x))r(x^{\prime},t)\quad(2) \\ \frac{dh(x,t)}{dt}=\frac{1-h(x,t)}{\tau_{d}}-g^{+}(x)h(x,t)r(x^{\prime},t)\quad(3) \\ r(x,t)={\frac{U^{2}(x,t)}{1+k\rho\int U^{2}(x,t)dx}}\quad(4) \end{gathered} $$ ## Programming in BrainPy ### Customize a ring CANN in brainpy In simulations, we can not simulate a CANN encoding features ranging $(-\inf, \inf)$. Instead, we simulate a ring attractor network which encodes features ranging $(-\pi, \pi)$. Note that the distance on a ring should be: $$ dist_{ring}(x,x&apos;) = min(|x-x&apos;|,2\pi-|x-x&apos;|) $$ ![Image Name](https://cdn.kesci.com/upload/s01apgi89t.png?imageView2/0/w/320/h/320) ```python class CANN1D(bp.NeuGroupNS): def __init__(self, num, tau=1., k=8.1, a=0.5, A=10., J0=4., z_min=-bm.pi, z_max=bm.pi, **kwargs): super(CANN1D, self).__init__(size=num, **kwargs) # 初始化参数 self.tau = tau self.k = k self.a = a self.A = A self.J0 = J0 # 初始化特征空间相关参数 self.z_min = z_min self.z_max = z_max self.z_range = z_max - z_min self.x = bm.linspace(z_min, z_max, num) self.rho = num / self.z_range self.dx = self.z_range / num # 初始化变量 self.u = bm.Variable(bm.zeros(num)) self.input = bm.Variable(bm.zeros(num)) self.conn_mat = self.make_conn(self.x) # 连接矩阵 # 定义积分函数 self.integral = bp.odeint(self.derivative) # 微分方程 @property def derivative(self): du = lambda u, t, Irec, Iext: (-u + Irec + Iext) / self.tau return du # 将距离转换到[-z_range/2, z_range/2)之间 def dist(self, d): d = bm.remainder(d, self.z_range) d = bm.where(d &amp;gt; 0.5 * self.z_range, d - self.z_range, d) return d # 计算连接矩阵 def make_conn(self, x): assert bm.ndim(x) == 1 d = self.dist(x - x[:, None]) # 距离矩阵 Jxx = self.J0 * bm.exp( -0.5 * bm.square(d / self.a)) / (bm.sqrt(2 * bm.pi) * self.a) return Jxx # 获取各个神经元到pos处神经元的输入 def get_stimulus_by_pos(self, pos): return self.A * bm.exp(-0.25 * bm.square(self.dist(self.x - pos) / self.a)) def update(self, x=None): _t = bp.share[&apos;t&apos;] u2 = bm.square(self.u) r = u2 / (1.0 + self.k * bm.sum(u2)) Irec = bm.dot(self.conn_mat, r) self.u[:] = self.integral(self.u, _t,Irec, self.input) self.input[:] = 0. # 重置外部电流 ``` ### Simulate the persistent activity of CANN after the removal of external input ```python def Persistent_Activity(k=0.1,J0=1.): # 生成CANN cann = CANN1D(num=512, k=k,J0=J0) # 生成外部刺激，从第2到12ms，持续10ms dur1, dur2, dur3 = 2., 10., 10. I1 = cann.get_stimulus_by_pos(0.) Iext, duration = bp.inputs.section_input(values=[0., I1, 0.], durations=[dur1, dur2, dur3], return_length=True) noise_level = 0.1 noise = bm.random.normal(0., noise_level, (int(duration / bm.get_dt()), len(I1))) Iext += noise # 运行数值模拟 runner = bp.DSRunner(cann, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;]) runner.run(duration) # 可视化 def plot_response(t): fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann.x, I, label=&apos;Iext&apos;) ax.plot(cann.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() # plt.savefig(f&apos;CANN_t={t}.pdf&apos;, transparent=True, dpi=500) plot_response(t=10.) plot_response(t=20.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=1, frame_delay=40, show=True, ) plt.show() Persistent_Activity(k=0.1) ``` ### Simulate the tracking behavior of CANN ```python def smooth_tracking(): cann = CANN1D(num=512, k=8.1) # 定义随时间变化的外部刺激 v_ext = 1e-3 dur1, dur2, dur3 = 10., 10., 20 num1 = int(dur1 / bm.get_dt()) num2 = int(dur2 / bm.get_dt()) num3 = int(dur3 / bm.get_dt()) position = bm.zeros(num1 + num2 + num3) position[num1: num1 + num2] = bm.linspace(0., 1.5 * bm.pi, num2) position[num1 + num2: ] = 1.5 * bm.pi position = position.reshape((-1, 1)) Iext = cann.get_stimulus_by_pos(position) # 运行模拟 runner = bp.DSRunner(cann, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;]) runner.run(dur1 + dur2 + dur3) # 可视化 def plot_response(t, extra_fun=None): fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann.x, I, label=&apos;Iext&apos;) ax.plot(cann.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() if extra_fun: extra_fun() # plt.savefig(f&apos;CANN_tracking_t={t}.pdf&apos;, transparent=True, dpi=500) plot_response(t=10.) def f(): plt.annotate(&apos;&apos;, xy=(1.5, 10), xytext=(0.5, 10), arrowprops=dict(arrowstyle=&quot;-&amp;gt;&quot;)) plot_response(t=15., extra_fun=f) def f(): plt.annotate(&apos;&apos;, xy=(-2, 10), xytext=(-3, 10), arrowprops=dict(arrowstyle=&quot;-&amp;gt;&quot;)) plot_response(t=20., extra_fun=f) plot_response(t=30.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=5, frame_delay=50, show=True, ) plt.show() smooth_tracking() ``` ### Customize a CANN with SFASimulate the spontaneous traveling wave ```python class CANN1D_SFA(bp.NeuGroupNS): def __init__(self, num, m = 0.1, tau=1., tau_v=10., k=8.1, a=0.5, A=10., J0=4., z_min=-bm.pi, z_max=bm.pi, **kwargs): super(CANN1D_SFA, self).__init__(size=num, **kwargs) # 初始化参数 self.tau = tau self.tau_v = tau_v #time constant of SFA self.k = k self.a = a self.A = A self.J0 = J0 self.m = m #SFA strength # 初始化特征空间相关参数 self.z_min = z_min self.z_max = z_max self.z_range = z_max - z_min self.x = bm.linspace(z_min, z_max, num) self.rho = num / self.z_range self.dx = self.z_range / num # 初始化变量 self.u = bm.Variable(bm.zeros(num)) self.v = bm.Variable(bm.zeros(num)) #SFA current self.input = bm.Variable(bm.zeros(num)) self.conn_mat = self.make_conn(self.x) # 连接矩阵 # 定义积分函数 self.integral = bp.odeint(self.derivative) # 微分方程 @property def derivative(self): du = lambda u, t, v, Irec, Iext: (-u + Irec + Iext-v) / self.tau dv = lambda v, t, u: (-v + self.m*u) / self.tau_v return bp.JointEq([du, dv]) # 将距离转换到[-z_range/2, z_range/2)之间 def dist(self, d): d = bm.remainder(d, self.z_range) d = bm.where(d &amp;gt; 0.5 * self.z_range, d - self.z_range, d) return d # 计算连接矩阵 def make_conn(self, x): assert bm.ndim(x) == 1 d = self.dist(x - x[:, None]) # 距离矩阵 Jxx = self.J0 * bm.exp( -0.5 * bm.square(d / self.a)) / (bm.sqrt(2 * bm.pi) * self.a) return Jxx # 获取各个神经元到pos处神经元的输入 def get_stimulus_by_pos(self, pos): return self.A * bm.exp(-0.25 * bm.square(self.dist(self.x - pos) / self.a)) def update(self, x=None): u2 = bm.square(self.u) r = u2 / (1.0 + self.k * bm.sum(u2)) Irec = bm.dot(self.conn_mat, r) u, v = self.integral(self.u, self.v, bp.share[&apos;t&apos;],Irec, self.input) self.u[:] = bm.where(u&amp;gt;0,u,0) self.v[:] = v self.input[:] = 0. # 重置外部电流 ``` ### Simulate the spontaneous traveling wave ```python def traveling_wave(num=512,m=0.1,k=0.1): # 生成CANN cann_sfa = CANN1D_SFA(num=num, m=m,k=k) # 生成外部刺激 dur = 1000. noise_level = 0.1 Iext = bm.random.normal(0., noise_level, (int(dur / bm.get_dt()), num)) duration = dur # 运行数值模拟 runner = bp.DSRunner(cann_sfa, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;]) runner.run(duration) # 可视化 def plot_response(t): fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann_sfa.x, I, label=&apos;Iext&apos;) ax.plot(cann_sfa.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() # plt.savefig(f&apos;CANN_t={t}.pdf&apos;, transparent=True, dpi=500) plot_response(t=100.) plot_response(t=150.) plot_response(t=200.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann_sfa.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann_sfa.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=1, frame_delay=40, show=True, ) plt.show() traveling_wave(num=512,m=0.5,k=0.1) ``` ### Simulate the anticipative tracking ```python def anticipative_tracking(m=10,v_ext=6*1e-3): cann_sfa = CANN1D_SFA(num=512, m=m) # 定义随时间变化的外部刺激 v_ext = v_ext dur1, dur2, = 10., 1000. num1 = int(dur1 / bm.get_dt()) num2 = int(dur2 / bm.get_dt()) position = np.zeros(num1 + num2) for i in range(num2): pos = position[i+num1-1]+v_ext*bm.dt # the periodical boundary pos = np.where(pos&amp;gt;np.pi, pos-2*np.pi, pos) pos = np.where(pos 0.5 * self.z_range, d - self.z_range, d) return d # 计算连接矩阵 def make_conn(self, x): assert bm.ndim(x) == 1 d = self.dist(x - x[:, None]) # 距离矩阵 Jxx = self.J0 * bm.exp( -0.5 * bm.square(d / self.a)) / (bm.sqrt(2 * bm.pi) * self.a) return Jxx # 获取各个神经元到pos处神经元的输入 def get_stimulus_by_pos(self, pos): return self.A * bm.exp(-0.25 * bm.square(self.dist(self.x - pos) / self.a)) def update(self, x=None): u2 = bm.square(self.u) r = u2 / (1.0 + self.k * bm.sum(u2)) Irec = bm.dot(self.conn_mat, (self.g + self.G * (1 - self.g))*self.h*r) u, g, h = self.integral(u=self.u, g=self.g, h=self.h, t=bp.share[&apos;t&apos;], Irec=Irec, Iext=self.input, r=r, dt=bm.dt) self.u[:] = bm.where(u&amp;gt;0,u,0) self.g.value = g self.h.value = h self.input[:] = 0. # 重置外部电流 ``` ### Simulate traveling wave in CANN with STP ```python def traveling_wave_STP(num=512,k=0.1,J0=12.,tau_d=1000,tau_f=1.,G=0.9): # 生成CANN cann_stp = CANN1D_STP(num=num, k=k,tau_d=tau_d,tau_f=tau_f,G=G, J0=J0) # 生成外部刺激 dur = 1000. noise_level = 0.1 Iext = bm.random.normal(0., noise_level, (int(dur / bm.get_dt()), num)) duration = dur # 运行数值模拟 runner = bp.DSRunner(cann_stp, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;,&apos;g&apos;,&apos;h&apos;]) runner.run(duration) fig,ax = plt.subplots(figsize=(3,3)) u = bm.as_numpy(runner.mon.u) max_index = np.argmax(u[1000,:]) print(max_index) ax.plot(runner.mon.g[:,max_index],label=&apos;g&apos;) ax.plot(runner.mon.h[:,max_index],label=&apos;h&apos;) ax.legend() # 可视化 def plot_response(t): fig, gs = bp.visualize.get_figure(1, 1, 3, 3) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann_stp.x, I, label=&apos;Iext&apos;) ax.plot(cann_stp.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() plot_response(t=100.) plot_response(t=200.) plot_response(t=300.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann_stp.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann_stp.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=1, frame_delay=40, show=True, ) plt.show() traveling_wave_STP(G=0.5,tau_d=50) ``` # Decision-Making Network ## LIP -&amp;gt; Decision-Making ### Coherent motion task 判断随机点(大部分点)的运动朝向 ![image-20230828100425871](/BrainPy-course-notes/master_content/Notes.assets/image-20230828100425871.png) coherence影响任务的难度 0%难，100%简单 ![image-20230828100516123](/BrainPy-course-notes/master_content/Notes.assets/image-20230828100516123.png) 编码决策的响应，不是运动 ### Reaction Time vs. Fixed Duration coherence越高，反应时间越短 Fixed Duration多了Delay time ![image-20230828100658772](/BrainPy-course-notes/master_content/Notes.assets/image-20230828100658772.png) 实验设计纯粹把decision-making给提取出来 #### Effect of Difficulty coherence越大，反应时间是越短，single neuron很难做到这么短的decision-making，考虑要建模的因素 ![image-20230828101103008](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101103008.png) ![image-20230828101058059](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101058059.png) #### Response of MT Neurons 记录MT的神经元，对这种运动的朝向刺激进行编码 线性编码coherence运动强度的方向 做决策在它的下游 ![image-20230828101303674](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101303674.png) #### Response of LIP Neurons MT的下游找到LIP的神经元 爬升到一定高度再做选择 coherence与爬升的斜率也会有影响，任务越难，爬升斜率越小 ![image-20230828101609881](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101609881.png) ### Ramping-to-threshold(perfect integrator) Model $$ \begin{aligned}\frac{dR}{dt}=I_A-I_B+\text{noise},\quad R(t)&amp;=(I_A-I_B)t+\int_0^tdt\text{noise}.\\\tau_\text{network}&amp;=\infty!\end{aligned} $$ 两种选择积分求和做积累，等到阈值做决策 Accumulates information (evidence) -&amp;gt; Ramping 直接保存信息，没有特别好的生物对应 ## A Spiking Network of DM ### A cortical microcircuit model ![image-20230828103055151](Notes.assets/image-20230828103055151.png) A=Upward motion B=Downward motion 2-population excitatory neurons (integrate-and-fire neurons driven by Poisson input) Slow reverberatory excitation mediated by the NMDA receptors at recurrent synapses AMPA receptors ($\tau _{syn}=$1 - 3 ms) NMDA receptors ($\tau _{syn}=$ 50 - 100 ms). 两群神经元分别做不同的选择，与自己对方都有连接 NMDA 缓慢的信号使得有慢慢增长的ramping的过程 interneurons的backward有抑制作用 #### Coherence-Dependent Input 线性编码运动朝向的信息，coherence强度影响firing rate，一系列泊松过程，同时还有noise。 本身两种信息还是有差异 ![image-20230828104054275](/BrainPy-course-notes/master_content/Notes.assets/image-20230828104054275.png) #### Duality of this model 不同coherence的神经元响应 ![image-20230828104432061](/BrainPy-course-notes/master_content/Notes.assets/image-20230828104432061.png) 两个group会竞争，当有一个group达到20%，进入这个窗口，就会直接发放上去 Spontaneous symmetry breaking and stochastic decision making ![image-20230828104600840](/BrainPy-course-notes/master_content/Notes.assets/image-20230828104600840.png) ## Simulation of Spiking DM ### A Cortical Microcircuit Model 用两个coherence生成出来的序列 ![image-20230828110300576](/BrainPy-course-notes/master_content/Notes.assets/image-20230828110300576.png) $$ \begin{gathered}C_m\frac{dV(t)}{dt}=-g_L(V(t)-V_L)-I_{syn}(t)\\I_{syn}(t)=I_{\mathrm{ext},\mathrm{AMPA}}\left(t\right)+I_{\mathrm{rec},AMPA}(t)+I_{\mathrm{rec},NMDA}(t)+I_{\mathrm{rec},\mathrm{GABA}}(t)\end{gathered} $$ $$ \begin{gathered} I_{\mathrm{ext},\mathrm{AMPA}}\left(t\right)=g_{\mathrm{ext},\mathrm{AMPA}}\left(V(t)-V_{E}\right)s^{\mathrm{ext},\mathrm{AMPA}}\left(t\right) \\ I_{\mathrm{rec},\mathrm{AMP}\Lambda}\left(t\right)=g_{\mathrm{rec},\mathrm{AMP}\Lambda}\left(V(t)-V_{E}\right)\sum_{j=1}^{Ce}w_{j}s_{j}^{AMPA}(t) \\ I_{\mathrm{rec},\mathrm{NMDA}}\left(t\right)=\frac{g_{\mathrm{NMDA}}(V(t)-V_{E})}{\left(1+\left[\mathrm{Mg}^{2+}\right]\exp(-0.062V(t))/3.57\right)}\sum_{j=1}^{\mathrm{C_E}}w_{j}s_{j}^{\mathrm{NMDA}}\left(t\right) \\ I_\mathrm{rec,GABA}(t)=g_\mathrm{GABA}(V(t)-V_l)\sum_{j=1}^{C_1}s_j^\mathrm{GABA}(t) \end{gathered} $$ $$ w_j=\left\{\begin{matrix}w_+&amp;gt;1,\\w_-E/I conn self.I2B = AMPA(self.I, self.B, &apos;all2all&apos;, 0.5, g_I2E_GABAa, tau=5., E=-70.) self.I2A = AMPA(self.I, self.A, &apos;all2all&apos;, 0.5, g_I2E_GABAa, tau=5., E=-70.) self.I2N = AMPA(self.I, self.N, &apos;all2all&apos;, 0.5, g_I2E_GABAa, tau=5., E=-70.) self.I2I = AMPA(self.I, self.I, &apos;all2all&apos;, 0.5, g_I2I_GABAa, tau=5., E=-70.) # define external projections #### TO DO!!!! self.noise2B = AMPA(self.noise_B, self.B, &apos;one2one&apos;, None, g_ext2E_AMPA, tau=2., E=0.) self.noise2A = AMPA(self.noise_A, self.A, &apos;one2one&apos;, None, g_ext2E_AMPA, tau=2., E=0.) self.noise2N = AMPA(self.noise_N, self.N, &apos;one2one&apos;, None, g_ext2E_AMPA, tau=2., E=0.) self.noise2I = AMPA(self.noise_I, self.I, &apos;one2one&apos;, None, g_ext2I_AMPA, tau=2., E=0.) ``` ```python class Tool: def __init__(self, pre_stimulus_period=100., stimulus_period=1000., delay_period=500.): self.pre_stimulus_period = pre_stimulus_period self.stimulus_period = stimulus_period self.delay_period = delay_period self.freq_variance = 10. self.freq_interval = 50. self.total_period = pre_stimulus_period + stimulus_period + delay_period def generate_freqs(self, mean): # stimulus period n_stim = int(self.stimulus_period / self.freq_interval) n_interval = int(self.freq_interval / bm.get_dt()) freqs_stim = np.random.normal(mean, self.freq_variance, (n_stim, 1)) freqs_stim = np.tile(freqs_stim, (1, n_interval)).flatten() # pre stimulus period freqs_pre = np.zeros(int(self.pre_stimulus_period / bm.get_dt())) # post stimulus period freqs_delay = np.zeros(int(self.delay_period / bm.get_dt())) all_freqs = np.concatenate([freqs_pre, freqs_stim, freqs_delay], axis=0) return bm.asarray(all_freqs) def visualize_results(self, mon, IA_freqs, IB_freqs, t_start=0., title=None): fig, gs = bp.visualize.get_figure(4, 1, 3, 10) axes = [fig.add_subplot(gs[i, 0]) for i in range(4)] ax = axes[0] bp.visualize.raster_plot(mon[&apos;ts&apos;], mon[&apos;A.spike&apos;], markersize=1, ax=ax) if title: ax.set_title(title) ax.set_ylabel(&quot;Group A&quot;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax = axes[1] bp.visualize.raster_plot(mon[&apos;ts&apos;], mon[&apos;B.spike&apos;], markersize=1, ax=ax) ax.set_ylabel(&quot;Group B&quot;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax = axes[2] rateA = bp.measure.firing_rate(mon[&apos;A.spike&apos;], width=10.) rateB = bp.measure.firing_rate(mon[&apos;B.spike&apos;], width=10.) ax.plot(mon[&apos;ts&apos;], rateA, label=&quot;Group A&quot;) ax.plot(mon[&apos;ts&apos;], rateB, label=&quot;Group B&quot;) ax.set_ylabel(&apos;Population activity [Hz]&apos;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax.legend() ax = axes[3] ax.plot(mon[&apos;ts&apos;], IA_freqs, label=&quot;group A&quot;) ax.plot(mon[&apos;ts&apos;], IB_freqs, label=&quot;group B&quot;) ax.set_ylabel(&quot;Input activity [Hz]&quot;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax.legend() ax.set_xlabel(&quot;Time [ms]&quot;) plt.show() ``` ```python tool = Tool() net = DecisionMakingNet() mu0 = 40. coherence = 25.6 IA_freqs = tool.generate_freqs(mu0 + mu0 / 100. * coherence) IB_freqs = tool.generate_freqs(mu0 - mu0 / 100. * coherence) def give_input(): i = bp.share[&apos;i&apos;] net.IA.freqs[0] = IA_freqs[i] net.IB.freqs[0] = IB_freqs[i] runner = bp.DSRunner(net, inputs=give_input, monitors=[&apos;A.spike&apos;, &apos;B.spike&apos;]) runner.run(tool.total_period) tool.visualize_results(runner.mon, IA_freqs, IB_freqs) ``` ### Results ![image-20230828112245950](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112245950.png) #### Stochastic Decision Making ![image-20230828112253619](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112253619.png) ## A Rate Network of DM ### Reduced Model 化简到只有两群神经元，只接受外界输入信号，互相影响对方 ![image-20230828112326267](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112326267.png) Synaptic variables $$ \begin{gathered} \frac{dS_{1}}{dt} =F(x_1)\gamma(1-S_1)-S_1/\tau_s \\ \frac{dS_2}{dt} =F(x_2)\gamma(1-S_2)-S_2/\tau_s \end{gathered} $$ Input current to each population $$ \begin{gathered} x_{1} =J_{E}S_{1}+J_{I}S_{2}+I_{0}+I_{noise1}+J_{\text{ext }\mu_{1}} \\ x_{2} =J_{E}S_{2}+J_{I}S_{1}+I_{0}+I_{noise2}+J_{\mathrm{ext}}\mu_{2} \end{gathered} $$ Background input $$ I_0+I_{noise}\\ \begin{gathered} dI_{noise1} =-I_{noise1}\frac{dt}{\tau_{0}}+\sigma dW \\ dI_{noise2} =-I_{noise2}\frac{dt}{\tau_{0}}+\sigma dW \end{gathered} $$ Firing rates $$ r_i=F(x_i)=\frac{ax_i-b}{1-\exp(-d(ax_i-b))} $$ Coherence-dependent inputs $$ \begin{array}{l}\mu_1=\mu_0\big(1+c&apos;/100\big)\\\mu_2=\mu_0\big(1-c&apos;/100\big)\end{array} $$ $$ \begin{aligned}&amp;\gamma,a,b,d,J_E,J_I,J_{\mathrm{ext}},I_0,\mu_0,\tau_{\mathrm{AMPA}},\sigma_{\mathrm{noise}}\\&amp;\text{are fixed parameters.}\end{aligned} $$ ```python class DecisionMakingRateModel(bp.dyn.NeuGroup): def __init__(self, size, coherence, JE=0.2609, JI=0.0497, Jext=5.2e-4, I0=0.3255, gamma=6.41e-4, tau=100., tau_n=2., sigma_n=0.02, a=270., b=108., d=0.154, noise_freq=2400., method=&apos;exp_auto&apos;, **kwargs): super(DecisionMakingRateModel, self).__init__(size, **kwargs) # 初始化参数 self.coherence = coherence self.JE = JE self.JI = JI self.Jext = Jext self.I0 = I0 self.gamma = gamma self.tau = tau self.tau_n = tau_n self.sigma_n = sigma_n self.a = a self.b = b self.d = d # 初始化变量 self.s1 = bm.Variable(bm.zeros(self.num) + 0.15) self.s2 = bm.Variable(bm.zeros(self.num) + 0.15) self.r1 = bm.Variable(bm.zeros(self.num)) self.r2 = bm.Variable(bm.zeros(self.num)) self.mu0 = bm.Variable(bm.zeros(self.num)) self.I1_noise = bm.Variable(bm.zeros(self.num)) self.I2_noise = bm.Variable(bm.zeros(self.num)) # 噪声输入的神经元 self.noise1 = bp.dyn.PoissonGroup(self.num, freqs=noise_freq) self.noise2 = bp.dyn.PoissonGroup(self.num, freqs=noise_freq) # 定义积分函数 self.integral = bp.odeint(self.derivative, method=method) @property def derivative(self): return bp.JointEq([self.ds1, self.ds2, self.dI1noise, self.dI2noise]) def ds1(self, s1, t, s2, mu0): I1 = self.Jext * mu0 * (1. + self.coherence / 100.) x1 = self.JE * s1 - self.JI * s2 + self.I0 + I1 + self.I1_noise r1 = (self.a * x1 - self.b) / (1. - bm.exp(-self.d * (self.a * x1 - self.b))) return - s1 / self.tau + (1. - s1) * self.gamma * r1 def ds2(self, s2, t, s1, mu0): I2=self.Jext*mu0*(1.- self.coherence / 100.) x2 = self.JE * s2 - self.JI * s1 + self.I0 + I2 + self.I2_noise r2 = (self.a * x2 - self.b) / (1. - bm.exp(-self.d * (self.a * x2 - self.b))) return - s2 / self.tau + (1. - s2) * self.gamma * r2 def dI1noise(self, I1_noise, t, noise1): return (- I1_noise + noise1.spike * bm.sqrt(self.tau_n * self.sigma_n * self.sigma_n)) / self.tau_n def dI2noise(self, I2_noise, t, noise2): return (- I2_noise + noise2.spike * bm.sqrt(self.tau_n * self.sigma_n * self.sigma_n)) / self.tau_n def update(self, tdi): # 更新噪声神经元以产生新的随机发放 self.noise1.update(tdi) self.noise2.update(tdi) # 更新s1、s2、I1_noise、I2_noise integral = self.integral(self.s1, self.s2, self.I1_noise, self.I2_noise, tdi.t, mu0=self.mu0, noise1=self.noise1, noise2=self.noise2, dt=tdi.dt) self.s1.value, self.s2.value, self.I1_noise.value, self.I2_noise.value = integral # 用更新后的s1、s2计算r1、r2 I1 = self.Jext * self.mu0 * (1. + self.coherence / 100.) x1 = self.JE * self.s1 + self.JI * self.s2 + self.I0 + I1 + self.I1_noise self.r1.value = (self.a * x1 - self.b) / (1. - bm.exp(-self.d * (self.a * x1 - self.b))) I2 = self.Jext * self.mu0 * (1. - self.coherence / 100.) x2 = self.JE * self.s2 + self.JI * self.s1 + self.I0 + I2 + self.I2_noise self.r2.value = (self.a * x2 - self.b) / (1. - bm.exp(-self.d * (self.a * x2 - self.b))) # 重置外部输入 self.mu0[:] = 0. ``` ```python # 定义各个阶段的时长 pre_stimulus_period, stimulus_period, delay_period = 100., 2000., 500. # 生成模型 dmnet = DecisionMakingRateModel(1, coherence=25.6, noise_freq=2400.) # 定义电流随时间的变化 inputs, total_period = bp.inputs.constant_input([(0., pre_stimulus_period), (20., stimulus_period), (0., delay_period)]) # 运行数值模拟 runner = bp.DSRunner(dmnet, monitors=[&apos;s1&apos;, &apos;s2&apos;, &apos;r1&apos;, &apos;r2&apos;], inputs=(&apos;mu0&apos;, inputs, &apos;iter&apos;)) runner.run(total_period) # 可视化 fig, gs = plt.subplots(2, 1, figsize=(6, 6), sharex=&apos;all&apos;) gs[0].plot(runner.mon.ts, runner.mon.s1, label=&apos;s1&apos;) gs[0].plot(runner.mon.ts, runner.mon.s2, label=&apos;s2&apos;) gs[0].axvline(pre_stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[0].axvline(pre_stimulus_period + stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[0].set_ylabel(&apos;gating variable $s$&apos;) gs[0].legend() gs[1].plot(runner.mon.ts, runner.mon.r1, label=&apos;r1&apos;) gs[1].plot(runner.mon.ts, runner.mon.r2, label=&apos;r2&apos;) gs[1].axvline(pre_stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[1].axvline(pre_stimulus_period + stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[1].set_xlabel(&apos;t (ms)&apos;) gs[1].set_ylabel(&apos;firing rate $r$&apos;) gs[1].legend() plt.subplots_adjust(hspace=0.1) plt.show() ``` ### Results ![image-20230828112555018](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112555018.png) ## Phase Plane Analysis 因为只有两个variable ### Model implementation ```python @bp.odeint def int_s1(s1, t, s2, coh=0.5, mu=20.): x1 = JE * s1 + JI * s2 + Ib + JAext * mu * (1. + coh/100) r1 = (a * x1 - b) / (1. - bm.exp(-d * (a * x1 - b))) return - s1 / tau + (1. - s1) * gamma * r1 @bp.odeint def int_s2(s2, t, s1, coh=0.5, mu=20.): x2 = JE * s2 + JI * s1 + Ib + JAext * mu * (1. - coh/100) r2 = (a * x2 - b) / (1. - bm.exp(-d * (a * x2 - b))) return - s2 / tau + (1. - s2) * gamma * r2 ``` ### Without / with input ![image-20230828112709355](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112709355.png) 只受扰动影响，有input后中间变得不稳定，但如果已经选择，网络仍维持之前选择的结果 ![image-20230828112811394](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112811394.png) ### Coherence 稳定点对网络的拉伸更强 ![image-20230828113031946](/BrainPy-course-notes/master_content/Notes.assets/image-20230828113031946.png) ![image-20230828113009219](/BrainPy-course-notes/master_content/Notes.assets/image-20230828113009219.png) # Reservoir Computing 引入训练 倾向于使用RNN ![image-20230828140305956](/BrainPy-course-notes/master_content/Notes.assets/image-20230828140305956.png) Connecting different units $$ \begin{aligned} &amp;\textsf{Input to unit i from unit j:} \\ &amp;&amp;&amp;I_{j\rightarrow i}=J_{ij}r_{j}(t) \\ &amp;\textsf{Total input to unit i:} \\ &amp;&amp;&amp;I_{i}^{(tot)}=\sum_{j=1}^{N}J_{ij}r_{j}(t)+I_{i}^{(ext)} \end{aligned} $$ $$ \textsf{Activation of unit i:} \\ \tau\frac{dx_{i}}{dt}=-x_{i}+\sum_{j=1}^{N}J_{ij}\frac{\phi(x_{j})}{1}+I_{i}^{(ext)}(t) $$ 训练范式 ![image-20230828140707998](/BrainPy-course-notes/master_content/Notes.assets/image-20230828140707998.png) ## Echo state machine ### Echo state machine 类似人工神经网络RNN，可以处理temporal信息 ![image-20230828140937455](/BrainPy-course-notes/master_content/Notes.assets/image-20230828140937455.png) $$ \begin{aligned} &amp;\mathbf{x}(n+1) =f(\mathbf{W}^{\mathrm{in}}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n)+\mathbf{W}^{\mathrm{back}}\mathbf{y}(n)) \\ &amp;\mathbf{y}(n+1) =\mathbf{W}^{\mathrm{out}}(\mathbf{u}(n+1),\mathbf{x}(n+1),\mathbf{y}(n)) \end{aligned} $$ For an RNN, the state of its internal neurons reflects the historical information of the external inputs. 反映的echo的历史信息，唯一依赖历史信息 Assuming that the updates of the network are discrete, the external input at the 𝑛th moment is u(𝑛) and the neuron state is x(𝑛), then x(𝑛) should be determined by u(𝑛), u(𝑛 - 1), ... uniquely determined. At this point, x(𝑛) can be regarded as an &quot;echo&quot; of the historical input signals. 不需要训练connection ### Echo state machine with leaky integrator 有一个leaky项，引入decay ### $$ \begin{aligned}\hat{h}(n)=\tanh(W^{in}x(n)+W^{rec}h(n-1)+W^{fb}y(n-1)+b^{rec})\\h(n)=(1-\alpha)x(n-1)+\alpha\hat{h}(n)\end{aligned} $$ where $h(n)$ is a vector of reservoir neuron activations, $W^{in}$ and $W^{rec}$ are the input and recurrent weight matrices respectively, and $\alpha\in(0,1]$ is the leaking rate. The model is also sometimes used without the leaky integration, which is a special case of $\alpha=1$ The linear readout layer is defined as $$ y(n)=W^{out}h(n)+b^{out} $$ where $y(n)$ is network output, $W^{out}$ the output weight matrix, and $b^out$ is the output bias ## Constraints of echo state machine ### Echo state property #### Theorem 1 For the echo state network defined above, the network will be echoey as long as the maximum singular value $\sigma_{max} Provement: &amp;gt; $$ &amp;gt; \begin{aligned} &amp;gt; d(\mathbf{x}(n+1),\mathbf{x}^{\prime}(n+1))&amp; =d(T(\mathbf{x}(n),\mathbf{u}(n+1)),T(\mathbf{x}&apos;(n),\mathbf{u}(n+1))) \\ &amp;gt; &amp;=d(f(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n)),f(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}&apos;(n))) \\ &amp;gt; &amp;\leq d(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n),\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}^{\prime}(n)) \\ &amp;gt; &amp;=d(\mathbf{W}\mathbf{x}(n),\mathbf{W}\mathbf{x}&apos;(n)) \\ &amp;gt; &amp;=||\mathbf{W}(\mathbf{x}(n)-\mathbf{x}^{\prime}(n))|| \\ &amp;gt; &amp;\leq\sigma_{\max}(\mathbf{W})d(\mathbf{x}(n),\mathbf{x}&apos;(n)) &amp;gt; \end{aligned} &amp;gt; $$ #### Theorem 2 For the echo state network defined above, as long as the spectral radius $|\lambda_{max}|$ of the recurrent connection matrix W &amp;gt; 1, then the network must not be echogenic. The spectral radius of the matrix is the absolute value of the largest eigenvalue $\lambda_{max}$. #### How to initialize Using these two theorems, how should we initialize W so that the network has an echo property? If we scale W, i.e., multiply it by a scaling factor $\alpha$, then $\sigma_{max}\alpha_{max}\text{,the network will not have the echo state.}\\\bullet&amp;\text{if}\alpha_{min}\le\alpha\le\alpha_{max}\text{,the network may have the echo state.}\end{array} $$ **$\alpha$设的略小于1** ![image-20230828142516052](/BrainPy-course-notes/master_content/Notes.assets/image-20230828142516052.png) ### Global parameters of reservoir 这些超参会影响reservoir network的性能，需要手动调参，很难自动去调整 - The size $N_x$ - General wisdom: the bigger the reservoir, the better the obtainable performance - Select global parameters with smaller reservoirs, then scale to bigger ones. - Sparsity - Distribution of nonzero elements: - Normal distribution - Uniform distribution - The width of the distributions does not matter - spectral radius of $W$ - scales the width of the distribution of its nonzero elements - determines how fast the influence of an input dies out in a reservoir with time, and how stable the reservoir activations are - The spectral radius should be larger in tasks requiring longer memory of the input - Scaling(-s) to $W^{in}$: - For uniform distributed $W^{in}$, $\alpha$ in the range of the interval $[-a;a]$. - For normal distributed $W^{in}$, one may take the standard deviation as a scaling measure. The leaking rate $\alpha$ ## Training of echo state machine ### Offline learning The advantage of the echo state network is that it does not train recurrent connections within the reservoir, but only the readout layer from the reservoir to the output. 线性层的优化方法是简单的 **Ridge regression** $$ \begin{aligned}\epsilon_{\mathrm{train}}(n)&amp;=\mathbf{y}(n)-\mathbf{\hat{y}}(n) \\&amp;=\mathbf{y}(n)-\mathbf{W}^{\mathrm{out}}\mathbf{x}(n) \\&amp;L_{\mathrm{ridge}}=\frac{1}{N}\sum_{i=1}^{N}\epsilon_{\mathrm{train}}^{2}(i)+\alpha||\mathbf{W^{out}}||^{2} \\\\W^{out}&amp;=Y^{target}X^T(XX^T+\beta I)^{-1}\end{aligned} $$ ```python trainer = bp.OfflineTrainer(model, fit_method=bp.algorithms.RidgeRegression(1e-7), dt=dt) ``` ### Online learning 来一个sample，进行一次training，对训练资源可以避免瓶颈 The training data is passed to the trainer in a certain sequence (e.g., time series), and the trainer continuously learns based on the new incoming data. **Recursive Least Squares (RLS) algorithm** $$ E(\mathbf{y},\mathbf{y}^\mathrm{target},n)=\frac{1}{N_\mathrm{y}}\sum_{i=1}^{N_\mathrm{y}}\sum_{j=1}^{n}\lambda^{n-j}\left(y_i(j)-y_i^\mathrm{target}(j)\right)^2, $$ ```python trainer = bp.OnlineTrainer(model, fit_method=bp.algorithms.RLS(), dt=dt) ``` ### Dataset 给定time sequence，可以让网络去预测regression ![image-20230828144309742](/BrainPy-course-notes/master_content/Notes.assets/image-20230828144309742.png) 用到BrainPy集成的`Neuromorphic and Cognitive Datasets` ### Other tasks `MNIST dataset` or `Fashion MNIST` Two aspect: - Running time - Memory Usage ## Echo state machine programming ```python import brainpy as bp import brainpy.math as bm import brainpy_datasets as bd import matplotlib.pyplot as plt # enable x64 computation bm.set_environment(x64=True, mode=bm.batching_mode) bm.set_platform(&apos;cpu&apos;) ``` ### Dataset ```python def plot_mackey_glass_series(ts, x_series, x_tau_series, num_sample): plt.figure(figsize=(13, 5)) plt.subplot(121) plt.title(f&quot;Timeserie - {num_sample} timesteps&quot;) plt.plot(ts[:num_sample], x_series[:num_sample], lw=2, color=&quot;lightgrey&quot;, zorder=0) plt.scatter(ts[:num_sample], x_series[:num_sample], c=ts[:num_sample], cmap=&quot;viridis&quot;, s=6) plt.xlabel(&quot;$t$&quot;) plt.ylabel(&quot;$P(t)$&quot;) ax = plt.subplot(122) ax.margins(0.05) plt.title(f&quot;Phase diagram: $P(t) = f(P(t-\\tau))$&quot;) plt.plot(x_tau_series[: num_sample], x_series[: num_sample], lw=1, color=&quot;lightgrey&quot;, zorder=0) plt.scatter(x_tau_series[:num_sample], x_series[: num_sample], lw=0.5, c=ts[:num_sample], cmap=&quot;viridis&quot;, s=6) plt.xlabel(&quot;$P(t-\\tau)$&quot;) plt.ylabel(&quot;$P(t)$&quot;) cbar = plt.colorbar() cbar.ax.set_ylabel(&apos;$t$&apos;) plt.tight_layout() plt.show() ``` ```python dt = 0.1 mg_data = bd.chaos.MackeyGlassEq(25000, dt=dt, tau=17, beta=0.2, gamma=0.1, n=10) ts = mg_data.ts xs = mg_data.xs ys = mg_data.ys plot_mackey_glass_series(ts, xs, ys, num_sample=int(1000 / dt)) ``` ![image-20230828151451523](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151451523.png) ### Prediction of Mackey-Glass timeseries #### Prepare the data ```python def get_data(t_warm, t_forcast, t_train, sample_rate=1): warmup = int(t_warm / dt) # warmup the reservoir forecast = int(t_forcast / dt) # predict 10 ms ahead train_length = int(t_train / dt) X_warm = xs[:warmup:sample_rate] X_warm = bm.expand_dims(X_warm, 0) X_train = xs[warmup: warmup+train_length: sample_rate] X_train = bm.expand_dims(X_train, 0) Y_train = xs[warmup+forecast: warmup+train_length+forecast: sample_rate] Y_train = bm.expand_dims(Y_train, 0) X_test = xs[warmup + train_length: -forecast: sample_rate] X_test = bm.expand_dims(X_test, 0) Y_test = xs[warmup + train_length + forecast::sample_rate] Y_test = bm.expand_dims(Y_test, 0) return X_warm, X_train, Y_train, X_test, Y_test ``` ```python # First warmup the reservoir using the first 100 ms # Then, train the network in 20000 ms to predict 1 ms chaotic series ahead x_warm, x_train, y_train, x_test, y_test = get_data(100, 1, 20000) ``` ```python sample = 3000 fig = plt.figure(figsize=(15, 5)) plt.plot(x_train[0, :sample], label=&quot;Training data&quot;) plt.plot(y_train[0, :sample], label=&quot;True prediction&quot;) plt.legend() plt.show() ``` ![image-20230828151606545](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151606545.png) #### Prepare the ESN ```python class ESN(bp.DynamicalSystemNS): def __init__(self, num_in, num_hidden, num_out, sr=1., leaky_rate=0.3, Win_initializer=bp.init.Uniform(0, 0.2)): super(ESN, self).__init__() self.r = bp.layers.Reservoir( num_in, num_hidden, Win_initializer=Win_initializer, spectral_radius=sr, leaky_rate=leaky_rate, ) self.o = bp.layers.Dense(num_hidden, num_out, mode=bm.training_mode) def update(self, x): return x &amp;gt;&amp;gt; self.r &amp;gt;&amp;gt; self.o ``` #### Train and test ```python model = ESN(1, 100, 1) model.reset_state(1) trainer = bp.RidgeTrainer(model, alpha=1e-6) ``` ```python # warmup _ = trainer.predict(x_warm) ``` ```python # train _ = trainer.fit([x_train, y_train]) ``` #### Test the training data ```python ys_predict = trainer.predict(x_train) ``` ```python start, end = 1000, 6000 plt.figure(figsize=(15, 7)) plt.subplot(211) plt.plot(bm.as_numpy(ys_predict)[0, start:end, 0], lw=3, label=&quot;ESN prediction&quot;) plt.plot(bm.as_numpy(y_train)[0, start:end, 0], linestyle=&quot;--&quot;, lw=2, label=&quot;True value&quot;) plt.title(f&apos;Mean Square Error: {bp.losses.mean_squared_error(ys_predict, y_train)}&apos;) plt.legend() plt.show() ``` ![image-20230828151747954](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151747954.png) #### Test the testing data ```python ys_predict = trainer.predict(x_test) start, end = 1000, 6000 plt.figure(figsize=(15, 7)) plt.subplot(211) plt.plot(bm.as_numpy(ys_predict)[0, start:end, 0], lw=3, label=&quot;ESN prediction&quot;) plt.plot(bm.as_numpy(y_test)[0,start:end, 0], linestyle=&quot;--&quot;, lw=2, label=&quot;True value&quot;) plt.title(f&apos;Mean Square Error: {bp.losses.mean_squared_error(ys_predict, y_test)}&apos;) plt.legend() plt.show() ``` ![image-20230828151824907](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151824907.png) ### JIT connection operators - Just-in-time randomly generated matrix. - Support for Mat@Vec and Mat@Mat. - Support different random generation methods.(homogenous, uniform, normal) ```python import math, random def jitconn_prob_homo(events, prob, weight, seed, outs): random.seed(seed) max_cdist= math.ceil(2/prob -1) for event in events: if event: post_i = random.randint(1, max_cdist) outs[post_i] += weight ``` ![image-20230828153353131](/BrainPy-course-notes/master_content/Notes.assets/image-20230828153353131.png) ## Applications ### From the perspective of kernel methods 维度扩张思想 Non-linear SVMs: Kernel Mapping ![image-20230828153621843](/BrainPy-course-notes/master_content/Notes.assets/image-20230828153621843.png) Kernel methods in neural system? **与维度扩张的思想相似** ![image-20230828153801285](/BrainPy-course-notes/master_content/Notes.assets/image-20230828153801285.png) ### Subcortical pathway for rapid motion processing The first two stages of subcortical visual pathway: Retina -&amp;gt; superior colliculus The first two stages of primary auditory pathway: Inner Ear -&amp;gt; Cochlear Nuclei 维度扩张在subcortical pathway中体现，reservoir 能够高维处理的更简单 ### Spatial-temporal tasks ![image-20230828154155803](/BrainPy-course-notes/master_content/Notes.assets/image-20230828154155803.png) 既有时间信息，又有空间信息的dataset，使用reservoir来处理高维信息，坐位 Dimension expansion ### Gait recognition input来了再做计算 ![image-20230828154352087](/BrainPy-course-notes/master_content/Notes.assets/image-20230828154352087.png) ### Spatial-temporal tasks large-scale，随size增大，accuracy增大 ![image-20230828154428762](/BrainPy-course-notes/master_content/Notes.assets/image-20230828154428762.png) ### Liquid state machine A liquid state machine (LSM) is a type of reservoir computer that uses a spiking neural network. 与ESN一样的范式，都是去做dimension expansion 很难去分析怎么work的">
<meta name="twitter:image" content="Notes.assets/image-20230826111831637.png">


<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/BrainPy-course-notes',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="routhleck.github.io/BrainPy-course-notes"/>





  <title>               | BrainPy course notes      </title>
  
















</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/BrainPy-course-notes/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">BrainPy course notes</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        
        
        <li class="menu-item menu-item-home">
          <a href="/BrainPy-course-notes/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        
        
        <li class="menu-item menu-item-about">
          <a href="/BrainPy-course-notes/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
<div id="posts" class="posts-expand">
<header class="post-header">

	<h1 class="post-title" itemprop="name headline">
    
      
    
  </h1>



</header>

  
  
    <p>[TOC]</p>

<h1 id="神经计算建模简介">神经计算建模简介</h1>

<h2 id="计算神经科学的背景与使命">计算神经科学的背景与使命</h2>

<p>计算神经科学是<strong>脑科学</strong>对<strong>类脑智能</strong>的<strong>桥梁</strong></p>

<h3 id="两大目标">两大目标</h3>

<ul>
  <li>用计算建模的方法来阐明大脑功能的计算原理</li>
  <li>发展类脑智能的模型和算法</li>
</ul>

<h3 id="prehistory">Prehistory</h3>

<ul>
  <li>1907 LIF model 
神经计算的本质</li>
  <li>1950s HH model 
电位定量化模型 最fundamental的</li>
  <li>1960s Roll’s cable equation 
描述信号在轴突和树突怎么传递</li>
  <li>1970s Amari, Wilson, Cowan et al.
现今建模的基础</li>
  <li>1982 Hopfield model(Amari-Hopfield model)
引入物理学技术，吸引子模型</li>
  <li>1988 Sejnowski et al. “Computational Neuroscience”(science)
提出计算神经科学概念</li>
</ul>

<p><strong>现在的计算神经科学对应于物理学的第谷-伽利略时代，对大脑工作原理还缺乏清晰的理论</strong></p>

<h3 id="three-levels-of-brain-science">Three levels of Brain Science</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823105226568.png" alt="image-20230823105226568" /></p>

<ul>
  <li>大脑做什么
Computational theory 
-&gt; Psychology &amp; Cognitive Science
-&gt; Human-like Cognitive function</li>
  <li>大脑怎么做
Representation &amp; Algorithm 
-&gt; Computational Neuroscience
-&gt; Brain-inspired model &amp; algorithm</li>
  <li>大脑怎么实现
Implementation
-&gt; Neuroscience
-&gt; Neuromorphic computing</li>
</ul>

<h3 id="mission-of-computational-neuroscience">Mission of Computational Neuroscience</h3>

<blockquote>
  <p>What I can not build a computational model, I do not understand</p>
</blockquote>

<h2 id="神经计算建模的目标与挑战">神经计算建模的目标与挑战</h2>

<h3 id="limitation-of-deep-learning">Limitation of Deep Learning</h3>

<ul>
  <li>不擅长对抗样本</li>
  <li>对图像的理解有限</li>
</ul>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823105836259.png" alt="image-20230823105836259" /></p>

<h3 id="brain-is-for-processing-dynamical-information">Brain is for Processing Dynamical Information</h3>

<p><strong>We never “see” a static image</strong></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823105918336.png" alt="image-20230823105918336" /></p>

<h3 id="the-missing-link">The missing link</h3>

<p>a computational model of higher cognitive functior</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823110617639.png" alt="image-20230823110617639" /></p>

<p>现在只是做的<strong>局部</strong>的网络，没有一个成功的模型，能<strong>从神经元出发构建网络，到系统层面上</strong></p>

<p><strong>原因</strong>: 因为神经科学底层数据的缺失，可以考虑数据驱动、大数据的方式来加快发展</p>

<h2 id="神经计算建模的工具">神经计算建模的工具</h2>

<blockquote>
  <p>工欲行其事，必先利其器
We need “PyTorch/TensorFlow” in Computational Neuroscience!</p>
</blockquote>

<h3 id="challenges-in-neural-modelling">Challenges in neural modelling</h3>

<p>有不同的尺度</p>

<ul>
  <li>Mutiple-scale</li>
  <li>Large-scale</li>
  <li>Multiple purposes</li>
</ul>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823111212460.png" alt="image-20230823111212460" /></p>

<blockquote>
  <p>The modeling targets and methods are extremely complex, and we need a general framework.</p>
</blockquote>

<h3 id="limitations-of-existing-brain-simulators">Limitations of Existing Brain Simulators</h3>

<p>现今的框架不能满足以上</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823111509523.png" alt="image-20230823111509523" /></p>

<h3 id="what-are-needed-for-a-brain-simulator">What are needed for a brain simulator</h3>

<ol>
  <li>Efficiency
High-speed simulation on parallel computing devices, etc.</li>
  <li>Integration
Integrated modeling of simulation, training, and analysis</li>
  <li>Flexibility
New models at all scales can be accommodated</li>
  <li>Extensibility
Extensible to new modeling methods(machine learning)</li>
</ol>

<p>需要新的范式</p>

<h3 id="our-solution-brainpy">Our solution: BrainPy</h3>

<p>4 levels</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823111903456.png" alt="image-20230823111903456" /></p>

<h2 id="神经计算建模举例">神经计算建模举例</h2>

<h3 id="image-understanding-an-ill-posed-problem">Image understanding: an ill-posed problem</h3>

<p>Image Understanding = image segmentation + image object recognition</p>

<blockquote>
  <p>Chicken vs. Egg dilemma</p>

  <ul>
    <li>Without segmentation, how to recognize</li>
    <li>Without recognition, how to segment</li>
  </ul>
</blockquote>

<p><strong>The solution of brain:</strong> Analysis-by-synthesis 猜测与验证方法</p>

<h3 id="reverse-hierarchy-theory">Reverse Hierarchy Theory</h3>

<p>人的感知是整体到局部</p>

<h3 id="two-pathways-for-visual-information-processing">Two pathways for visual information processing</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823114517888.png" alt="image-20230823114517888" /></p>

<h3 id="key-computational-issues-for-global-to-local-neural-information-processing">Key Computational Issues for Global-to-local Neural Information Processing</h3>

<ul>
  <li>What are global and local features</li>
  <li>How to rapidly extract global features</li>
  <li>How to generate global hypotheses</li>
  <li>How to implement from global to local processing</li>
  <li>The interplay between global and local features</li>
  <li>Others</li>
</ul>

<h4 id="how-to-extract-global-features">How to extract global features</h4>

<p><strong>Global first = Topology first</strong>(大范围首先，陈霖)
视觉系统更敏感于拓扑性质的差异</p>

<blockquote>
  <p>DNNs has difficulty to recognize topology</p>
</blockquote>

<p><strong>A retina-SC network for topology detection</strong></p>

<p>视网膜到上丘的检测，Gap junction coupling …</p>

<h3 id="a-model-for-motion-pattern-recognition">A Model for Motion Pattern Recognition</h3>

<p>Reservoir Module
Decision-making Module</p>

<h3 id="how-to-generate-global-hypotheses-in-the-representation-space">How to generate “global” hypotheses in the representation space</h3>

<p>Attractor neural network</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823115853980.png" alt="image-20230823115853980" /></p>

<p>Levy Flight in Animal Behaviors</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823120000911.png" alt="image-20230823120000911" /></p>

<h3 id="how-to-process-information-from-global-to-local">How to process information from global to local</h3>

<p>Push-pull Feedback</p>

<p>A hierarchical Hopfield Model</p>

<h3 id="interplay-between-global-and-local-features">Interplay between global and local features</h3>

<p>A two-pathway model for object recognition</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823120750349.png" alt="image-20230823120750349" /></p>

<p>Modeling visual masking 可以用two-pathway很好解释</p>

<h1 id="programming-basics">Programming basics</h1>

<h2 id="python-basics">Python Basics</h2>

<h3 id="values">Values</h3>

<ul>
  <li>Boolean</li>
  <li>String</li>
  <li>Integer</li>
  <li>Float</li>
  <li>…</li>
</ul>

<h3 id="keywords">Keywords</h3>

<p>Not allowed to use keywords, they define structure and rules of a language.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">help</span><span class="p">(</span><span class="s">"keywords"</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="operators">Operators</h3>

<p>数据之间的操作</p>

<h4 id="for-integers-and-floats">For Integers and Floats</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="o">=</span><span class="mi">5</span>
<span class="n">b</span><span class="o">=</span><span class="mi">3</span>
<span class="c1"># addition +
</span><span class="k">print</span><span class="p">(</span><span class="s">"a+b="</span><span class="p">,</span><span class="n">atb</span><span class="p">)</span>
<span class="c1"># subtraction -
</span><span class="k">print</span><span class="p">(</span><span class="s">"a-b="</span><span class="p">,</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="p">)</span>
<span class="c1"># multiplication *
</span><span class="k">print</span><span class="p">(</span><span class="s">"axb="</span><span class="n">a</span><span class="o">*</span><span class="n">b</span><span class="p">)</span>
<span class="c1"># division /
</span><span class="k">print</span><span class="p">(</span><span class="s">"a/b="</span><span class="p">,</span><span class="n">a</span><span class="o">/</span><span class="n">b</span><span class="p">)</span>
<span class="c1"># power **
</span><span class="k">print</span><span class="p">(</span><span class="s">"a**b="</span><span class="p">,</span><span class="n">a</span><span class="o">**</span><span class="n">b</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="booleans">Booleans</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Boolean experssions
# equals: ==
</span><span class="k">print</span><span class="p">(</span><span class="s">"5==5"</span><span class="p">,</span><span class="mi">5</span><span class="o">==</span><span class="mi">5</span><span class="p">)</span> 
<span class="c1"># do not equal: !=
</span><span class="k">print</span><span class="p">(</span><span class="s">"5!-5"</span><span class="p">,</span><span class="mi">5</span><span class="o">!=</span><span class="mi">5</span><span class="p">)</span>
<span class="c1"># greater than: &gt;
</span><span class="k">print</span><span class="p">(</span><span class="s">"5&gt;5"</span><span class="p">,</span><span class="mi">5</span><span class="o">&gt;</span><span class="mi">5</span><span class="p">)</span>
<span class="c1"># greater than or equal: &gt;=
</span><span class="k">print</span><span class="p">(</span><span class="s">"5&gt;=5”5&gt;=5)
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># logica operators
</span><span class="k">print</span><span class="p">(</span><span class="s">"True and False:"</span><span class="p">,</span> <span class="bp">True</span> <span class="ow">and</span> <span class="bp">False</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"True or False:"</span><span class="p">,</span> <span class="bp">True</span> <span class="ow">or</span> <span class="bp">False</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"not False:"</span><span class="p">,</span> <span class="ow">not</span> <span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="modules">Modules</h3>

<p>Not all functionality available comes automatically when starting python.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">match</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="k">print</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">pi</span>
<span class="k">print</span><span class="p">(</span><span class="n">pi</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="o">*</span>
<span class="k">print</span><span class="p">(</span><span class="n">pi</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="control-statements">Control statements</h3>

<h4 id="if">If</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="mi">5</span>
<span class="c1"># In Python, blocks of code are defined using indentation.
</span><span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
	<span class="k">print</span><span class="p">(</span><span class="s">"ok"</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>ok</p>
</blockquote>

<h4 id="for">For</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># range(5) means a list with integers, 0, 1, 2, 3, 4
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>0
1
2
3
4</p>
</blockquote>

<h4 id="while">While</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">i</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">while</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="mi">100</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">+=</span> <span class="n">i</span><span class="o">**</span><span class="mi">3</span> <span class="c1"># a += b is short for a = a+b
</span></code></pre></div></div>

<blockquote>
  <p>1
8
1000</p>
</blockquote>

<h3 id="functions">Functions</h3>

<ul>
  <li>Functions are used to abstract components of a program.</li>
  <li>Much like a mathematical function, they take some input and then find the result. start a function definition with a keyword def</li>
  <li>Then comes the function name, with arguments in braces, and then a colon.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">args1</span><span class="p">,</span> <span class="n">args2</span><span class="p">):</span>
    <span class="k">pass</span>
</code></pre></div></div>

<h3 id="data-types">Data types</h3>

<h4 id="list">List</h4>

<ul>
  <li>Group variables together</li>
  <li>Specific order</li>
  <li>Access item with brankets: [ ]</li>
  <li>List can be sliced</li>
  <li>List can be multiplied</li>
  <li>List can be added</li>
  <li>Lists are mutable</li>
  <li>Copying a list</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">myList</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span><span class="s">"name"</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s">"myList[0]:"</span><span class="p">,</span> <span class="n">myList</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"myList[1]:"</span><span class="p">,</span> <span class="n">myList</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"myList[3]:"</span><span class="p">,</span> <span class="n">myList</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"myList[-1]:"</span><span class="p">,</span> <span class="n">myList</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"myList[-2]:"</span><span class="p">,</span> <span class="n">myList</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
</code></pre></div></div>

<blockquote>
  <p>myList[0]: 0
myList[1]: 1
myList[3]: name
myList[-1]: name
myList[-2]: 2.0</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">myList</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s">"hello"</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s">"myList[0:2]:"</span><span class="p">,</span> <span class="n">mylist</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"myList*2:"</span><span class="p">,</span> <span class="n">myList</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span>
<span class="n">myList2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="s">"yes"</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s">"myList+myList2:"</span><span class="p">,</span> <span class="n">myList</span><span class="o">+</span><span class="n">myList2</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>myList[0:2]: [0，1.0]
myList*2: [0，1.0， hello’，0，1.0， hello’]
myList+myList2: [0，1.0，’hello’，2，yes’]</p>
</blockquote>

<h4 id="tuple">tuple</h4>

<p>Tuples are immutable.</p>

<h4 id="dictionary">dictionary</h4>

<p>A dictionary is a collection of key-value pairs</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">d</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">d</span><span class="p">[</span><span class="s">"a"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">3</span>

<span class="k">print</span><span class="p">(</span><span class="s">"d: "</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="s">"a"</span><span class="p">:</span><span class="mi">3</span><span class="p">}</span>
<span class="k">print</span><span class="p">(</span><span class="s">"c: "</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"c[1]: "</span><span class="p">,</span> <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<blockquote>
  <p>d: {1: 2, ‘a’: 3}
c: {1: 2, ‘a’: 3}
c[1]: 2</p>
</blockquote>

<h3 id="class">Class</h3>

<p>In Python, everything is an object. Classes are objects, instances of
classes are objects, modules are objects, and functions are objects.</p>

 	1.  a <strong>type</strong>
 	2.  an internal <strong>data representation</strong> (primitive or composite)
 	3.  a set of procedures for <strong>interaction</strong> with the object

<p><strong>a simple example</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># define class
</span>
<span class="k">class</span> <span class="nc">Linear</span><span class="p">():</span>
    <span class="k">pass</span>

<span class="c1"># instantiate object
</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">layer1</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p><code class="language-plaintext highlighter-rouge">&lt;__main__.Linear object at 0x7f88ad6c61d0&gt;</code></p>
</blockquote>

<h4 id="initializing-an-object">Initializing an object</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># define class
</span><span class="k">class</span> <span class="nc">Linear</span><span class="p">():</span>
    <span class="c1"># It refers to the object (instance) itself
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_input</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_input</span> <span class="o">=</span> <span class="n">n_input</span>

<span class="n">layer1</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">layer2</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"layer1 : "</span><span class="p">,</span> <span class="n">layer1</span><span class="p">.</span><span class="n">n_input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"layer2 : "</span><span class="p">,</span> <span class="n">layer2</span><span class="p">.</span><span class="n">n_input</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>layer1 : 100
layer2 : 1000</p>
</blockquote>

<h4 id="class-has-methods-similar-to-functions">Class has methods (similar to functions)</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># define class
</span><span class="k">class</span> <span class="nc">Linear</span><span class="p">():</span>
    <span class="c1">### It refers to the the object (instance) itself
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_input</span><span class="p">,</span> <span class="n">n_output</span><span class="p">):</span>
	    <span class="bp">self</span><span class="p">.</span><span class="n">n_input</span> <span class="o">=</span> <span class="n">n_input</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_output</span> <span class="o">=</span> <span class="n">n_output</span>
	<span class="k">def</span> <span class="nf">compute</span> <span class="n">n</span> <span class="n">params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">num_params</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_input</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_output</span>
        <span class="k">return</span> <span class="n">num_params</span>
<span class="n">layerl</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">layerl</span><span class="p">.</span><span class="n">compute_n_params</span><span class="p">())</span>
</code></pre></div></div>

<blockquote>
  <p>1000</p>
</blockquote>

<h2 id="numpy-basic">NumPy Basic</h2>

<h3 id="numpy-introduction">Numpy Introduction</h3>

<ul>
  <li>Fundamental package for scientific computing with Python</li>
  <li>N-dimensional array object</li>
  <li>Linear algebra, frontier transform, random number capacities</li>
  <li>Building block for other packages (e.g. Scipy)</li>
</ul>

<h3 id="array">Array</h3>

<ul>
  <li>Arrays are mutable</li>
  <li>Arrays attributes</li>
  <li>…</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>[[0. 0.]
	[0. 0.]]</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="p">.</span><span class="n">ndim</span>		<span class="c1"># 2 dimension
</span><span class="n">a</span><span class="p">.</span><span class="n">shape</span>		<span class="c1"># (2, 5) shape of array
</span><span class="n">a</span><span class="p">.</span><span class="n">size</span>		<span class="c1"># 10 $ of elements
</span><span class="n">a</span><span class="p">.</span><span class="n">T</span>			<span class="c1"># transpose
</span><span class="n">a</span><span class="p">.</span><span class="n">dtype</span>		<span class="c1"># data type
</span></code></pre></div></div>

<h4 id="array-broadcasting">Array broadcasting</h4>

<p>When operating on two arrays, numpy compares shapes. Two dimensions are compatible when</p>
<ol>
  <li>They are of equal size</li>
  <li>One of them is 1</li>
</ol>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823143622229.png" alt="image-20230823143622229" /></p>

<h3 id="vector-operations">Vector operations</h3>

<ul>
  <li>Inner product</li>
  <li>Outer product</li>
  <li>Dot product (matrix multiplication)</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">u</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">v</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">np</span><span class="p">.</span><span class="n">inner</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="n">np</span><span class="p">.</span><span class="n">outer</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>6
array([[1, 1, 1],
			[2, 2, 2],
			[3, 3, 3]])
6</p>
</blockquote>

<h3 id="matrix-operations">Matrix operations</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">np.ones</code></li>
  <li><code class="language-plaintext highlighter-rouge">.T</code></li>
  <li><code class="language-plaintext highlighter-rouge">np.dot</code></li>
  <li><code class="language-plaintext highlighter-rouge">np.eye</code></li>
  <li><code class="language-plaintext highlighter-rouge">np.trace</code></li>
  <li><code class="language-plaintext highlighter-rouge">np.row_stack</code></li>
  <li><code class="language-plaintext highlighter-rouge">np.column_stack</code></li>
</ul>

<h3 id="operations-along-axes">Operations along axes</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="n">a</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>

<span class="n">a</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">a</span><span class="p">.</span><span class="n">cumsum</span><span class="p">()</span>

<span class="n">a</span><span class="p">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="slicing-arrays">Slicing arrays</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> 	<span class="c1"># first row, all columns
</span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> 	<span class="c1"># first and second rows, al columns
</span><span class="n">a</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span><span class="c1"># all rows, second and third columns
</span></code></pre></div></div>

<h3 id="reshape">Reshape</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">a</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="linear-algebra">Linear algebra</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">qr</span>				<span class="c1"># Computes the QR decomposition
</span><span class="n">cholesky</span>		<span class="c1"># Computes the Cholesky decomposition
</span><span class="n">inv</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>			<span class="c1"># Inverse
</span><span class="n">solve</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>		<span class="c1"># Solves Ax = b for A full rank
</span><span class="n">lstsq</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>		<span class="c1"># Solves arg minx //Ax - b//2
</span><span class="n">eig</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>			<span class="c1"># Eigenvalue decomposition
</span><span class="n">eigvals</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>		<span class="c1"># Computes eigenvalues
</span><span class="n">svd</span><span class="p">(</span><span class="n">A</span><span class="err">，</span><span class="n">full</span><span class="p">)</span>		<span class="c1"># Sinqular value decomposition
</span><span class="n">pinv</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>			<span class="c1"># Computes pseudo-inverse of A
</span></code></pre></div></div>

<h3 id="fourier-transform">Fourier transform</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy.fft</span>
<span class="n">fft</span>		<span class="c1"># 1-dimensional DFT
</span><span class="n">fft2</span>	<span class="c1"># 2-dimensional DFT
</span><span class="n">fftn</span>	<span class="c1"># N-dimensional DFT
</span><span class="n">ifft</span>	<span class="c1"># 1-dimensional inverse DFT (etc.)
</span><span class="n">rfft</span>	<span class="c1"># Real DFT (1-dim)
</span></code></pre></div></div>

<h3 id="random-sampling">Random sampling</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy.random</span>
<span class="n">rand</span><span class="p">(</span><span class="n">d0</span><span class="p">,</span> <span class="n">d1</span><span class="p">,</span> <span class="p">...,</span> <span class="n">dn</span><span class="p">)</span>		<span class="c1"># Random values in a given shape
</span><span class="n">randn</span><span class="p">(</span><span class="n">d0</span><span class="p">,</span> <span class="n">d1</span><span class="p">,</span> <span class="p">...,</span> <span class="n">dn</span><span class="p">)</span>		<span class="c1"># Random standard normal
</span><span class="n">randint</span><span class="p">(</span><span class="n">lo</span><span class="p">,</span> <span class="n">hi</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>		<span class="c1"># Random integers [lo hi)
</span><span class="n">choice</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">repl</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>	<span class="c1"># Sample from a
</span><span class="n">shuffle</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>					<span class="c1"># Permutation (in-place)
</span><span class="n">permutation</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>				<span class="c1"># Permutation (new array)
</span></code></pre></div></div>

<h3 id="distributions-in-random">Distributions in random</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy.random</span>
<span class="n">beta</span>
<span class="n">binomial</span>
<span class="n">chisquare</span>
<span class="n">exponential</span>
<span class="n">dirichlet</span>
<span class="n">gamma</span>
<span class="n">laplace</span>
<span class="n">lognormal</span>
<span class="p">...</span>
</code></pre></div></div>

<h3 id="scipy">Scipy</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">SciPy</code> is a library of algorithms and mathematical tools built to work with <code class="language-plaintext highlighter-rouge">NumPy </code> arrays.</li>
  <li><code class="language-plaintext highlighter-rouge">scipy.linalg linear algebra</code></li>
  <li><code class="language-plaintext highlighter-rouge">scipy.stats statistics</code></li>
  <li><code class="language-plaintext highlighter-rouge">scipy.optimize optimization</code></li>
  <li><code class="language-plaintext highlighter-rouge">scipy.sparse sparse matrices</code></li>
  <li><code class="language-plaintext highlighter-rouge">scipy.signal signal processing</code></li>
  <li>etc.</li>
</ul>

<h2 id="brainpy-introduction">BrainPy introduction</h2>

<h3 id="modeling-demands">Modeling demands</h3>

<ul>
  <li>Large-scale</li>
  <li>Multi-scale</li>
  <li>Methods</li>
</ul>

<h3 id="brainpy-architecture">BrainPy Architecture</h3>

<ul>
  <li>Infrastructure</li>
  <li>Functions</li>
  <li>Just-in-time compilation</li>
  <li>Devices</li>
</ul>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823145349681.png" alt="image-20230823145349681" /></p>

<h3 id="main-features">Main features</h3>

<h4 id="dense-operators">Dense operators</h4>

<ul>
  <li>Compatible with <code class="language-plaintext highlighter-rouge">NumPy</code>, <code class="language-plaintext highlighter-rouge">TensorFlow</code>, <code class="language-plaintext highlighter-rouge">PyTorch</code> and other dense matrix operator syntax.</li>
  <li>Users do not need to learn and get started programming directly.</li>
</ul>

<h4 id="dedicated-operatorsq">Dedicated operatorsq</h4>

<ul>
  <li>Applies brain dynamics sparse connectivity properties with event-driven computational features.</li>
  <li>Reduce the complexity of brain dynamics simulations by several orders of magnitude.</li>
</ul>

<h4 id="numerical-integrators">Numerical Integrators</h4>

<ul>
  <li>Ordinary differential equations: brainpy.odeint</li>
  <li>Stochastic differential equations: brainpy.sdeint</li>
  <li>Fractional differential equations: brainpy.fdeint</li>
  <li>Delayed differential equations</li>
</ul>

<h4 id="modular-and-composable">Modular and composable</h4>

<p>从微观到宏观</p>

<p><strong>brainpy.DynamicalSystem</strong></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823151159786.png" alt="image-20230823151159786" /></p>

<h4 id="jit-of-object-oriented">JIT of object-oriented</h4>

<p>BrainPy provides object-oriented transformations:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">brainpy.math.jit</code></li>
  <li><code class="language-plaintext highlighter-rouge">brainpy.math.grad</code></li>
  <li><code class="language-plaintext highlighter-rouge">brainpy.math.for_loop</code></li>
  <li><code class="language-plaintext highlighter-rouge">brainpy.math.ifelse</code></li>
</ul>

<h2 id="brainpy-programming-basics">BrainPy Programming Basics</h2>

<h3 id="just-in-time-compilation">Just-in-Time compilation</h3>

<p>Just In Time Compilation (JIT, or Dynamic Translation), is compilation that is being done during the execution of a program.</p>

<p>JIT compilation attempts to use <strong>the benefits of both</strong>. While the interpreted program is being run, the JIT compiler determines the most frequently used code and compiles it to machine code.</p>

<p>The advantages of a JIT are due to the fact that since the compilation takes place in run time, a JIT compiler has access to dynamic runtime information enabling it to make better optimizations (such as inlining functions).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">sqrt</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span>
    <span class="n">cdf</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">bm</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">sqrt</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.044715</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">3</span><span class="p">))))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span><span class="n">cdf</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">gelu_jit</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">jit</span><span class="p">(</span><span class="n">gelu</span><span class="p">)</span> <span class="c1"># 使用JIT
</span></code></pre></div></div>

<h3 id="object-oriented-jit-compilation">Object-oriented JIT compilation</h3>

<ul>
  <li>The class object must be inherited from brainpy.BrainPyObject, the base class of BrainPy, whose methods will be automatically JIT compiled.</li>
  <li>All time-dependent variables must be defined as brainpy.math.Variable.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">BrainPyObject</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dimension</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        
        <span class="c1"># parameters
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dimension</span> <span class="o">=</span> <span class="n">dimension</span>
        
        <span class="c1"># variables
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dimension</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.3</span><span class="p">)</span>
        
	<span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">dot</span><span class="p">(((</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">Y</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">w</span><span class="p">)))</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">Y</span><span class="p">),</span> <span class="n">X</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">w</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">w</span> <span class="o">-</span> <span class="n">u</span> <span class="c1"># in-place update
</span></code></pre></div></div>

<p><strong>ExampleL Run a neuron model</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">neurons</span><span class="p">.</span><span class="n">HH</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span> <span class="c1">#一共1000个神经元
</span><span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="s">'input'</span><span class="p">,</span> <span class="mf">10.</span><span class="p">))</span> <span class="c1"># jit默认为True
</span><span class="n">runner</span><span class="p">(</span><span class="n">duration</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">eval_time</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1">#模拟 1000ms
</span></code></pre></div></div>

<p>禁用JIT来debug</p>

<h3 id="data-operations">Data operations</h3>

<h4 id="array-1">Array</h4>

<p>等价于<code class="language-plaintext highlighter-rouge">numpy</code>的<code class="language-plaintext highlighter-rouge">array</code></p>

<h4 id="brainpy-arrays--jax-arrays">BrainPy arrays &amp; JAX arrays</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t1</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">t1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">t1</span><span class="p">.</span><span class="n">value</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>JaxArray([0, 1, 2], dtype=int32)
DeviceArray([0, 1, 2], dtype=int32)</p>
</blockquote>

<h4 id="variables">Variables</h4>

<p>Arrays that are not marked as dynamic variables will be JIT-compiled as static arrays, and modifications to static arrays will not be valid in the JIT compilation environment.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">v</span><span class="p">.</span><span class="n">value</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>Variable([0, 1, 2, 3], dtype=int32)
DeviceArray([0, 1, 2, 3], dtype=int32)</p>
</blockquote>

<h3 id="variables-1">Variables</h3>

<p><strong>In-place updating</strong> 就地更新</p>

<h4 id="indexing-and-slicing">Indexing and slicing</h4>

<ul>
  <li>Indexing: <code class="language-plaintext highlighter-rouge">v[i] = a</code> or <code class="language-plaintext highlighter-rouge">v[(1, 3)] = c</code></li>
  <li>Slicing: <code class="language-plaintext highlighter-rouge">v[i:j] = b</code></li>
  <li>Slicing all values <code class="language-plaintext highlighter-rouge">v[:] = d</code>, <code class="language-plaintext highlighter-rouge">v[...] = e</code></li>
</ul>

<h4 id="augmented-assignment">Augmented assignment</h4>

<ul>
  <li>add</li>
  <li>subtract</li>
  <li>divide</li>
  <li>multiply</li>
  <li>floor divide</li>
  <li>modulo</li>
  <li>power</li>
  <li>and</li>
  <li>or</li>
  <li>xor</li>
  <li>left shift</li>
  <li>right shift</li>
</ul>

<h4 id="value-assignment">Value assignment</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">v</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">check_no_change</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="update-assignment">Update assignment</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">v</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="control-flows">Control flows</h3>

<h4 id="if-else">If-else</h4>

<p><code class="language-plaintext highlighter-rouge">brainpy.math.where</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">a</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>DeviceArray(1., dtype=float32, weak_type=True)</p>
</blockquote>

<p><code class="language-plaintext highlighter-rouge">brainpy.math.ifelse</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ifelse</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">branches</span><span class="p">,</span> <span class="n">operands</span><span class="p">):</span>
	<span class="n">true_fun</span><span class="p">,</span> <span class="n">false_fun</span> <span class="o">=</span> <span class="n">branches</span>
    <span class="k">if</span> <span class="n">condition</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">true_fun</span><span class="p">(</span><span class="n">operands</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">false_fun</span><span class="p">(</span><span class="n">operands</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="for-loop">For loop</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">brainpy.math</span>
<span class="n">hist_of_out_vars</span> <span class="o">=</span> <span class="n">brainpy</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">for_loop</span><span class="p">(</span><span class="n">body_fun</span><span class="p">,</span> <span class="n">operands</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="while-loop">While loop</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">i</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">counter</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">cond_f</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">10</span>

<span class="k">def</span> <span class="nf">body_f</span><span class="p">():</span>
    <span class="n">i</span><span class="p">.</span><span class="n">value</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">counter</span><span class="p">.</span><span class="n">value</span> <span class="o">+=</span> <span class="n">i</span>

<span class="n">bm</span><span class="p">.</span><span class="n">while_loop</span><span class="p">(</span><span class="n">body_f</span><span class="p">,</span> <span class="n">cond_f</span><span class="p">,</span> <span class="n">operands</span><span class="o">=</span><span class="p">())</span>

<span class="k">print</span><span class="p">(</span><span class="n">counter</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="single-neuron-modeling-conductance-based-models">Single Neuron Modeling: Conductance-Based Models</h1>

<h2 id="neuronal-structureing-potential-and-equivalent-circuits">Neuronal structure,ing potential, and equivalent circuits</h2>

<h3 id="neuronal-structure">Neuronal structure</h3>

<ul>
  <li>Cell body/soma</li>
  <li>Axon</li>
  <li>Dendrites</li>
  <li>Synapses</li>
</ul>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824100656397.png" alt="image-20230824100656397" /></p>

<h3 id="resting-potential">Resting potential</h3>

<p>Transport proteins for ions in neuron cell membranes:</p>

<ul>
  <li>Ion channels: Na + channels, K + channels, … (gated/non-gated)</li>
  <li>Ion pumps: the Na + -K + pump</li>
</ul>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824100812962.png" alt="image-20230824100812962" /></p>

<p>离子浓度在胞内外的差异产生的电势差</p>

<ul>
  <li>
    <p>Ion concentration difference → chemical gradient → electrical gradient</p>
  </li>
  <li>
    <p>Nernst Equation:</p>
  </li>
</ul>

\[E=\dfrac{RT}{zF}\ln\dfrac{[\mathrm{ion}]_{\mathrm{out}}}{[\mathrm{ion}]_{\mathrm{in}}}\]

<ul>
  <li>Goldman-Hodgkin-Katz (GHK) Equation:</li>
</ul>

\[V_m=\frac{RT}{F}\ln\left(\frac{P_{\mathrm{Na}}[\mathrm{Na}^+]_{\mathrm{out}}+P_{\mathrm{K}}[\mathrm{K}^+]_{\mathrm{out}}+P_{\mathrm{Cl}}[\mathrm{Cl}^-]_{\mathrm{in}}}{P_{\mathrm{Na}}[\mathrm{Na}^+]_{\mathrm{in}}+P_{\mathrm{K}}[\mathrm{K}^+]_{\mathrm{in}}+P_{\mathrm{Cl}}[\mathrm{Cl}^-]_{\mathrm{out}}}\right)\]

<h3 id="equivalent-circuits">Equivalent circuits</h3>

<p>Components of an equivalent circuit:</p>

<ul>
  <li>Battery</li>
  <li>Capacitor</li>
  <li>Resistor</li>
</ul>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824101350048.png" alt="image-20230824101350048" /></p>

<p>Considering the potassium channel <strong>ONLY</strong>:</p>

\[\begin{gathered}
0=I_{\mathrm{cap}}+I_{K}=c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+{\frac{V_{\mathrm{M}}-E_{\mathrm{K}}}{R_{\mathrm{K}}}}, \\
c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}=-{\frac{V_{\mathrm{M}}-E_{\mathrm{K}}}{R_{\mathrm{K}}}}=-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}}). 
\end{gathered}\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824101433908.png" alt="image-20230824101433908" /></p>

<p><strong>Considering the Na + , K + , and Cl - channels and the external current I(t):</strong></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824101648270.png" alt="image-20230824101648270" /></p>

\[\begin{aligned}
\frac{I(t)}{A}&amp; =c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+i_{\mathrm{ion}}  \\
\Rightarrow\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}}&amp; =-g_{\mathrm{Cl}}(V_{\mathrm{M}}-E_{\mathrm{Cl}})-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}})-g_{\mathrm{Na}}(V_{\mathrm{M}}-E_{\mathrm{Na}})+\frac{I(t)}{A} 
\end{aligned}\]

<p>Steady-state membrane potential given a constant current input I:</p>

\[\begin{array}{rcl}\Rightarrow&amp;c_{M}\frac{\mathrm{d}V_{M}}{\mathrm{d}t}=-(g_{C1}+g_{K}+g_{Na})V_{M}+g_{C1}E_{C1}+g_{K}E_{K}+g_{Na}E_{Na}+\frac{I(t)}{A}\\\\V_{sS}=\frac{g_{CM}E_{C1}+g_{K}E_{K}+g_{Na}E_{Na}+I/A}{g_{C1}+g_{K}+g_{Na}}&amp;
\xrightarrow{I=0}
&amp;V_{sN,I=0}=E_{R}=\frac{g_{CC}E_{C1}+g_{K}E_{K}+g_{Na}E_{Na}}{g_{C1}+g_{K}+g_{Na}}\end{array}\]

<h2 id="cable-theory--passive-conduction">Cable Theory &amp; passive conduction</h2>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824102017607.png" alt="image-20230824102017607" /></p>

<p>Considering the axon as a long cylindrical cable:</p>

\[I_{\mathrm{cross}}(x,t)={I_{\mathrm{cross}}(x+\Delta x,t)}+I_{\mathrm{ion}}(x,t)+I_{\mathrm{cap}}(x,t)\]

\[V(x+\Delta x,t)-V(x,t)=-I_{\mathrm{cross}}(x,t)R_{\mathrm{L}}=-I_{\mathrm{cross}}(x,t)\frac{\Delta x}{\pi a^{2}}\rho_{\mathrm{L}} \\
{I_{\mathrm{cross}}(x,t)} =-\frac{\pi a^{2}}{\rho_{\mathrm{L}}}\frac{\partial V(x,t)}{\partial x}  \\
{I_{\mathrm{ion}}} =(2\pi a\Delta x)i_{\mathrm{ion}}  \\
I_{\mathrm{cap}}(x,t) =(2\pi a\Delta x)c_{\mathrm{M}}\frac{\partial V(x,t)}{\partial t}\]

<p>-&gt; 
\((2\pi a\Delta x)c_{\mathrm{M}}\frac{\partial V(x,t)}{\partial t}+(2\pi a\Delta x)i_{\mathrm{ion}}=\frac{\pi a^{2}}{\rho_{\mathrm{L}}}\frac{\partial V(x+\Delta x,t)}{\partial x}-\frac{\pi a^{2}}{\rho_{\mathrm{L}}}\frac{\partial V(x,t)}{\partial x}\)</p>

<p><strong>Cable Equation</strong></p>

\[c_\mathrm{M}\frac{\partial V(x,t)}{\partial t}=\frac{a}{2\rho_\mathrm{L}}\frac{\partial^2V(x,t)}{\partial x^2}-i_\mathrm{ion}\]

<p>电流在通过长直导体时会泄露电流，如何记录膜电位，可以使用此方程来描述</p>

<p><strong>Passive conduction:</strong> ion currents are caused by leaky channels exclusively</p>

<p>\(i_{\mathrm{ion}}=V(x,t)/r_{\mathrm{M}}\)
-&gt;</p>

<p>\(\begin{aligned}c_\mathrm{M}\frac{\partial V(x,t)}{\partial t}&amp;=\frac{a}{2\rho_\mathrm{L}}\frac{\partial^2V(x,t)}{\partial x^2}-\frac{V(x,t)}{r_\mathrm{M}}\\\\\tau\frac{\partial V(x,t)}{\partial t}&amp;=\lambda^2\frac{\partial^2V(x,t)}{\partial x^2}-V(x,t)\quad\lambda=\sqrt{0.5ar_\mathrm{M}/\rho_\mathrm{L}}\end{aligned}\)
没有动作电位，单纯通过电缆传输</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824102932665.png" alt="image-20230824102932665" /></p>

<p>If a constant external current is applied to 𝑥 = 0  the steady-state membrane potential $𝑉_{ss}(𝑥)$ is</p>

\[\lambda^2\frac{\mathrm{d}^2V_{\mathrm{ss}}(x)}{\mathrm{d}x^2}-V_{\mathrm{ss}}(x)=0\longrightarrow V_{\mathrm{ss}}(x)=\frac{\lambda\rho_{\mathrm{L}}}{\pi a^2}I_0e^{-x/\lambda}\]

<p>电信号无衰减传播: 动作电位</p>

<h2 id="action-potential--active-transport">Action potential &amp; active transport</h2>

<p>Steps of an action potential:</p>

<ul>
  <li>Depolarization</li>
  <li>Repolarization</li>
  <li>Hyperpolarization</li>
  <li>Resting</li>
</ul>

<p>Characteristics:</p>

<ul>
  <li>All-or-none</li>
  <li>Fixed shape</li>
  <li>Active electrical property</li>
</ul>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824103322522.png" alt="image-20230824103322522" /></p>

<p>How to simulate an action potential?</p>

\[\begin{aligned}
\frac{I(t)}{A}&amp; =c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+i_{\mathrm{ion}}  \\
\Rightarrow\quad c_{\mathrm{M}}\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}&amp; =-g_{\mathrm{Cl}}(V_{\mathrm{M}}-E_{\mathrm{Cl}})-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}})-g_{\mathrm{Na}}(V_{\mathrm{M}}-E_{\mathrm{Na}})+\frac{I(t)}{A} 
\end{aligned}\]

<p>离子通道的开闭会随着电压而变化，电导也随着电压而变化</p>

<p>Mechanism: voltage-gated ion channels</p>

<p><strong>HH建模思路：通过电导</strong></p>

<h3 id="nodes-of-ranvier">Nodes of Ranvier</h3>

<p>Saltatory conduction with a much higher speed and less energy consumption</p>

<p>两个郎飞结之间会有离子通道，既有被动传导，也有主动的防止衰减</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824104220106.png" alt="image-20230824104220106" /></p>

<h2 id="the-hodgkin-huxley-model">The Hodgkin-Huxley Model</h2>

<h3 id="modeling-of-each-ion-channel">Modeling of each ion channel</h3>

<p>Modeling of each ion channel:</p>

\[g_m=\bar{g}_mm^x\]

<p>Modeling of each ion gate:</p>

\[\mathcal{C}\underset{}{\operatorname*{\overset{\alpha(\mathrm{V})}{\underset{\beta(\mathrm{V})}{\operatorname*{\longrightarrow}}}}\mathcal{O}}

\\
\Rightarrow
\begin{aligned}
\frac{\mathrm{d}m}{\mathrm{d}t}&amp; =\alpha(V)(1-m)-\beta(V)m  \\
&amp;=\frac{m_{\infty}(V)-m}{\tau_{m}(V)}
\end{aligned}

\\
\\

\begin{aligned}m_\infty(V)&amp;=\frac{\alpha(V)}{\alpha(V)+\beta(V)}.\\\tau_m(V)&amp;=\frac{1}{\alpha(V)+\beta(V)}\end{aligned}\]

\[\text{If}\ V\text{ is constant:}m(t)=m_\infty(V)+(m_0-m_\infty(V))\mathrm{e}^{-t/\tau_m(V)}\]

<h3 id="voltage-clamp">Voltage clamp</h3>

\[\begin{aligned}
\frac{I(t)}{A}&amp; =c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+i_{\mathrm{ion}}  \\
\Rightarrow\quad c_{\mathrm{M}}\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}&amp; =-g_{\mathrm{Cl}}(V_{\mathrm{M}}-E_{\mathrm{Cl}})-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}})-g_{\mathrm{Na}}(V_{\mathrm{M}}-E_{\mathrm{Na}})+\frac{I(t)}{A} 
\end{aligned}\]

<ul>
  <li>The membrane potential is kept constant</li>
  <li>The current from capacitors is excluded</li>
  <li>Currents must come from leaky/voltage-gated ion channels</li>
</ul>

\[\begin{aligned}I_{\mathrm{cap}}&amp;=c\frac{dV}{dt}=0\\I_{\mathrm{fb}}&amp;=\quad i_{\mathrm{ion}}=g_{\mathrm{Na}}(V-E_{\mathrm{Na}})+g_{\mathrm{K}}(V-E_{\mathrm{K}})+g_{\mathrm{L}}(V-E_{\mathrm{L}})\end{aligned}\]

<p>只测量一个离子通道就可以很容易得到电导</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824111620056.png" alt="image-20230824111620056" /></p>

<h3 id="leaky-channel">Leaky channel</h3>

<p>Hyperpolarization → the sodium and potassium channels are closed</p>

\[I_{\mathrm{fb}}=g_{\mathrm{Na}}(V-E_{\mathrm{Na}})+g_{\mathrm{K}}(V-E_{\mathrm{K}})+g_{\mathrm{L}}(V-E_{\mathrm{L}})\]

\[\Rightarrow I_{\mathrm{fb}}=g_L(V-E_L)\]

\[g_\mathrm{L}=0.3\mathrm{mS/cm}^2,E_\mathrm{L}=-54.4\mathrm{mV}\]

<h4 id="potassium-and-sodium-channels">Potassium and sodium channels</h4>

<p>Potassium channels: Use choline to eliminate the inward current of Na +
Na + current: $I_{fb} - I_{K}$</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824112328953.png" alt="image-20230824112328953" /></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824112333144.png" alt="image-20230824112333144" /></p>

<p>转化速率和电导率两个因素</p>

<p>Potassium channels</p>

<ul>
  <li>Resting state (gate closed)</li>
  <li>Activated state (gate open)</li>
</ul>

<p>→ Activation gate: $g_{\mathrm{K}}=\bar{g}_{K}n^{x}$</p>

<p>Sodium channels</p>

<ul>
  <li>Resting state (gate closed)</li>
  <li>Activated state (gate open)</li>
  <li>Inactivated state (gate blocked)</li>
</ul>

<p>→ Activation gate + inactivation gate: $g_{\mathrm{Na}}=\bar{g}_\text{Na}m^3h$</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824113116329.png" alt="image-20230824113116329" /></p>

<p>The gates of sodium channels</p>

<p>Modeling of each ion gate:</p>

\[\begin{aligned}
&amp;\text{gk}&amp;&amp; =\bar{g}_{K}n^{x}  \\
&amp;\text{gNa}&amp;&amp; =\bar{g}_{\mathrm{Na}}m^{3}h  \\
&amp;\frac{\mathrm{d}n}{\mathrm{d}t}&amp;&amp; =\alpha_{n}(V)(1-n)-\beta_{n}(V)n  \\
&amp;\frac{\mathrm{d}m}{\mathrm{d}t}&amp;&amp; =\alpha_{m}(V)(1-m)-\beta_{m}(V)m  \\
&amp;\frac{\mathrm{d}h}{\mathrm{d}t}&amp;&amp; =\alpha_{h}(V)(1-h)-\beta_{h}(V)h 
\end{aligned}\]

\[\begin{aligned}
\frac{\mathrm{d}m}{\mathrm{d}t}&amp; =\alpha(V)(1-m)-\beta(V)m  \\
&amp;=\frac{m_{\infty}(V)-m}{\tau_{m}(V)}
\end{aligned}\]

\[\begin{aligned}m_\infty(V)&amp;=\frac{\alpha(V)}{\alpha(V)+\beta(V)}\\\tau_m(V)&amp;=\frac{1}{\alpha(V)+\beta(V)}\end{aligned}.\]

\[m(t)=m_\infty(V)+(m_0-m_\infty(V))\mathrm{e}^{-t/\tau_m(V)}\]

<h3 id="the-hodgkin-huxleyhh-model">The Hodgkin-Huxley(HH) Model</h3>

\[c_\mathrm{M}\frac{\mathrm{d}V_\mathrm{M}}{\mathrm{d}t}=-g_\mathrm{Cl}(V_\mathrm{M}-E_\mathrm{Cl})-g_\mathrm{K}(V_\mathrm{M}-E_\mathrm{K})-g_\mathrm{Na}(V_\mathrm{M}-E_\mathrm{Na})+\frac{I(t)}{A}\]

<p>本质是4个微分方程联立在一起</p>

\[\left\{\begin{aligned}&amp;c\frac{\mathrm{d}V}{\mathrm{d}t}=-\bar{g}_\text{Na}m^3h(V-E_\text{Na})-\bar{g}_\text{K}n^4(V-E_\text{K})-\bar{g}_\text{L}(V-E_\text{L})+I_\text{ext},\\&amp;\frac{\mathrm{d}n}{\mathrm{d}t}=\phi\left[\alpha_n(V)(1-n)-\beta_n(V)n\right]\\&amp;\frac{\mathrm{d}m}{\mathrm{d}t}=\phi\left[\alpha_m(V)(1-m)-\beta_m(V)m\right],\\&amp;\frac{\mathrm{d}h}{\mathrm{d}t}=\phi\left[\alpha_h(V)(1-h)-\beta_h(V)h\right],\end{aligned}\right.\]

\[\begin{aligned}\alpha_n(V)&amp;=\frac{0.01(V+55)}{1-\exp\left(-\frac{V+55}{10}\right)},\quad\beta_n(V)&amp;=0.125\exp\left(-\frac{V+65}{80}\right),\\\alpha_h(V)&amp;=0.07\exp\left(-\frac{V+65}{20}\right),\quad\beta_n(V)&amp;=\frac{1}{\left(\exp\left(-\frac{V+55}{10}\right)+1\right)},\\\alpha_m(V)&amp;=\frac{0.1(V+40)}{1-\exp\left(-(V+40)/10\right)},\quad\beta_m(V)&amp;=4\exp\left(-(V+65)/18\right).\end{aligned}\]

\[\phi=Q_{10}^{(T-T_{\mathrm{base}})/10}\]

<p>每一步符合生物学</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824113714178.png" alt="image-20230824113714178" /></p>

<h4 id="how-to-fit-each-gating-variable">How to fit each gating variable?</h4>

<p><strong>Fitting n:</strong> $g_{\mathbf{K}}=\bar{g}<em>{K}n^{x}\quad m(t)=m</em>{\infty}(V)+(m_{0}-\color{red}{\boxed{m_{\infty}(V)}})\mathrm{e}^{-t/\pi_{m}(V)}$</p>

<p>→ $g_\mathrm{K}(V,t)=\bar{g}<em>\mathrm{K}\left[n</em>\infty(V)-(n_\infty(V)-n_0(V))\mathrm{e}^{-\frac{t}{\tau_n(V)}}\right]^x$</p>

<p>by $g_{\mathrm{K}\infty}=\bar{g}<em>{\mathrm{K}}n</em>{\infty}^{x},g_{\mathrm{K}0}=\bar{g}<em>{\mathrm{K}}n</em>{0}^{x}$</p>

<p>→ $g_{\mathrm{K}}(V,t)=\left[g_{\mathrm{K}\infty}^{1/x}-(g_{\mathrm{K}\infty}^{1/x}-g_{\mathrm{K}0}^{1/x})\mathrm{e}^{-\frac{t}{\tau_{n}(V)}}\right]^{x}$</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824114623467.png" alt="image-20230824114623467" /></p>

<h1 id="hodgkin-huxley-brain-dynamics-programming">Hodgkin-Huxley brain dynamics programming</h1>

<h2 id="dynamics-programming-basics">Dynamics Programming Basics</h2>

<h3 id="integrators">Integrators</h3>

<p>微分器</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824140806650.png" alt="image-20230824140806650" /></p>

<p><strong>example</strong></p>

<p>FitzHugh-Nagumo equation</p>

\[\begin{aligned}\tau\dot{w}&amp;=v+a-bw,\\\dot{v}&amp;=v-\frac{\nu^3}{3}-w+I_{\mathrm{ext}}.\end{aligned}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s">'Euler'</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">integral</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">Iext</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>
    <span class="n">dw</span> <span class="o">=</span> <span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span> <span class="o">*</span> <span class="n">w</span><span class="p">)</span> <span class="o">/</span> <span class="n">tau</span>
    <span class="n">dV</span> <span class="o">=</span> <span class="n">V</span> <span class="o">-</span> <span class="n">V</span> <span class="o">*</span> <span class="n">V</span> <span class="o">*</span> <span class="n">V</span> <span class="o">/</span> <span class="mi">3</span> <span class="o">-</span> <span class="n">w</span> <span class="o">+</span> <span class="n">Iext</span>
    <span class="k">return</span> <span class="n">dV</span><span class="p">,</span> <span class="n">dw</span>
</code></pre></div></div>

<p><strong>JointEq</strong></p>

<p>In a dynamical system, there may be multiple variables that change dynamically over time. Sometimes these variables are interrelated, and updating one variable requires other variables as inputs. For better integration accuracy, we recommend that you use <code class="language-plaintext highlighter-rouge">brainpy.JointEq</code> to jointly solve interrelated differential equations.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.20</span>
<span class="n">dV</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">V</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">Iext</span><span class="p">:</span> <span class="mf">0.04</span> <span class="o">*</span> <span class="n">V</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="mi">140</span> <span class="o">-</span> <span class="n">w</span> <span class="o">+</span> <span class="n">Iext</span>	<span class="c1"># 第一个方程
</span><span class="n">dw</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">:</span> <span class="n">a</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">V</span> <span class="o">-</span> <span class="n">w</span><span class="p">)</span>								<span class="c1"># 第二个方程
</span><span class="n">joint_eq</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">JointEq</span><span class="p">(</span><span class="n">dV</span><span class="p">,</span> <span class="n">dw</span><span class="p">)</span>										<span class="c1"># 联合微分方程
</span><span class="n">integral2</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="n">joint_eq</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'rk2'</span><span class="p">)</span>						<span class="c1"># 定义该联合微分方程的数值积分方法
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 声明积分运行器
</span><span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">integrators</span><span class="p">.</span><span class="n">IntegratorRunner</span><span class="p">(</span>
	<span class="n">integral</span><span class="p">,</span>
    <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'V'</span><span class="p">]</span>
    <span class="n">inits</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">V</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="n">args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">Iext</span><span class="o">=</span><span class="n">Iext</span><span class="p">),</span>
    <span class="n">dt</span><span class="o">=</span><span class="mf">0.01</span>
<span class="p">)</span>

<span class="c1"># 使用积分运行器来进行模拟100ms，结合步长dt=0.01
</span><span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="mf">100.</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">V</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824142019832.png" alt="image-20230824142019832" /></p>

<h3 id="dynamicalsystem"><code class="language-plaintext highlighter-rouge">DynamicalSystem</code></h3>

<p>BrainPy provides a generic <code class="language-plaintext highlighter-rouge">SynamicalSystem</code> class to define various types of dynamical models.</p>

<p>BrainPy supports modelings in brain simulation and brain-inspired computing.</p>

<p>All these supports are based on one common concept: <strong>Dynamical System</strong> via <code class="language-plaintext highlighter-rouge">brainpy.DynamicalSystem</code>.</p>

<h4 id="what-is-dynamicalsystem">What is <code class="language-plaintext highlighter-rouge">DynamicalSystem</code></h4>

<p>A <code class="language-plaintext highlighter-rouge">DynamicalSystem</code> defines the updating rule of the model at single time step.</p>

<ol>
  <li>For models with state, <code class="language-plaintext highlighter-rouge">DynamicalSystem</code> defines the state transition from $t$ to $t + dt$, i.e., $S(t+dt)=F(S(t),x,t,dt)$, where $S$ is the state, $x$ is input, $t$ is the time, and $dt$ is the time step. This is the case for recurrent neural networks (like GRU, LSTM), neuron models (like HH, LIF), or synapse models which are widely used in brain simulation.</li>
  <li>However, for models in deep learning, like convolution and fully-connected linear layers, <code class="language-plaintext highlighter-rouge">DynamicalSystem</code> defines the input-to-output mapping, i.e., $y=F(x,t)$.</li>
</ol>

<p><img src="https://brainpy.readthedocs.io/en/latest/_images/dynamical_system.png" alt="img" /></p>

<h4 id="how-to-define-dynamicalsystem">How to define <code class="language-plaintext highlighter-rouge">DynamicalSystem</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">YourDynamicalSystem</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">DynamicalSystem</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="p">...</span>
</code></pre></div></div>

<p>Instead of input x, there are shared arguments across all nodes/layers in the network:</p>

<ul>
  <li>the current time <code class="language-plaintext highlighter-rouge">t</code>, or</li>
  <li>the current running index <code class="language-plaintext highlighter-rouge">i</code>, or</li>
  <li>the current time step <code class="language-plaintext highlighter-rouge">dt</code>, or</li>
  <li>the current phase of training or testing <code class="language-plaintext highlighter-rouge">fit=True/False</code>.</li>
</ul>

<p>Here, it is necessary to explain the usage of <code class="language-plaintext highlighter-rouge">bp.share</code>.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">bp.share.save( )</code>: The function saves shared arguments in the global context. User can save shared arguments in tow ways, for example, if user want to set the current time <code class="language-plaintext highlighter-rouge">t=100</code>, the current time step <code class="language-plaintext highlighter-rouge">dt=0.1</code>,the user can use <code class="language-plaintext highlighter-rouge">bp.share.save("t",100,"dt",0.1)</code> or <code class="language-plaintext highlighter-rouge">bp.share.save(t=100,dt=0.1)</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">bp.share.load( )</code>: The function gets the shared data by the <code class="language-plaintext highlighter-rouge">key</code>, for example, <code class="language-plaintext highlighter-rouge">bp.share.load("t")</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">bp.share.clear_shargs( )</code>: The function clears the specific shared arguments in the global context, for example, <code class="language-plaintext highlighter-rouge">bp.share.clear_shargs("t")</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">bp.share.clear( )</code>: The function clears all shared arguments in the global context.</li>
</ul>

<h4 id="how-to-run-dynamicalsystem">How to run <code class="language-plaintext highlighter-rouge">DynamicalSystem</code></h4>

<p>As we have stated above that <code class="language-plaintext highlighter-rouge">DynamicalSystem</code> only defines the updating rule at single time step, to run a <code class="language-plaintext highlighter-rouge">DynamicalSystem</code> instance over time, we need a for loop mechanism.</p>

<p><img src="https://brainpy.readthedocs.io/en/latest/_images/dynamical_system_and_dsrunner.png" alt="img" /></p>

<h5 id="brainpymathfor_loop"><code class="language-plaintext highlighter-rouge">brainpy.math.for_loop</code></h5>

<p><code class="language-plaintext highlighter-rouge">for_loop</code> is a structural control flow API which runs a function with the looping over the inputs. Moreover, this API just-in-time compile the looping process into the machine code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">section_input</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">100.</span><span class="p">,</span> <span class="mf">200.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">])</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">neu</span><span class="p">.</span><span class="n">step_run</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">neu</span><span class="p">.</span><span class="n">V</span><span class="p">.</span><span class="n">value</span>

<span class="n">vs</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">for_loop</span><span class="p">(</span><span class="n">run</span><span class="p">,</span> <span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">inputs</span><span class="p">),</span> <span class="n">progress_bar</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<h5 id="brainpyloopovertime"><code class="language-plaintext highlighter-rouge">brainpy.LoopOverTime</code></h5>

<p>Different from <code class="language-plaintext highlighter-rouge">for_loop</code>, <code class="language-plaintext highlighter-rouge">brainpy.LoopOverTime</code> is used for constructing a dynamical system that automatically loops the model over time when receiving an input.</p>

<p><code class="language-plaintext highlighter-rouge">for_loop</code> runs the model over time. While <code class="language-plaintext highlighter-rouge">brainpy.LoopOverTime</code> creates a model which will run the model over time when calling it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net2</span><span class="p">.</span><span class="n">reset_state</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">looper</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">LoopOverTime</span><span class="p">(</span><span class="n">net2</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">looper</span><span class="p">(</span><span class="n">currents</span><span class="p">)</span>
</code></pre></div></div>

<h5 id="brainpydsrunner"><code class="language-plaintext highlighter-rouge">brainpy.DSRunner</code></h5>

<p><strong>Initializing a <code class="language-plaintext highlighter-rouge">DSRunner</code></strong></p>

<p>Generally, we can initialize a runner for dynamical systems with the format of:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>runner = DSRunner(target=instance_of_dynamical_system,
                  inputs=inputs_for_target_DynamicalSystem,
                  monitors=interested_variables_to_monitor,
                  dyn_vars=dynamical_changed_variables,
                  jit=enable_jit_or_not,
                  progress_bar=report_the_running_progress,
                  numpy_mon_after_run=transform_into_numpy_ndarray
                  )
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">target</code> specifies the model to be simulated. It must an instance of brainpy.DynamicalSystem.</li>
  <li><code class="language-plaintext highlighter-rouge">inputs</code> is used to define the input operations for specific variables.
    <ul>
      <li>It should be the format of <code class="language-plaintext highlighter-rouge">[(target, value, [type, operation])]</code>, where <code class="language-plaintext highlighter-rouge">target</code> is the input target, <code class="language-plaintext highlighter-rouge">value</code> is the input value, <code class="language-plaintext highlighter-rouge">type</code> is the input type (such as “fix”, “iter”, “func”), <code class="language-plaintext highlighter-rouge">operation</code> is the operation for inputs (such as “+”, “-”, “*”, “/”, “=”). Also, if you want to specify multiple inputs, just give multiple <code class="language-plaintext highlighter-rouge">(target, value, [type, operation])</code>, such as <code class="language-plaintext highlighter-rouge">[(target1, value1), (target2, value2)]</code>.</li>
      <li>It can also be a function, which is used to manually specify the inputs for the target variables. This input function should receive one argument <code class="language-plaintext highlighter-rouge">tdi</code> which contains the shared arguments like time <code class="language-plaintext highlighter-rouge">t</code>, time step <code class="language-plaintext highlighter-rouge">dt</code>, and index <code class="language-plaintext highlighter-rouge">i</code>.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">monitors</code> is used to define target variables in the model. During the simulation, the history values of the monitored variables will be recorded. It can also to monitor variables by callable functions and it should be a <code class="language-plaintext highlighter-rouge">dict</code>. The <code class="language-plaintext highlighter-rouge">key</code> should be a string for later retrieval by <code class="language-plaintext highlighter-rouge">runner.mon[key]</code>. The <code class="language-plaintext highlighter-rouge">value</code> should be a callable function which receives an argument: <code class="language-plaintext highlighter-rouge">tdt</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">dyn_vars</code> is used to specify all the dynamically changed <a href="https://brainpy.readthedocs.io/en/latest/tutorial_math/variables.html">variables</a> used in the <code class="language-plaintext highlighter-rouge">target</code> model.</li>
  <li><code class="language-plaintext highlighter-rouge">jit</code> determines whether to use JIT compilation during the simulation.</li>
  <li><code class="language-plaintext highlighter-rouge">progress_bar</code> determines whether to use progress bar to report the running progress or not.</li>
  <li><code class="language-plaintext highlighter-rouge">numpy_mon_after_run</code> determines whether to transform the JAX arrays into numpy ndarray or not when the network finishes running.</li>
</ul>

<p><strong>Running a <code class="language-plaintext highlighter-rouge">DSRunner</code></strong></p>

<p>After initialization of the runner, users can call <code class="language-plaintext highlighter-rouge">.run()</code> function to run the simulation. The format of function <code class="language-plaintext highlighter-rouge">.run()</code> is showed as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">duration</span><span class="o">=</span><span class="n">simulation_time_length</span><span class="p">,</span>
           <span class="n">inputs</span><span class="o">=</span><span class="n">input_data</span><span class="p">,</span>
           <span class="n">reset_state</span><span class="o">=</span><span class="n">whether_reset_the_model_states</span><span class="p">,</span>
           <span class="n">shared_args</span><span class="o">=</span><span class="n">shared_arguments_across_different_layers</span><span class="p">,</span>
           <span class="n">progress_bar</span><span class="o">=</span><span class="n">report_the_running_progress</span><span class="p">,</span>
           <span class="n">eval_time</span><span class="o">=</span><span class="n">evaluate_the_running_time</span>
           <span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">duration</code> is the simulation time length.</li>
  <li><code class="language-plaintext highlighter-rouge">inputs</code> is the input data. If <code class="language-plaintext highlighter-rouge">inputs_are_batching=True</code>, <code class="language-plaintext highlighter-rouge">inputs</code> must be a PyTree of data with two dimensions: <code class="language-plaintext highlighter-rouge">(num_sample, num_time, ...)</code>. Otherwise, the <code class="language-plaintext highlighter-rouge">inputs</code> should be a PyTree of data with one dimension: <code class="language-plaintext highlighter-rouge">(num_time, ...)</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">reset_state</code> determines whether to reset the model states.</li>
  <li><code class="language-plaintext highlighter-rouge">shared_args</code> is shared arguments across different layers. All the layers can access the elements in <code class="language-plaintext highlighter-rouge">shared_args</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">progress_bar</code> determines whether to use progress bar to report the running progress or not.</li>
  <li><code class="language-plaintext highlighter-rouge">eval_time</code> determines whether to evaluate the running time.</li>
</ul>

<h3 id="monitors">Monitors</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># initialize monitor through a list of strings
</span><span class="n">runner1</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">net</span><span class="p">,</span>
                      <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'E.spike'</span><span class="p">,</span> <span class="s">'E.V'</span><span class="p">,</span> <span class="s">'I.spike'</span><span class="p">,</span> <span class="s">'I.V'</span><span class="p">],</span>  <span class="c1"># 4 elements in monitors
</span>                      <span class="n">inputs</span><span class="o">=</span><span class="p">[(</span><span class="s">'E.input'</span><span class="p">,</span> <span class="mf">20.</span><span class="p">),</span> <span class="p">(</span><span class="s">'I.input'</span><span class="p">,</span> <span class="mf">20.</span><span class="p">)],</span>
                      <span class="n">jit</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>Once we call the runner with a given time duration, the monitor will automatically record the variable evolutions in the corresponding models. Afterwards, users can access these variable trajectories by using .mon.[variable_name]. The default history times .mon.ts will also be generated after the model finishes its running. Let’s see an example.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">runner1</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="mf">100.</span><span class="p">)</span>
<span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">raster_plot</span><span class="p">(</span><span class="n">runner1</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner1</span><span class="p">.</span><span class="n">mon</span><span class="p">[</span><span class="s">'E.spike'</span><span class="p">],</span> <span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Initialization with index specification</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">monitors</span><span class="o">=</span><span class="p">[(</span><span class="s">'E.spike'</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span>  <span class="c1"># monitor values of Variable at index of [1, 2, 3]
</span>                                <span class="s">'E.V'</span><span class="p">],</span>  <span class="c1"># monitor all values of Variable 'V'
</span>
</code></pre></div></div>

<blockquote>
  <p>The monitor shape of “E.V” is (run length, variable size) = (1000, 3200)
The monitor shape of “E.spike” is (run length, index size) = (1000, 3)</p>
</blockquote>

<p><strong>Explicit monitor target</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">monitors</span><span class="o">=</span><span class="p">{</span><span class="s">'spike'</span><span class="p">:</span> <span class="n">net</span><span class="p">.</span><span class="n">E</span><span class="p">.</span><span class="n">spike</span><span class="p">,</span> <span class="s">'V'</span><span class="p">:</span> <span class="n">net</span><span class="p">.</span><span class="n">E</span><span class="p">.</span><span class="n">V</span><span class="p">},</span>
</code></pre></div></div>

<blockquote>
  <p>The monitor shape of “V” is = (1000, 3200)
The monitor shape of “spike” is = (1000, 3200)</p>
</blockquote>

<p><strong>Explicit monitor target with index specification</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">monitors</span><span class="o">=</span><span class="p">{</span><span class="s">'E.spike'</span><span class="p">:</span> <span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">E</span><span class="p">.</span><span class="n">spike</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span>  <span class="c1"># monitor values of Variable at index of [1, 2]
</span>                                <span class="s">'E.V'</span><span class="p">:</span> <span class="n">net</span><span class="p">.</span><span class="n">E</span><span class="p">.</span><span class="n">V</span><span class="p">},</span>  <span class="c1"># monitor all values of Variable 'V'
</span></code></pre></div></div>

<blockquote>
  <p>The monitor shape of “E.V” is = (1000, 3200)
The monitor shape of “E.spike” is = (1000, 2)</p>
</blockquote>

<h3 id="inputs">Inputs</h3>

<p>In brain dynamics simulation, various inputs are usually given to different units of the dynamical system. In BrainPy, <code class="language-plaintext highlighter-rouge">inputs</code> can be specified to runners for dynamical systems. The aim of <code class="language-plaintext highlighter-rouge">inputs</code> is to mimic the input operations in experiments like Transcranial Magnetic Stimulation (TMS) and patch clamp recording.</p>

<p><code class="language-plaintext highlighter-rouge">inputs</code> should have the format like <code class="language-plaintext highlighter-rouge">(target, value, [type, operation])</code>, where</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">target</code> is the target variable to inject the input.</li>
  <li><code class="language-plaintext highlighter-rouge">value</code> is the input value. It can be a scalar, a tensor, or a iterable object/function.</li>
  <li><code class="language-plaintext highlighter-rouge">type</code> is the type of the input value. It support two types of input: <code class="language-plaintext highlighter-rouge">fix</code> and <code class="language-plaintext highlighter-rouge">iter</code>. The first one means that the data is static; the second one denotes the data can be iterable, no matter whether the input value is a tensor or a function. The <code class="language-plaintext highlighter-rouge">iter</code> type must be explicitly stated.</li>
  <li><code class="language-plaintext highlighter-rouge">operation</code> is the input operation on the target variable. It should be set as one of <code class="language-plaintext highlighter-rouge">{ + , - , * , / , = }</code>, and if users do not provide this item explicitly, it will be set to ‘+’ by default, which means that the target variable will be updated as <code class="language-plaintext highlighter-rouge">val = val + input</code>.</li>
</ul>

<h4 id="static-inputs">Static inputs</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">runner6</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">net</span><span class="p">,</span>
                      <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'E.spike'</span><span class="p">],</span>
                      <span class="n">inputs</span><span class="o">=</span><span class="p">[(</span><span class="s">'E.input'</span><span class="p">,</span> <span class="mf">20.</span><span class="p">),</span> <span class="p">(</span><span class="s">'I.input'</span><span class="p">,</span> <span class="mf">20.</span><span class="p">)],</span>  <span class="c1"># static inputs
</span>                      <span class="n">jit</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">runner6</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="mf">100.</span><span class="p">)</span>
<span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">raster_plot</span><span class="p">(</span><span class="n">runner6</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner6</span><span class="p">.</span><span class="n">mon</span><span class="p">[</span><span class="s">'E.spike'</span><span class="p">])</span>
</code></pre></div></div>

<h4 id="iterable-inputs">Iterable inputs</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">I</span><span class="p">,</span> <span class="n">length</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">section_input</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">20.</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                                    <span class="n">durations</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
                                    <span class="n">return_length</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                    <span class="n">dt</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">runner7</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">net</span><span class="p">,</span>
                      <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'E.spike'</span><span class="p">],</span>
                      <span class="n">inputs</span><span class="o">=</span><span class="p">[(</span><span class="s">'E.input'</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="s">'iter'</span><span class="p">),</span> <span class="p">(</span><span class="s">'I.input'</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="s">'iter'</span><span class="p">)],</span>  <span class="c1"># iterable inputs
</span>                      <span class="n">jit</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">runner7</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
<span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">raster_plot</span><span class="p">(</span><span class="n">runner7</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner7</span><span class="p">.</span><span class="n">mon</span><span class="p">[</span><span class="s">'E.spike'</span><span class="p">])</span>
</code></pre></div></div>

<h2 id="run-a-built-in-hh-model">Run a built-in HH model</h2>

<p><a href="https://brainpy.readthedocs.io/en/latest/tutorial_building/overview_of_dynamic_model.html">Using Built-in Models — BrainPy documentation</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">brainpy</span> <span class="k">as</span> <span class="n">bp</span>
<span class="kn">import</span> <span class="nn">brainpy.math</span> <span class="k">as</span> <span class="n">bm</span>

<span class="n">current</span><span class="p">,</span> <span class="n">length</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">section_input</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="n">bm</span><span class="p">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">15.</span><span class="p">]),</span> <span class="mf">0.</span><span class="p">],</span>
                                         <span class="n">durations</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">25</span><span class="p">],</span>
                                         <span class="n">return_length</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">hh_neurons</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">neurons</span><span class="p">.</span><span class="n">HH</span><span class="p">(</span><span class="n">current</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">hh_neurons</span><span class="p">,</span> <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'V'</span><span class="p">,</span> <span class="s">'m'</span><span class="p">,</span> <span class="s">'h'</span><span class="p">,</span> <span class="s">'n'</span><span class="p">],</span> <span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="s">'input'</span><span class="p">,</span> <span class="n">current</span><span class="p">,</span> <span class="s">'iter'</span><span class="p">))</span>

<span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="run-a-hh-model-from-scratch">Run a HH model from scratch</h2>

<p>The mathematic expression of the HH model</p>

\[\left\{\begin{aligned}&amp;c\frac{\mathrm{d}V}{\mathrm{d}t}=-\bar{g}_\text{Na}m^3h(V-E_\text{Na})-\bar{g}_\text{K}n^4(V-E_\text{K})-\bar{g}_\text{L}(V-E_\text{L})+I_\text{ext},\\&amp;\frac{\mathrm{d}n}{\mathrm{d}t}=\phi\left[\alpha_n(V)(1-n)-\beta_n(V)n\right]\\&amp;\frac{\mathrm{d}m}{\mathrm{d}t}=\phi\left[\alpha_m(V)(1-m)-\beta_m(V)m\right],\\&amp;\frac{\mathrm{d}h}{\mathrm{d}t}=\phi\left[\alpha_h(V)(1-h)-\beta_h(V)h\right],\end{aligned}\right.\]

\[\begin{aligned}\alpha_n(V)&amp;=\frac{0.01(V+55)}{1-\exp\left(-\frac{V+55}{10}\right)},\quad\beta_n(V)&amp;=0.125\exp\left(-\frac{V+65}{80}\right),\\\alpha_h(V)&amp;=0.07\exp\left(-\frac{V+65}{20}\right),\quad\beta_n(V)&amp;=\frac{1}{\left(\exp\left(-\frac{V+55}{10}\right)+1\right)},\\\alpha_m(V)&amp;=\frac{0.1(V+40)}{1-\exp\left(-(V+40)/10\right)},\quad\beta_m(V)&amp;=4\exp\left(-(V+65)/18\right).\end{aligned}\]

\[\phi=Q_{10}^{(T-T_{\mathrm{base}})/10}\]

<p>V: the membrane potential</p>

<p>n: activation variable of the Kt channel</p>

<p>m: activation variable of the Nat channel</p>

<p>h; inactivation variable of the Nat channe</p>

<h3 id="define-hh-model-class">Define HH model <code class="language-plaintext highlighter-rouge">class</code></h3>

<ul>
  <li>Inherit <code class="language-plaintext highlighter-rouge">bp.dyn.NeuDyn</code></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">brainpy</span> <span class="k">as</span> <span class="n">bp</span>
<span class="kn">import</span> <span class="nn">brainpy.math</span> <span class="k">as</span> <span class="n">bm</span>

<span class="k">class</span> <span class="nc">HH</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">NeuDyn</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span>
                <span class="n">ENa</span><span class="o">=</span><span class="mf">50.</span><span class="p">,</span> <span class="n">gNa</span><span class="o">=</span><span class="mf">120.</span><span class="p">,</span>
                <span class="n">Ek</span><span class="o">=-</span><span class="mf">77.</span><span class="p">,</span> <span class="n">gK</span><span class="o">=</span><span class="mf">36.</span><span class="p">,</span>
                <span class="n">EL</span><span class="o">=-</span><span class="mf">54.387</span><span class="p">,</span> <span class="n">gL</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span>
                <span class="n">V_th</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mf">6.3</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HH</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="initialization">Initialization</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">brainpy</span> <span class="k">as</span> <span class="n">bp</span>
<span class="kn">import</span> <span class="nn">brainpy.math</span> <span class="k">as</span> <span class="n">bm</span>

<span class="k">class</span> <span class="nc">HH</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">NeuDyn</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span>
                <span class="n">ENa</span><span class="o">=</span><span class="mf">50.</span><span class="p">,</span> <span class="n">gNa</span><span class="o">=</span><span class="mf">120.</span><span class="p">,</span>
                <span class="n">Ek</span><span class="o">=-</span><span class="mf">77.</span><span class="p">,</span> <span class="n">gK</span><span class="o">=</span><span class="mf">36.</span><span class="p">,</span>
                <span class="n">EL</span><span class="o">=-</span><span class="mf">54.387</span><span class="p">,</span> <span class="n">gL</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span>
                <span class="n">V_th</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mf">6.3</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HH</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
        
        <span class="c1"># parameters
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">ENa</span> <span class="o">=</span> <span class="n">ENa</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">EK</span> <span class="o">=</span> <span class="n">EK</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">EL</span> <span class="o">=</span> <span class="n">EL</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gNA</span> <span class="o">=</span> <span class="n">gNa</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gK</span> <span class="o">=</span> <span class="n">gK</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gL</span> <span class="o">=</span> <span class="n">gL</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">C</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">V_th</span> <span class="o">=</span> <span class="n">V_th</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">T_base</span> <span class="o">=</span> <span class="mf">6.3</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">phi</span> <span class="o">=</span> <span class="mf">3.0</span> <span class="o">**</span> <span class="p">((</span><span class="n">T</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">T_base</span><span class="p">)</span> <span class="o">/</span> <span class="mf">10.0</span><span class="p">)</span>
        
        <span class="c1"># variable
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="o">-</span><span class="mf">70.68</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">0.0266</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">0.772</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">0.235</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="nb">input</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">spike</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">t_last_spike</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e7</span><span class="p">)</span>
        
        <span class="c1"># 定义积分函数
</span>    	<span class="bp">self</span><span class="p">.</span><span class="n">integral</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">derivative</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'exp_auto'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="define-the-derivative-function">Define the derivative function</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="nb">property</span>
<span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">bp</span><span class="p">.</span><span class="n">JointEq</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dV</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dm</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dh</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dn</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">dV</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">Iext</span><span class="p">):</span>
    <span class="n">I_Na</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">gNa</span> <span class="o">*</span> <span class="n">m</span> <span class="o">**</span> <span class="mf">3.0</span> <span class="o">*</span> <span class="n">h</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">ENa</span><span class="p">)</span>
    <span class="n">I_K</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">gK</span> <span class="o">*</span> <span class="n">n</span> <span class="o">**</span> <span class="mf">4.0</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">EK</span><span class="p">)</span>
    <span class="n">I_leak</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">gL</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">EL</span><span class="p">)</span>
    <span class="n">dVdt</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span> <span class="n">I_Na</span> <span class="o">-</span> <span class="n">I_K</span> <span class="o">-</span> <span class="n">I_leak</span> <span class="o">+</span> <span class="n">Iext</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">C</span>
    <span class="k">return</span> <span class="n">dVdt</span>

<span class="k">def</span> <span class="nf">dm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">40</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">40</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="mf">4.0</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">65</span><span class="p">)</span> <span class="o">/</span> <span class="mi">18</span><span class="p">)</span>
    <span class="n">dmdt</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">m</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">phi</span> <span class="o">*</span> <span class="n">dmdt</span>

<span class="k">def</span> <span class="nf">dh</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.07</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">65</span><span class="p">)</span> <span class="o">/</span> <span class="mf">20.</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">35</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">dhdt</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">)</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">h</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">phi</span> <span class="o">*</span> <span class="n">dhdt</span>

<span class="k">def</span> <span class="nf">dn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">55</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">55</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="mf">0.125</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">65</span><span class="p">)</span> <span class="o">/</span> <span class="mi">80</span><span class="p">)</span>
    <span class="n">dndt</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">n</span><span class="p">)</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">n</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">phi</span> <span class="o">*</span> <span class="n">dndt</span>
</code></pre></div></div>

<h3 id="complete-the-update-function">Complete the <code class="language-plaintext highlighter-rouge">update()</code> function</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'t'</span><span class="p">)</span>
    <span class="n">dt</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'dt'</span><span class="p">)</span>
    <span class="c1"># TODO: 更新变量V, m, h, n, 暂存在V, m, h, n中
</span>    <span class="n">V</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">V</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="n">dt</span><span class="p">)</span>

    <span class="c1">#判断是否发生动作电位
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">spike</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">logical_and</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">V</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_th</span><span class="p">,</span> <span class="n">V</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_th</span><span class="p">)</span>
    <span class="c1"># 更新最后一次脉冲发放时间
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">t_last_spike</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">spike</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">t_last_spike</span><span class="p">)</span>

    <span class="c1"># TODO: 更新变量V, m, h, n的值
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">V</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">V</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">m</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">m</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">h</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">n</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">n</span>

    <span class="c1">#重置输入
</span>    <span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div></div>

<h3 id="simulation">Simulation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">current</span><span class="p">,</span> <span class="n">length</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">section_input</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="n">bm</span><span class="p">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">15.</span><span class="p">]),</span> <span class="mf">0.</span><span class="p">],</span>
                                          <span class="n">durations</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">25</span><span class="p">],</span>
                                          <span class="n">return_length</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">hh_neurons</span> <span class="o">=</span> <span class="n">HH</span><span class="p">(</span><span class="n">current</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">hh_neurons</span><span class="p">,</span> <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'V'</span><span class="p">,</span> <span class="s">'m'</span><span class="p">,</span> <span class="s">'h'</span><span class="p">,</span> <span class="s">'n'</span><span class="p">],</span> <span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="s">'input'</span><span class="p">,</span> <span class="n">current</span><span class="p">,</span> <span class="s">'iter'</span><span class="p">))</span>

<span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="visualization">Visualization</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">line_plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">V</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s">'V (mV)'</span><span class="p">,</span> <span class="n">plot_ids</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">current</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">current</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="mf">90.</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">m</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">h</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">n</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">([</span><span class="s">'m'</span><span class="p">,</span> <span class="s">'h'</span><span class="p">,</span> <span class="s">'n'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Time (ms)'</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="customize-a-conductance-based-model">Customize a conductance-based model</h2>

<p>电路模拟，写成电导形式</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824180831033.png" alt="image-20230824180831033" /></p>

\[\begin{aligned}
\text{gK}&amp; =\bar{g}_\text{K}n^4,  \\
\frac{\mathrm{d}n}{\mathrm{d}t}&amp; =\phi[\alpha_n(V)(1-n)-\beta_n(V)n], 
\end{aligned}\]

<p>动力学形式描述，引入门框变量$n$</p>

\[\begin{aligned}
&amp;\alpha_{n}(V) =\frac{0.01(V+55)}{1-\exp(-\frac{V+55}{10})},  \\
&amp;\beta_{n}(V) =0.125\exp\left(-\frac{V+65}{80}\right). 
\end{aligned}\]

<p>由此式来建模钾离子通道</p>

<h3 id="programming-an-ion-channel">Programming an ion channel</h3>

<h4 id="three-ion-channel">Three ion channel</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">brainpy</span> <span class="k">as</span> <span class="n">bp</span>
<span class="kn">import</span> <span class="nn">brainpy.math</span> <span class="k">as</span> <span class="n">bm</span>

<span class="k">class</span> <span class="nc">IK</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">IonChannel</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">E</span><span class="o">=-</span><span class="mf">77.</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">36.</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'exp_auto'</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">IK</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">g_max</span> <span class="o">=</span> <span class="n">g_max</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">=</span> <span class="n">E</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">phi</span> <span class="o">=</span> <span class="n">phi</span>

    <span class="bp">self</span><span class="p">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>  <span class="c1"># variables should be packed with bm.Variable
</span>    
    <span class="bp">self</span><span class="p">.</span><span class="n">integral</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dn</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">dn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="n">alpha_n</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">55</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">55</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">beta_n</span> <span class="o">=</span> <span class="mf">0.125</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">65</span><span class="p">)</span> <span class="o">/</span> <span class="mi">80</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">phi</span> <span class="o">*</span> <span class="p">(</span><span class="n">alpha_n</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">n</span><span class="p">)</span> <span class="o">-</span> <span class="n">beta_n</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'t'</span><span class="p">)</span>
    <span class="n">dt</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'dt'</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">n</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="n">dt</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">current</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">g_max</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">n</span> <span class="o">**</span> <span class="mi">4</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">-</span> <span class="n">V</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">INa</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">IonChannel</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span> <span class="mf">50.</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">120.</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'exp_auto'</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">INa</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">g_max</span> <span class="o">=</span> <span class="n">g_max</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">=</span> <span class="n">E</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">phi</span> <span class="o">=</span> <span class="n">phi</span>

    <span class="bp">self</span><span class="p">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>  <span class="c1"># variables should be packed with bm.Variable
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>
    
    <span class="bp">self</span><span class="p">.</span><span class="n">integral_m</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dm</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">integral_h</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dh</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">dm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="c1"># TODO: 计算dm/dt
</span>    <span class="n">alpha_m</span> <span class="o">=</span> <span class="mf">0.11</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">40</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">40</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">beta_m</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">65</span><span class="p">)</span> <span class="o">/</span> <span class="mi">18</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">phi</span> <span class="o">*</span> <span class="p">(</span><span class="n">alpha_m</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span> <span class="o">-</span> <span class="n">beta_m</span> <span class="o">*</span> <span class="n">m</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">dh</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="c1"># TODO: 计算dh/dt
</span>    <span class="n">alpha_h</span> <span class="o">=</span> <span class="mf">0.07</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">65</span><span class="p">)</span> <span class="o">/</span> <span class="mi">20</span><span class="p">)</span>
    <span class="n">beta_h</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">35</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">phi</span> <span class="o">*</span> <span class="p">(</span><span class="n">alpha_h</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">h</span><span class="p">)</span> <span class="o">-</span> <span class="n">beta_h</span> <span class="o">*</span> <span class="n">h</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'t'</span><span class="p">)</span>
    <span class="n">dt</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'dt'</span><span class="p">)</span>
    <span class="c1"># TODO: 更新self.m, self.h
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">m</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral_m</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">m</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="n">dt</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral_h</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="n">dt</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">current</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">g_max</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">m</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">h</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">-</span> <span class="n">V</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">IL</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">IonChannel</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">E</span><span class="o">=-</span><span class="mf">54.39</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">0.03</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">IL</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">g_max</span> <span class="o">=</span> <span class="n">g_max</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">=</span> <span class="n">E</span>

  <span class="k">def</span> <span class="nf">current</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">g_max</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">-</span> <span class="n">V</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="k">pass</span>
</code></pre></div></div>

<h4 id="build-a-hh-model-with-ion-channels">Build a HH model with ion channels</h4>

<p><strong>Using customized ion channels</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">HH</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">CondNeuGroup</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">HH</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">V_initializer</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">Uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">80</span><span class="p">,</span> <span class="o">-</span><span class="mf">60.</span><span class="p">))</span>
    <span class="c1"># TODO: 初始化三个离子通道
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">IK</span> <span class="o">=</span> <span class="n">IK</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">E</span><span class="o">=-</span><span class="mf">77.</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">36.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">INa</span> <span class="o">=</span> <span class="n">INa</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">50.</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">120.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">IL</span> <span class="o">=</span> <span class="n">IL</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">E</span><span class="o">=-</span><span class="mf">54.39</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">0.03</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Using built-in ion channels</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">HH</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">CondNeuGroup</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">INa</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">channels</span><span class="p">.</span><span class="n">INa_HH1952</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">IK</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">channels</span><span class="p">.</span><span class="n">IK_HH1952</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">IL</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">cahnnels</span><span class="p">.</span><span class="n">IL</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">E</span><span class="o">=-</span><span class="mf">54.387</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">0.03</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="simulation-1">Simulation</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">neu</span> <span class="o">=</span> <span class="n">HH</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span>
    <span class="n">neu</span><span class="p">,</span> 
    <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'V'</span><span class="p">,</span> <span class="s">'IK.n'</span><span class="p">,</span> <span class="s">'INa.m'</span><span class="p">,</span> <span class="s">'INa.h'</span><span class="p">],</span> 
    <span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="s">'input'</span><span class="p">,</span> <span class="mf">1.698</span><span class="p">)</span>  <span class="c1"># near the threshold current
</span><span class="p">)</span>

<span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>  <span class="c1"># the running time is 200 ms
</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">[</span><span class="s">'ts'</span><span class="p">],</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">[</span><span class="s">'V'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'t (ms)'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'V (mV)'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">"HH.jpg"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">[</span><span class="s">'ts'</span><span class="p">],</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">[</span><span class="s">'IK.n'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'n'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">[</span><span class="s">'ts'</span><span class="p">],</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">[</span><span class="s">'INa.m'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'m'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">[</span><span class="s">'ts'</span><span class="p">],</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">[</span><span class="s">'INa.h'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'h'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'t (ms)'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">"HH_channels.jpg"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824184016011.png" alt="image-20230824184016011" /></p>

<h1 id="simple-neuron-modeling-simplified-models">Simple Neuron Modeling: Simplified Models</h1>

<h2 id="the-leaky-integrate-and-firelif-neuron-model">The Leaky Integrate-and-Fire(LIF) Neuron Model</h2>

<h3 id="the-lif-neuron-model">The LIF neuron model</h3>

\[\begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-(V-V_{\mathrm{rest}})+RI(t)\\\\\mathrm{if}V&amp;&gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\text{last}\ {t_{ref}}\end{aligned}\]

<p>只有一个微分方程，要加一个不应期(<strong>t refractory period</strong>)，膜电位不发生任何改变，认为离子通道只有泄露通道</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825101057570.png" alt="image-20230825101057570" /></p>

<p>Given a constant current input:</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825101410745.png" alt="image-20230825101410745" /></p>

<p>没有建模准确变化，只提供什么时候膜电位的变化</p>

<h3 id="the-dynamic-features-of-the-lif-model">The dynamic features of the LIF model</h3>

<p><strong>General solution (constant input):</strong>$V(t)=V_{\text{reset}}+RI_{\text{c}}(1-\mathrm{e}^{-\frac{t-t_0}{\tau}})$</p>

<p><strong>Firing frequency:</strong></p>

\[\begin{aligned}T&amp;=-\tau\ln\left(1-\frac{V_{\phi h}-V_{\mathrm{rest}}}{RI_{\varsigma}}\right)\\f&amp;=\frac{1}{T+t_{\mathrm{ref}}}=\frac{1}{t_{\mathrm{ref}}-\tau\ln\left(1-\frac{V_{0}-V_{\mathrm{rest}}}{RI_{\varsigma}}\right)}\end{aligned}\]

<p><strong>Rheobase current (minimal current):</strong></p>

\[I_{\theta}=\frac{V_{\mathrm{th}}-V_{\mathrm{reset}}}{R}\]

<p>基强电流，如果小于它将无法发放</p>

<h3 id="strengths--weaknesses-of-the-lif-model">Strengths &amp; weaknesses of the LIF model</h3>

<h4 id="strengths">Strengths</h4>

<ul>
  <li>Simple, high simulation efficiency</li>
  <li>Intuitive</li>
  <li>Fits well the subthreshold membrane potential</li>
</ul>

<h4 id="weaknesses">Weaknesses</h4>

<ul>
  <li>The shape of action potentials is over-simplified</li>
  <li>Has no memory of the spiking history</li>
  <li>Cannot reproduce diverse firing patterns</li>
</ul>

<h3 id="other-univariate-neuron-models">Other Univariate neuron models</h3>

<h4 id="the-quadratic-integrate-and-fire-qof-model">The Quadratic Integrate-and-Fire (QOF) model:</h4>

\[\begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=a_{0}(V-V_{\mathrm{re}t})(V-V_{\mathrm{c}})+RI(t)\\&amp;\text{if }V&gt;\theta,\quad V\leftarrow V_{\mathrm{re}set}\quad\text{last}\quad t_{\mathrm{ref}}\end{aligned}\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825103243039.png" alt="image-20230825103243039" /></p>

<p>膜电位仍需要手动重置</p>

<h4 id="the-theta-neuron-model">The Theta neuron model</h4>

\[\frac{\mathrm{d}\theta}{\mathrm{d}t}=1-\cos\theta+(1+\cos\theta)(\beta+I(t))\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825103331170.png" alt="image-20230825103331170" /></p>

<p>隐式表达，不具有物理意义，但也会进行整合发放</p>

<h4 id="the-exponential-integrate-and-fire-expif-model">The Exponential Integrate-and-Fire (ExpIF) model</h4>

\[\begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-\left(V-V_{\mathrm{res}t}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{3T}}+RI(t)\\\mathrm{if~}V&amp;&gt;\theta,\quad V\leftarrow V_{\mathrm{res}t}\mathrm{last}t_{\mathrm{ref}}\end{aligned}\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825103501912.png" alt="image-20230825103501912" /></p>

<p>仍需要手动重置膜电位</p>

<h2 id="the-adaptive-exponential-integrate-and-fireadex-neuron-model">The Adaptive Exponential Integrate-and-Fire(AdEx) Neuron Model</h2>

<h3 id="the-adex-neuron-model">The AdEx neuron model</h3>

<p>Two variables:</p>

<ul>
  <li>𝑉: membrane potential</li>
  <li>𝑤: adaptation variable</li>
</ul>

\[\begin{aligned}
\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{S_{T}}}-Rw+RI(t)  \\
\tau_{w}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{\mathrm{w}}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right)  \\
\mathrm{if}V&amp; &gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} 
\end{aligned}\]

<p>不为零，就会衰减到$-w$</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825103840880.png" alt="image-20230825103840880" /></p>

<ul>
  <li>A larger 𝑤 suppresses 𝑉 from increasing</li>
  <li>𝑤 decays exponentially while having a sudden increase when the neuron fires</li>
</ul>

<p><strong>Firing patterns of the AdEx model</strong></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825104254936.png" alt="image-20230825104254936" /></p>

<p><strong>Categorization of firing patterns</strong></p>

<p>According to the steady-state firing time intervals:</p>

<ul>
  <li>Tonic/regular spiking</li>
  <li>Adapting</li>
  <li>Bursting</li>
  <li>Irregular spiking</li>
</ul>

<p>According to the initial-state features:</p>

<ul>
  <li>Tonic/classic spiking</li>
  <li>Initial burst</li>
  <li>Delayed spiking</li>
</ul>

<h3 id="other-multivariate-neuron-models">Other multivariate neuron models</h3>

<h4 id="the-izhikevich-model">The Izhikevich model</h4>

\[\begin{aligned}
&amp;\frac{dV}{dt} =0.04V^{2}+5V+140-u+I  \\
&amp;\frac{\mathrm{d}u}{\mathrm{d}t} =a\left(bV-u\right)  \\
&amp;\operatorname{if}V &gt;\theta,\quad V\leftarrow c,u\leftarrow u+d\text{ last }t_{\mathrm{ref}} 
\end{aligned}\]

<p>二次整合发放多加了一个$u$</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825104832770.png" alt="image-20230825104832770" /></p>

<h4 id="the-fitzhughnagumo-fhn-model">The FitzHugh–Nagumo (FHN) model</h4>

\[\begin{aligned}\dot{v}&amp;=v-\frac{v^3}3-w+RI_{\mathrm{ext}}\\\tau\dot{w}&amp;=v+a-bw.\end{aligned}\]

<p>没有对膜电位进行人为的重置，可以更好的进行动力学分析，没有打破微分方程的连续性</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825104922636.png" alt="image-20230825104922636" /></p>

<h4 id="the-generalized-integrate-and-fire-gif-model">The Generalized Integrate-and-Fire (GIF) model</h4>

<p>n+2个变量</p>

\[\begin{aligned}
&amp;\tau{\frac{\mathrm{d}V}{\mathrm{d}t}} =-\left(V-V_{\mathrm{rest}}\right)+R\sum_{j}I_{j}+RI  \\
&amp;\frac{\mathrm{d}\Theta}{\mathrm{d}t} =a\left(V-V_{\mathrm{rest}}\right)-b\left(\Theta-\Theta_{\infty}\right)  \\
&amp;\frac{\mathrm{d}l_{j}}{\mathrm{d}t} =-k_{j}I_{j},\quad j=1,2,...,n  \\
&amp;\operatorname{if}V &gt;\Theta,\quad I_{j}\leftarrow R_{j}I_{j}+A_{j},V\leftarrow V_{\mathrm{reset}},\Theta\leftarrow max(\Theta_{\mathrm{reset}},\Theta) 
\end{aligned}\]

<p>每个变量都是线性的，泛化性体现在重置条件上</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825105035349.png" alt="image-20230825105035349" /></p>

<h2 id="dynamic-analysis-phase-plane-analysis">Dynamic analysis: phase-plane analysis</h2>

<h3 id="phase-plane-analysis">Phase plane analysis</h3>

<p>对动力学系统的行为来分析，普遍对两个变量来进行分析</p>

<p>Analyzes the behavior of a dynamical system with (usually two) variables described by ordinary differential equations</p>

\[\begin{aligned}
&amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp;&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{S_{T}}}-Rw+RI(t)  \\
&amp;\tau_{W}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp;&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right)  \\
&amp;\mathrm{if}V&amp;&amp; &gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} 
\end{aligned}\]

<p><strong>Elements:</strong></p>

<ul>
  <li>Nullclines: $\mathrm{d}V/\mathrm{d}t=0;\mathrm{d}w/\mathrm{d}t=0$</li>
  <li>Fixed points: $\mathrm{d}V/\mathrm{d}t=0\mathrm{~and~}\mathrm{d}w/\mathrm{d}t=0$</li>
  <li>The vector field</li>
  <li>The trajectory of variables</li>
</ul>

<p>假设外部电流恒定</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825110708994.png" alt="image-20230825110708994" /></p>

<h3 id="phase-plane-analysis-for-the-adex-neuron-model">Phase plane analysis for the AdEx neuron model</h3>

\[\begin{aligned}
&amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp;&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{\Lambda_{T}}}-Rw+RI(t)  \\
&amp;\tau_{w}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp;&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right)  \\
&amp;\text{ifV}&amp;&amp; &gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} 
\end{aligned}\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825110811399.png" alt="image-20230825110811399" /></p>

<h4 id="tonic">Tonic</h4>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825112857175.png" alt="image-20230825112857175" /></p>

<h4 id="adaptation">Adaptation</h4>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825112918815.png" alt="image-20230825112918815" /></p>

<h4 id="bursting">Bursting</h4>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825112933938.png" alt="image-20230825112933938" /></p>

<h4 id="transient-spiking">Transient spiking</h4>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825112950297.png" alt="image-20230825112950297" /></p>

<h2 id="dynamic-analysis-bifurcation-analysis">Dynamic analysis: bifurcation analysis</h2>

<h3 id="bifurcation-analysis">Bifurcation analysis</h3>

<p>Quantitative analysis of the existence and the properties of fixed points in a dynamical system with a changing parameter</p>

<p>某个外界条件变化时，固定点的变化</p>

<p>Elements:</p>

<ul>
  <li>Lines of fixed points</li>
  <li>Stability properties of fixed points</li>
</ul>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825114510710.png" alt="image-20230825114510710" /></p>

<h3 id="bifurcation-analysis-for-the-adex-neuron-model">Bifurcation analysis for the AdEx Neuron model</h3>

<p>bifurcation analysis for 2 variables
Variables: 𝑉 and 𝑤
Parameters: $I_{ext}$</p>

\[\begin{aligned}
&amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}=-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{ST}}}-Rw+RI(t) \\
&amp;\text{-} {\frac{\mathrm{d}w}{\mathrm{d}t}}=a(V-V_{\mathrm{rest}})-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right)  \\
&amp;\mathrm{if}V&gt;\theta,\quad V\leftarrow V_{\mathrm{reset}}\ \mathrm{last}\ t_{\mathrm{ref}}
\end{aligned}\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825114801456.png" alt="image-20230825114801456" /></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825114742740.png" alt="image-20230825114742740" /></p>

<p><strong>Subjects: two variables (𝑉 and 𝑤)</strong></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825114856403.png" alt="image-20230825114856403" /></p>

<h3 id="extended-the-limit-cycle">Extended: The limit cycle</h3>

<p>The FitzHugh–Nagumo (FHN) model</p>

\[\begin{aligned}\dot{v}&amp;=v-\frac{v^3}3-w+RI_\mathrm{ext}\\\tau\dot{w}&amp;=v+a-bw.\end{aligned}\]

<p>This dynamical system, in certain conditions, exhibits a cyclic pattern of variable changes which can be visualized as a closed trajectory in the phase plane.</p>

<p>变化锁定到环中</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825115348008.png" alt="image-20230825115348008" /></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825115354146.png" alt="image-20230825115354146" /></p>

<h1 id="reduced-models---brain-dynamics-programming">Reduced Models - brain dynamics programming</h1>

<h2 id="lif-neuron-models-programming">LIF neuron models programming</h2>

<h3 id="define-lif-class">Define LIF <code class="language-plaintext highlighter-rouge">class</code></h3>

\[\begin{aligned}&amp;\tau\frac{\mathrm{d}V}{\mathrm{d}t}=-(V-V_{\mathrm{rest}})+RI(t)\\&amp;\text{if }V&gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\text{last}t_{\mathrm{ref}}\end{aligned}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LIF</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">NeuDyn</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">V_rest</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">R</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">t_ref</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># 初始化父类
</span>        <span class="nb">super</span><span class="p">(</span><span class="n">LIF</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="initialization-1">Initialization</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LIF</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">NeuDyn</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">V_rest</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">R</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">t_ref</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># 初始化父类
</span>        <span class="nb">super</span><span class="p">(</span><span class="n">LIF</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="c1"># 初始化参数
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">V_rest</span> <span class="o">=</span> <span class="n">V_rest</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">V_reset</span> <span class="o">=</span> <span class="n">V_reset</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">V_th</span> <span class="o">=</span> <span class="n">V_th</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">R</span> <span class="o">=</span> <span class="n">R</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">t_ref</span> <span class="o">=</span> <span class="n">t_ref</span>  <span class="c1"># 不应期时长
</span>        
        <span class="c1"># 初始化变量
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">)</span> <span class="o">+</span> <span class="n">V_reset</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="nb">input</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">t_last_spike</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e7</span><span class="p">)</span>  <span class="c1"># 上一次脉冲发放时间
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">refractory</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">))</span>  <span class="c1"># 是否处于不应期
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">spike</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">))</span>  <span class="c1"># 脉冲发放状态
</span>        
        <span class="c1"># 使用指数欧拉方法进行积分
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">integral</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">derivative</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'exponential_euler'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="define-the-derivative-function-1">Define the derivative function</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 定义膜电位关于时间变化的微分方程
</span><span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">Iext</span><span class="p">):</span>
    <span class="n">dVdt</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">V</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_rest</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">R</span> <span class="o">*</span> <span class="n">Iext</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau</span>
    <span class="k">return</span> <span class="n">dVdt</span>
</code></pre></div></div>

<h3 id="complete-the-update-function-1">Complete the <code class="language-plaintext highlighter-rouge">update()</code> function</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">t</span><span class="p">,</span> <span class="n">dt</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'t'</span><span class="p">],</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'dt'</span><span class="p">]</span>
    <span class="c1"># 以数组的方式对神经元进行更新
</span>    <span class="n">refractory</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">t_last_spike</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="p">.</span><span class="n">t_ref</span>  <span class="c1"># 判断神经元是否处于不应期
</span>    <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">V</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="n">dt</span><span class="p">)</span>  <span class="c1"># 根据时间步长更新膜电位
</span>    <span class="n">V</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">refractory</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">V</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>  <span class="c1"># 若处于不应期，则返回原始膜电位self.V，否则返回更新后的膜电位V
</span>    <span class="n">spike</span> <span class="o">=</span> <span class="n">V</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_th</span>  <span class="c1"># 将大于阈值的神经元标记为发放了脉冲
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">spike</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">spike</span>  <span class="c1"># 更新神经元脉冲发放状态
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">t_last_spike</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">spike</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">t_last_spike</span><span class="p">)</span>  <span class="c1"># 更新最后一次脉冲发放时间
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">V</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">spike</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_reset</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>  <span class="c1"># 将发放了脉冲的神经元膜电位置为V_reset，其余不变
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">refractory</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">refractory</span><span class="p">,</span> <span class="n">spike</span><span class="p">)</span>  <span class="c1"># 更新神经元是否处于不应期
</span>    <span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">[:]</span> <span class="o">=</span> <span class="mf">0.</span>  <span class="c1"># 重置外界输入
</span></code></pre></div></div>

<h3 id="simulation-2">Simulation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">run_LIF</span><span class="p">():</span>
  <span class="c1"># 运行LIF模型
</span>
  <span class="n">group</span> <span class="o">=</span> <span class="n">LIF</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'V'</span><span class="p">],</span> <span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="s">'input'</span><span class="p">,</span> <span class="mf">22.</span><span class="p">))</span>
  <span class="n">runner</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>  <span class="c1"># 运行时长为200ms
</span>
  <span class="c1"># 结果可视化
</span>  <span class="n">fig</span><span class="p">,</span> <span class="n">gs</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">get_figure</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
  <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">V</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$t$ (ms)'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$V$ (mV)'</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'top'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'right'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825141201825.png" alt="image-20230825141201825" /></p>

<h3 id="input-current--firing-frequency">Input current &amp; firing frequency</h3>

\[\begin{gathered}
V(t)=V_{\mathrm{reset}}+RI_{\mathrm{c}}(1-\mathrm{e}^{-\frac{t-t_{0}}{\tau}}). \\
T=-\tau\ln\left[1-\frac{V_{\mathrm{th}}-V_{\mathrm{rest}}}{RI_{\mathrm{c}}}\right] \\
f={\frac{1}{T+t_{\mathrm{ref}}}}={\frac{1}{t_{\mathrm{ref}}-\tau\ln\left[1-{\frac{V_{\mathrm{th}}-V_{\mathrm{rest}}}{RI_{c}}}\right]}} 
\end{gathered}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 输入与频率的关系
</span>
<span class="n">current</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">600</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">duration</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">LIF_neuron</span> <span class="o">=</span> <span class="n">LIF</span><span class="p">(</span><span class="n">current</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">runner_2</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">LIF_neurons</span><span class="p">,</span> <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'spike'</span><span class="p">],</span> <span class="n">inputs</span><span class="o">=</span><span class="p">{</span><span class="s">'input'</span><span class="p">,</span> <span class="n">current</span><span class="p">},</span> <span class="n">dt</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="n">runner_2</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">duration</span><span class="p">)</span>

<span class="n">freqs</span> <span class="o">=</span> <span class="n">runner_2</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">spike</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">duration</span><span class="o">/</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">freqs</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'inputs'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'frequencies'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825143405952.png" alt="image-20230825143405952" /></p>

<h3 id="other-univariate-neuron-models-1">Other Univariate neuron models</h3>

<p><strong>The Quadratic Integrate-and-Fire (QIF) model</strong></p>

\[\begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=a_{0}(V-V_{\mathrm{res}t})(V-V_{c})+RI(t)\\\mathrm{if~}V&amp;&gt;\theta,\quad V\leftarrow V_{\mathrm{reset~last~}t_{\mathrm{ref}}}\end{aligned}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">I</span><span class="p">):</span>
    <span class="n">dVdt</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">c</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_reset</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_c</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">R</span> <span class="o">*</span> <span class="n">I</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau</span>
    <span class="k">return</span> <span class="n">dVdt</span>
</code></pre></div></div>

<p><strong>The Exponential Integrate-and-Fire (ExpIF) model</strong></p>

\[\begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{\delta_{T}}}+RI(t)\\&amp;\mathrm{if~}V&gt;\theta,\quad V\leftarrow V_{\mathrm{reset}}\mathrm{last}t_{\mathrm{ref}}\end{aligned}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">I</span><span class="p">):</span>
    <span class="n">exp_v</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">delta_T</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">((</span><span class="n">V</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_T</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">delta_T</span><span class="p">)</span>
    <span class="n">dvdt</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span> <span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_rest</span><span class="p">)</span> <span class="o">+</span> <span class="n">exp_v</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">R</span> <span class="o">*</span> <span class="n">I</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau</span>
    <span class="k">return</span> <span class="n">dvdt</span>
</code></pre></div></div>

<h2 id="adex-neuron-models-programming">AdEx neuron models programming</h2>

\[\begin{gathered}
\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}=-(V-V_{\mathrm{rest}})+\Delta_{T}\mathrm{e}^{\Delta T}}}-Rw+RI(t), \\
\tau_{w}\frac{\mathrm{d}w}{\mathrm{d}t}=a(V-V_{\mathrm{rest}})-w+b\tau_{w}\sum_{t^{(f)}}\delta(t-t^{(f)})), \\
\mathrm{if~}V&gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\mathrm{last}t_{\mathrm{ref}}. 
\end{gathered}\]

<h3 id="define-adex-class">Define AdEx <code class="language-plaintext highlighter-rouge">class</code></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AdEx</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">NeuDyn</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span>
                <span class="n">V_rest</span><span class="o">=-</span><span class="mi">65</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mi">68</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=-</span><span class="mi">30</span><span class="p">,</span> <span class="n">V_T</span><span class="o">=-</span><span class="mf">59.9</span><span class="p">,</span> <span class="n">delta_T</span><span class="o">=</span><span class="mf">3.48</span>
                <span class="n">a</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">R</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">tau_w</span><span class="o">=</span><span class="mf">30.</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># 初始化父类
</span>        <span class="nb">super</span><span class="p">(</span><span class="n">AdEx</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="initialization-2">Initialization</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AdEx</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">NeuDyn</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span>
                <span class="n">V_rest</span><span class="o">=-</span><span class="mi">65</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mi">68</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=-</span><span class="mi">30</span><span class="p">,</span> <span class="n">V_T</span><span class="o">=-</span><span class="mf">59.9</span><span class="p">,</span> <span class="n">delta_T</span><span class="o">=</span><span class="mf">3.48</span>
                <span class="n">a</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">R</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">tau_w</span><span class="o">=</span><span class="mf">30.</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># 初始化父类
</span>        <span class="nb">super</span><span class="p">(</span><span class="n">AdEx</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="c1"># 初始化参数
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">V_rest</span> <span class="o">=</span> <span class="n">V_rest</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">V_reset</span> <span class="o">=</span> <span class="n">V_reset</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">V_th</span> <span class="o">=</span> <span class="n">V_th</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">V_T</span> <span class="o">=</span> <span class="n">V_T</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">delta_T</span> <span class="o">=</span> <span class="n">delta_T</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">a</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">R</span> <span class="o">=</span> <span class="n">R</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tau_w</span> <span class="o">=</span> <span class="n">tau_w</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">tau_ref</span> <span class="o">=</span> <span class="n">tau_ref</span>
        
        <span class="c1"># 初始化变量
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">)</span> <span class="o">-</span> <span class="mf">65.</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="nb">input</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">t_last_spike</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e7</span><span class="p">)</span>  <span class="c1"># 上一次脉冲发放时间
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">refractory</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">))</span>  <span class="c1"># 是否处于不应期
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">spike</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">))</span>  <span class="c1"># 脉冲发放状态
</span>        
        <span class="c1"># 定义积分器
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">integral</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">derivative</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'exp_auto'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="define-the-derivative-function-2">Define the derivative function</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">dV</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">I</span><span class="p">):</span>
	<span class="n">exp</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">delta_T</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">((</span><span class="n">V</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_T</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">delta_T</span><span class="p">)</span>
    <span class="n">dVdt</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">V</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_rest</span> <span class="o">+</span> <span class="n">exp</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">R</span> <span class="o">*</span> <span class="n">w</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">R</span> <span class="o">*</span> <span class="n">I</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau</span>
    <span class="k">return</span> <span class="n">dVdt</span>

<span class="k">def</span> <span class="nf">dw</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="n">dwdt</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_rest</span><span class="p">)</span> <span class="o">-</span> <span class="n">w</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau_w</span>
    <span class="k">return</span> <span class="n">dwdt</span>

<span class="o">@</span><span class="nb">property</span>
<span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">bp</span><span class="p">.</span><span class="n">JointEq</span><span class="p">([</span><span class="bp">self</span><span class="p">.</span><span class="n">dV</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dw</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="complete-the-update-function-2">Complete the <code class="language-plaintext highlighter-rouge">update()</code> function</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">t</span><span class="p">,</span> <span class="n">dt</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'t'</span><span class="p">],</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'dt'</span><span class="p">]</span>
    <span class="n">V</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">V</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">w</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="n">dt</span><span class="p">)</span>
    <span class="c1"># 以数组的方式对神经元进行更新
</span>    <span class="n">refractory</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">t_last_spike</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="p">.</span><span class="n">t_ref</span>  <span class="c1"># 判断神经元是否处于不应期
</span>    <span class="n">V</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">refractory</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">V</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>  <span class="c1"># 若处于不应期，则返回原始膜电位self.V，否则返回更新后的膜电位V
</span>    <span class="n">spike</span> <span class="o">=</span> <span class="n">V</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_th</span>  <span class="c1"># 将大于阈值的神经元标记为发放了脉冲
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">spike</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">spike</span>  <span class="c1"># 更新神经元脉冲发放状态
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">t_last_spike</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">spike</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">t_last_spike</span><span class="p">)</span>  <span class="c1"># 更新最后一次脉冲发放时间
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">V</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">spike</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_reset</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>  <span class="c1"># 将发放了脉冲的神经元膜电位置为V_reset，其余不变
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">w</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">spike</span><span class="p">,</span> <span class="n">w</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>  <span class="c1">#更新自适应电流
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">refractory</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">refractory</span><span class="p">,</span> <span class="n">spike</span><span class="p">)</span>  <span class="c1"># 更新神经元是否处于不应期
</span>    <span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">[:]</span> <span class="o">=</span> <span class="mf">0.</span>  <span class="c1"># 重置外界输入
</span></code></pre></div></div>

<h3 id="simulation-3">Simulation</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825145518709.png" alt="image-20230825145518709" /></p>

<h3 id="other-multivariate-neuron-models-1">Other multivariate neuron models</h3>

<p><strong>The Izhikevich model</strong></p>

\[\begin{aligned}
&amp;\frac{dV}{dt} =0.04V^{2}+5V+140-u+I  \\
&amp;\frac{\mathrm{d}u}{\mathrm{d}t} =a\left(bV-u\right)  \\
&amp;\operatorname{if}V &gt;\theta,\quad V\leftarrow c,u\leftarrow u+d\mathrm{last}t_{\mathrm{ref}} 
\end{aligned}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">dV</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">I</span><span class="p">):</span>
    <span class="n">dVdt</span> <span class="o">=</span> <span class="mf">0.04</span> <span class="o">*</span> <span class="n">V</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="mi">140</span> <span class="o">-</span> <span class="n">u</span> <span class="o">+</span> <span class="n">I</span>
    <span class="k">return</span> <span class="n">dVdt</span>

<span class="k">def</span> <span class="nf">du</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="n">dudt</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">b</span> <span class="o">*</span> <span class="n">V</span> <span class="o">-</span> <span class="n">u</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dudt</span>
</code></pre></div></div>

<p><strong>The Generalized Integrate-and-Fire (GIF) model</strong></p>

\[\begin{aligned}
&amp;\tau{\frac{\mathrm{d}V}{\mathrm{d}t}} =-\left(V-V_{\mathrm{rest}}\right)+R\sum_{j}I_{j}+RI  \\
&amp;\frac{\mathrm{d}\Theta}{\mathrm{d}t} =a\left(V-V_{\mathrm{est}}\right)-b\left(\Theta-\Theta_{\infty}\right)  \\
&amp;\frac{\mathrm{d}I_j}{\mathrm{d}r} =-k_jI_j,\quad j=1,2,\ldots,n  \\
&amp;\text{if V} &gt;\Theta,\quad I_{j}\leftarrow R_{j}I_{j}+A_{j},V\leftarrow V_{\mathrm{reset}},\Theta\leftarrow max\left(\Theta_{\mathrm{reset}},\Theta\right) 
\end{aligned}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">dI1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">I1</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">k1</span> <span class="o">*</span> <span class="n">I1</span>

<span class="k">def</span> <span class="nf">dI2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">I2</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">k2</span> <span class="o">*</span> <span class="n">I2</span>

<span class="k">def</span> <span class="nf">dVth</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V_th</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">v_rest</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span> <span class="o">*</span> <span class="p">(</span><span class="n">V_th</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_th_inf</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">dV</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">I1</span><span class="p">,</span> <span class="n">I2</span><span class="p">,</span> <span class="n">I</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="o">-</span> <span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_rest</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">R</span> <span class="o">*</span> <span class="p">(</span><span class="n">I</span> <span class="o">+</span> <span class="n">I1</span> <span class="o">+</span> <span class="n">I2</span><span class="p">))</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau</span>
</code></pre></div></div>

<p><strong>Built-in reduced neuron models</strong></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825145947800.png" alt="image-20230825145947800" /></p>

<h2 id="dynamic-analysis-phase-plane-analysis-1">Dynamic analysis: phase-plane analysis</h2>

<h3 id="simple-case">Simple case</h3>

\[\frac{dx}{dt}=\sin(x)+I,\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">bp</span><span class="p">.</span><span class="n">odeint</span>
<span class="k">def</span> <span class="nf">int_x</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">Iext</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">bp</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">Iext</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pp</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">analysis</span><span class="p">.</span><span class="n">PhasePlane1D</span><span class="p">(</span>
	<span class="n">model</span><span class="o">=</span><span class="n">int_x</span><span class="p">,</span>
	<span class="n">target_vars</span><span class="o">=</span><span class="p">{</span><span class="s">'x'</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]},</span>
	<span class="n">pars_update</span><span class="o">=</span><span class="p">{</span><span class="s">'Iext'</span><span class="p">:</span> <span class="mf">0.</span><span class="p">},</span>
    <span class="n">resolutions</span><span class="o">=</span><span class="p">{</span><span class="s">'x'</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">}</span>
<span class="p">)</span>
<span class="n">pp</span><span class="p">.</span><span class="n">plot_vector_field</span><span class="p">()</span>
<span class="n">pp</span><span class="p">.</span><span class="n">plot_fixed_point</span><span class="p">(</span><span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825152003373.png" alt="image-20230825152003373" /></p>

<ul>
  <li>Nullcline: The zero-growth isoclines, such as $f(x,y) = 0$ and $g(x,y) = 0$</li>
  <li>Fixed points: The equilibrium points of the system, which are located at all the nullclines intersect.</li>
  <li>Vector field: The vector field of the system.</li>
  <li>Limit cycles: The limit cycles.</li>
  <li>Trajectories: A simulation trajectory with the given initial values</li>
</ul>

<h3 id="phase-plane-analysis-for-adex">Phase plane analysis for AdEx</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ppa_AdEx</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
    <span class="n">bm</span><span class="p">.</span><span class="n">enable_x64</span><span class="p">()</span>
    
    <span class="n">v_range</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">70.</span><span class="p">,</span> <span class="o">-</span><span class="mf">40.</span><span class="p">]</span>
    <span class="n">w_range</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">10.</span><span class="p">,</span> <span class="mf">50.</span><span class="p">]</span>
    
    <span class="n">phase_plane_analyzer</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">analysis</span><span class="p">.</span><span class="n">PhasePlane2D</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">group</span><span class="p">,</span>
        <span class="n">target_vars</span><span class="o">=</span><span class="p">{</span><span class="s">'V'</span><span class="p">:</span> <span class="n">v_range</span><span class="p">,</span> <span class="s">'w'</span><span class="p">:</span> <span class="n">w_range</span><span class="p">,</span> <span class="p">},</span>  <span class="c1"># 待分析变量
</span>        <span class="n">pars_update</span><span class="o">=</span><span class="p">{</span><span class="s">'I'</span><span class="p">:</span> <span class="n">Iext</span><span class="p">},</span>  <span class="c1"># 需要更新的变量
</span>        <span class="n">resolutions</span><span class="o">=</span><span class="mf">0.05</span>
    <span class="p">)</span>

    <span class="c1"># 画出V, w的零增长曲线
</span>    <span class="n">phase_plane_analyzer</span><span class="p">.</span><span class="n">plot_nullcline</span><span class="p">()</span>
    <span class="c1"># 画出奇点
</span>    <span class="n">phase_plane_analyzer</span><span class="p">.</span><span class="n">plot_fixed_point</span><span class="p">()</span>
    <span class="c1"># 画出向量场
</span>    <span class="n">phase_plane_analyzer</span><span class="p">.</span><span class="n">plot_vector_field</span><span class="p">()</span>
    
    <span class="c1"># 分段画出V, w的变化轨迹
</span>    <span class="n">group</span><span class="p">.</span><span class="n">V</span><span class="p">[:],</span> <span class="n">group</span><span class="p">.</span><span class="n">w</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">group</span><span class="p">.</span><span class="n">V_reset</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'V'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">,</span> <span class="s">'spike'</span><span class="p">],</span> <span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="s">'input'</span><span class="p">,</span> <span class="n">Iext</span><span class="p">))</span>
    <span class="n">runner</span><span class="p">(</span><span class="mi">500</span><span class="p">)</span>
    <span class="n">spike</span> <span class="o">=</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">spike</span><span class="p">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="n">s_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">spike</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># 找到所有发放动作电位对应的index
</span>    <span class="n">s_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">(([</span><span class="mi">0</span><span class="p">],</span> <span class="n">s_idx</span><span class="p">,</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">spike</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]))</span>  <span class="c1"># 加上起始点和终止点的index
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s_idx</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">vs</span> <span class="o">=</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">V</span><span class="p">[</span><span class="n">s_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span> <span class="n">s_idx</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]]</span>
        <span class="n">ws</span> <span class="o">=</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">w</span><span class="p">[</span><span class="n">s_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span> <span class="n">s_idx</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]]</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">vs</span><span class="p">,</span> <span class="n">ws</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'darkslateblue'</span><span class="p">)</span>
        
    <span class="c1"># 画出虚线 x = V_reset
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">([</span><span class="n">group</span><span class="p">.</span><span class="n">V_reset</span><span class="p">,</span> <span class="n">group</span><span class="p">.</span><span class="n">V_reset</span><span class="p">],</span> <span class="n">w_range</span><span class="p">,</span> <span class="s">'--'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'grey'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825152925463.png" alt="image-20230825152925463" /></p>

<h2 id="dynamic-analysis-bifurcation-analysis-1">Dynamic analysis: bifurcation analysis</h2>

<h3 id="simple-case-1">Simple case</h3>

\[\frac{dx}{dt}=\sin(x)+I,\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bif</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">analysis</span><span class="p">.</span><span class="n">Bifurcation1D</span><span class="p">(</span>
	<span class="n">model</span><span class="o">=</span><span class="n">int_x</span><span class="p">,</span>
	<span class="n">target_vars</span><span class="o">=</span><span class="p">{</span><span class="s">'x'</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]},</span>
	<span class="n">target_pars</span><span class="o">=</span><span class="p">{</span><span class="s">'Iext'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]},</span>
	<span class="n">resolutions</span><span class="o">=</span><span class="p">{</span><span class="s">'Iext'</span><span class="p">:</span> <span class="mf">0.005</span><span class="p">,</span> <span class="s">'x'</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">}</span>
<span class="p">)</span>
<span class="n">bif</span><span class="p">.</span><span class="n">plot_bifurcation</span><span class="p">(</span><span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825154227567.png" alt="image-20230825154227567" /></p>

<h1 id="synapse-models-and-their-programming">Synapse models and their programming</h1>

<h2 id="the-biology-of-synapses">The biology of synapses</h2>

<h3 id="neurotransmitter--synapse">Neurotransmitter &amp; Synapse</h3>

<p>When the action potential invades the axon terminals, it causes voltage-gated 𝐶𝐶𝑎𝑎 2+ channels to open (1), which triggers vesicles to bind to the presynaptic membrane (2). Neurotransmitter is released into the synaptic cleft by exocytosis and diffuses across the cleft (3). Binding of the neurotransmitter to receptor molecules in the postsynaptic membrane completes the process of transmission (4).</p>

<p>去极化时钙离子内流，与囊泡相结合，…，与受体结合，打开离子通道，超极化、去极化现象</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826100321307.png" alt="image-20230826100321307" /></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826100418911.png" alt="image-20230826100418911" /></p>

<p><strong>Neurotransmitter leading to postsynaptic potential.</strong></p>

<p>The binding of neurotransmitter to the postsynaptic membrane receptors changes the membrane potential ($V_m$). These postsynaptic potentials can be either excitatory (depolarizing the membrane), as shown here, or inhibitory (hyperpolarizing the membrane).</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826100531535.png" alt="image-20230826100531535" /></p>

<h3 id="neurotransmitters">Neurotransmitters</h3>

<p>兴奋性神经递质：</p>

<ul>
  <li>乙酰胆碱 (ACh)</li>
  <li>儿茶酚胺 (catecholamines)</li>
  <li>谷氨酸 (glutamate)</li>
  <li>组胺 (histamine)</li>
  <li>5-羟色胺 (serotonin)</li>
  <li>某些神经肽类 (some of neuropeptides)</li>
</ul>

<p>抑制性神经递质：</p>

<ul>
  <li>GABA</li>
  <li>甘氨酸 (glycine)</li>
  <li>某些神经肽类 (some of peptides)</li>
</ul>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826100609904.png" alt="image-20230826100609904" /></p>

<h3 id="the-postsynaptic-response">The postsynaptic response</h3>

<p>The aim of a synapse model is to describe accurately the postsynaptic response generated by the arrival of an action potential at a presynaptic terminal.</p>

<ol>
  <li>The fundamental quantity to be modelled is the time course of the postsynaptic receptor conductance</li>
  <li>The models:
    <ul>
      <li>Simple phenomenological waveforms</li>
      <li>More complex kinetic schemes that are analogous to the models of membrane- bound ion channels</li>
    </ul>
  </li>
</ol>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826100701580.png" alt="image-20230826100701580" /></p>

<p>建模这种响应模式，打开关闭的概率…</p>

<h2 id="phenomenological-synapse-models">Phenomenological synapse models</h2>

<h3 id="exponential-model">Exponential Model</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826100738460.png" alt="image-20230826100738460" /></p>

<p><strong>Assumption</strong>:</p>

<ul>
  <li>The release of neurotransmitter, its diffusion across the cleft, the receptor binding, and channel opening all happen very quickly, so that the channels instantaneously jump from the closed to the open state. channel会瞬间增加然后逐渐关闭</li>
</ul>

\[g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}e^{-(t-t_{0})/\tau}
\\
\begin{matrix}\bullet&amp;\tau \ \text{is the time constant}\\\bullet&amp;t_0 \ \text{is the time of the pre-synaptic spike}\\\bullet&amp;\bar{g_{syn}}\ \text{is the maximal conductance}\end{matrix}\]

<p>-&gt; corresponding differential equation</p>

\[\tau\frac{dg_{\mathrm{syn}}(t)}{dt}=-g_{\mathrm{syn}}(t)+\bar{g}_{\mathrm{syn}}\delta\left(t_{0}-t\right)\]

<ul>
  <li>Can fit with experimental data.</li>
  <li>A good approximation for GABA A and AMPA, because the rising phase is much shorter than their decay phase.</li>
</ul>

<h3 id="dual-exponential-model">Dual Exponential Model</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826101203059.png" alt="image-20230826101203059" /></p>

<p>exponential model上升的太快，不太符合某些synapse</p>

<p>Dual exponential synapse provides a general way to describe the synaptic conductance with different rising and decay time constants.</p>

\[g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}\frac{\tau_{1}\tau_{2}}{\tau_{1}-\tau_{2}}\left(\exp\left(-\frac{t-t_{0}}{\tau_{1}}\right)-\exp\left(-\frac{t-t_{0}}{\tau_{2}}\right)\right)
\\
\begin{matrix}
\bullet &amp;t_1\ \text{is the decay synaptic time constant} \\
\bullet &amp;\tau_2\ \text{is the rise synaptic time constant} \\
\bullet &amp;t_0\ \text{is the time of the pre-synaptic spike} \\
\bullet &amp;\bar{g}_{syn}\ \text{is the maximal conductance}
\end{matrix}\]

<p>-&gt;corresponding differential equation</p>

\[\begin{aligned}
&amp;g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}g \\
&amp;\frac{dg}{dt}=-\frac{g}{\tau_{\mathrm{decay}}}+h \\
&amp;\frac{dh}{dt}&amp; =-\frac{h}{\tau_{\mathrm{rise}}}+\delta\left(t_{0}-t\right), 
\end{aligned}\]

<p>The time course of most synaptic conductance can be well described by this sum of two exponentials.</p>

<h3 id="synaptic-time-constants">Synaptic time constants</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826101544786.png" alt="image-20230826101544786" /></p>

<p>http://compneuro.uwaterloo.ca/research/constants-constraints/neurotransmitter-time-constants-pscs.html</p>

<h4 id="ampa-synapse">AMPA synapse</h4>

<ul>
  <li>$t_{decay}$ = 0.18 ms in the auditory system of the chick nucleus magnocellularis (Trussell, 1999).</li>
  <li>$t_{rise}$ 25 ms and $\tau_{decay}$ =0.77 ms in dentate gyrus basket cells (Geiger et al., 1997).</li>
  <li>$t_{rise}$ = 0.2 ms and $\tau_{decay}$ =1.7 ms in in neocortical layer 5 pyramidal neurons (Hausser and Roth, 1997b).</li>
  <li>Reversal potential is nearly 0 mV.</li>
</ul>

<h4 id="nmda-synapse">NMDA synapse</h4>

<ul>
  <li>The decay time constants (at near-physiological temperature):
    <ul>
      <li>19 ms in dentate gyrus basket cells (Geiger et al., 1997),</li>
      <li>26 ms in neocortical layer 2/3 pyramidal neurons (Feldmeyer et al., 2002),</li>
      <li>89 ms in CA1 pyramidal cells (Diamond, 2001).</li>
    </ul>
  </li>
  <li>The rise time constants are about 2 ms (Feldmeyer et al., 2002).</li>
  <li>Reversal potential is nearly 0 mV.</li>
</ul>

<h4 id="gaba_a-synapse">GABA$_A$ synapse</h4>

<ul>
  <li>GABAergic synapses from dentate gyrus basket cells onto other basket cells are faster: $t_{rise}$ = 0.3 ms and $t_{decay}$ = 2.5 ms (Bartos et al., 2001) than synapses from basket cells to granule cells: $t_{rise}$ = 0.26 ms and $t_{decay}$ = 6.5 ms (Kraushaar and Jonas, 2000).</li>
  <li>Reversal potential is nearly -80 mV.</li>
</ul>

<h4 id="gaba_b-synapse">GABA$_B$ synapse</h4>

<ul>
  <li>Common models use models with a rise time of about 25-50 ms, a fast decay time in the range of 100-300ms and a slow decay time of 500-1000 ms.</li>
  <li>Reversal potential is nearly -90 mV.</li>
</ul>

<h3 id="general-property-of-synaptic-time-constants">General property of synaptic time constants</h3>

<ul>
  <li>The time constants of synaptic conductance vary widely among synapse types.</li>
  <li>The synaptic kinetics tends to accelerate during development (T. Takahashi, Neuroscience Research, 2005) .</li>
  <li>The synaptic kinetics becomes substantially faster with increasing temperature.</li>
</ul>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826102033433.png" alt="image-20230826102033433" /></p>

<h3 id="current--and-conductance-based-response">Current- and Conductance-based Response</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826102042614.png" alt="image-20230826102042614" /></p>

<h4 id="conductance-based-response">Conductance-based Response</h4>

<p>Most synaptic ion channels, such as AMPA and GABA, display an approximately linear current-voltage relationship when they open.</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826102113670.png" alt="image-20230826102113670" /></p>

<p><strong>For example</strong>:
The synapse is located on a thin dendrite, because the local membrane potential V changes considerably when the synapse is activated.</p>

<h4 id="current-based-response">Current-based Response</h4>

<p>In some case, we can also approximate the synapses as sources of current and not a conductance.</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826102150487.png" alt="image-20230826102150487" /></p>

<p><strong>For example</strong>:</p>

<p>The excitatory synapse on a large compartment, because the depolarization of the membrane is small.</p>

<h2 id="programming-of-phenomenological-synapse-models">Programming of phenomenological synapse models</h2>

<h3 id="projalignpostmg2"><code class="language-plaintext highlighter-rouge">ProjAlignPostMg2</code></h3>

<p><img src="https://cdn.kesci.com/upload/rzz4o4uyar.png?imageView2/0/w/960/h/960" alt="Image Name" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">brainpy</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPostMg2</span><span class="p">(</span>  
   <span class="n">pre</span><span class="p">,</span>  
   <span class="n">delay</span><span class="p">,</span>  
   <span class="n">comm</span><span class="p">,</span>  
   <span class="n">syn</span><span class="p">,</span>  
   <span class="n">out</span><span class="p">,</span>  
   <span class="n">post</span>  
<span class="p">)</span>  
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">pre (JointType[DynamicalSystem, AutoDelaySupp])</code>: The pre-synaptic neuron group.</li>
  <li><code class="language-plaintext highlighter-rouge">delay (Union[None, int, float])</code>: The synaptic delay.</li>
  <li><code class="language-plaintext highlighter-rouge">comm (DynamicalSystem)</code>: The synaptic communication.</li>
  <li><code class="language-plaintext highlighter-rouge">syn (ParamDescInit)</code>: The synaptic dynamics.</li>
  <li><code class="language-plaintext highlighter-rouge">out (ParamDescInit)</code>: The synaptic output.</li>
  <li><code class="language-plaintext highlighter-rouge">post (DynamicalSystem)</code> The post-synaptic neuron group.</li>
</ul>

<p>只需要建模所有post的neurons</p>

<h3 id="csr-matrix">CSR matrix</h3>

<p><img src="https://cdn.kesci.com/upload/rzz4on32hr.png?imageView2/0/w/960/h/960" alt="Image Name" /></p>

<h3 id="exponential-model-1">Exponential Model</h3>

<p>The single exponential decay synapse model assumes the release of neurotransmitter, its diffusion across the cleft, the receptor binding, and channel opening all happen very quickly, so that the channels instantaneously jump from the closed to the open state. Therefore, its expression is given by</p>

\[g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} e^{-\left(t-t_{0}\right) / \tau}\]

<p>where $\tau$ is the time constant, $t_0$ is the time of the pre-synaptic spike, $\bar{g}_{\mathrm{syn}}$ is the maximal conductance.</p>

<p>The corresponding differential equation:</p>

\[\frac{d g}{d t} = -\frac{g}{\tau_{decay}}+\sum_{k} \delta(t-t_{j}^{k}).\]

<h4 id="coba">COBA</h4>

<p>Given the synaptic conductance, the COBA model outputs the post-synaptic current with</p>

\[I_{syn}(t) = g_{\mathrm{syn}}(t) (E - V(t))\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ExponSparseCOBA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">Projection</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">g_max</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">E</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    
    <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPostMg2</span><span class="p">(</span>
      <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">,</span> 
      <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">,</span> 
      <span class="n">comm</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">EventCSRLinear</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="n">FixedProb</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">),</span> <span class="n">g_max</span><span class="p">),</span>
      <span class="n">syn</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">Expon</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">),</span>
      <span class="n">out</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">COBA</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">E</span><span class="o">=</span><span class="n">E</span><span class="p">),</span>
      <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">,</span> 
    <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SimpleNet</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">DynSysGroup</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">pre</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">SpikeTimeGroup</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">times</span><span class="o">=</span><span class="p">(</span><span class="mf">10.</span><span class="p">,</span> <span class="mf">30.</span><span class="p">,</span> <span class="mf">50.</span><span class="p">,</span> <span class="mf">70.</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">post</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">LifRef</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">V_rest</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=-</span><span class="mf">50.</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span>
                              <span class="n">V_initializer</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">Constant</span><span class="p">(</span><span class="o">-</span><span class="mf">60.</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">syn</span> <span class="o">=</span> <span class="n">ExponSparseCOBA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">prob</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="n">E</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">pre</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">syn</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">()</span>
    
    <span class="c1"># monitor the following variables
</span>    <span class="n">conductance</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">syn</span><span class="p">.</span><span class="n">proj</span><span class="p">.</span><span class="n">refs</span><span class="p">[</span><span class="s">'syn'</span><span class="p">].</span><span class="n">g</span>
    <span class="n">current</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">sum_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">V</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">conductance</span><span class="p">,</span> <span class="n">current</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">V</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">run_a_net</span><span class="p">(</span><span class="n">net</span><span class="p">):</span>
  <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>  <span class="c1"># 100 ms
</span>  <span class="n">conductances</span><span class="p">,</span> <span class="n">currents</span><span class="p">,</span> <span class="n">potentials</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">for_loop</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">step_run</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">progress_bar</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  <span class="n">ts</span> <span class="o">=</span> <span class="n">indices</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">()</span>
  
  <span class="c1"># --- similar to: 
</span>  <span class="c1"># runner = bp.DSRunner(net)
</span>  <span class="c1"># conductances, currents, potentials = runner.run(100.)
</span>  
  <span class="n">fig</span><span class="p">,</span> <span class="n">gs</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">get_figure</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
  <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">conductances</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Syn conductance'</span><span class="p">)</span>
  <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">currents</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Syn current'</span><span class="p">)</span>
  <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">potentials</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Post V'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="cuba">CUBA</h4>

<p>Given the conductance, this model outputs the post-synaptic current with a identity function:</p>

\[I_{\mathrm{syn}}(t) = g_{\mathrm{syn}}(t)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ExponSparseCUBA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">Projection</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">g_max</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    
    <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPostMg2</span><span class="p">(</span>
      <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">,</span> 
      <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">,</span> 
      <span class="n">comm</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">EventCSRLinear</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="n">FixedProb</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">),</span> <span class="n">g_max</span><span class="p">),</span>
      <span class="n">syn</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">Expon</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">),</span>
      <span class="n">out</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">CUBA</span><span class="p">.</span><span class="n">desc</span><span class="p">(),</span>
      <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">,</span> 
    <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SimpleNet2</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">DynSysGroup</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    
    <span class="bp">self</span><span class="p">.</span><span class="n">pre</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">SpikeTimeGroup</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">times</span><span class="o">=</span><span class="p">(</span><span class="mf">10.</span><span class="p">,</span> <span class="mf">30.</span><span class="p">,</span> <span class="mf">50.</span><span class="p">,</span> <span class="mf">70.</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">post</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">LifRef</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">V_rest</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=-</span><span class="mf">50.</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span>
                              <span class="n">V_initializer</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">Constant</span><span class="p">(</span><span class="o">-</span><span class="mf">60.</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">syn</span> <span class="o">=</span> <span class="n">ExponSparseCUBA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">prob</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="n">g_max</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">5.</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">pre</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">syn</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">()</span>
    
    <span class="c1"># monitor the following variables
</span>    <span class="n">conductance</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">syn</span><span class="p">.</span><span class="n">proj</span><span class="p">.</span><span class="n">refs</span><span class="p">[</span><span class="s">'syn'</span><span class="p">].</span><span class="n">g</span>
    <span class="n">current</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">sum_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">V</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">conductance</span><span class="p">,</span> <span class="n">current</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">V</span>
</code></pre></div></div>

<h4 id="dense-connections">Dense connections</h4>

<p>Exponential synapse model with the conductance-based (COBA) output current and dense connections.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ExponDenseCOBA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">Projection</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">g_max</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">E</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    
    <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPostMg2</span><span class="p">(</span>
      <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">,</span> 
      <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">,</span> 
      <span class="n">comm</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">MaskedLinear</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="n">FixedProb</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">),</span> <span class="n">g_max</span><span class="p">),</span>
      <span class="n">syn</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">Expon</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">),</span>
      <span class="n">out</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">COBA</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">E</span><span class="o">=</span><span class="n">E</span><span class="p">),</span>
      <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">,</span> 
    <span class="p">)</span>
</code></pre></div></div>

<p><img src="https://cdn.kesci.com/upload/rzz4p7x6dl.png?imageView2/0/w/960/h/960" alt="Image Name" /></p>

<p>Exponential synapse model with the current-based (COBA) output current and dense connections.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ExponDenseCUBA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">Projection</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">g_max</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">E</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    
    <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPostMg2</span><span class="p">(</span>
      <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">,</span> 
      <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">,</span> 
      <span class="n">comm</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">MaskedLinear</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="n">FixedProb</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">),</span> <span class="n">g_max</span><span class="p">),</span>
      <span class="n">syn</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">Expon</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">),</span>
      <span class="n">out</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">CUBA</span><span class="p">.</span><span class="n">desc</span><span class="p">(),</span>
      <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">,</span> 
    <span class="p">)</span>
</code></pre></div></div>

<h3 id="projalignpremg2"><code class="language-plaintext highlighter-rouge">ProjAlignPreMg2</code></h3>

<p>Synaptic projection which defines the synaptic computation with the dimension of presynaptic neuron group.</p>

<p><img src="https://cdn.kesci.com/upload/rzz4pj1qmk.png?imageView2/0/w/960/h/960" alt="Image Name" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">brainpy</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPreMg2</span><span class="p">(</span>  
   <span class="n">pre</span><span class="p">,</span>  
   <span class="n">delay</span><span class="p">,</span>  
   <span class="n">syn</span><span class="p">,</span>  
   <span class="n">comm</span><span class="p">,</span>  
   <span class="n">out</span><span class="p">,</span>  
   <span class="n">post</span>  
<span class="p">)</span>  
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">pre (JointType[DynamicalSystem, AutoDelaySupp])</code>: The pre-synaptic neuron group.</li>
  <li><code class="language-plaintext highlighter-rouge">delay (Union[None, int, float])</code>: The synaptic delay.</li>
  <li><code class="language-plaintext highlighter-rouge">syn (ParamDescInit)</code>: The synaptic dynamics.</li>
  <li><code class="language-plaintext highlighter-rouge">comm (DynamicalSystem)</code>: The synaptic communication.</li>
  <li><code class="language-plaintext highlighter-rouge">out (ParamDescInit)</code>: The synaptic output.</li>
  <li><code class="language-plaintext highlighter-rouge">post (DynamicalSystem)</code> The post-synaptic neuron group.</li>
</ul>

<h3 id="dual-exponential-model-1">Dual Exponential Model</h3>

<p>The dual exponential synapse model, also named as <strong>difference of two exponentials model</strong>, is given by:</p>

\[g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} \frac{\tau_{1} \tau_{2}}{\tau_{1}-\tau_{2}}\left(\exp \left(-\frac{t-t_{0}}{\tau_{1}}\right)-\exp \left(-\frac{t-t_{0}}{\tau_{2}}\right)\right)\]

<p>where $\tau_1$ is the time constant of the decay phase, $\tau_2$ is the time constant of the rise phase, $t_0$ is the time of the pre-synaptic spike, $\bar{g}_{\mathrm{syn}}$ is the maximal conductance.</p>

<p>The corresponding differential equation:</p>

\[\begin{aligned}  
&amp;g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} g \\  
&amp;\frac{d g}{d t}=-\frac{g}{\tau_{\mathrm{decay}}}+h \\  
&amp;\frac{d h}{d t}=-\frac{h}{\tau_{\text {rise }}}+ \delta\left(t_{0}-t\right),  
\end{aligned}\]

<p>The alpha function is retrieved in the limit when both time constants are equal.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DualExpSparseCOBA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">Projection</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">g_max</span><span class="p">,</span> <span class="n">tau_decay</span><span class="p">,</span> <span class="n">tau_rise</span><span class="p">,</span> <span class="n">E</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    
    <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPreMg2</span><span class="p">(</span>
      <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">,</span> 
      <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">,</span> 
      <span class="n">syn</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">DualExpon</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">tau_decay</span><span class="o">=</span><span class="n">tau_decay</span><span class="p">,</span> <span class="n">tau_rise</span><span class="o">=</span><span class="n">tau_rise</span><span class="p">),</span>
      <span class="n">comm</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">CSRLinear</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="n">FixedProb</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">),</span> <span class="n">g_max</span><span class="p">),</span>
      <span class="n">out</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">COBA</span><span class="p">(</span><span class="n">E</span><span class="o">=</span><span class="n">E</span><span class="p">),</span>
      <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">,</span> 
    <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SimpleNet4</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">DynSysGroup</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    
    <span class="bp">self</span><span class="p">.</span><span class="n">pre</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">SpikeTimeGroup</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">times</span><span class="o">=</span><span class="p">(</span><span class="mf">10.</span><span class="p">,</span> <span class="mf">30.</span><span class="p">,</span> <span class="mf">50.</span><span class="p">,</span> <span class="mf">70.</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">post</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">LifRef</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">V_rest</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=-</span><span class="mf">50.</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span>
                              <span class="n">V_initializer</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">Constant</span><span class="p">(</span><span class="o">-</span><span class="mf">60.</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">syn</span> <span class="o">=</span> <span class="n">DualExpSparseCOBA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">prob</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> 
                                 <span class="n">tau_decay</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">tau_rise</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="n">E</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">pre</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">syn</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">()</span>
    
    <span class="c1"># monitor the following variables
</span>    <span class="n">conductance</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">syn</span><span class="p">.</span><span class="n">proj</span><span class="p">.</span><span class="n">refs</span><span class="p">[</span><span class="s">'syn'</span><span class="p">].</span><span class="n">g</span>
    <span class="n">current</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">sum_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">V</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">conductance</span><span class="p">,</span> <span class="n">current</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">V</span>
</code></pre></div></div>

<h2 id="biophysical-synapse-models">Biophysical synapse models</h2>

<h3 id="limitations-of-phenomenological-models">Limitations of phenomenological models</h3>

<p>打开的数量是有限的，而且有饱和期</p>

<ol>
  <li>Saturation of postsynaptic receptors by previously released transmitter.</li>
  <li>Certain receptor types also exhibit desensitization that prevents them (re)opening for a period after transmitter-binding, like sodium channels underlying action potential.</li>
</ol>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826111443117.png" alt="image-20230826111443117" /></p>

<h3 id="lineticmarkov-models">Linetic/Markov models</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826111733654.png" alt="image-20230826111733654" /></p>

<ul>
  <li>The simplest kinetic model is a two-state scheme in which receptors can be either closed, 𝐶, or open, 𝑂, and the transition between states depends on transmitter concentration, [𝑇], in the synaptic cleft:</li>
  <li>𝛼 and 𝛽 are voltage-independent forward and backward rate constants.</li>
  <li>𝐶 and 𝑂 can range from 0 to 1, and describe the fraction of receptors in the closed and open states, respectively.</li>
  <li>The synaptic conductance is: $g_{syn}(t)=\bar{g}_{max}g(t)$</li>
</ul>

<h3 id="ampagaba_a-synapse-model">AMPA/GABA$_A$ synapse model</h3>

<p><img src="Notes.assets/image-20230826111831637.png" alt="image-20230826111831637" style="zoom:50%;" /></p>

<p><img src="Notes.assets/image-20230826111841073.png" alt="image-20230826111841073" style="zoom:50%;" /></p>

\[\begin{aligned}\frac{ds}{dt}&amp;=\alpha[T](1-s)-\beta s\\I&amp;=\tilde{g}s(V-E)\end{aligned}\]

<ul>
  <li>𝛼[𝑇] denotes the transition probability from state (1−𝑠) to state (𝑠)</li>
  <li>𝛽 represents the transition probability of the other direction</li>
  <li>𝐸 is a reverse potential, which can determine whether the direction of 𝐼 is inhibition or excitation.</li>
  <li>𝐸 = 0 𝑚𝑚𝑉𝑉 =&gt; Excitatory synapse [AMPA]</li>
  <li>𝐸 = −80 𝑚𝑚𝑉𝑉 =&gt; Inhibitory synapse [GABA A ]</li>
</ul>

<h3 id="comparison">Comparison</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826111950713.png" alt="image-20230826111950713" /></p>

<h3 id="nmda-synapse-model">NMDA synapse model</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826112027689.png" alt="image-20230826112027689" /></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826112034481.png" alt="image-20230826112034481" /></p>

\[\begin{aligned}
&amp;\frac{ds}{dt} =\alpha[T](1-s)-\beta s  \\
&amp;I=\tilde{g}sB(V)(V-E) \\
&amp;B(V )=\frac{1}{1+\exp(-0.062V)[Mg^{2+}]_{o}/3.57} 
\end{aligned}\]

<p>The magnesium block of the NMDA receptor channel is an extremely fast process compared to the other kinetics of the receptor (Jahr and Stevens 1990a, 1990b). The block can therefore be accurately modeled as an instantaneous function of voltage(Jahr and Stevens 1990b).</p>

<p>where $[Mg^{2+}]$ is the external magnesium concentration (1 to 2mM inphysiological conditions)</p>

<h2 id="programming-of-biophysical-synapse-models">Programming of biophysical synapse models</h2>

<h3 id="ampa-synapse-model">AMPA synapse model</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AMPA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">Projection</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">g_max</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPreMg2</span><span class="p">(</span>
          <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">,</span> 
          <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">,</span> 
          <span class="n">syn</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">AMPA</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.18</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">T_dur</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
          <span class="n">comm</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">CSRLinear</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="n">FixedProb</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">),</span> <span class="n">g_max</span><span class="p">),</span>
          <span class="n">out</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">COBA</span><span class="p">(</span><span class="n">E</span><span class="o">=</span><span class="n">E</span><span class="p">),</span>
          <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">,</span> 
        <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SimpleNet</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">DynSysGroup</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">syn_cls</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">pre</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">SpikeTimeGroup</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">times</span><span class="o">=</span><span class="p">(</span><span class="mf">10.</span><span class="p">,</span> <span class="mf">30.</span><span class="p">,</span> <span class="mf">50.</span><span class="p">,</span> <span class="mf">70.</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">post</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">LifRef</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">V_rest</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=-</span><span class="mf">50.</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span>
                              <span class="n">V_initializer</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">Constant</span><span class="p">(</span><span class="o">-</span><span class="mf">60.</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">syn</span> <span class="o">=</span> <span class="n">syn_cls</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">prob</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">pre</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">syn</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">()</span>
    
    <span class="c1"># monitor the following variables
</span>    <span class="n">conductance</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">syn</span><span class="p">.</span><span class="n">proj</span><span class="p">.</span><span class="n">refs</span><span class="p">[</span><span class="s">'syn'</span><span class="p">].</span><span class="n">g</span>
    <span class="n">current</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">sum_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">V</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">conductance</span><span class="p">,</span> <span class="n">current</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">V</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">run_a_net</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">duration</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
  <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">duration</span><span class="o">/</span><span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">()))</span>  <span class="c1"># duration ms
</span>  <span class="n">conductances</span><span class="p">,</span> <span class="n">currents</span><span class="p">,</span> <span class="n">potentials</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">for_loop</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">step_run</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">progress_bar</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  <span class="n">ts</span> <span class="o">=</span> <span class="n">indices</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">()</span>
  
  <span class="c1"># --- similar to: 
</span>  <span class="c1"># runner = bp.DSRunner(net)
</span>  <span class="c1"># conductances, currents, potentials = runner.run(100.)
</span>  
  <span class="n">fig</span><span class="p">,</span> <span class="n">gs</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">get_figure</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
  <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">conductances</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Syn conductance'</span><span class="p">)</span>
  <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">currents</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Syn current'</span><span class="p">)</span>
  <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">potentials</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Post V'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="textgaba_a-synapse-model">$\text{GABA}_A$ synapse model</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GABAa</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">Projection</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">g_max</span><span class="p">,</span> <span class="n">E</span><span class="o">=-</span><span class="mf">80.</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPreMg2</span><span class="p">(</span>
          <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">,</span> 
          <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">,</span> 
          <span class="n">syn</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">GABAa</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.53</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.18</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">T_dur</span><span class="o">=</span><span class="mf">1.0</span><span class="p">),</span>
          <span class="n">comm</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">CSRLinear</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="n">FixedProb</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">),</span> <span class="n">g_max</span><span class="p">),</span>
          <span class="n">out</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">COBA</span><span class="p">(</span><span class="n">E</span><span class="o">=</span><span class="n">E</span><span class="p">),</span>
          <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">,</span> 
        <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">run_a_net</span><span class="p">(</span><span class="n">SimpleNet</span><span class="p">(</span><span class="n">syn_cls</span><span class="o">=</span><span class="n">GABAa</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="nmda-synapse-model-1">NMDA synapse model</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">NMDA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">Projection</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">g_max</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPreMg2</span><span class="p">(</span>
          <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">,</span> 
          <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">,</span> 
          <span class="n">syn</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">NMDA</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">tau_decay</span><span class="o">=</span><span class="mf">100.</span><span class="p">,</span> <span class="n">tau_rise</span><span class="o">=</span><span class="mf">2.</span><span class="p">),</span> 
          <span class="n">comm</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">CSRLinear</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="n">FixedProb</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">),</span> <span class="n">g_max</span><span class="p">),</span> 
          <span class="n">out</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">MgBlock</span><span class="p">(</span><span class="n">E</span><span class="o">=</span><span class="n">E</span><span class="p">),</span> 
          <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">,</span> 
        <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">run_a_net</span><span class="p">(</span><span class="n">SimpleNet</span><span class="p">(</span><span class="n">NMDA</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="kinetic-synapse-models-are-more-realistic">Kinetic synapse models are more realistic</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SimpleNet5</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">DynSysGroup</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">freqs</span><span class="o">=</span><span class="mf">10.</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    
    <span class="bp">self</span><span class="p">.</span><span class="n">pre</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">PoissonGroup</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">freqs</span><span class="o">=</span><span class="n">freqs</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">post</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">LifRef</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span>  <span class="n">V_rest</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=-</span><span class="mf">50.</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span>
                              <span class="n">V_initializer</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">Constant</span><span class="p">(</span><span class="o">-</span><span class="mf">60.</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">syn</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">prob</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">pre</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">syn</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">()</span>
    
    <span class="c1"># monitor the following variables
</span>    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">syn</span><span class="p">.</span><span class="n">proj</span><span class="p">.</span><span class="n">refs</span><span class="p">[</span><span class="s">'syn'</span><span class="p">].</span><span class="n">g</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">V</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compare_freqs</span><span class="p">(</span><span class="n">freqs</span><span class="p">):</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">get_figure</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mf">6.</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">freqs</span><span class="p">:</span>
      <span class="n">net</span> <span class="o">=</span> <span class="n">SimpleNet5</span><span class="p">(</span><span class="n">freqs</span><span class="o">=</span><span class="n">freq</span><span class="p">)</span>
      <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>  <span class="c1"># 100 ms
</span>      <span class="n">conductances</span><span class="p">,</span> <span class="n">potentials</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">for_loop</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">step_run</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">progress_bar</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="n">ts</span> <span class="o">=</span> <span class="n">indices</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">()</span>
      <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">conductances</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">freq</span><span class="si">}</span><span class="s"> Hz'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'g'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">compare_freqs</span><span class="p">([</span><span class="mf">10.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">1000.</span><span class="p">,</span> <span class="mf">10000.</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="how-to-customize-a-synapse">How to customize a synapse</h3>

<h4 id="preparations">Preparations</h4>

<p><code class="language-plaintext highlighter-rouge">ProjAlignPostMg2</code> and <code class="language-plaintext highlighter-rouge">ProjAlignPreMg2</code></p>

<h4 id="exponential-model-2">Exponential Model</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Exponen</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">SynDyn</span><span class="p">,</span> <span class="n">bp</span><span class="p">.</span><span class="n">mixin</span><span class="p">.</span><span class="n">AlignPost</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    
    <span class="c1"># parameters
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>
    
    <span class="c1"># variables
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">g</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
    
    <span class="c1"># integral
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">integral</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="k">lambda</span> <span class="n">g</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="o">-</span><span class="n">g</span><span class="o">/</span><span class="n">tau</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'exp_auto'</span><span class="p">)</span>  
  
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre_spike</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral</span><span class="p">(</span><span class="n">g</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'t'</span><span class="p">],</span> <span class="n">dt</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'dt'</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">pre_spike</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="p">.</span><span class="n">add_current</span><span class="p">(</span><span class="n">pre_spike</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">.</span><span class="n">value</span>
      
  <span class="k">def</span> <span class="nf">add_current</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>  <span class="c1"># specical for bp.mixin.AlignPost
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">g</span> <span class="o">+=</span> <span class="n">x</span>
    
  <span class="k">def</span> <span class="nf">return_info</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">g</span>
</code></pre></div></div>

<h4 id="ampa-model">AMPA Model</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AMPA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">SynDyn</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span> <span class="mf">0.98</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.18</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">T_dur</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>

    <span class="c1"># parameters
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">T</span> <span class="o">=</span> <span class="n">T</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">T_duration</span> <span class="o">=</span> <span class="n">T_dur</span>

    <span class="c1"># functions
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">integral</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s">'exp_auto'</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dg</span><span class="p">)</span>
    
    <span class="c1"># variables
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">g</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">spike_arrival_time</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e7</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">dg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">TT</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">TT</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">g</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">*</span> <span class="n">g</span>
  
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre_spike</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">spike_arrival_time</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">pre_spike</span><span class="p">,</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'t'</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">spike_arrival_time</span><span class="p">)</span>
    <span class="n">TT</span> <span class="o">=</span> <span class="p">((</span><span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">spike_arrival_time</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">T_duration</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">T</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">,</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'t'</span><span class="p">],</span> <span class="n">TT</span><span class="p">,</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'dt'</span><span class="p">])</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">.</span><span class="n">value</span>

  <span class="k">def</span> <span class="nf">return_info</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">g</span>
</code></pre></div></div>

<h4 id="synapse-outputs">Synapse outputs</h4>

<p><strong>COBA</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">COBA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">SynOut</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">E</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">=</span> <span class="n">E</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">conductance</span><span class="p">,</span> <span class="n">potential</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">conductance</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">-</span> <span class="n">potential</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>CUBA</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CUBA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">SynOut</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">E</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">=</span> <span class="n">E</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">conductance</span><span class="p">,</span> <span class="n">potential</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">conductance</span>
</code></pre></div></div>

<h4 id="mg-blocking">Mg Blocking</h4>

<p>The voltage dependence is due to the blocking of the pore of the NMDA receptor from the outside by a positively charged magnesium ion. The channel is nearly completely blocked at resting potential, but the magnesium block is relieved if the cell is depolarized. The fraction of channels $B(V)$ that are not blocked by magnesium can be fitted to</p>

\[B(V) = {1 \over 1 + \exp(-0.062V) [Mg^{2+}]_o/3.57}\]

<p>Here, $[{Mg}^{2+}]_{o}$ is the extracellular magnesium concentration, usually 1 mM.</p>

<p>If we make the approximation that the magnesium block changes instantaneously with voltage and is independent of the gating of the channel, the net NMDA receptor-mediated synaptic current is given by</p>

\[I=\bar{g}sB(V)(V-E)\]

<p>where $V(t)$ is the post-synaptic neuron potential, $E$ is the reversal potential.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MgBlock</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">SynOut</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">cc_Mg</span><span class="o">=</span> <span class="mf">1.2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span> <span class="mf">0.062</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span> <span class="mf">3.57</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">=</span> <span class="n">E</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">cc_Mg</span> <span class="o">=</span> <span class="n">cc_Mg</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">conductance</span><span class="p">,</span> <span class="n">potential</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">conductance</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">-</span> <span class="n">potential</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">cc_Mg</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">potential</span><span class="p">))</span>
</code></pre></div></div>

<h4 id="masked-matrix">Masked matrix</h4>

<p><img src="https://cdn.kesci.com/upload/rzz4kw4ybf.png?imageView2/0/w/960/h/960" alt="Image Name" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MaskedLinear</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">conn</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    
    <span class="c1"># connection and weight
</span>    <span class="n">weight</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">parameter</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="p">(</span><span class="n">conn</span><span class="p">.</span><span class="n">pre_num</span><span class="p">,</span> <span class="n">conn</span><span class="p">.</span><span class="n">post_num</span><span class="p">))</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">mode</span><span class="p">,</span> <span class="n">bm</span><span class="p">.</span><span class="n">TrainingMode</span><span class="p">):</span>
      <span class="n">weight</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">TrainVar</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span>

    <span class="c1"># connection
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">conn</span> <span class="o">=</span> <span class="n">conn</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="n">partition</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="n">require</span><span class="p">(</span><span class="s">'conn_mat'</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">mask</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="short-term-synaptic-plasticity">Short-term Synaptic Plasticity</h1>

<h2 id="synaptic-transmission-and-plasticity">Synaptic transmission and plasticity</h2>

<h3 id="process-of-chemical-synaptic-transmission">Process of Chemical synaptic transmission</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826140307862.png" alt="image-20230826140307862" /></p>

<h3 id="epsp-and-epsc">EPSP and EPSC</h3>

<p>EPSP: Excitatory Post Synaptic Potential
EPSC: Excitatory Post Synaptic Current</p>

<p>Post synaptic current: $I(t)=g(t)\bigl[V_{\mathrm{post}}(\mathrm{t})-E_{reversal}\bigr]$</p>

<p>Dynamics of post-synaptic conductance (exponential model): $\frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{S}}+A\delta(t-t_{sp})$</p>

<p>The synaptic strength is characterized as EPSC, which refers to the post synaptic current increment at each spike, $EPSC_{n}=A(V_{\mathrm{rest}}-E_{reversal})$</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826140558289.png" alt="image-20230826140558289" /></p>

<h3 id="synaptic-plasticity">Synaptic plasticity</h3>

<p>实际上突触强度会一直变换</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826140745735.png" alt="image-20230826140745735" /></p>

<h2 id="phenomenological-model-of-stp">Phenomenological model of STP</h2>

<h3 id="short-term-depression-observed-between-pyramidal-cells">Short-term depression observed between pyramidal cells</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826140825584.png" alt="image-20230826140825584" /></p>

<p>膜片前技术，记录膜电位变化</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826140947752.png" alt="image-20230826140947752" /></p>

<h3 id="modeling-neuro-transmitter-consumption">Modeling neuro-transmitter consumption</h3>

<p>Dynamics of three-factor STD:</p>

\[\begin{gathered}
\frac{dx(t)}{dt}=\frac{z(t)}{\tau_{rec}}-U_{SE}x(t)\delta\big(t-t_{sp}\big), \\
\frac{dy(t)}{dt}=-\frac{y(t)}{\tau_{in}}+U_{SE}x(t)\delta\big(t-t_{sp}\big), \\
x(t)+y(t)+z(t)=1, \\
\frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{s}}+g_{max}y(t), 
\end{gathered}\]

<p>$x$: Fraction of available neuro-transmitter
$y$: Fraction of active neuro-transmitter
$z$: Fraction of inactive neuro-transmitter
$U_{se}$: Release probability of active neuro-transmitter
$t_{sp}$: Pre-synaptic spike time
$g(t)$:  Post-synaptic conductance
$A$: total amount of neuro-transmitter
$\tau_{in}$ &amp; $\tau_{rec}$ &amp; $\tau_s$: Time constants</p>

<h3 id="simulate-the-three-factor-std">Simulate the three-factor STD</h3>

\[\begin{gathered}
\frac{dx(t)}{dt}=\frac{z(t)}{\tau_{rec}}-U_{SE}x(t)\delta\big(t-t_{sp}\big), \\
\frac{dy(t)}{dt}=-\frac{y(t)}{\tau_{in}}+U_{SE}x(t)\delta\big(t-t_{sp}\big), \\
x(t)+y(t)+z(t)=1, \\
\frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{s}}+Ay(t), 
\end{gathered}\]

\[\tau_{rec}=500ms,\quad\tau_{in}=3ms,\quad fr=20hz\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826142313326.png" alt="image-20230826142313326" /></p>

<h3 id="simplify-the-dynamics-of-neuro-transmitter-consumption">Simplify the dynamics of neuro-transmitter consumption</h3>

<p>变量太多了，模型复杂</p>

<p>In general, the inactivation time constants is much shorter (3ms) than the spike time interval, i.e., 𝜏 2- ≪ Δ𝑡, so the formulation can be approximately simplified,</p>

\[\begin{aligned}\frac{dy(t)}{dt}&amp;=-\frac{y(t)}{\tau_{in}}+U_{SE}x(t)\delta\big(t-t_{sp}\big)\\&amp;\Longrightarrow\color{red}{\left\{\begin{array}{c}y(t)=U_{SE}x^-\delta_1(t-t_{sp}),\\x^-=\lim_{t-t_{sp}\to0^-}x(t)\end{array}\right.}\end{aligned}\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826142632613.png" alt="image-20230826142632613" /></p>

<p>Simplified model:</p>

\[\begin{gathered}
\frac{dx(t)}{dt} =\frac{1-x(t)}{\tau_{rec}}-U_{SE}x^{-}\delta\big(t-t_{sp}\big), \\
\frac{dg(t)}{dt} =-\frac{g(t)}{\tau_{s}}+AU_{SE}x^{-}\delta\big(t-t_{sp}\big), \\
EPSC=AU_{SE}x^{-}, 
\end{gathered}\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826143004941.png" alt="image-20230826143004941" /></p>

<h3 id="infer-model-parameters-from-experimental-data">Infer model parameters from experimental data</h3>

<p>推断超参，EPSC的理论解</p>

<p>Short term depression model:</p>

\[\begin{aligned}\frac{dx(t)}{dt}&amp;=\frac{1-x(t)}{\tau_{rec}}-U_{SE}x^{-}\delta(t-t_{sp}),\\EPSC&amp;=AU_{SE}x^{-},\end{aligned}\]

<p>Iterative expression for EPSCs:</p>

\[x_{1}^{-}=1, EPSC_{1}=AU_{SE},  \\
x_{n+1}^{-}=1-x_{n}^{-}(1-U_{SE})\mathrm{e}^{-\frac{\Delta t}{\tau_{rec}}} \\
EPSC_{n+1}=AU_{SE}-EPSC_{n}(1-U_{SE})e^{-\frac{\Delta t}{\tau_{rec}}}\]

<h4 id="short-term-facilitation-observed-between-pyramidal-cells-and-interneurons">Short-term facilitation observed between pyramidal cells and interneurons</h4>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826143735036.png" alt="image-20230826143735036" /></p>

<p>短时程增强</p>

<h3 id="modeling-neuro-transmitter-release-probability">Modeling neuro-transmitter release probability</h3>

<p>先前漏掉释放概率的建模</p>

<p>The release probability can also be modelled as a dynamical variable 𝑢(𝑡),</p>

\[\begin{gathered}
\frac{du(t)}{dt}=\frac{-u(t)}{\tau_{f}}+U_{SE}(1-u^{-})\delta\big(t-t_{sp}\big), \\
\frac{dx(t)}{dt}=\frac{1-x(t)}{\tau_{d}}-u(t)x^{-}\delta\big(t-t_{sp}+\delta t\big), \\
\frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{S}}+Au(t)x^{-}\delta\big(t-t_{sp}+\delta t\big), \\
EPSC=Au(t)x^{-}, 
\end{gathered}\]

<p>$U_{SE}$ might reflect the concentration of $Ca^{2+}$</p>

<p>The release probability can also be modelled as a dynamical variable 𝑢(𝑡),</p>

\[\begin{gathered}
\frac{du(t)}{dt}=\frac{-u(t)}{\tau_{f}}+U_{SE}(1-u^{-})\delta\big(t-t_{sp}\big), \\
\frac{dx(t)}{dt}=\frac{1-x(t)}{\tau_{d}}-u^{+}x^{-}\delta\big(t-t_{sp}\big), \\
\frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{s}}+Au^{+}x^{-}\delta\big(t-t_{sp}\big), \\
{EPSC=Au^{+}x^{-},\quad u^{+}=\lim_{t-t_{sp}\to0^{+}}u(t),} 
\end{gathered}\]

<h3 id="std-and-stf-under-different-parameter-regime">STD and STF under different parameter regime</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826145216601.png" alt="image-20230826145216601" /></p>

<h3 id="derivation-of-iterative-expressions-for-epscs">Derivation of iterative expressions for EPSCs</h3>

\[\begin{gathered}
\mathrm{Iterative~expression~for~}x_{n},u_{n},EPSC_{n}; \\
u_{1}^{+}=U_{SE},\quad x_{1}^{-}=1, \\
x_{n+1}^{-}=1-x_{n}^{-}(1-u_{n}^{+})\mathrm{e}^{-\frac{\Delta t}{\tau_{rec}}}, \\
u_{n+1}^{+}=u_{n}^{+}e^{-\frac{\Delta t}{\tau_{f}}}+U_{SE}\left(1-u_{n}^{+}e^{-\frac{\Delta t}{\tau_{f}}}\right), \\
EPSC_{n+1}=Au_{n}^{+}x_{n}^{-}, 
\end{gathered}\]

\[\begin{gathered}
\mathrm{Steady~state~of~}x_{n},u_{n},EPSC_{n}; \\
u_{st}^{+}=\frac{U_{SE}}{1-(1-U_{SE})e^{-\frac{\Delta t}{\tau_{f}}}}\geq U_{SE}=u_{1}^{+}, \\
x_{st}^{-}=\frac{1}{1+(1-u_{st}^{+})\mathrm{e}^{-\frac{\Delta t}{\tau_{rec}}}}\leq1=x_{1}^{-}, \\
EPSC_{st}=Au_{st}^{+}x_{st}^{-} 
\end{gathered}\]

<h3 id="prediction-for-complex-post-synaptic-patterns">Prediction for complex post-synaptic patterns</h3>

<p>Infer model parameters by fitting experiments:</p>

\[EPSC_{n+1} = Au_nx_n\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826145401318.png" alt="image-20230826145401318" /></p>

<p>Simulate with complex pre-synaptic spike trains and compare with vitro experimental results (patch-clamp)</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826145419287.png" alt="image-20230826145419287" /><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826145424279.png" alt="image-20230826145424279" /></p>

<h2 id="effects-on-information-transmission">Effects on information transmission</h2>

<h3 id="mean-field-analysis-of-stp-model">Mean-field Analysis of STP model</h3>

<p>STP based on spiking time</p>

\[\begin{gathered}
\frac{du(t)}{dt}=\frac{-u(t)}{\tau_{f}}+U_{sE}(1-u^{-})\delta\big(t-t_{sp}\big), \\
\frac{dx(t)}{dt}=\frac{1-x(t)}{\tau_{d}}-u^{+}x^{-}\delta\big(t-t_{sp}\big), \\
\frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{s}}+Au^{+}x^{-}\delta\big(t-t_{sp}\big), \\
u^{+}=\lim_{t-t_{sp\rightarrow0^{+}}}u(t), 
\end{gathered}\]

<p>-&gt;做时间平均</p>

<p>STP based on firing rate</p>

\[\begin{gathered}
\frac{du(t)}{dt}=\frac{-u(t)}{\tau_{f}}+U_{sE}(1-u^{-})\delta\big(t-t_{sp}\big), \\
\frac{dx(t)}{dt}=\frac{1-x(t)}{\tau_{d}}-u^{+}x^{-}\delta\big(t-t_{sp}\big), \\
\frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{s}}+Au^{+}x^{-}\delta\big(t-t_{sp}\big), \\
u^{+}=\lim_{t-t_{sp\rightarrow0^{+}}}u(t), 
\end{gathered}\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826155016657.png" alt="image-20230826155016657" /></p>

<p>丢掉时间变化的具体细节，抓住了重要趋势</p>

<h3 id="theoretical-analysis-of-the-rate-model">Theoretical analysis of the rate model</h3>

<p>Suppose the pre-synaptic firing rate keeps as constant, we can calculate the stationary response</p>

\[u_{st}=\frac{U_{SE}R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}},\quad u_{st}^{+}=U_{SE}\frac{1+R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}},\quad x_{st}=\frac{1}{1+u_{st}^{+}\tau_{d}R_{0}},\]

\[EPSC_{st}=Au_{st}^{+}x_{st}=A\frac{u_{st}^{+}}{1+u_{st}^{+}\tau_{d}R_{0}},\quad PSV_{st}\propto g_{st}=\tau_{s}Au_{st}^{+}x_{st}R_{0}=A\frac{u_{st}^{+}R_{0}}{1+u_{st}^{+}\tau_{d}R_{0}},\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826155234134.png" alt="image-20230826155234134" /></p>

<h3 id="frequency-dependent-gain-control-of-spike-information">Frequency-dependent Gain control of spike information</h3>

\[\begin{gathered}
u_{st}^{+}=U_{SE}\frac{1+R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}}, \\
x_{st}=\frac{1}{1+u_{st}^{+}\tau_{d}R_{0}}, \\
EPSC_{st}=Au_{st}^{+}x_{st}=A\frac{u_{st}^{+}}{1+u_{st}^{+}\tau_{d}R_{0}}, 
\end{gathered}\]

<p>Peak frequency: $\theta\sim\frac{1}{\sqrt{U\tau_{f}\tau_{d}}}$</p>

<h3 id="simulation-of-frequency-dependent-gain-control">Simulation of Frequency-dependent Gain control</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826155715445.png" alt="image-20230826155715445" /></p>

<h2 id="effects-on-network-dynamics">Effects on network dynamics</h2>

<h3 id="stp-modeling-working-memory">STP modeling Working memory</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826160102332.png" alt="image-20230826160102332" /></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826160113456.png" alt="image-20230826160113456" /></p>

<h1 id="e-i-balanced-neural-network">E-I Balanced Neural Network</h1>

<h2 id="irregular-spiking-of-neurons">Irregular Spiking of Neurons</h2>

<h3 id="signal-process-of-single-neuron">Signal process of single neuron</h3>

<p>External Stimulus -&gt;</p>

<p>Single neuron model</p>

\[\begin{aligned}\tau&amp;\frac{\mathrm{d}V}{\mathrm{d}t}=-(V-V_\text{rest })+RI(t)\\\\&amp;\text{if}V&gt;V_\text{th},\quad V\leftarrow V_\text{reset }\text{last}t_\text{ref}\end{aligned}\]

<p>-&gt; … -&gt; Perception or action</p>

<p>真正的神经元并不是LIF model的输出</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827100647851.png" alt="image-20230827100647851" /></p>

<p>Simulation</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827100706310.png" alt="image-20230827100706310" /></p>

<p>Neuron recorded in vivo</p>

<h3 id="irregular-spiking-of-neurons-1">Irregular Spiking of Neurons</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827092807270.png" alt="image-20230827092807270" /></p>

<h4 id="statistical-description-of-spikes">Statistical Description of Spikes</h4>

<p>用以下的变量来进行统计描述</p>

<ul>
  <li>Firing Rate
Rate = average over time(single neuron, single run)
Spike count $v=\frac{n_{sp}}{T}$</li>
  <li>ISI(Interspike interval distributions)
average ISI $\overline{\Delta t}=\frac{1}{n_{sp}-1}\sum_{i=1}^{n_{sp}-1}\Delta t_{i}$
standard deviation ISI: $\sigma_{\Delta t}^{2}=\frac{1}{n_{sp}-1}\sum_{i=1}^{n_{sp}-1}(\Delta t_{i}-\overline{\Delta t})^{2}$</li>
  <li>$C_V$(Coefficient of variation, Fano factor) <strong>窄还是宽的分布</strong> 信息表征有多强的不稳定性
$C_{V}=\sigma_{\Delta t}^{2}/\overline{\Delta t}$</li>
</ul>

<h4 id="poisson-process">Poisson Process</h4>

<p>In probability theory and statistics, the Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known <strong>constant mean rate</strong> and <strong>independently</strong> of the time since the last event。</p>

\[\begin{aligned}
&amp;P(X=k\mathrm{~events~in~interval~}t)=e^{-rt}\frac{(rt)^{k}}{k!} \\
&amp;\mathrm{mean:}\quad\overline{X}=rt \\
&amp;\mathrm{variance}:\quad\sigma^{2}=rt\\
&amp;\mathrm{Fano factor:}\quad\frac{\sigma^{2}}{X}=1
\end{aligned}\]

<p>Fano factor -&gt; noise-to-signal ratio</p>

<h4 id="irregular-spiking-of-neurons-2">Irregular Spiking of Neurons</h4>

<p>LIF在单个神经元的情况下是基本没有太大问题的，在整个网络中会受网络信息调控</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827093614607.png" alt="image-20230827093614607" /></p>

<h4 id="why-irregular">Why Irregular?</h4>

<ul>
  <li>不完全是input影响的</li>
  <li>不能简单来衡量</li>
</ul>

<p>On average, a cortical neuron receives inputs from 1000~10000 connected neurons. -&gt; averaged noise ~ 0</p>

<h2 id="e-i-balanced-network">E-I Balanced Network</h2>

\[\begin{gathered}
\tau\frac{du_{i}^{E}}{dt}=-u_{i}^{E}+\sum_{j=1}^{K_{E}}J_{EE}r_{j}^{E}+\sum_{j=1}^{K_{I}}J_{EI}r_{j}^{I}+I_{i}^{E} \\
\tau\frac{du_{i}^{I}}{dt}=-u_{i}^{I}+\sum_{j=1}^{K_{I}}J_{II}r_{j}^{I}+\sum_{j=1}^{K_{E}}J_{IE}r_{j}^{E}+I_{i}^{I} 
\end{gathered}\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827093708220.png" alt="image-20230827093708220" /></p>

<p>Sparse &amp; random connections:$1\ll K_{\mathrm{E}},K_{1}\ll N_{\mathrm{E}},N_{\mathrm{I}}$ . Neurons fire largely independently to each other.</p>

\[\begin{gathered}
\text{Single neuron fires irregularly } r_j^E, r_j^{\prime} \text{with mean rate } \mu \text{and variance } \sigma^2.\\
\text{The mean of recurrent input received by E neuron:} \\
\sim K_{E}J_{EE}\mu-K_{I}J_{EI}\mu  \\
\text{The variance of recurrent input received by E neuron:} \\
\sim K_{E}(J_{EE})^{2}\sigma^{2}+K_{I}(J_{EI})^{2}\sigma^{2} \\
\begin{gathered} \\
\text{The balanced condition:} \\
K_{E}J_{EE}-K_{l}J_{El}{\sim}0(1) \\
J_{EE}=\frac{1}{\sqrt{K_{E}}},J_{EI}=\frac{1}{\sqrt{K_{I}}},K_{E}(J_{EE})^{2}\sigma^{2}+K_{I}(J_{EI})^{2}\sigma^{2}\sim O(1) 
\end{gathered}
\end{gathered}\]

\[\begin{aligned}\frac{I_E}{I_I}&amp;&gt;\frac{J_E}{J_I}&amp;&gt;1\\\\J_E&amp;&gt;1\\\\\text{r not too big}\end{aligned}\]

\[\overline{I_a}=\overline{F_a}+\overline{R_a}=\sqrt{N}(f_a\mu_0+w_{aE}r_E+w_{aI}r_I),\quad a=E,I,\\
\begin{gathered}
w_{ab}~=~p_{ab}j_{ab}q_{b} \\
J_{ij}^{ab}~=~j_{ab}/\sqrt{N}; \\
\frac{f_{E}}{f_{I}}&gt;\frac{w_{EI}}{w_{II}}&gt;\frac{w_{EE}}{w_{IE}}. 
\end{gathered}\]

<h2 id="brainpy-simulation">BrainPy Simulation</h2>

<h3 id="simulation-4">Simulation</h3>

<p>LIF neuron 4000 (E/I=4/1, P=0.02)
𝜏 = 20 ms
𝑉𝑟𝑒𝑠𝑡 = -60 mV
Spiking threshold: -50 mV
Refractory period: 5 ms</p>

\[\begin{gathered}
\tau\frac{dV}{dt}=(V_{\mathrm{rest}}-V)+I \\
I=g_{exc}(E_{exc}-V)+g_{inh}(E_{inh}-V)+I_{\mathrm{ext}} 
\end{gathered} \ \ \ \ \ \
\begin{aligned}\tau_{exc}&amp;\frac{dg_{exc}}{dt}=-g_{exc}\\\tau_{inh}&amp;\frac{dg_{inh}}{dt}=-g_{inh}\end{aligned}\]

\[\begin{array}{l}E_\mathrm{exc}=0\text{mV}\mathrm{and}E_\mathrm{inh}=-80\text{mV},I_\mathrm{ext}=20.\\\tau_\mathrm{exc}=5\text{ ms},\tau_\mathrm{inh}=10\text{ ms},\Delta g_\mathrm{exc}=0.6\text{ and}\Delta g_\mathrm{inh}=6.7.\end{array}\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827094502860.png" alt="image-20230827094502860" /></p>

<h3 id="synaptic-computation">Synaptic Computation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 基于 align post Exponential synaptic computation
</span><span class="k">class</span> <span class="nc">Exponential</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">Projection</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">g_max</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">E</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pron</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPost2</span><span class="p">(</span>
        	<span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">,</span>
            <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">,</span>
            <span class="n">comm</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">EventCSRLinear</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="n">FixedProb</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">),</span> <span class="n">g_max</span><span class="p">),</span> <span class="c1"># 随机连接
</span>            <span class="n">syn</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">Expon</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">),</span> <span class="c1"># Exponential synapse
</span>            <span class="n">out</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">COBA</span><span class="p">(</span><span class="n">E</span><span class="o">=</span><span class="n">E</span><span class="p">),</span> <span class="c1"># COBA network
</span>            <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">,</span>
            <span class="n">out_label</span><span class="o">=</span><span class="n">label</span>
        <span class="p">)</span>
</code></pre></div></div>

<h3 id="e-i-balanced-network-1">E-I Balanced Network</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 构建 E-I Balanced Network
</span>
<span class="k">class</span> <span class="nc">EINet</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">DynamicalSystem</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ne</span><span class="o">=</span><span class="mi">3200</span><span class="p">,</span> <span class="n">ni</span><span class="o">=</span><span class="mi">800</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        
        <span class="c1"># bp.neurons.LIF()
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">LifRef</span><span class="p">(</span><span class="n">ne</span><span class="p">,</span> <span class="n">V_rest</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=-</span><span class="mf">50.</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span>
                              <span class="n">V_initializer</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="o">-</span><span class="mf">55.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">I</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">LifRef</span><span class="p">(</span><span class="n">ni</span><span class="p">,</span> <span class="n">V_rest</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=-</span><span class="mf">50.</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span>
                              <span class="n">V_initializer</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="o">-</span><span class="mf">55.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">))</span>
    	<span class="c1">#### E2E, E2I, I2E, I2I Exponential synaptic computation
</span>        <span class="c1"># delay=0, prob=0.02, g_max_E=0.6, g_max_I=6.7, tau_E=5, tau_I=10,
</span>        <span class="c1"># reversal potentials E_E=0, E_E=-80, label=EE,EI,IE,II
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">E2E</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="s">'EE'</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">E2I</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="s">'EI'</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">I2E</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">6.7</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="o">-</span><span class="mf">80.</span><span class="p">,</span> <span class="s">'IE'</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">I2I</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">6.7</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="o">-</span><span class="mf">80.</span><span class="p">,</span> <span class="s">'II'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
    <span class="c1"># 更新突触传入电流
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">E2E</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">E2I</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">I2E</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">I2I</span><span class="p">()</span>
    
    <span class="c1"># 更新神经元群体
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
    
    <span class="c1"># 记录需要 monitor的变量
</span>    <span class="n">E_E_inp</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">.</span><span class="n">sum_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">.</span><span class="n">V</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'EE'</span><span class="p">)</span> <span class="c1">#E2E的输入
</span>    <span class="n">I_E_inp</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">.</span><span class="n">sum_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">.</span><span class="n">V</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'IE'</span><span class="p">)</span> <span class="c1"># I2E的输入
</span>    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">.</span><span class="n">spike</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">.</span><span class="n">spike</span><span class="p">,</span> <span class="n">E_E_inp</span><span class="p">,</span> <span class="n">I_E_inp</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827110737553.png" alt="image-20230827110737553" /></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827110746410.png" alt="image-20230827110746410" /></p>

<h2 id="properties-of-e-i-balanced-network">Properties of E-I Balanced Network</h2>

<ul>
  <li>Linear encoding
External input strength is “linearly” encoded by the mean firing rate of the neural population</li>
  <li>Fast Response
The network responds rapidly to abrupt changes of the input</li>
</ul>

<h3 id="noise-speeds-up-computation">Noise speeds up computation</h3>

<p>快速相应的原理，均匀分布在阈值下面的空间</p>

<ul>
  <li>A neural ensemble jointly encodes stimulus information;</li>
  <li>Noise randomizes the distribution of neuronal membrane potentials;</li>
  <li>Those neurons (red circle) whose potentials are close to the threshold will fire rapidly;</li>
  <li>If the noisy environment is proper, even for a small input, a certain number of neurons will fire instantly to report the presence of a stimulus.</li>
</ul>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827113451626.png" alt="image-20230827113451626" /></p>

<h1 id="continuous-attractor-neural-network">Continuous Attractor Neural Network</h1>

<h2 id="attractor-models">Attractor Models</h2>

<h3 id="the-concept-of-attractor-dynamics">The concept of attractor dynamics</h3>

<p>Different types of attractors:
Point attractors, Line attractors, Ring attractors, Plane attractors, Cyclic attractors, Chaotic attractors</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827140250173.png" alt="image-20230827140250173" /></p>

<p>稳态，能量梯度吸引到attractor</p>

<h3 id="discrete-attractor-network-model-hopfield-model">Discrete attractor Network Model: Hopfield Model</h3>

<p>$S_i=\pm1$: the neuronal state
$W_{ij}$ : the neuronal connection</p>

<p>The network dynamics:</p>

\[S_{i}=\mathrm{sign}\bigg(\sum_{j}w_{ij}S_{j}-\theta\bigg),\quad\mathrm{sign}(x)=1,\mathrm{for}x&gt;0;-1,\mathrm{otherwise}\]

<p>Updating rule: synchronous or asynchronous
Consider the network stores $p$ pattern, $\xi_{i}^{\mu},\mathrm{for}\mu=1,\ldots p;i=1,\ldots N$
Setting $w_{ij}=\frac{1}{N}\sum_{\mu=1}^{p}\xi_{i}^{\mu}\xi_{j}^{\mu}$</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827140827784.png" alt="image-20230827140827784" /></p>

<h4 id="energy-space-of-hopfield-network">Energy space of Hopfield network</h4>

\[\begin{aligned}
&amp;\text{Energy function: }E=-\frac{1}{2}\sum_{i,j}w_{ij}S_{i}S_{j}+\theta\sum_{i}S_{i} \\
&amp;\mathrm{Consider}S_{i}\mathrm{~is~updated},S_{i}(t+1)=sign[\sum_{j}w_{ij}S_{j}(t)-\theta] \\
&amp;\Delta E=E(t+1)-E(t)\\
&amp;=-[S_{i}(t+1)-S_{i}(t)]\sum_{j}w_{ij}S_{j}(t)+\theta\left[S_{i}(t+1)-S_{i}(t)\right] \\
&amp;=-[S_{i}(t+1)-S_{i}(t)][\sum_{j}w_{ij}S_{j}(t)-\theta] \\
&amp;\leq0
\end{aligned}\]

<p>同样激活同样pattern的神经元，~吸引子</p>

<h4 id="auto-associative-memory-in-hopfield-network">Auto-associative memory in Hopfield Network</h4>

<p>A partial/noisy input can retrieve the related memory pattern</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827141253326.png" alt="image-20230827141253326" /></p>

<h4 id="persistent-activity-in-working-memory">Persistent activity in working memory</h4>

<p>After the removal of external input, the neurons in the network encoding the stimulus continue to fire persistently.</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827141421796.png" alt="image-20230827141421796" /></p>

<h2 id="continuous-attractor-neural-network-1">Continuous Attractor Neural Network</h2>

<h3 id="neural-coding">Neural coding</h3>

<h4 id="low-dimensional-continuous-feature">Low-dimensional continuous feature</h4>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827142520189.png" alt="image-20230827142520189" /></p>

<h4 id="continuous-attractor-neural-network-2">Continuous Attractor neural network</h4>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827142606695.png" alt="image-20230827142606695" /></p>

<h3 id="cann-a-rate-based-recurrent-circuit-model">CANN: A rate-based recurrent circuit model</h3>

<p><img src="Notes.assets/image-20230827142918529.png" alt="image-20230827142918529" style="zoom:50%;" /></p>

\[\begin{aligned}\tau\frac{\partial U(x,t)}{\partial t}&amp;=-U(x,t)+\rho\int f(x,x')r(x',t)dx'+l^{ext}(1)\\r(x,t)&amp;=\frac{U^2(x,t)}{1+k\rho\int U^2(x,t)dx}\quad(2)\\J(x,x')&amp;=\frac{J_0}{\sqrt{2\pi}a}\exp\left[-\frac{(x-x')^2}{2a^2}\right](3)\end{aligned}\]

<p>r频率，J强度，U decay</p>

<p><img src="Notes.assets/image-20230827143435002.png" alt="image-20230827143435002" style="zoom:50%;" /></p>

<h4 id="a-continuous-family-of-attractor-states">A Continuous family of attractor states</h4>

<p>做平移的改变，变化会被保留，line attractor，受到编码连续刺激</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827143707784.png" alt="image-20230827143707784" /></p>

<h4 id="stability-analysis-derive-continuous-attractor-dynamics">Stability analysis derive continuous attractor dynamics</h4>

<p>只需要看在原始状态加入一个小量项，再代入回</p>

<p>Consider small fluctuations around a stationary state at z:</p>

<p>Projecting $\delta U$ on the $i$th right eigenvector of $F(\delta U)_i(t)=(\delta U)_i(0)e^{-(1-\lambda _i)t/\tau}$</p>

<p>Two cases:</p>

<ul>
  <li>If $\lambda _i &lt; 1$, the projection decays exponentially</li>
  <li>If $\lambda _i$ = 1, the projection is sustained</li>
</ul>

<h4 id="spectra-of-the-kernel-f">Spectra of the kernel F</h4>

\[\begin{aligned}\bullet&amp;\lambda_0=1-2k\rho A\sqrt{2\pi}a&lt;1,\quad&amp;\mathbf{u}_0(x\mid z)=\overline{\mathbf{U}}(x\mid z);\\\bullet&amp;\lambda_1=1,\quad&amp;\mathbf{u}_1(x\mid z)=\frac{d\overline{\mathbf{U}}(x\mid z)}{dz},\text{the tangent of the valley}\\\bullet&amp;\lambda_n=\frac1{2^{n-2}},\quad&amp;\mathbf{u}_n(z)=\text{Combination of }\mathbf{v}_n(z)\end{aligned}\]

<p>$\mathbf{v}_{n}(z)\sim e^{-(c-z)^{2}/4a^{2}}(\frac{d}{dc})^{n}e^{-(c-z)^{2}/2a^{2}},$ the wave functions of quantumn harmonic osscilator</p>

<p>Note the decay time constant is: $\frac{\tau}{1-\lambda _n}$</p>

<h4 id="only-bump-position-shift-survives">Only bump position shift survives</h4>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827144540676.png" alt="image-20230827144540676" /></p>

<p>Ring attractor network for head-direction cell in fruit fly</p>

<p><img src="Notes.assets/image-20230827144557359.png" alt="image-20230827144557359" style="zoom:50%;" /></p>

<h2 id="computation-with-cann">Computation with CANN</h2>

<h3 id="persistent-activity-for-working-memory">Persistent activity for working memory</h3>

<p>When the global inhibition is not too strong, the network spontaneously hold bump activity:</p>

\[k&lt;\frac{\rho J_{0}^{2}}{8\sqrt{2\pi}a}\]

\[\begin{aligned}
&amp;\tilde{U}(x|z) =\quad U_{0}\exp\left[-\frac{(x-z)^{2}}{4a^{2}}\right],  \\
&amp;\tilde{r}(x|z) =\quad r_{0}\exp\left[-\frac{(x-z)^{2}}{2a^{2}}\right],  \\
&amp;U_{0}=[1+(1-k/k_{c})^{1/2}]A/(4\sqrt{\pi}ak) \\
&amp;r_0=[1+(1-k/k_{c})^{1/2}]/(2\sqrt{2\pi}ak\rho).
\end{aligned}\]

<h3 id="smooth-tracking-by-cann">Smooth tracking by CANN</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827145334859.png" alt="image-20230827145334859" /></p>

<p>Project the network dynamics on $v_1(t)$
$\tau{\frac{\partial\mathbf{U}<em>\mathbf{v}_{1}}{\partial t}}=-\mathbf{U}</em>\mathbf{v}<em>{1}+(\mathbf{J}*\mathbf{r})*\mathbf{v}</em>{1}+\mathbf{I}^{ext}*\mathbf{v}_{1}$</p>

<p>Consider</p>

<p>\(\begin{aligned}&amp;I^{ext}(t)=\alpha\overline{U}(x\mid z_0)+\sigma\xi_c(t)\\&amp;\mathbf{U}*\mathbf{v}_1\equiv\int dxU(x\mid z)\nu_1(x\mid z)\\\\&amp;\tau\frac{dz}{dt}=-\alpha(z-z_0)e^{-(z-z_0)^2/8a^2}+\beta\xi(t)\end{aligned}\)
1st term: the force of the signal that pulls the bump back to the stimulus position
2nd term: random shift</p>

<h3 id="population-decoding-via-template-matching">Population decoding via template matching</h3>

\[\hat{x}=\max_{z}\sum_{i}r_{i}f_{i}(z)\]

<ul>
  <li>The noisy bump is the population activity when the stimulus $x=0$</li>
  <li>Among three positions, the red one($z=0$) has the maximum overlap with the observed data.</li>
</ul>

<h2 id="computation-and-dynamics-of-adaptive-cann">Computation and Dynamics of Adaptive CANN</h2>

<h3 id="adaptive-continuous-attractor-neural-network">Adaptive Continuous Attractor neural network</h3>

\[\begin{aligned}
&amp;\tau{\frac{dU(x,t)}{dt}} =-U(x,t)+\rho\int dx'J(x-x^{\prime})r(x',t)-V(x,t)+I^{ext}(x,t)  \\
&amp;\tau_{_{\nu}}\frac{dV(x,t)}{dt} =-V(x,t)+mU(x,t) 
\end{aligned}\]

<p>$V(x,t)$ represents the SFA effect,</p>

<p>$V(x,t)=\frac{m}{\tau_{\nu}}\int_{-\infty}^{‘}e^{-\frac{t-t’}{\tau_{\nu}}}U(x,t’)dt’$</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827150142710.png" alt="image-20230827150142710" /></p>

<p>SFA(Spike frequency Adaptation):</p>

<ul>
  <li>Neuronal response attenuates after experiencing prolonged firing.</li>
  <li>Slow negative feedback modulation to neuronal response.</li>
</ul>

<h3 id="intrinsic-mobility-of-a-cann">Intrinsic mobility of A-CANN</h3>

<p>Traveling Wave: a moving bump in the network without relying on external drive</p>

<p>The mechanism: SFA suppresses localized neural activity and triggers</p>

<p>$m&gt;\frac{\tau}{\tau _v}$, Travelling wave</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827150543244.png" alt="image-20230827150543244" /></p>

<h4 id="levy-flights-vs-brownian-motion">Levy flights vs. Brownian motion</h4>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827150851309.png" alt="image-20230827150851309" /></p>

<h4 id="lévy-flights-in-ecology-and-human-cognidve-behaviors">Lévy flights in ecology and human cogniDve behaviors</h4>

<p>生物学大多运动服从levy flights</p>

<h3 id="noisy-adaptation-generates-levy-flight-in-cann">Noisy adaptation generates Levy flight in CANN</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827151343126.png" alt="image-20230827151343126" /></p>

<h3 id="time-delay-in-neural-signal-transmission">Time Delay in Neural Signal Transmission</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827151622032.png" alt="image-20230827151622032" /></p>

<h3 id="anticipatory-head-direction-signals-in-anterior-thalamus">Anticipatory Head Direction Signals in Anterior Thalamus</h3>

<p>有预测策略，实现抵消信息传递的delay</p>

<p>CANN加入负反馈机制是可以实现预测的</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827152731895.png" alt="image-20230827152731895" /></p>

<h3 id="cann-with-stp">CANN with STP</h3>

\[\begin{gathered}
\tau{\frac{\mathrm{d}U(x,t)}{\mathrm{d}t}} {\cal O}=-U(x,t)+\rho\int g^{+}(x)h(x^{\prime},t)J(x,x^{\prime})r(x^{\prime},t)dx^{\prime}+I^{ext}(x,t)(1) \\
\frac{dg(x,t)}{dt}=-\frac{g(x,t)}{\tau_{f}}+G(1-g^{-}(x))r(x^{\prime},t)\quad(2) \\
\frac{dh(x,t)}{dt}=\frac{1-h(x,t)}{\tau_{d}}-g^{+}(x)h(x,t)r(x^{\prime},t)\quad(3) \\
r(x,t)={\frac{U^{2}(x,t)}{1+k\rho\int U^{2}(x,t)dx}}\quad(4) 
\end{gathered}\]

<h2 id="programming-in-brainpy">Programming in BrainPy</h2>

<h3 id="customize-a-ring-cann-in-brainpy">Customize a ring CANN in brainpy</h3>

<p>In simulations, we can not simulate a CANN encoding features ranging $(-\inf, \inf)$. Instead, we simulate a ring attractor network which encodes features ranging $(-\pi, \pi)$. Note that the distance on a ring should be:</p>

\[dist_{ring}(x,x') = min(|x-x'|,2\pi-|x-x'|)\]

<p><img src="https://cdn.kesci.com/upload/s01apgi89t.png?imageView2/0/w/320/h/320" alt="Image Name" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CANN1D</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">NeuGroupNS</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">8.1</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">A</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">J0</span><span class="o">=</span><span class="mf">4.</span><span class="p">,</span>
               <span class="n">z_min</span><span class="o">=-</span><span class="n">bm</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">z_max</span><span class="o">=</span><span class="n">bm</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">CANN1D</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">num</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># 初始化参数
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">a</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">A</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">J0</span> <span class="o">=</span> <span class="n">J0</span>

    <span class="c1"># 初始化特征空间相关参数
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">z_min</span> <span class="o">=</span> <span class="n">z_min</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">z_max</span> <span class="o">=</span> <span class="n">z_max</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span> <span class="o">=</span> <span class="n">z_max</span> <span class="o">-</span> <span class="n">z_min</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">z_min</span><span class="p">,</span> <span class="n">z_max</span><span class="p">,</span> <span class="n">num</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">rho</span> <span class="o">=</span> <span class="n">num</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">dx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span> <span class="o">/</span> <span class="n">num</span>

    <span class="c1"># 初始化变量
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">u</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="nb">input</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">conn_mat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">make_conn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 连接矩阵
</span>
    <span class="c1"># 定义积分函数
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">integral</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">derivative</span><span class="p">)</span>

  <span class="c1"># 微分方程
</span>  <span class="o">@</span><span class="nb">property</span>
  <span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">du</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">u</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">Irec</span><span class="p">,</span> <span class="n">Iext</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="n">u</span> <span class="o">+</span> <span class="n">Irec</span> <span class="o">+</span> <span class="n">Iext</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau</span>
    <span class="k">return</span> <span class="n">du</span>

  <span class="c1"># 将距离转换到[-z_range/2, z_range/2)之间
</span>  <span class="k">def</span> <span class="nf">dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">d</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span><span class="p">,</span> <span class="n">d</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">d</span>

  <span class="c1"># 计算连接矩阵
</span>  <span class="k">def</span> <span class="nf">make_conn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">bm</span><span class="p">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">d</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dist</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">])</span>  <span class="c1"># 距离矩阵
</span>    <span class="n">Jxx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">J0</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span>
      <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">d</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">Jxx</span>

  <span class="c1"># 获取各个神经元到pos处神经元的输入
</span>  <span class="k">def</span> <span class="nf">get_stimulus_by_pos</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pos</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.25</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dist</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">pos</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">_t</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span>
    <span class="n">u2</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">u</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">u2</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">k</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">u2</span><span class="p">))</span>
    <span class="n">Irec</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conn_mat</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">u</span><span class="p">[:]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">u</span><span class="p">,</span> <span class="n">_t</span><span class="p">,</span><span class="n">Irec</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">[:]</span> <span class="o">=</span> <span class="mf">0.</span>  <span class="c1"># 重置外部电流
</span></code></pre></div></div>

<h3 id="simulate-the-persistent-activity-of-cann-after-the-removal-of-external-input">Simulate the persistent activity of CANN after the removal of external input</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">Persistent_Activity</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">J0</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
    <span class="c1"># 生成CANN
</span>    <span class="n">cann</span> <span class="o">=</span> <span class="n">CANN1D</span><span class="p">(</span><span class="n">num</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span><span class="n">J0</span><span class="o">=</span><span class="n">J0</span><span class="p">)</span>

    <span class="c1"># 生成外部刺激，从第2到12ms，持续10ms
</span>    <span class="n">dur1</span><span class="p">,</span> <span class="n">dur2</span><span class="p">,</span> <span class="n">dur3</span> <span class="o">=</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">10.</span>
    <span class="n">I1</span> <span class="o">=</span> <span class="n">cann</span><span class="p">.</span><span class="n">get_stimulus_by_pos</span><span class="p">(</span><span class="mf">0.</span><span class="p">)</span>
    <span class="n">Iext</span><span class="p">,</span> <span class="n">duration</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">section_input</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="n">I1</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
                                             <span class="n">durations</span><span class="o">=</span><span class="p">[</span><span class="n">dur1</span><span class="p">,</span> <span class="n">dur2</span><span class="p">,</span> <span class="n">dur3</span><span class="p">],</span>
                                             <span class="n">return_length</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">noise_level</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">noise_level</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">duration</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">()),</span> <span class="nb">len</span><span class="p">(</span><span class="n">I1</span><span class="p">)))</span>
    <span class="n">Iext</span> <span class="o">+=</span> <span class="n">noise</span>
    <span class="c1"># 运行数值模拟
</span>    <span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">cann</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s">'input'</span><span class="p">,</span> <span class="n">Iext</span><span class="p">,</span> <span class="s">'iter'</span><span class="p">],</span> <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'u'</span><span class="p">])</span>
    <span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">duration</span><span class="p">)</span>

    <span class="c1"># 可视化
</span>    <span class="k">def</span> <span class="nf">plot_response</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">gs</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">get_figure</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">ts</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">t</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">())</span>
        <span class="n">I</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="n">Iext</span><span class="p">[</span><span class="n">ts</span><span class="p">],</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">u</span><span class="p">[</span><span class="n">ts</span><span class="p">]</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cann</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Iext'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cann</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'U'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">'$t$'</span> <span class="o">+</span> <span class="s">' = {} ms'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$x$'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'top'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'right'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="c1"># plt.savefig(f'CANN_t={t}.pdf', transparent=True, dpi=500)
</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">20.</span><span class="p">)</span>

    <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">animate_1D</span><span class="p">(</span>
        <span class="n">dynamical_vars</span><span class="o">=</span><span class="p">[{</span><span class="s">'ys'</span><span class="p">:</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">u</span><span class="p">,</span> <span class="s">'xs'</span><span class="p">:</span> <span class="n">cann</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="s">'legend'</span><span class="p">:</span> <span class="s">'u'</span><span class="p">},</span>
                        <span class="p">{</span><span class="s">'ys'</span><span class="p">:</span> <span class="n">Iext</span><span class="p">,</span> <span class="s">'xs'</span><span class="p">:</span> <span class="n">cann</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="s">'legend'</span><span class="p">:</span> <span class="s">'Iext'</span><span class="p">}],</span>
        <span class="n">frame_step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">frame_delay</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
        <span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">Persistent_Activity</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="simulate-the-tracking-behavior-of-cann">Simulate the tracking behavior of CANN</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">smooth_tracking</span><span class="p">():</span>
    <span class="n">cann</span> <span class="o">=</span> <span class="n">CANN1D</span><span class="p">(</span><span class="n">num</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">8.1</span><span class="p">)</span>

    <span class="c1"># 定义随时间变化的外部刺激
</span>    <span class="n">v_ext</span> <span class="o">=</span> <span class="mf">1e-3</span>
    <span class="n">dur1</span><span class="p">,</span> <span class="n">dur2</span><span class="p">,</span> <span class="n">dur3</span> <span class="o">=</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mi">20</span>
    <span class="n">num1</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">dur1</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">())</span>
    <span class="n">num2</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">dur2</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">())</span>
    <span class="n">num3</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">dur3</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">())</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num1</span> <span class="o">+</span> <span class="n">num2</span> <span class="o">+</span> <span class="n">num3</span><span class="p">)</span>
    <span class="n">position</span><span class="p">[</span><span class="n">num1</span><span class="p">:</span> <span class="n">num1</span> <span class="o">+</span> <span class="n">num2</span><span class="p">]</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">num2</span><span class="p">)</span>
    <span class="n">position</span><span class="p">[</span><span class="n">num1</span> <span class="o">+</span> <span class="n">num2</span><span class="p">:</span> <span class="p">]</span> <span class="o">=</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">pi</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">position</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">Iext</span> <span class="o">=</span> <span class="n">cann</span><span class="p">.</span><span class="n">get_stimulus_by_pos</span><span class="p">(</span><span class="n">position</span><span class="p">)</span>

    <span class="c1"># 运行模拟
</span>    <span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">cann</span><span class="p">,</span>
                         <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s">'input'</span><span class="p">,</span> <span class="n">Iext</span><span class="p">,</span> <span class="s">'iter'</span><span class="p">],</span>
                         <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'u'</span><span class="p">])</span>
    <span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">dur1</span> <span class="o">+</span> <span class="n">dur2</span> <span class="o">+</span> <span class="n">dur3</span><span class="p">)</span>

    <span class="c1"># 可视化
</span>    <span class="k">def</span> <span class="nf">plot_response</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">extra_fun</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">gs</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">get_figure</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">ts</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">t</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">())</span>
        <span class="n">I</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="n">Iext</span><span class="p">[</span><span class="n">ts</span><span class="p">],</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">u</span><span class="p">[</span><span class="n">ts</span><span class="p">]</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cann</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Iext'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cann</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'U'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">'$t$'</span> <span class="o">+</span> <span class="s">' = {} ms'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$x$'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'top'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'right'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">extra_fun</span><span class="p">:</span> <span class="n">extra_fun</span><span class="p">()</span>
        <span class="c1"># plt.savefig(f'CANN_tracking_t={t}.pdf', transparent=True, dpi=500)
</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">f</span><span class="p">():</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">''</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s">"-&gt;"</span><span class="p">))</span>

    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">15.</span><span class="p">,</span> <span class="n">extra_fun</span><span class="o">=</span><span class="n">f</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">f</span><span class="p">():</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">''</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s">"-&gt;"</span><span class="p">))</span>

    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">extra_fun</span><span class="o">=</span><span class="n">f</span><span class="p">)</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">30.</span><span class="p">)</span>

    <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">animate_1D</span><span class="p">(</span>
        <span class="n">dynamical_vars</span><span class="o">=</span><span class="p">[{</span><span class="s">'ys'</span><span class="p">:</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">u</span><span class="p">,</span> <span class="s">'xs'</span><span class="p">:</span> <span class="n">cann</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="s">'legend'</span><span class="p">:</span> <span class="s">'u'</span><span class="p">},</span>
                        <span class="p">{</span><span class="s">'ys'</span><span class="p">:</span> <span class="n">Iext</span><span class="p">,</span> <span class="s">'xs'</span><span class="p">:</span> <span class="n">cann</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="s">'legend'</span><span class="p">:</span> <span class="s">'Iext'</span><span class="p">}],</span>
        <span class="n">frame_step</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">frame_delay</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">smooth_tracking</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="customize-a-cann-with-sfasimulate-the-spontaneous-traveling-wave">Customize a CANN with SFASimulate the spontaneous traveling wave</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CANN1D_SFA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">NeuGroupNS</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">tau_v</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">8.1</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">A</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">J0</span><span class="o">=</span><span class="mf">4.</span><span class="p">,</span>
               <span class="n">z_min</span><span class="o">=-</span><span class="n">bm</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">z_max</span><span class="o">=</span><span class="n">bm</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">CANN1D_SFA</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">num</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># 初始化参数
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">tau_v</span> <span class="o">=</span> <span class="n">tau_v</span> <span class="c1">#time constant of SFA
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">a</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">A</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">J0</span> <span class="o">=</span> <span class="n">J0</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">m</span> <span class="c1">#SFA strength
</span>      
    <span class="c1"># 初始化特征空间相关参数
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">z_min</span> <span class="o">=</span> <span class="n">z_min</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">z_max</span> <span class="o">=</span> <span class="n">z_max</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span> <span class="o">=</span> <span class="n">z_max</span> <span class="o">-</span> <span class="n">z_min</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">z_min</span><span class="p">,</span> <span class="n">z_max</span><span class="p">,</span> <span class="n">num</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">rho</span> <span class="o">=</span> <span class="n">num</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">dx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span> <span class="o">/</span> <span class="n">num</span>

    <span class="c1"># 初始化变量
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">u</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num</span><span class="p">))</span> <span class="c1">#SFA current
</span>    <span class="bp">self</span><span class="p">.</span><span class="nb">input</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">conn_mat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">make_conn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 连接矩阵
</span>
    <span class="c1"># 定义积分函数
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">integral</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">derivative</span><span class="p">)</span>

  <span class="c1"># 微分方程
</span>  <span class="o">@</span><span class="nb">property</span>
  <span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">du</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">u</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">Irec</span><span class="p">,</span> <span class="n">Iext</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="n">u</span> <span class="o">+</span> <span class="n">Irec</span> <span class="o">+</span> <span class="n">Iext</span><span class="o">-</span><span class="n">v</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau</span>
    <span class="n">dv</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">v</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">u</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="n">v</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">m</span><span class="o">*</span><span class="n">u</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau_v</span>
    <span class="k">return</span> <span class="n">bp</span><span class="p">.</span><span class="n">JointEq</span><span class="p">([</span><span class="n">du</span><span class="p">,</span> <span class="n">dv</span><span class="p">])</span>

  <span class="c1"># 将距离转换到[-z_range/2, z_range/2)之间
</span>  <span class="k">def</span> <span class="nf">dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">d</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span><span class="p">,</span> <span class="n">d</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">d</span>

  <span class="c1"># 计算连接矩阵
</span>  <span class="k">def</span> <span class="nf">make_conn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">bm</span><span class="p">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">d</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dist</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">])</span>  <span class="c1"># 距离矩阵
</span>    <span class="n">Jxx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">J0</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span>
      <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">d</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">Jxx</span>

  <span class="c1"># 获取各个神经元到pos处神经元的输入
</span>  <span class="k">def</span> <span class="nf">get_stimulus_by_pos</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pos</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.25</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dist</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">pos</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">u2</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">u</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">u2</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">k</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">u2</span><span class="p">))</span>
    <span class="n">Irec</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conn_mat</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>
    <span class="n">u</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">u</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">,</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'t'</span><span class="p">],</span><span class="n">Irec</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">u</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">u</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">,</span><span class="n">u</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">v</span>
    <span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">[:]</span> <span class="o">=</span> <span class="mf">0.</span>  <span class="c1"># 重置外部电流
</span></code></pre></div></div>

<h3 id="simulate-the-spontaneous-traveling-wave">Simulate the spontaneous traveling wave</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">traveling_wave</span><span class="p">(</span><span class="n">num</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span><span class="n">m</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="c1"># 生成CANN
</span>    <span class="n">cann_sfa</span> <span class="o">=</span> <span class="n">CANN1D_SFA</span><span class="p">(</span><span class="n">num</span><span class="o">=</span><span class="n">num</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="n">m</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>

    <span class="c1"># 生成外部刺激
</span>    <span class="n">dur</span> <span class="o">=</span> <span class="mf">1000.</span>
    <span class="n">noise_level</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">Iext</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">noise_level</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">dur</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">()),</span> <span class="n">num</span><span class="p">))</span>
    <span class="n">duration</span> <span class="o">=</span> <span class="n">dur</span>
    <span class="c1"># 运行数值模拟
</span>    <span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">cann_sfa</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s">'input'</span><span class="p">,</span> <span class="n">Iext</span><span class="p">,</span> <span class="s">'iter'</span><span class="p">],</span> <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'u'</span><span class="p">])</span>
    <span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">duration</span><span class="p">)</span>

    <span class="c1"># 可视化
</span>    <span class="k">def</span> <span class="nf">plot_response</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">gs</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">get_figure</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">ts</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">t</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">())</span>
        <span class="n">I</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="n">Iext</span><span class="p">[</span><span class="n">ts</span><span class="p">],</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">u</span><span class="p">[</span><span class="n">ts</span><span class="p">]</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cann_sfa</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Iext'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cann_sfa</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'U'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">'$t$'</span> <span class="o">+</span> <span class="s">' = {} ms'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$x$'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'top'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'right'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="c1"># plt.savefig(f'CANN_t={t}.pdf', transparent=True, dpi=500)
</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">100.</span><span class="p">)</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">150.</span><span class="p">)</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">200.</span><span class="p">)</span>

    <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">animate_1D</span><span class="p">(</span>
        <span class="n">dynamical_vars</span><span class="o">=</span><span class="p">[{</span><span class="s">'ys'</span><span class="p">:</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">u</span><span class="p">,</span> <span class="s">'xs'</span><span class="p">:</span> <span class="n">cann_sfa</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="s">'legend'</span><span class="p">:</span> <span class="s">'u'</span><span class="p">},</span>
                        <span class="p">{</span><span class="s">'ys'</span><span class="p">:</span> <span class="n">Iext</span><span class="p">,</span> <span class="s">'xs'</span><span class="p">:</span> <span class="n">cann_sfa</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="s">'legend'</span><span class="p">:</span> <span class="s">'Iext'</span><span class="p">}],</span>
        <span class="n">frame_step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">frame_delay</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
        <span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
<span class="n">traveling_wave</span><span class="p">(</span><span class="n">num</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span><span class="n">m</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="simulate-the-anticipative-tracking">Simulate the anticipative tracking</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">anticipative_tracking</span><span class="p">(</span><span class="n">m</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">v_ext</span><span class="o">=</span><span class="mi">6</span><span class="o">*</span><span class="mf">1e-3</span><span class="p">):</span>
    <span class="n">cann_sfa</span> <span class="o">=</span> <span class="n">CANN1D_SFA</span><span class="p">(</span><span class="n">num</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="n">m</span><span class="p">)</span>
    
    <span class="c1"># 定义随时间变化的外部刺激
</span>    <span class="n">v_ext</span> <span class="o">=</span> <span class="n">v_ext</span>
    <span class="n">dur1</span><span class="p">,</span> <span class="n">dur2</span><span class="p">,</span> <span class="o">=</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">1000.</span>
    <span class="n">num1</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">dur1</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">())</span>
    <span class="n">num2</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">dur2</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">())</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num1</span> <span class="o">+</span> <span class="n">num2</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num2</span><span class="p">):</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">position</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">num1</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">v_ext</span><span class="o">*</span><span class="n">bm</span><span class="p">.</span><span class="n">dt</span>
        <span class="c1"># the periodical boundary
</span>        <span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">pos</span><span class="o">&gt;</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">pos</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">pos</span><span class="p">)</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">pos</span><span class="o">&lt;-</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">pos</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">pos</span><span class="p">)</span>
        <span class="c1"># update
</span>        <span class="n">position</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">num1</span><span class="p">]</span> <span class="o">=</span> <span class="n">pos</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">position</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">Iext</span> <span class="o">=</span> <span class="n">cann_sfa</span><span class="p">.</span><span class="n">get_stimulus_by_pos</span><span class="p">(</span><span class="n">position</span><span class="p">)</span>

    <span class="c1"># 运行模拟
</span>    <span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">cann_sfa</span><span class="p">,</span>
                         <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s">'input'</span><span class="p">,</span> <span class="n">Iext</span><span class="p">,</span> <span class="s">'iter'</span><span class="p">],</span>
                         <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'u'</span><span class="p">],</span>
                         <span class="n">dyn_vars</span><span class="o">=</span><span class="n">cann_sfa</span><span class="p">.</span><span class="nb">vars</span><span class="p">())</span>
    <span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">dur1</span> <span class="o">+</span> <span class="n">dur2</span><span class="p">)</span>

    <span class="c1"># 可视化
</span>    <span class="k">def</span> <span class="nf">plot_response</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">extra_fun</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">gs</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">get_figure</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">ts</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">t</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">())</span>
        <span class="n">I</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="n">Iext</span><span class="p">[</span><span class="n">ts</span><span class="p">],</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">u</span><span class="p">[</span><span class="n">ts</span><span class="p">]</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cann_sfa</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Iext'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cann_sfa</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="mi">10</span><span class="o">*</span><span class="n">u</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'U'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">'$t$'</span> <span class="o">+</span> <span class="s">' = {} ms'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$x$'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'top'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'right'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">200.</span><span class="p">)</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">400.</span><span class="p">)</span>
    <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">animate_1D</span><span class="p">(</span>
        <span class="n">dynamical_vars</span><span class="o">=</span><span class="p">[{</span><span class="s">'ys'</span><span class="p">:</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">u</span><span class="p">,</span> <span class="s">'xs'</span><span class="p">:</span> <span class="n">cann_sfa</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="s">'legend'</span><span class="p">:</span> <span class="s">'u'</span><span class="p">},</span>
                        <span class="p">{</span><span class="s">'ys'</span><span class="p">:</span> <span class="n">Iext</span><span class="p">,</span> <span class="s">'xs'</span><span class="p">:</span> <span class="n">cann_sfa</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="s">'legend'</span><span class="p">:</span> <span class="s">'Iext'</span><span class="p">}],</span>
        <span class="n">frame_step</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">frame_delay</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">anticipative_tracking</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="customize-a-cann-with-stp">Customize a CANN with STP</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CANN1D_STP</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">NeuGroupNS</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">tau_f</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">tau_d</span><span class="o">=</span><span class="mf">30.</span><span class="p">,</span> <span class="n">G</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">8.1</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">A</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">J0</span><span class="o">=</span><span class="mf">12.</span><span class="p">,</span>
               <span class="n">z_min</span><span class="o">=-</span><span class="n">bm</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">z_max</span><span class="o">=</span><span class="n">bm</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">CANN1D_STP</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">num</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># 初始化参数
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">tau_f</span> <span class="o">=</span> <span class="n">tau_f</span> <span class="c1">#time constant of u
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">tau_d</span> <span class="o">=</span> <span class="n">tau_d</span> <span class="c1">#time constant of h
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">G</span> <span class="o">=</span> <span class="n">G</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">a</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">A</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">J0</span> <span class="o">=</span> <span class="n">J0</span>
      
    <span class="c1"># 初始化特征空间相关参数
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">z_min</span> <span class="o">=</span> <span class="n">z_min</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">z_max</span> <span class="o">=</span> <span class="n">z_max</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span> <span class="o">=</span> <span class="n">z_max</span> <span class="o">-</span> <span class="n">z_min</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">z_min</span><span class="p">,</span> <span class="n">z_max</span><span class="p">,</span> <span class="n">num</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">rho</span> <span class="o">=</span> <span class="n">num</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">dx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span> <span class="o">/</span> <span class="n">num</span>

    <span class="c1"># 初始化变量
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">u</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">g</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num</span><span class="p">))</span> <span class="c1">#neuro-transmitter release probability
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num</span><span class="p">))</span> <span class="c1">#neuro-transmitter available fraction
</span>    <span class="bp">self</span><span class="p">.</span><span class="nb">input</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">conn_mat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">make_conn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 连接矩阵
</span>
    <span class="c1"># 定义积分函数
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">integral</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">derivative</span><span class="p">)</span>

  <span class="c1"># 微分方程
</span>  <span class="o">@</span><span class="nb">property</span>
  <span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">du</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">u</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">Irec</span><span class="p">,</span> <span class="n">Iext</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="n">u</span> <span class="o">+</span> <span class="n">Irec</span> <span class="o">+</span> <span class="n">Iext</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau</span>
    <span class="n">dg</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">g</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">r</span><span class="p">:</span> <span class="o">-</span><span class="n">g</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau_f</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">G</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">r</span> 
    <span class="n">dh</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">r</span><span class="p">:</span>  <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau_d</span> <span class="o">-</span> <span class="p">(</span><span class="n">g</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">G</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">g</span><span class="p">))</span> <span class="o">*</span> <span class="n">h</span> <span class="o">*</span><span class="n">r</span>
    <span class="k">return</span> <span class="n">bp</span><span class="p">.</span><span class="n">JointEq</span><span class="p">([</span><span class="n">du</span><span class="p">,</span> <span class="n">dg</span><span class="p">,</span> <span class="n">dh</span><span class="p">])</span>

  <span class="c1"># 将距离转换到[-z_range/2, z_range/2)之间
</span>  <span class="k">def</span> <span class="nf">dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">d</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span><span class="p">,</span> <span class="n">d</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">d</span>

  <span class="c1"># 计算连接矩阵
</span>  <span class="k">def</span> <span class="nf">make_conn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">bm</span><span class="p">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">d</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dist</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">])</span>  <span class="c1"># 距离矩阵
</span>    <span class="n">Jxx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">J0</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span>
      <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">d</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">Jxx</span>

  <span class="c1"># 获取各个神经元到pos处神经元的输入
</span>  <span class="k">def</span> <span class="nf">get_stimulus_by_pos</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pos</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.25</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dist</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">pos</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">u2</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">u</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">u2</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">k</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">u2</span><span class="p">))</span> 
    <span class="n">Irec</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conn_mat</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">g</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">G</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">))</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="o">*</span><span class="n">r</span><span class="p">)</span>
    <span class="n">u</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral</span><span class="p">(</span><span class="n">u</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">u</span><span class="p">,</span> <span class="n">g</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'t'</span><span class="p">],</span> <span class="n">Irec</span><span class="o">=</span><span class="n">Irec</span><span class="p">,</span> <span class="n">Iext</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="n">r</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="n">bm</span><span class="p">.</span><span class="n">dt</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">u</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">u</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">,</span><span class="n">u</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">g</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">h</span>
    <span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">[:]</span> <span class="o">=</span> <span class="mf">0.</span>  <span class="c1"># 重置外部电流
</span></code></pre></div></div>

<h3 id="simulate-traveling-wave-in-cann-with-stp">Simulate traveling wave in CANN with STP</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">traveling_wave_STP</span><span class="p">(</span><span class="n">num</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">J0</span><span class="o">=</span><span class="mf">12.</span><span class="p">,</span><span class="n">tau_d</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span><span class="n">tau_f</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span><span class="n">G</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
    <span class="c1"># 生成CANN
</span>    <span class="n">cann_stp</span> <span class="o">=</span> <span class="n">CANN1D_STP</span><span class="p">(</span><span class="n">num</span><span class="o">=</span><span class="n">num</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span><span class="n">tau_d</span><span class="o">=</span><span class="n">tau_d</span><span class="p">,</span><span class="n">tau_f</span><span class="o">=</span><span class="n">tau_f</span><span class="p">,</span><span class="n">G</span><span class="o">=</span><span class="n">G</span><span class="p">,</span> <span class="n">J0</span><span class="o">=</span><span class="n">J0</span><span class="p">)</span>

    <span class="c1"># 生成外部刺激
</span>    <span class="n">dur</span> <span class="o">=</span> <span class="mf">1000.</span>
    <span class="n">noise_level</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">Iext</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">noise_level</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">dur</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">()),</span> <span class="n">num</span><span class="p">))</span>
    <span class="n">duration</span> <span class="o">=</span> <span class="n">dur</span>
    <span class="c1"># 运行数值模拟
</span>    <span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">cann_stp</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s">'input'</span><span class="p">,</span> <span class="n">Iext</span><span class="p">,</span> <span class="s">'iter'</span><span class="p">],</span> <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'u'</span><span class="p">,</span><span class="s">'g'</span><span class="p">,</span><span class="s">'h'</span><span class="p">])</span>
    <span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">duration</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">as_numpy</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">u</span><span class="p">)</span>
    <span class="n">max_index</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="mi">1000</span><span class="p">,:])</span>
    <span class="k">print</span><span class="p">(</span><span class="n">max_index</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">g</span><span class="p">[:,</span><span class="n">max_index</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s">'g'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">h</span><span class="p">[:,</span><span class="n">max_index</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s">'h'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="c1"># 可视化
</span>    <span class="k">def</span> <span class="nf">plot_response</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">gs</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">get_figure</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">ts</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">t</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">())</span>
        <span class="n">I</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="n">Iext</span><span class="p">[</span><span class="n">ts</span><span class="p">],</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">u</span><span class="p">[</span><span class="n">ts</span><span class="p">]</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cann_stp</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Iext'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cann_stp</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'U'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">'$t$'</span> <span class="o">+</span> <span class="s">' = {} ms'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$x$'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'top'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'right'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">100.</span><span class="p">)</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">200.</span><span class="p">)</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">300.</span><span class="p">)</span>

    <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">animate_1D</span><span class="p">(</span>
        <span class="n">dynamical_vars</span><span class="o">=</span><span class="p">[{</span><span class="s">'ys'</span><span class="p">:</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">u</span><span class="p">,</span> <span class="s">'xs'</span><span class="p">:</span> <span class="n">cann_stp</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="s">'legend'</span><span class="p">:</span> <span class="s">'u'</span><span class="p">},</span>
                        <span class="p">{</span><span class="s">'ys'</span><span class="p">:</span> <span class="n">Iext</span><span class="p">,</span> <span class="s">'xs'</span><span class="p">:</span> <span class="n">cann_stp</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="s">'legend'</span><span class="p">:</span> <span class="s">'Iext'</span><span class="p">}],</span>
        <span class="n">frame_step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">frame_delay</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
        <span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
<span class="n">traveling_wave_STP</span><span class="p">(</span><span class="n">G</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">tau_d</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="decision-making-network">Decision-Making Network</h1>

<h2 id="lip---decision-making">LIP -&gt; Decision-Making</h2>

<h3 id="coherent-motion-task">Coherent motion task</h3>

<p>判断随机点(大部分点)的运动朝向</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828100425871.png" alt="image-20230828100425871" /></p>

<p>coherence影响任务的难度
0%难，100%简单</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828100516123.png" alt="image-20230828100516123" /></p>

<p>编码决策的响应，不是运动</p>

<h3 id="reaction-time-vs-fixed-duration">Reaction Time vs. Fixed Duration</h3>

<p>coherence越高，反应时间越短</p>

<p>Fixed Duration多了Delay time</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828100658772.png" alt="image-20230828100658772" /></p>

<p>实验设计纯粹把decision-making给提取出来</p>

<h4 id="effect-of-difficulty">Effect of Difficulty</h4>

<p>coherence越大，反应时间是越短，single neuron很难做到这么短的decision-making，考虑要建模的因素</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828101103008.png" alt="image-20230828101103008" /></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828101058059.png" alt="image-20230828101058059" /></p>

<h4 id="response-of-mt-neurons">Response of MT Neurons</h4>

<p>记录MT的神经元，对这种运动的朝向刺激进行编码</p>

<p>线性编码coherence运动强度的方向</p>

<p>做决策在它的下游</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828101303674.png" alt="image-20230828101303674" /></p>

<h4 id="response-of-lip-neurons">Response of LIP Neurons</h4>

<p>MT的下游找到LIP的神经元</p>

<p>爬升到一定高度再做选择</p>

<p>coherence与爬升的斜率也会有影响，任务越难，爬升斜率越小</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828101609881.png" alt="image-20230828101609881" /></p>

<h3 id="ramping-to-thresholdperfect-integrator-model">Ramping-to-threshold(perfect integrator) Model</h3>

\[\begin{aligned}\frac{dR}{dt}=I_A-I_B+\text{noise},\quad R(t)&amp;=(I_A-I_B)t+\int_0^tdt\text{noise}.\\\tau_\text{network}&amp;=\infty!\end{aligned}\]

<p>两种选择积分求和做积累，等到阈值做决策</p>

<p>Accumulates information (evidence) -&gt; Ramping</p>

<p>直接保存信息，没有特别好的生物对应</p>

<h2 id="a-spiking-network-of-dm">A Spiking Network of DM</h2>

<h3 id="a-cortical-microcircuit-model">A cortical microcircuit model</h3>

<p><img src="Notes.assets/image-20230828103055151.png" alt="image-20230828103055151" /></p>

<p>A=Upward motion B=Downward motion</p>

<p>2-population excitatory neurons (integrate-and-fire neurons driven by Poisson input)
Slow reverberatory excitation mediated by the NMDA receptors at recurrent synapses
AMPA receptors ($\tau _{syn}=$1 - 3 ms)
NMDA receptors ($\tau _{syn}=$ 50 - 100 ms).</p>

<p>两群神经元分别做不同的选择，与自己对方都有连接
NMDA 缓慢的信号使得有慢慢增长的ramping的过程
interneurons的backward有抑制作用</p>

<h4 id="coherence-dependent-input">Coherence-Dependent Input</h4>

<p>线性编码运动朝向的信息，coherence强度影响firing rate，一系列泊松过程，同时还有noise。</p>

<p>本身两种信息还是有差异</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828104054275.png" alt="image-20230828104054275" /></p>

<h4 id="duality-of-this-model">Duality of this model</h4>

<p>不同coherence的神经元响应</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828104432061.png" alt="image-20230828104432061" /></p>

<p>两个group会竞争，当有一个group达到20%，进入这个窗口，就会直接发放上去</p>

<p>Spontaneous symmetry breaking and stochastic decision making</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828104600840.png" alt="image-20230828104600840" /></p>

<h2 id="simulation-of-spiking-dm">Simulation of Spiking DM</h2>

<h3 id="a-cortical-microcircuit-model-1">A Cortical Microcircuit Model</h3>

<p>用两个coherence生成出来的序列</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828110300576.png" alt="image-20230828110300576" /></p>

\[\begin{gathered}C_m\frac{dV(t)}{dt}=-g_L(V(t)-V_L)-I_{syn}(t)\\I_{syn}(t)=I_{\mathrm{ext},\mathrm{AMPA}}\left(t\right)+I_{\mathrm{rec},AMPA}(t)+I_{\mathrm{rec},NMDA}(t)+I_{\mathrm{rec},\mathrm{GABA}}(t)\end{gathered}\]

\[\begin{gathered}
I_{\mathrm{ext},\mathrm{AMPA}}\left(t\right)=g_{\mathrm{ext},\mathrm{AMPA}}\left(V(t)-V_{E}\right)s^{\mathrm{ext},\mathrm{AMPA}}\left(t\right) \\
I_{\mathrm{rec},\mathrm{AMP}\Lambda}\left(t\right)=g_{\mathrm{rec},\mathrm{AMP}\Lambda}\left(V(t)-V_{E}\right)\sum_{j=1}^{Ce}w_{j}s_{j}^{AMPA}(t) \\
I_{\mathrm{rec},\mathrm{NMDA}}\left(t\right)=\frac{g_{\mathrm{NMDA}}(V(t)-V_{E})}{\left(1+\left[\mathrm{Mg}^{2+}\right]\exp(-0.062V(t))/3.57\right)}\sum_{j=1}^{\mathrm{C_E}}w_{j}s_{j}^{\mathrm{NMDA}}\left(t\right) \\
I_\mathrm{rec,GABA}(t)=g_\mathrm{GABA}(V(t)-V_l)\sum_{j=1}^{C_1}s_j^\mathrm{GABA}(t) 
\end{gathered}\]

\[w_j=\left\{\begin{matrix}w_+&gt;1,\\w_-&lt;1,\\others=1.\end{matrix}\right.\]

<p>四类神经元，三类信号</p>

<p>外界输入的信号，recurrent信号，其它神经元的信号，抑制神经元的信号
都有AMPA和NMDA这两个synapse，还有抑制的GABA</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AMPA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">Projection</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">conn</span><span class="p">,</span> <span class="n">delay</span><span class="p">,</span> <span class="n">g_max</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">E</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">conn</span> <span class="o">==</span> <span class="s">'all2all'</span><span class="p">:</span>
      <span class="n">comm</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">AllToAll</span><span class="p">(</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">g_max</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">conn</span> <span class="o">==</span> <span class="s">'one2one'</span><span class="p">:</span>
      <span class="n">comm</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">OneToOne</span><span class="p">(</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">g_max</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="nb">ValueError</span>
    <span class="n">syn</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">Expon</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">COBA</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">E</span><span class="o">=</span><span class="n">E</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPostMg2</span><span class="p">(</span>
      <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">,</span> <span class="n">comm</span><span class="o">=</span><span class="n">comm</span><span class="p">,</span>
      <span class="n">syn</span><span class="o">=</span><span class="n">syn</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">,</span> <span class="n">post</span><span class="o">=</span><span class="n">post</span>
    <span class="p">)</span>


<span class="k">class</span> <span class="nc">NMDA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">Projection</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">conn</span><span class="p">,</span> <span class="n">delay</span><span class="p">,</span> <span class="n">g_max</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">conn</span> <span class="o">==</span> <span class="s">'all2all'</span><span class="p">:</span>
      <span class="n">comm</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">AllToAll</span><span class="p">(</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">g_max</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">conn</span> <span class="o">==</span> <span class="s">'one2one'</span><span class="p">:</span>
      <span class="n">comm</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">OneToOne</span><span class="p">(</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">g_max</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="nb">ValueError</span>
    <span class="n">syn</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">NMDA</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">tau_decay</span><span class="o">=</span><span class="mf">100.</span><span class="p">,</span> <span class="n">tau_rise</span><span class="o">=</span><span class="mf">2.</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">MgBlock</span><span class="p">(</span><span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">cc_Mg</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPreMg2</span><span class="p">(</span>
      <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">,</span> <span class="n">syn</span><span class="o">=</span><span class="n">syn</span><span class="p">,</span>
      <span class="n">comm</span><span class="o">=</span><span class="n">comm</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">,</span> <span class="n">post</span><span class="o">=</span><span class="n">post</span>
    <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DecisionMakingNet</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">DynSysGroup</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="mf">0.15</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    <span class="c1"># 网络中各组神经元的数目
</span>    <span class="n">num_exc</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1600</span> <span class="o">*</span> <span class="n">scale</span><span class="p">)</span>
    <span class="n">num_I</span><span class="p">,</span> <span class="n">num_A</span><span class="p">,</span> <span class="n">num_B</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">400</span> <span class="o">*</span> <span class="n">scale</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">f</span> <span class="o">*</span> <span class="n">num_exc</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">f</span> <span class="o">*</span> <span class="n">num_exc</span><span class="p">)</span>
    <span class="n">num_N</span> <span class="o">=</span> <span class="n">num_exc</span> <span class="o">-</span> <span class="n">num_A</span> <span class="o">-</span> <span class="n">num_B</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">num_A</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_B</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_N</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_I</span> <span class="o">=</span> <span class="n">num_A</span><span class="p">,</span> <span class="n">num_B</span><span class="p">,</span> <span class="n">num_N</span><span class="p">,</span> <span class="n">num_I</span>

    <span class="n">poisson_freq</span> <span class="o">=</span> <span class="mf">2400.</span>  <span class="c1"># Hz
</span>    <span class="n">w_pos</span> <span class="o">=</span> <span class="mf">1.7</span>
    <span class="n">w_neg</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">f</span> <span class="o">*</span> <span class="p">(</span><span class="n">w_pos</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">f</span><span class="p">)</span>
    <span class="n">g_ext2E_AMPA</span> <span class="o">=</span> <span class="mf">2.1</span>  <span class="c1"># nS
</span>    <span class="n">g_ext2I_AMPA</span> <span class="o">=</span> <span class="mf">1.62</span>  <span class="c1"># nS
</span>    <span class="n">g_E2E_AMPA</span> <span class="o">=</span> <span class="mf">0.05</span> <span class="o">/</span> <span class="n">scale</span>  <span class="c1"># nS
</span>    <span class="n">g_E2I_AMPA</span> <span class="o">=</span> <span class="mf">0.04</span> <span class="o">/</span> <span class="n">scale</span>  <span class="c1"># nS
</span>    <span class="n">g_E2E_NMDA</span> <span class="o">=</span> <span class="mf">0.165</span> <span class="o">/</span> <span class="n">scale</span>  <span class="c1"># nS
</span>    <span class="n">g_E2I_NMDA</span> <span class="o">=</span> <span class="mf">0.13</span> <span class="o">/</span> <span class="n">scale</span>  <span class="c1"># nS
</span>    <span class="n">g_I2E_GABAa</span> <span class="o">=</span> <span class="mf">1.3</span> <span class="o">/</span> <span class="n">scale</span>  <span class="c1"># nS
</span>    <span class="n">g_I2I_GABAa</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">scale</span>  <span class="c1"># nS
</span>
    <span class="n">neu_par</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">V_rest</span><span class="o">=-</span><span class="mf">70.</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mf">55.</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=-</span><span class="mf">50.</span><span class="p">,</span> <span class="n">V_initializer</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">OneInit</span><span class="p">(</span><span class="o">-</span><span class="mf">70.</span><span class="p">))</span>

    <span class="c1"># E neurons/pyramid neurons
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">LifRef</span><span class="p">(</span><span class="n">num_A</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">R</span><span class="o">=</span><span class="mf">0.04</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="o">**</span><span class="n">neu_par</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">LifRef</span><span class="p">(</span><span class="n">num_B</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">R</span><span class="o">=</span><span class="mf">0.04</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="o">**</span><span class="n">neu_par</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">LifRef</span><span class="p">(</span><span class="n">num_N</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">R</span><span class="o">=</span><span class="mf">0.04</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="o">**</span><span class="n">neu_par</span><span class="p">)</span>

    <span class="c1"># I neurons/interneurons
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">I</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">LifRef</span><span class="p">(</span><span class="n">num_I</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">R</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="o">**</span><span class="n">neu_par</span><span class="p">)</span>

    <span class="c1"># poisson stimulus  # 'freqs' as bm.Variable
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">IA</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">PoissonGroup</span><span class="p">(</span><span class="n">num_A</span><span class="p">,</span> <span class="n">freqs</span><span class="o">=</span><span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">IB</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">PoissonGroup</span><span class="p">(</span><span class="n">num_B</span><span class="p">,</span> <span class="n">freqs</span><span class="o">=</span><span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>

    <span class="c1"># noise neurons
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">noise_B</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">PoissonGroup</span><span class="p">(</span><span class="n">num_B</span><span class="p">,</span> <span class="n">freqs</span><span class="o">=</span><span class="n">poisson_freq</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">noise_A</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">PoissonGroup</span><span class="p">(</span><span class="n">num_A</span><span class="p">,</span> <span class="n">freqs</span><span class="o">=</span><span class="n">poisson_freq</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">noise_N</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">PoissonGroup</span><span class="p">(</span><span class="n">num_N</span><span class="p">,</span> <span class="n">freqs</span><span class="o">=</span><span class="n">poisson_freq</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">noise_I</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">PoissonGroup</span><span class="p">(</span><span class="n">num_I</span><span class="p">,</span> <span class="n">freqs</span><span class="o">=</span><span class="n">poisson_freq</span><span class="p">)</span>

    <span class="c1"># define external inputs
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">IA2A</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">IA</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="s">'one2one'</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">g_ext2E_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">IB2B</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">IB</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="s">'one2one'</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">g_ext2E_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>

    <span class="c1"># define AMPA projections from N
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">N2B_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_AMPA</span> <span class="o">*</span> <span class="n">w_neg</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">N2A_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_AMPA</span> <span class="o">*</span> <span class="n">w_neg</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">N2N_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">N2I_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2I_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>

    <span class="c1"># define NMDA projections from N
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">N2B_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_NMDA</span> <span class="o">*</span> <span class="n">w_neg</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">N2A_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_NMDA</span> <span class="o">*</span> <span class="n">w_neg</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">N2N_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_NMDA</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">N2I_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2I_NMDA</span><span class="p">)</span>

    <span class="c1"># define AMPA projections from B
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">B2B_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_AMPA</span> <span class="o">*</span> <span class="n">w_pos</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">B2A_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_AMPA</span> <span class="o">*</span> <span class="n">w_neg</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">B2N_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">B2I_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2I_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>

    <span class="c1"># define NMDA projections from B
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">B2B_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_NMDA</span> <span class="o">*</span> <span class="n">w_pos</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">B2A_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_NMDA</span> <span class="o">*</span> <span class="n">w_neg</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">B2N_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_NMDA</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">B2I_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2I_NMDA</span><span class="p">)</span>

    <span class="c1"># define AMPA projections from A
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">A2B_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_AMPA</span> <span class="o">*</span> <span class="n">w_neg</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">A2A_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_AMPA</span> <span class="o">*</span> <span class="n">w_pos</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">A2N_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">A2I_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2I_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>

    <span class="c1"># define NMDA projections from A
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">A2B_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_NMDA</span> <span class="o">*</span> <span class="n">w_neg</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">A2A_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_NMDA</span> <span class="o">*</span> <span class="n">w_pos</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">A2N_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_NMDA</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">A2I_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2I_NMDA</span><span class="p">)</span>

    <span class="c1"># define I-&gt;E/I conn
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">I2B</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_I2E_GABAa</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">E</span><span class="o">=-</span><span class="mf">70.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">I2A</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_I2E_GABAa</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">E</span><span class="o">=-</span><span class="mf">70.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">I2N</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_I2E_GABAa</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">E</span><span class="o">=-</span><span class="mf">70.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">I2I</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_I2I_GABAa</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">E</span><span class="o">=-</span><span class="mf">70.</span><span class="p">)</span>

    <span class="c1"># define external projections
</span>    <span class="c1">#### TO DO!!!!
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">noise2B</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">noise_B</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="s">'one2one'</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">g_ext2E_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">noise2A</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">noise_A</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="s">'one2one'</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">g_ext2E_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">noise2N</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">noise_N</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="s">'one2one'</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">g_ext2E_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">noise2I</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">noise_I</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="s">'one2one'</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">g_ext2I_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Tool</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre_stimulus_period</span><span class="o">=</span><span class="mf">100.</span><span class="p">,</span> <span class="n">stimulus_period</span><span class="o">=</span><span class="mf">1000.</span><span class="p">,</span> <span class="n">delay_period</span><span class="o">=</span><span class="mf">500.</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span> <span class="o">=</span> <span class="n">pre_stimulus_period</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">stimulus_period</span> <span class="o">=</span> <span class="n">stimulus_period</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">delay_period</span> <span class="o">=</span> <span class="n">delay_period</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">freq_variance</span> <span class="o">=</span> <span class="mf">10.</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">freq_interval</span> <span class="o">=</span> <span class="mf">50.</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">total_period</span> <span class="o">=</span> <span class="n">pre_stimulus_period</span> <span class="o">+</span> <span class="n">stimulus_period</span> <span class="o">+</span> <span class="n">delay_period</span>

  <span class="k">def</span> <span class="nf">generate_freqs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mean</span><span class="p">):</span>
    <span class="c1"># stimulus period
</span>    <span class="n">n_stim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">stimulus_period</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">freq_interval</span><span class="p">)</span>
    <span class="n">n_interval</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">freq_interval</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">())</span>
    <span class="n">freqs_stim</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">freq_variance</span><span class="p">,</span> <span class="p">(</span><span class="n">n_stim</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">freqs_stim</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">freqs_stim</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_interval</span><span class="p">)).</span><span class="n">flatten</span><span class="p">()</span>
    <span class="c1"># pre stimulus period
</span>    <span class="n">freqs_pre</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">()))</span>
    <span class="c1"># post stimulus period
</span>    <span class="n">freqs_delay</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">delay_period</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">()))</span>
    <span class="n">all_freqs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">freqs_pre</span><span class="p">,</span> <span class="n">freqs_stim</span><span class="p">,</span> <span class="n">freqs_delay</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">bm</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">all_freqs</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">visualize_results</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mon</span><span class="p">,</span> <span class="n">IA_freqs</span><span class="p">,</span> <span class="n">IB_freqs</span><span class="p">,</span> <span class="n">t_start</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">gs</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">get_figure</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="p">[</span><span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">raster_plot</span><span class="p">(</span><span class="n">mon</span><span class="p">[</span><span class="s">'ts'</span><span class="p">],</span> <span class="n">mon</span><span class="p">[</span><span class="s">'A.spike'</span><span class="p">],</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">title</span><span class="p">:</span> <span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Group A"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">t_start</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">total_period</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">stimulus_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">delay_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">raster_plot</span><span class="p">(</span><span class="n">mon</span><span class="p">[</span><span class="s">'ts'</span><span class="p">],</span> <span class="n">mon</span><span class="p">[</span><span class="s">'B.spike'</span><span class="p">],</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Group B"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">t_start</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">total_period</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">stimulus_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">delay_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">rateA</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">measure</span><span class="p">.</span><span class="n">firing_rate</span><span class="p">(</span><span class="n">mon</span><span class="p">[</span><span class="s">'A.spike'</span><span class="p">],</span> <span class="n">width</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>
    <span class="n">rateB</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">measure</span><span class="p">.</span><span class="n">firing_rate</span><span class="p">(</span><span class="n">mon</span><span class="p">[</span><span class="s">'B.spike'</span><span class="p">],</span> <span class="n">width</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mon</span><span class="p">[</span><span class="s">'ts'</span><span class="p">],</span> <span class="n">rateA</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Group A"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mon</span><span class="p">[</span><span class="s">'ts'</span><span class="p">],</span> <span class="n">rateB</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Group B"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Population activity [Hz]'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">t_start</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">total_period</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">stimulus_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">delay_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mon</span><span class="p">[</span><span class="s">'ts'</span><span class="p">],</span> <span class="n">IA_freqs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"group A"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mon</span><span class="p">[</span><span class="s">'ts'</span><span class="p">],</span> <span class="n">IB_freqs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"group B"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Input activity [Hz]"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">t_start</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">total_period</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">stimulus_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">delay_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Time [ms]"</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tool</span> <span class="o">=</span> <span class="n">Tool</span><span class="p">()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">DecisionMakingNet</span><span class="p">()</span>

<span class="n">mu0</span> <span class="o">=</span> <span class="mf">40.</span>
<span class="n">coherence</span> <span class="o">=</span> <span class="mf">25.6</span>
<span class="n">IA_freqs</span> <span class="o">=</span> <span class="n">tool</span><span class="p">.</span><span class="n">generate_freqs</span><span class="p">(</span><span class="n">mu0</span> <span class="o">+</span> <span class="n">mu0</span> <span class="o">/</span> <span class="mf">100.</span> <span class="o">*</span> <span class="n">coherence</span><span class="p">)</span>
<span class="n">IB_freqs</span> <span class="o">=</span> <span class="n">tool</span><span class="p">.</span><span class="n">generate_freqs</span><span class="p">(</span><span class="n">mu0</span> <span class="o">-</span> <span class="n">mu0</span> <span class="o">/</span> <span class="mf">100.</span> <span class="o">*</span> <span class="n">coherence</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">give_input</span><span class="p">():</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'i'</span><span class="p">]</span>
    <span class="n">net</span><span class="p">.</span><span class="n">IA</span><span class="p">.</span><span class="n">freqs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">IA_freqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">net</span><span class="p">.</span><span class="n">IB</span><span class="p">.</span><span class="n">freqs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">IB_freqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">give_input</span><span class="p">,</span> <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'A.spike'</span><span class="p">,</span> <span class="s">'B.spike'</span><span class="p">])</span>
<span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">tool</span><span class="p">.</span><span class="n">total_period</span><span class="p">)</span>
<span class="n">tool</span><span class="p">.</span><span class="n">visualize_results</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">,</span> <span class="n">IA_freqs</span><span class="p">,</span> <span class="n">IB_freqs</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="results">Results</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828112245950.png" alt="image-20230828112245950" /></p>

<h4 id="stochastic-decision-making">Stochastic Decision Making</h4>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828112253619.png" alt="image-20230828112253619" /></p>

<h2 id="a-rate-network-of-dm">A Rate Network of DM</h2>

<h3 id="reduced-model">Reduced Model</h3>

<p>化简到只有两群神经元，只接受外界输入信号，互相影响对方</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828112326267.png" alt="image-20230828112326267" /></p>

<p>Synaptic variables</p>

\[\begin{gathered}
\frac{dS_{1}}{dt} =F(x_1)\gamma(1-S_1)-S_1/\tau_s \\
\frac{dS_2}{dt} =F(x_2)\gamma(1-S_2)-S_2/\tau_s 
\end{gathered}\]

<p>Input current to each population</p>

<p>\(\begin{gathered}
x_{1} =J_{E}S_{1}+J_{I}S_{2}+I_{0}+I_{noise1}+J_{\text{ext }\mu_{1}} \\
x_{2} =J_{E}S_{2}+J_{I}S_{1}+I_{0}+I_{noise2}+J_{\mathrm{ext}}\mu_{2} 
\end{gathered}\)
Background input</p>

<p>\(I_0+I_{noise}\\
\begin{gathered}
dI_{noise1} =-I_{noise1}\frac{dt}{\tau_{0}}+\sigma dW \\
dI_{noise2} =-I_{noise2}\frac{dt}{\tau_{0}}+\sigma dW 
\end{gathered}\)
Firing rates</p>

<p>\(r_i=F(x_i)=\frac{ax_i-b}{1-\exp(-d(ax_i-b))}\)
Coherence-dependent inputs</p>

\[\begin{array}{l}\mu_1=\mu_0\big(1+c'/100\big)\\\mu_2=\mu_0\big(1-c'/100\big)\end{array}\]

\[\begin{aligned}&amp;\gamma,a,b,d,J_E,J_I,J_{\mathrm{ext}},I_0,\mu_0,\tau_{\mathrm{AMPA}},\sigma_{\mathrm{noise}}\\&amp;\text{are fixed parameters.}\end{aligned}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DecisionMakingRateModel</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">NeuGroup</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">coherence</span><span class="p">,</span> <span class="n">JE</span><span class="o">=</span><span class="mf">0.2609</span><span class="p">,</span> <span class="n">JI</span><span class="o">=</span><span class="mf">0.0497</span><span class="p">,</span> <span class="n">Jext</span><span class="o">=</span><span class="mf">5.2e-4</span><span class="p">,</span> <span class="n">I0</span><span class="o">=</span><span class="mf">0.3255</span><span class="p">,</span>
                 <span class="n">gamma</span><span class="o">=</span><span class="mf">6.41e-4</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">100.</span><span class="p">,</span> <span class="n">tau_n</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">sigma_n</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">270.</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">108.</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mf">0.154</span><span class="p">,</span>
                 <span class="n">noise_freq</span><span class="o">=</span><span class="mf">2400.</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'exp_auto'</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DecisionMakingRateModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="c1"># 初始化参数
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">coherence</span> <span class="o">=</span> <span class="n">coherence</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">JE</span> <span class="o">=</span> <span class="n">JE</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">JI</span> <span class="o">=</span> <span class="n">JI</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">Jext</span> <span class="o">=</span> <span class="n">Jext</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">I0</span> <span class="o">=</span> <span class="n">I0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tau_n</span> <span class="o">=</span> <span class="n">tau_n</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">sigma_n</span> <span class="o">=</span> <span class="n">sigma_n</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">a</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">d</span>
        
        <span class="c1"># 初始化变量
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">s1</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.15</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">s2</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.15</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">r1</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">r2</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mu0</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">I1_noise</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">I2_noise</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        
        <span class="c1"># 噪声输入的神经元
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">noise1</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">PoissonGroup</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">freqs</span><span class="o">=</span><span class="n">noise_freq</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">noise2</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">PoissonGroup</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">freqs</span><span class="o">=</span><span class="n">noise_freq</span><span class="p">)</span>
        
        <span class="c1"># 定义积分函数
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">integral</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">derivative</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">)</span>
        
    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">bp</span><span class="p">.</span><span class="n">JointEq</span><span class="p">([</span><span class="bp">self</span><span class="p">.</span><span class="n">ds1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">ds2</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dI1noise</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dI2noise</span><span class="p">])</span>
        
    <span class="k">def</span> <span class="nf">ds1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">mu0</span><span class="p">):</span>
        <span class="n">I1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">Jext</span> <span class="o">*</span> <span class="n">mu0</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">coherence</span> <span class="o">/</span> <span class="mf">100.</span><span class="p">)</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">JE</span> <span class="o">*</span> <span class="n">s1</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">JI</span> <span class="o">*</span> <span class="n">s2</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">I0</span> <span class="o">+</span> <span class="n">I1</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">I1_noise</span>
        <span class="n">r1</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">d</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span><span class="p">)))</span>
        <span class="k">return</span> <span class="o">-</span> <span class="n">s1</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">s1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">r1</span>
    
    <span class="k">def</span> <span class="nf">ds2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">mu0</span><span class="p">):</span>
        <span class="n">I2</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">Jext</span><span class="o">*</span><span class="n">mu0</span><span class="o">*</span><span class="p">(</span><span class="mf">1.</span><span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">coherence</span> <span class="o">/</span> <span class="mf">100.</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">JE</span> <span class="o">*</span> <span class="n">s2</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">JI</span> <span class="o">*</span> <span class="n">s1</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">I0</span> <span class="o">+</span> <span class="n">I2</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">I2_noise</span>
        <span class="n">r2</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">d</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span><span class="p">)))</span> 
        <span class="k">return</span> <span class="o">-</span> <span class="n">s2</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">s2</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">r2</span>

    <span class="k">def</span> <span class="nf">dI1noise</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">I1_noise</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise1</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="o">-</span> <span class="n">I1_noise</span> <span class="o">+</span> <span class="n">noise1</span><span class="p">.</span><span class="n">spike</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">tau_n</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigma_n</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigma_n</span><span class="p">))</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau_n</span>
    
    <span class="k">def</span> <span class="nf">dI2noise</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">I2_noise</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise2</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="o">-</span> <span class="n">I2_noise</span> <span class="o">+</span> <span class="n">noise2</span><span class="p">.</span><span class="n">spike</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">tau_n</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigma_n</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigma_n</span><span class="p">))</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau_n</span>
    
    
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tdi</span><span class="p">):</span>
        <span class="c1"># 更新噪声神经元以产生新的随机发放 self.noise1.update(tdi) self.noise2.update(tdi)
</span>        <span class="c1"># 更新s1、s2、I1_noise、I2_noise
</span>        <span class="n">integral</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">s1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">s2</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I1_noise</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I2_noise</span><span class="p">,</span> <span class="n">tdi</span><span class="p">.</span><span class="n">t</span><span class="p">,</span> <span class="n">mu0</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">mu0</span><span class="p">,</span>
                             <span class="n">noise1</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">noise1</span><span class="p">,</span> <span class="n">noise2</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">noise2</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="n">tdi</span><span class="p">.</span><span class="n">dt</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">s1</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">s2</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I1_noise</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I2_noise</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">integral</span>
        
        <span class="c1"># 用更新后的s1、s2计算r1、r2
</span>        <span class="n">I1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">Jext</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">mu0</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">coherence</span> <span class="o">/</span> <span class="mf">100.</span><span class="p">)</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">JE</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">s1</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">JI</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">s2</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">I0</span> <span class="o">+</span> <span class="n">I1</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">I1_noise</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">r1</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">d</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span><span class="p">)))</span>
        <span class="n">I2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">Jext</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">mu0</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">coherence</span> <span class="o">/</span> <span class="mf">100.</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">JE</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">s2</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">JI</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">s1</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">I0</span> <span class="o">+</span> <span class="n">I2</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">I2_noise</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">r2</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">d</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span><span class="p">)))</span>
        
        <span class="c1"># 重置外部输入 
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">mu0</span><span class="p">[:]</span> <span class="o">=</span> <span class="mf">0.</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 定义各个阶段的时长
</span><span class="n">pre_stimulus_period</span><span class="p">,</span> <span class="n">stimulus_period</span><span class="p">,</span> <span class="n">delay_period</span> <span class="o">=</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">2000.</span><span class="p">,</span> <span class="mf">500.</span>

<span class="c1"># 生成模型
</span><span class="n">dmnet</span> <span class="o">=</span> <span class="n">DecisionMakingRateModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">coherence</span><span class="o">=</span><span class="mf">25.6</span><span class="p">,</span> <span class="n">noise_freq</span><span class="o">=</span><span class="mf">2400.</span><span class="p">)</span>

<span class="c1"># 定义电流随时间的变化
</span><span class="n">inputs</span><span class="p">,</span> <span class="n">total_period</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">constant_input</span><span class="p">([(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">pre_stimulus_period</span><span class="p">),</span>
                                                 <span class="p">(</span><span class="mf">20.</span><span class="p">,</span> <span class="n">stimulus_period</span><span class="p">),</span>
                                                 <span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">delay_period</span><span class="p">)])</span>
<span class="c1"># 运行数值模拟
</span><span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">dmnet</span><span class="p">,</span>
                     <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'s1'</span><span class="p">,</span> <span class="s">'s2'</span><span class="p">,</span> <span class="s">'r1'</span><span class="p">,</span> <span class="s">'r2'</span><span class="p">],</span>
                     <span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="s">'mu0'</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="s">'iter'</span><span class="p">))</span>
<span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">total_period</span><span class="p">)</span>

<span class="c1"># 可视化
</span><span class="n">fig</span><span class="p">,</span> <span class="n">gs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="s">'all'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">s1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'s1'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">s2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'s2'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">axvline</span><span class="p">(</span><span class="n">pre_stimulus_period</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sa">u</span><span class="s">'#444444'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">axvline</span><span class="p">(</span><span class="n">pre_stimulus_period</span> <span class="o">+</span> <span class="n">stimulus_period</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sa">u</span><span class="s">'#444444'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'gating variable $s$'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">legend</span><span class="p">()</span>

<span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">r1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'r1'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">r2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'r2'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">axvline</span><span class="p">(</span><span class="n">pre_stimulus_period</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sa">u</span><span class="s">'#444444'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">axvline</span><span class="p">(</span><span class="n">pre_stimulus_period</span> <span class="o">+</span> <span class="n">stimulus_period</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sa">u</span><span class="s">'#444444'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'t (ms)'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'firing rate $r$'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="results-1">Results</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828112555018.png" alt="image-20230828112555018" /></p>

<h2 id="phase-plane-analysis-1">Phase Plane Analysis</h2>

<p>因为只有两个variable</p>

<h3 id="model-implementation">Model implementation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">bp</span><span class="p">.</span><span class="n">odeint</span>
<span class="k">def</span> <span class="nf">int_s1</span><span class="p">(</span><span class="n">s1</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">coh</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mf">20.</span><span class="p">):</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">JE</span> <span class="o">*</span> <span class="n">s1</span> <span class="o">+</span> <span class="n">JI</span> <span class="o">*</span> <span class="n">s2</span> <span class="o">+</span> <span class="n">Ib</span> <span class="o">+</span> <span class="n">JAext</span> <span class="o">*</span> <span class="n">mu</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">coh</span><span class="o">/</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">r1</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">d</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">-</span> <span class="n">b</span><span class="p">)))</span>
    <span class="k">return</span> <span class="o">-</span> <span class="n">s1</span> <span class="o">/</span> <span class="n">tau</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">s1</span><span class="p">)</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">r1</span>

<span class="o">@</span><span class="n">bp</span><span class="p">.</span><span class="n">odeint</span>
<span class="k">def</span> <span class="nf">int_s2</span><span class="p">(</span><span class="n">s2</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">coh</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mf">20.</span><span class="p">):</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">JE</span> <span class="o">*</span> <span class="n">s2</span> <span class="o">+</span> <span class="n">JI</span> <span class="o">*</span> <span class="n">s1</span> <span class="o">+</span> <span class="n">Ib</span> <span class="o">+</span> <span class="n">JAext</span> <span class="o">*</span> <span class="n">mu</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">coh</span><span class="o">/</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">r2</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">d</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">-</span> <span class="n">b</span><span class="p">)))</span>
    <span class="k">return</span> <span class="o">-</span> <span class="n">s2</span> <span class="o">/</span> <span class="n">tau</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">s2</span><span class="p">)</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">r2</span>
</code></pre></div></div>

<h3 id="without--with-input">Without / with input</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828112709355.png" alt="image-20230828112709355" /></p>

<p>只受扰动影响，有input后中间变得不稳定，但如果已经选择，网络仍维持之前选择的结果</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828112811394.png" alt="image-20230828112811394" /></p>

<h3 id="coherence">Coherence</h3>

<p>稳定点对网络的拉伸更强</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828113031946.png" alt="image-20230828113031946" /></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828113009219.png" alt="image-20230828113009219" /></p>

<h1 id="reservoir-computing">Reservoir Computing</h1>

<p>引入训练</p>

<p>倾向于使用RNN</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828140305956.png" alt="image-20230828140305956" /></p>

<p>Connecting different units</p>

\[\begin{aligned}
&amp;\textsf{Input to unit i from unit j:} \\
&amp;&amp;&amp;I_{j\rightarrow i}=J_{ij}r_{j}(t) \\
&amp;\textsf{Total input to unit i:} \\
&amp;&amp;&amp;I_{i}^{(tot)}=\sum_{j=1}^{N}J_{ij}r_{j}(t)+I_{i}^{(ext)} 
\end{aligned}\]

\[\textsf{Activation of unit i:}
\\
\tau\frac{dx_{i}}{dt}=-x_{i}+\sum_{j=1}^{N}J_{ij}\frac{\phi(x_{j})}{1}+I_{i}^{(ext)}(t)\]

<p>训练范式</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828140707998.png" alt="image-20230828140707998" /></p>

<h2 id="echo-state-machine">Echo state machine</h2>

<h3 id="echo-state-machine-1">Echo state machine</h3>

<p>类似人工神经网络RNN，可以处理temporal信息</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828140937455.png" alt="image-20230828140937455" /></p>

\[\begin{aligned}
&amp;\mathbf{x}(n+1) =f(\mathbf{W}^{\mathrm{in}}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n)+\mathbf{W}^{\mathrm{back}}\mathbf{y}(n))  \\
&amp;\mathbf{y}(n+1) =\mathbf{W}^{\mathrm{out}}(\mathbf{u}(n+1),\mathbf{x}(n+1),\mathbf{y}(n)) 
\end{aligned}\]

<p>For an RNN, the state of its internal neurons reflects the historical information of the external inputs.</p>

<p>反映的echo的历史信息，唯一依赖历史信息</p>

<p>Assuming that the updates of the network are discrete, the external input at the 𝑛th moment is u(𝑛) and the neuron state is x(𝑛), then x(𝑛) should be determined by u(𝑛), u(𝑛 - 1), … uniquely determined. At this point, x(𝑛) can be regarded as an “echo” of the historical input signals.</p>

<p>不需要训练connection</p>

<h3 id="echo-state-machine-with-leaky-integrator">Echo state machine with leaky integrator</h3>

<p>有一个leaky项，引入decay</p>

<p>###</p>

\[\begin{aligned}\hat{h}(n)=\tanh(W^{in}x(n)+W^{rec}h(n-1)+W^{fb}y(n-1)+b^{rec})\\h(n)=(1-\alpha)x(n-1)+\alpha\hat{h}(n)\end{aligned}\]

<p>where $h(n)$ is a vector of reservoir neuron activations, $W^{in}$ and $W^{rec}$ are the input and recurrent weight matrices respectively, and $\alpha\in(0,1]$ is the leaking rate. The model is also sometimes used without the leaky integration, which is a special case of $\alpha=1$</p>

<p>The linear readout layer is defined as</p>

\[y(n)=W^{out}h(n)+b^{out}\]

<p>where $y(n)$ is network output, $W^{out}$ the output weight matrix, and $b^out$ is the output bias</p>

<h2 id="constraints-of-echo-state-machine">Constraints of echo state machine</h2>

<h3 id="echo-state-property">Echo state property</h3>

<h4 id="theorem-1">Theorem 1</h4>

<p>For the echo state network defined above, the network will be echoey as long as the maximum singular value  $\sigma_{max}&lt;1$ of the recurrent connectivity matrix W .</p>

<blockquote>
  <p>Provement:
\(\begin{aligned}
d(\mathbf{x}(n+1),\mathbf{x}^{\prime}(n+1))&amp; =d(T(\mathbf{x}(n),\mathbf{u}(n+1)),T(\mathbf{x}'(n),\mathbf{u}(n+1)))  \\
&amp;=d(f(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n)),f(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}'(n))) \\
&amp;\leq d(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n),\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}^{\prime}(n)) \\
&amp;=d(\mathbf{W}\mathbf{x}(n),\mathbf{W}\mathbf{x}'(n)) \\
&amp;=||\mathbf{W}(\mathbf{x}(n)-\mathbf{x}^{\prime}(n))|| \\
&amp;\leq\sigma_{\max}(\mathbf{W})d(\mathbf{x}(n),\mathbf{x}'(n))
\end{aligned}\)</p>
</blockquote>

<h4 id="theorem-2">Theorem 2</h4>

<table>
  <tbody>
    <tr>
      <td>For the echo state network defined above, as long as the spectral radius $</td>
      <td>\lambda_{max}</td>
      <td>$ of the recurrent connection matrix W &gt; 1, then the network must not be echogenic. The spectral radius of the matrix is the absolute value of the largest eigenvalue $\lambda_{max}$.</td>
    </tr>
  </tbody>
</table>

<h4 id="how-to-initialize">How to initialize</h4>

<p>Using these two theorems, how should we initialize W so that the network has an echo property?
If we scale W, i.e., multiply it by a scaling factor $\alpha$, then $\sigma_{max}&lt;1$ and $\lambda_{max}$ will also be scaled $\alpha$.</p>

\[\text{For any square matrix, we have}\sigma_{max}\geq|\lambda_{max}|.\\
\text{Therefore we set}\alpha_{min}=1/\sigma_{max}(W),\alpha_{max}=1/|\lambda_{max}|(W).\mathrm{Then},
\\
\begin{array}{ll}\bullet&amp;\text{if}\alpha&lt;\alpha_{min}\text{,the network must have the echo state.}\\\bullet&amp;\text{if}\alpha&gt;\alpha_{max}\text{,the network will not have the echo state.}\\\bullet&amp;\text{if}\alpha_{min}\le\alpha\le\alpha_{max}\text{,the network may have the echo state.}\end{array}\]

<p><strong>$\alpha$设的略小于1</strong></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828142516052.png" alt="image-20230828142516052" /></p>

<h3 id="global-parameters-of-reservoir">Global parameters of reservoir</h3>

<p>这些超参会影响reservoir network的性能，需要手动调参，很难自动去调整</p>

<ul>
  <li>The size $N_x$
    <ul>
      <li>General wisdom: the bigger the reservoir, the better the obtainable performance</li>
      <li>Select global parameters with smaller reservoirs, then scale to bigger ones.</li>
    </ul>
  </li>
  <li>Sparsity</li>
  <li>Distribution of nonzero elements:
    <ul>
      <li>Normal distribution</li>
      <li>Uniform distribution</li>
      <li>The width of the distributions does not matter</li>
    </ul>
  </li>
  <li>spectral radius of $W$
    <ul>
      <li>scales the width of the distribution of its nonzero elements</li>
      <li>determines how fast the influence of an input dies out in a reservoir with time, and how stable the reservoir activations are</li>
      <li>The spectral radius should be larger in tasks requiring longer memory of the input</li>
    </ul>
  </li>
  <li>Scaling(-s) to $W^{in}$:
    <ul>
      <li>For uniform distributed $W^{in}$, $\alpha$ in the range of the interval $[-a;a]$.</li>
      <li>For normal distributed $W^{in}$, one may take the standard deviation as a scaling measure.</li>
    </ul>
  </li>
</ul>

<p>The leaking rate $\alpha$</p>

<h2 id="training-of-echo-state-machine">Training of echo state machine</h2>

<h3 id="offline-learning">Offline learning</h3>

<p>The advantage of the echo state network is that it does not train recurrent connections within the reservoir, but only the readout layer from the reservoir to the output.</p>

<p>线性层的优化方法是简单的</p>

<p><strong>Ridge regression</strong></p>

\[\begin{aligned}\epsilon_{\mathrm{train}}(n)&amp;=\mathbf{y}(n)-\mathbf{\hat{y}}(n)
\\&amp;=\mathbf{y}(n)-\mathbf{W}^{\mathrm{out}}\mathbf{x}(n)
\\&amp;L_{\mathrm{ridge}}=\frac{1}{N}\sum_{i=1}^{N}\epsilon_{\mathrm{train}}^{2}(i)+\alpha||\mathbf{W^{out}}||^{2}
\\\\W^{out}&amp;=Y^{target}X^T(XX^T+\beta I)^{-1}\end{aligned}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trainer</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">OfflineTrainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">fit_method</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">algorithms</span><span class="p">.</span><span class="n">RidgeRegression</span><span class="p">(</span><span class="mf">1e-7</span><span class="p">),</span> <span class="n">dt</span><span class="o">=</span><span class="n">dt</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="online-learning">Online learning</h3>

<p>来一个sample，进行一次training，对训练资源可以避免瓶颈</p>

<p>The training data is passed to the trainer in a certain sequence (e.g., time series), and the trainer continuously learns based on the new incoming data.</p>

<p><strong>Recursive Least Squares (RLS) algorithm</strong></p>

\[E(\mathbf{y},\mathbf{y}^\mathrm{target},n)=\frac{1}{N_\mathrm{y}}\sum_{i=1}^{N_\mathrm{y}}\sum_{j=1}^{n}\lambda^{n-j}\left(y_i(j)-y_i^\mathrm{target}(j)\right)^2,\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trainer</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">OnlineTrainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">fit_method</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">algorithms</span><span class="p">.</span><span class="n">RLS</span><span class="p">(),</span> <span class="n">dt</span><span class="o">=</span><span class="n">dt</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="dataset">Dataset</h3>

<p>给定time sequence，可以让网络去预测regression</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828144309742.png" alt="image-20230828144309742" /></p>

<p>用到BrainPy集成的<code class="language-plaintext highlighter-rouge">Neuromorphic and Cognitive Datasets</code></p>

<h3 id="other-tasks">Other tasks</h3>

<p><code class="language-plaintext highlighter-rouge">MNIST dataset</code> or <code class="language-plaintext highlighter-rouge">Fashion MNIST</code></p>

<p>Two aspect:</p>

<ul>
  <li>Running time</li>
  <li>Memory Usage</li>
</ul>

<h2 id="echo-state-machine-programming">Echo state machine programming</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">brainpy</span> <span class="k">as</span> <span class="n">bp</span>
<span class="kn">import</span> <span class="nn">brainpy.math</span> <span class="k">as</span> <span class="n">bm</span>
<span class="kn">import</span> <span class="nn">brainpy_datasets</span> <span class="k">as</span> <span class="n">bd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># enable x64 computation
</span><span class="n">bm</span><span class="p">.</span><span class="n">set_environment</span><span class="p">(</span><span class="n">x64</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">bm</span><span class="p">.</span><span class="n">batching_mode</span><span class="p">)</span>
<span class="n">bm</span><span class="p">.</span><span class="n">set_platform</span><span class="p">(</span><span class="s">'cpu'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="dataset-1">Dataset</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_mackey_glass_series</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">x_series</span><span class="p">,</span> <span class="n">x_tau_series</span><span class="p">,</span> <span class="n">num_sample</span><span class="p">):</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

  <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">"Timeserie - </span><span class="si">{</span><span class="n">num_sample</span><span class="si">}</span><span class="s"> timesteps"</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ts</span><span class="p">[:</span><span class="n">num_sample</span><span class="p">],</span> <span class="n">x_series</span><span class="p">[:</span><span class="n">num_sample</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"lightgrey"</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">ts</span><span class="p">[:</span><span class="n">num_sample</span><span class="p">],</span> <span class="n">x_series</span><span class="p">[:</span><span class="n">num_sample</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">ts</span><span class="p">[:</span><span class="n">num_sample</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s">"viridis"</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"$t$"</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"$P(t)$"</span><span class="p">)</span>

  <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">margins</span><span class="p">(</span><span class="mf">0.05</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">"Phase diagram: $P(t) = f(P(t-</span><span class="se">\\</span><span class="s">tau))$"</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_tau_series</span><span class="p">[:</span> <span class="n">num_sample</span><span class="p">],</span> <span class="n">x_series</span><span class="p">[:</span> <span class="n">num_sample</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"lightgrey"</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_tau_series</span><span class="p">[:</span><span class="n">num_sample</span><span class="p">],</span> <span class="n">x_series</span><span class="p">[:</span> <span class="n">num_sample</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">ts</span><span class="p">[:</span><span class="n">num_sample</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s">"viridis"</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"$P(t-</span><span class="se">\\</span><span class="s">tau)$"</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"$P(t)$"</span><span class="p">)</span>
  <span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">colorbar</span><span class="p">()</span>
  <span class="n">cbar</span><span class="p">.</span><span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'$t$'</span><span class="p">)</span>

  <span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dt</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">mg_data</span> <span class="o">=</span> <span class="n">bd</span><span class="p">.</span><span class="n">chaos</span><span class="p">.</span><span class="n">MackeyGlassEq</span><span class="p">(</span><span class="mi">25000</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="n">dt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">17</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ts</span> <span class="o">=</span> <span class="n">mg_data</span><span class="p">.</span><span class="n">ts</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">mg_data</span><span class="p">.</span><span class="n">xs</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">mg_data</span><span class="p">.</span><span class="n">ys</span>

<span class="n">plot_mackey_glass_series</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">num_sample</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mi">1000</span> <span class="o">/</span> <span class="n">dt</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828151451523.png" alt="image-20230828151451523" /></p>

<h3 id="prediction-of-mackey-glass-timeseries">Prediction of Mackey-Glass timeseries</h3>

<h4 id="prepare-the-data">Prepare the data</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_data</span><span class="p">(</span><span class="n">t_warm</span><span class="p">,</span> <span class="n">t_forcast</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="n">sample_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">warmup</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">t_warm</span> <span class="o">/</span> <span class="n">dt</span><span class="p">)</span>  <span class="c1"># warmup the reservoir
</span>    <span class="n">forecast</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">t_forcast</span> <span class="o">/</span> <span class="n">dt</span><span class="p">)</span>  <span class="c1"># predict 10 ms ahead
</span>    <span class="n">train_length</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">t_train</span> <span class="o">/</span> <span class="n">dt</span><span class="p">)</span>

    <span class="n">X_warm</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[:</span><span class="n">warmup</span><span class="p">:</span><span class="n">sample_rate</span><span class="p">]</span>
    <span class="n">X_warm</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">X_warm</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">X_train</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[</span><span class="n">warmup</span><span class="p">:</span> <span class="n">warmup</span><span class="o">+</span><span class="n">train_length</span><span class="p">:</span> <span class="n">sample_rate</span><span class="p">]</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">Y_train</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[</span><span class="n">warmup</span><span class="o">+</span><span class="n">forecast</span><span class="p">:</span> <span class="n">warmup</span><span class="o">+</span><span class="n">train_length</span><span class="o">+</span><span class="n">forecast</span><span class="p">:</span> <span class="n">sample_rate</span><span class="p">]</span>
    <span class="n">Y_train</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">Y_train</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">X_test</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[</span><span class="n">warmup</span> <span class="o">+</span> <span class="n">train_length</span><span class="p">:</span> <span class="o">-</span><span class="n">forecast</span><span class="p">:</span> <span class="n">sample_rate</span><span class="p">]</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">Y_test</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[</span><span class="n">warmup</span> <span class="o">+</span> <span class="n">train_length</span> <span class="o">+</span> <span class="n">forecast</span><span class="p">::</span><span class="n">sample_rate</span><span class="p">]</span>
    <span class="n">Y_test</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">X_warm</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># First warmup the reservoir using the first 100 ms
# Then, train the network in 20000 ms to predict 1 ms chaotic series ahead
</span><span class="n">x_warm</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">20000</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sample</span> <span class="o">=</span> <span class="mi">3000</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="n">sample</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">"Training data"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="n">sample</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">"True prediction"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828151606545.png" alt="image-20230828151606545" /></p>

<h4 id="prepare-the-esn">Prepare the ESN</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ESN</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">DynamicalSystemNS</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_in</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">,</span> <span class="n">num_out</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">leaky_rate</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
               <span class="n">Win_initializer</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">Uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ESN</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">r</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Reservoir</span><span class="p">(</span>
        <span class="n">num_in</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">,</span>
        <span class="n">Win_initializer</span><span class="o">=</span><span class="n">Win_initializer</span><span class="p">,</span>
        <span class="n">spectral_radius</span><span class="o">=</span><span class="n">sr</span><span class="p">,</span>
        <span class="n">leaky_rate</span><span class="o">=</span><span class="n">leaky_rate</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">o</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">num_hidden</span><span class="p">,</span> <span class="n">num_out</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">bm</span><span class="p">.</span><span class="n">training_mode</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">&gt;&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">r</span> <span class="o">&gt;&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">o</span>
</code></pre></div></div>

<h4 id="train-and-test">Train and test</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">ESN</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">reset_state</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">RidgeTrainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># warmup
</span><span class="n">_</span> <span class="o">=</span> <span class="n">trainer</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_warm</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># train
</span><span class="n">_</span> <span class="o">=</span> <span class="n">trainer</span><span class="p">.</span><span class="n">fit</span><span class="p">([</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">])</span>
</code></pre></div></div>

<h4 id="test-the-training-data">Test the training data</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ys_predict</span> <span class="o">=</span> <span class="n">trainer</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">6000</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">as_numpy</span><span class="p">(</span><span class="n">ys_predict</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
         <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"ESN prediction"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">as_numpy</span><span class="p">(</span><span class="n">y_train</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">"--"</span><span class="p">,</span>
         <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"True value"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">'Mean Square Error: </span><span class="si">{</span><span class="n">bp</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">ys_predict</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828151747954.png" alt="image-20230828151747954" /></p>

<h4 id="test-the-testing-data">Test the testing data</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ys_predict</span> <span class="o">=</span> <span class="n">trainer</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>

<span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">6000</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">as_numpy</span><span class="p">(</span><span class="n">ys_predict</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"ESN prediction"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">as_numpy</span><span class="p">(</span><span class="n">y_test</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">"--"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"True value"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">'Mean Square Error: </span><span class="si">{</span><span class="n">bp</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">ys_predict</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828151824907.png" alt="image-20230828151824907" /></p>

<h3 id="jit-connection-operators">JIT connection operators</h3>

<ul>
  <li>Just-in-time randomly generated matrix.</li>
  <li>Support for Mat@Vec and Mat@Mat.</li>
  <li>Support different random generation methods.(homogenous, uniform, normal)</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">math</span><span class="p">,</span> <span class="n">random</span>

<span class="k">def</span> <span class="nf">jitconn_prob_homo</span><span class="p">(</span><span class="n">events</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">outs</span><span class="p">):</span>
    <span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">max_cdist</span><span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="n">ceil</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">prob</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span>  <span class="n">events</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">event</span><span class="p">:</span>
            <span class="n">post_i</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_cdist</span><span class="p">)</span>
            <span class="n">outs</span><span class="p">[</span><span class="n">post_i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">weight</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828153353131.png" alt="image-20230828153353131" /></p>

<h2 id="applications">Applications</h2>

<h3 id="from-the-perspective-of-kernel-methods">From the perspective of kernel methods</h3>

<p>维度扩张思想</p>

<p>Non-linear SVMs: Kernel Mapping</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828153621843.png" alt="image-20230828153621843" /></p>

<p>Kernel methods in neural system? <strong>与维度扩张的思想相似</strong></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828153801285.png" alt="image-20230828153801285" /></p>

<h3 id="subcortical-pathway-for-rapid-motion-processing">Subcortical pathway for rapid motion processing</h3>

<p>The first two stages of subcortical visual pathway:
Retina -&gt; superior colliculus</p>

<p>The first two stages of primary auditory pathway:
Inner Ear -&gt; Cochlear Nuclei</p>

<p>维度扩张在subcortical pathway中体现，reservoir 能够高维处理的更简单</p>

<h3 id="spatial-temporal-tasks">Spatial-temporal tasks</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828154155803.png" alt="image-20230828154155803" /></p>

<p>既有时间信息，又有空间信息的dataset，使用reservoir来处理高维信息，坐位 Dimension expansion</p>

<h3 id="gait-recognition">Gait recognition</h3>

<p>input来了再做计算</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828154352087.png" alt="image-20230828154352087" /></p>

<h3 id="spatial-temporal-tasks-1">Spatial-temporal tasks</h3>

<p>large-scale，随size增大，accuracy增大</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828154428762.png" alt="image-20230828154428762" /></p>

<h3 id="liquid-state-machine">Liquid state machine</h3>

<p>A liquid state machine (LSM) is a type of reservoir computer that uses a spiking neural network.</p>

<p>与ESN一样的范式，都是去做dimension expansion</p>

<p>很难去分析怎么work的</p>

  
</div>


          </div>
          


          

        </div>
        
          

  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="https://github.com/brainpy.png"
               alt="Sichao He" />
          <p class="site-author-name" itemprop="name">Sichao He</p>
           
              <p class="site-description motion-element" itemprop="description">Notes from the first training course on Neural Computational Modeling</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          

        </nav>

        
        
        
          <div class="feed-link motion-element">
            <a href="/BrainPy-course-notes/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>

        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Sichao He</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://jekyllrb.com">Jekyll</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/simpleyyt/jekyll-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>





















  
   
  
  
  
  
  
  <script type="text/javascript" src="/BrainPy-course-notes/assets/lib/jquery/index.js?v=2.1.3"></script>

  
  
  
  
  
  <script type="text/javascript" src="/BrainPy-course-notes/assets/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  
  
  
  
  <script type="text/javascript" src="/BrainPy-course-notes/assets/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  
  
  
  
  <script type="text/javascript" src="/BrainPy-course-notes/assets/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  
  
  
  
  <script type="text/javascript" src="/BrainPy-course-notes/assets/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  
  
  
  
  <script type="text/javascript" src="/BrainPy-course-notes/assets/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/BrainPy-course-notes/assets/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/BrainPy-course-notes/assets/js/src/motion.js?v=5.1.1"></script>



  
  

  <script type="text/javascript" src="/BrainPy-course-notes/assets/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/BrainPy-course-notes/assets/js/src/post-details.js?v=5.1.1"></script>


  


  <script type="text/javascript" src="/BrainPy-course-notes/assets/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  











  




  

    

  







  






  

  

  
  


  

  

  

</body>
</html>



<!doctype html>














<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/BrainPy-course-notes/assets/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/BrainPy-course-notes/assets/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/BrainPy-course-notes/assets/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Jekyll, NexT" />





  <link rel="alternate" href="/BrainPy-course-notes/atom.xml" title="BrainPy course notes" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/BrainPy-course-notes/assets/favicon.ico?v=5.1.1" />
















<meta name="description" content="[TOC] # ç¥žç»è®¡ç®—å»ºæ¨¡ç®€ä»‹ ## è®¡ç®—ç¥žç»ç§‘å­¦çš„èƒŒæ™¯ä¸Žä½¿å‘½ è®¡ç®—ç¥žç»ç§‘å­¦æ˜¯**è„‘ç§‘å­¦**å¯¹**ç±»è„‘æ™ºèƒ½**çš„**æ¡¥æ¢** ### ä¸¤å¤§ç›®æ ‡ - ç”¨è®¡ç®—å»ºæ¨¡çš„æ–¹æ³•æ¥é˜æ˜Žå¤§è„‘åŠŸèƒ½çš„è®¡ç®—åŽŸç† - å‘å±•ç±»è„‘æ™ºèƒ½çš„æ¨¡åž‹å’Œç®—æ³• ### Prehistory - 1907 LIF model ç¥žç»è®¡ç®—çš„æœ¬è´¨ - 1950s HH model ç”µä½å®šé‡åŒ–æ¨¡åž‹ æœ€fundamentalçš„ - 1960s Roll&apos;s cable equation æè¿°ä¿¡å·åœ¨è½´çªå’Œæ ‘çªæ€Žä¹ˆä¼ é€’ - 1970s Amari, Wilson, Cowan et al. çŽ°ä»Šå»ºæ¨¡çš„åŸºç¡€ - 1982 Hopfield model(Amari-Hopfield model) å¼•å…¥ç‰©ç†å­¦æŠ€æœ¯ï¼Œå¸å¼•å­æ¨¡åž‹ - 1988 Sejnowski et al. &quot;Computational Neuroscience&quot;(science) æå‡ºè®¡ç®—ç¥žç»ç§‘å­¦æ¦‚å¿µ **çŽ°åœ¨çš„è®¡ç®—ç¥žç»ç§‘å­¦å¯¹åº”äºŽç‰©ç†å­¦çš„ç¬¬è°·-ä¼½åˆ©ç•¥æ—¶ä»£ï¼Œå¯¹å¤§è„‘å·¥ä½œåŽŸç†è¿˜ç¼ºä¹æ¸…æ™°çš„ç†è®º** ### Three levels of Brain Science ![image-20230823105226568](/BrainPy-course-notes/master_content/Notes.assets/image-20230823105226568.png) - å¤§è„‘åšä»€ä¹ˆ Computational theory -&amp;gt; Psychology &amp; Cognitive Science -&amp;gt; Human-like Cognitive function - å¤§è„‘æ€Žä¹ˆåš Representation &amp; Algorithm -&amp;gt; Computational Neuroscience -&amp;gt; Brain-inspired model &amp; algorithm - å¤§è„‘æ€Žä¹ˆå®žçŽ° Implementation -&amp;gt; Neuroscience -&amp;gt; Neuromorphic computing ### Mission of Computational Neuroscience &amp;gt; What I can not build a computational model, I do not understand ## ç¥žç»è®¡ç®—å»ºæ¨¡çš„ç›®æ ‡ä¸ŽæŒ‘æˆ˜ ### Limitation of Deep Learning - ä¸æ“…é•¿å¯¹æŠ—æ ·æœ¬ - å¯¹å›¾åƒçš„ç†è§£æœ‰é™ ![image-20230823105836259](/BrainPy-course-notes/master_content/Notes.assets/image-20230823105836259.png) ### Brain is for Processing Dynamical Information **We never &quot;see&quot; a static image** ![image-20230823105918336](/BrainPy-course-notes/master_content/Notes.assets/image-20230823105918336.png) ### The missing link a computational model of higher cognitive functior ![image-20230823110617639](/BrainPy-course-notes/master_content/Notes.assets/image-20230823110617639.png) çŽ°åœ¨åªæ˜¯åšçš„**å±€éƒ¨**çš„ç½‘ç»œï¼Œæ²¡æœ‰ä¸€ä¸ªæˆåŠŸçš„æ¨¡åž‹ï¼Œèƒ½**ä»Žç¥žç»å…ƒå‡ºå‘æž„å»ºç½‘ç»œï¼Œåˆ°ç³»ç»Ÿå±‚é¢ä¸Š** **åŽŸå› **: å› ä¸ºç¥žç»ç§‘å­¦åº•å±‚æ•°æ®çš„ç¼ºå¤±ï¼Œå¯ä»¥è€ƒè™‘æ•°æ®é©±åŠ¨ã€å¤§æ•°æ®çš„æ–¹å¼æ¥åŠ å¿«å‘å±• ## ç¥žç»è®¡ç®—å»ºæ¨¡çš„å·¥å…· &amp;gt; å·¥æ¬²è¡Œå…¶äº‹ï¼Œå¿…å…ˆåˆ©å…¶å™¨ &amp;gt; We need &quot;PyTorch/TensorFlow&quot; in Computational Neuroscience! ### Challenges in neural modelling æœ‰ä¸åŒçš„å°ºåº¦ - Mutiple-scale - Large-scale - Multiple purposes ![image-20230823111212460](/BrainPy-course-notes/master_content/Notes.assets/image-20230823111212460.png) &amp;gt; The modeling targets and methods are extremely complex, and we need a general framework. ### Limitations of Existing Brain Simulators çŽ°ä»Šçš„æ¡†æž¶ä¸èƒ½æ»¡è¶³ä»¥ä¸Š ![image-20230823111509523](/BrainPy-course-notes/master_content/Notes.assets/image-20230823111509523.png) ### What are needed for a brain simulator 1. Efficiency High-speed simulation on parallel computing devices, etc. 2. Integration Integrated modeling of simulation, training, and analysis 3. Flexibility New models at all scales can be accommodated 4. Extensibility Extensible to new modeling methods(machine learning) éœ€è¦æ–°çš„èŒƒå¼ ### Our solution: BrainPy 4 levels ![image-20230823111903456](/BrainPy-course-notes/master_content/Notes.assets/image-20230823111903456.png) ## ç¥žç»è®¡ç®—å»ºæ¨¡ä¸¾ä¾‹ ### Image understanding: an ill-posed problem Image Understanding = image segmentation + image object recognition &amp;gt; Chicken vs. Egg dilemma &amp;gt; &amp;gt; - Without segmentation, how to recognize &amp;gt; - Without recognition, how to segment **The solution of brain:** Analysis-by-synthesis çŒœæµ‹ä¸ŽéªŒè¯æ–¹æ³• ### Reverse Hierarchy Theory äººçš„æ„ŸçŸ¥æ˜¯æ•´ä½“åˆ°å±€éƒ¨ ### Two pathways for visual information processing ![image-20230823114517888](/BrainPy-course-notes/master_content/Notes.assets/image-20230823114517888.png) ### Key Computational Issues for Global-to-local Neural Information Processing - What are global and local features - How to rapidly extract global features - How to generate global hypotheses - How to implement from global to local processing - The interplay between global and local features - Others #### How to extract global features **Global first = Topology first**(å¤§èŒƒå›´é¦–å…ˆï¼Œé™ˆéœ–) è§†è§‰ç³»ç»Ÿæ›´æ•æ„ŸäºŽæ‹“æ‰‘æ€§è´¨çš„å·®å¼‚ &amp;gt; DNNs has difficulty to recognize topology **A retina-SC network for topology detection** è§†ç½‘è†œåˆ°ä¸Šä¸˜çš„æ£€æµ‹ï¼ŒGap junction coupling ... ### A Model for Motion Pattern Recognition Reservoir Module Decision-making Module ### How to generate &quot;global&quot; hypotheses in the representation space Attractor neural network ![image-20230823115853980](/BrainPy-course-notes/master_content/Notes.assets/image-20230823115853980.png) Levy Flight in Animal Behaviors ![image-20230823120000911](/BrainPy-course-notes/master_content/Notes.assets/image-20230823120000911.png) ### How to process information from global to local Push-pull Feedback A hierarchical Hopfield Model ### Interplay between global and local features A two-pathway model for object recognition ![image-20230823120750349](/BrainPy-course-notes/master_content/Notes.assets/image-20230823120750349.png) Modeling visual masking å¯ä»¥ç”¨two-pathwayå¾ˆå¥½è§£é‡Š # Programming basics ## Python Basics ### Values - Boolean - String - Integer - Float - ... ### Keywords Not allowed to use keywords, they define structure and rules of a language. ```python help(&quot;keywords&quot;) ``` ### Operators æ•°æ®ä¹‹é—´çš„æ“ä½œ #### For Integers and Floats ```python a=5 b=3 # addition + print(&quot;a+b=&quot;,atb) # subtraction - print(&quot;a-b=&quot;,a-b) # multiplication * print(&quot;axb=&quot;a*b) # division / print(&quot;a/b=&quot;,a/b) # power ** print(&quot;a**b=&quot;,a**b) ``` #### Booleans ```python #Boolean experssions # equals: == print(&quot;5==5&quot;,5==5) # do not equal: != print(&quot;5!-5&quot;,5!=5) # greater than: &amp;gt; print(&quot;5&amp;gt;5&quot;,5&amp;gt;5) # greater than or equal: &amp;gt;= print(&quot;5&amp;gt;=5â€5&amp;gt;=5) ``` ```python # logica operators print(&quot;True and False:&quot;, True and False) print(&quot;True or False:&quot;, True or False) print(&quot;not False:&quot;, not False) ``` ### Modules Not all functionality available comes automatically when starting python. ```python import match import numpy as np print(math.pi) print(np.pi) from numpy import pi print(pi) from numpy import * print(pi) ``` ### Control statements #### If ```python a = 5 # In Python, blocks of code are defined using indentation. if a == 5: print(&quot;ok&quot;) ``` &amp;gt; ok #### For ```python # range(5) means a list with integers, 0, 1, 2, 3, 4 for i in range(5): print(i) ``` &amp;gt; 0 &amp;gt; 1 &amp;gt; 2 &amp;gt; 3 &amp;gt; 4 #### While ```python i = 1 while i 1 &amp;gt; 8 &amp;gt; 1000 ### Functions - Functions are used to abstract components of a program. - Much like a mathematical function, they take some input and then find the result. start a function definition with a keyword def - Then comes the function name, with arguments in braces, and then a colon. ```python def func(args1, args2): pass ``` ### Data types #### List - Group variables together - Specific order - Access item with brankets: [ ] - List can be sliced - List can be multiplied - List can be added - Lists are mutable - Copying a list ```python myList = [0, 1, 2, 0,&quot;name&quot;] print(&quot;myList[0]:&quot;, myList[0]) print(&quot;myList[1]:&quot;, myList[1]) print(&quot;myList[3]:&quot;, myList[3]) print(&quot;myList[-1]:&quot;, myList[-1]) print(&quot;myList[-2]:&quot;, myList[-2]) ``` &amp;gt; myList[0]: 0 &amp;gt; myList[1]: 1 &amp;gt; myList[3]: name &amp;gt; myList[-1]: name &amp;gt; myList[-2]: 2.0 ```python myList = [0, 1.0, &quot;hello&quot;] print(&quot;myList[0:2]:&quot;, mylist[0:2]) print(&quot;myList*2:&quot;, myList*2) myList2 = [2,&quot;yes&quot;] print(&quot;myList+myList2:&quot;, myList+myList2) ``` &amp;gt; myList[0:2]: [0ï¼Œ1.0] &amp;gt; myList*2: [0ï¼Œ1.0ï¼Œ hello&apos;ï¼Œ0ï¼Œ1.0ï¼Œ hello&apos;] &amp;gt; myList+myList2: [0ï¼Œ1.0ï¼Œ&apos;hello&apos;ï¼Œ2ï¼Œyes&apos;] #### tuple Tuples are immutable. #### dictionary A dictionary is a collection of key-value pairs ```python d = {} d[1] = 2 d[&quot;a&quot;] = 3 print(&quot;d: &quot;, d) c = {1:2, &quot;a&quot;:3} print(&quot;c: &quot;, c) print(&quot;c[1]: &quot;, c[1]) ``` &amp;gt; d: {1: 2, &apos;a&apos;: 3} &amp;gt; c: {1: 2, &apos;a&apos;: 3} &amp;gt; c[1]: 2 ### Class In Python, everything is an object. Classes are objects, instances of classes are objects, modules are objects, and functions are objects. 1. a **type** 2. an internal **data representation** (primitive or composite) 3. a set of procedures for **interaction** with the object **a simple example** ```python # define class class Linear(): pass # instantiate object layer1 = Linear() print(layer1) ``` &amp;gt; `` #### Initializing an object ```python # define class class Linear(): # It refers to the object (instance) itself def __init__(self, n_input): self.n_input = n_input layer1 = Linear(100) layer2 = Linear(1000) print(&quot;layer1 : &quot;, layer1.n_input) print(&quot;layer2 : &quot;, layer2.n_input) ``` &amp;gt; layer1 : 100 &amp;gt; layer2 : 1000 #### Class has methods (similar to functions) ```python # define class class Linear(): ### It refers to the the object (instance) itself def __init__(self, n_input, n_output): self.n_input = n_input self.n_output = n_output def compute n params(self): num_params = self.n_input * self.n_output return num_params layerl = Linear(10,100) print(layerl.compute_n_params()) ``` &amp;gt; 1000 ## NumPy Basic ### Numpy Introduction - Fundamental package for scientific computing with Python - N-dimensional array object - Linear algebra, frontier transform, random number capacities - Building block for other packages (e.g. Scipy) ### Array - Arrays are mutable - Arrays attributes - ... ```python A = np.zeros((2, 2)) print(A) ``` &amp;gt; [[0. 0.] &amp;gt; [0. 0.]] ```python a.ndim # 2 dimension a.shape # (2, 5) shape of array a.size # 10 $ of elements a.T # transpose a.dtype # data type ``` #### Array broadcasting When operating on two arrays, numpy compares shapes. Two dimensions are compatible when 1. They are of equal size 2. One of them is 1 ![image-20230823143622229](/BrainPy-course-notes/master_content/Notes.assets/image-20230823143622229.png) ### Vector operations - Inner product - Outer product - Dot product (matrix multiplication) ```python u = [1, 2, 3] v = [1, 1, 1] np.inner(u, v) np.outer(u, v) np.dot(u, v) ``` &amp;gt; 6 &amp;gt; array([[1, 1, 1], &amp;gt; [2, 2, 2], &amp;gt; [3, 3, 3]]) &amp;gt; 6 ### Matrix operations - `np.ones` - `.T` - `np.dot` - `np.eye` - `np.trace` - `np.row_stack` - `np.column_stack` ### Operations along axes ```python a = np.ones((2, 3)) print(a) a.sum() a.sum(axis=0) a.cumsum() a.cumsum(axis=0) ``` ### Slicing arrays ```python a = np.random.random((2, 3)) print(a) a[0,:] # first row, all columns a[0:2] # first and second rows, al columns a[:,1:3]# all rows, second and third columns ``` ### Reshape ```python a = np.ones((10,1)) a.reshape(2,5) ``` ### Linear algebra ```python qr # Computes the QR decomposition cholesky # Computes the Cholesky decomposition inv(A) # Inverse solve(A,b) # Solves Ax = b for A full rank lstsq(A,b) # Solves arg minx //Ax - b//2 eig(A) # Eigenvalue decomposition eigvals(A) # Computes eigenvalues svd(Aï¼Œfull) # Sinqular value decomposition pinv(A) # Computes pseudo-inverse of A ``` ### Fourier transform ```python import numpy.fft fft # 1-dimensional DFT fft2 # 2-dimensional DFT fftn # N-dimensional DFT ifft # 1-dimensional inverse DFT (etc.) rfft # Real DFT (1-dim) ``` ### Random sampling ```python import numpy.random rand(d0, d1, ..., dn) # Random values in a given shape randn(d0, d1, ..., dn) # Random standard normal randint(lo, hi, size) # Random integers [lo hi) choice(a, size, repl, p) # Sample from a shuffle(a) # Permutation (in-place) permutation(a) # Permutation (new array) ``` ### Distributions in random ```python import numpy.random beta binomial chisquare exponential dirichlet gamma laplace lognormal ... ``` ### Scipy - `SciPy` is a library of algorithms and mathematical tools built to work with `NumPy ` arrays. - `scipy.linalg linear algebra` - `scipy.stats statistics` - `scipy.optimize optimization` - `scipy.sparse sparse matrices` - `scipy.signal signal processing` - etc. ## BrainPy introduction ### Modeling demands - Large-scale - Multi-scale - Methods ### BrainPy Architecture - Infrastructure - Functions - Just-in-time compilation - Devices ![image-20230823145349681](/BrainPy-course-notes/master_content/Notes.assets/image-20230823145349681.png) ### Main features #### Dense operators - Compatible with `NumPy`, `TensorFlow`, `PyTorch` and other dense matrix operator syntax. - Users do not need to learn and get started programming directly. #### Dedicated operatorsq - Applies brain dynamics sparse connectivity properties with event-driven computational features. - Reduce the complexity of brain dynamics simulations by several orders of magnitude. #### Numerical Integrators - Ordinary differential equations: brainpy.odeint - Stochastic differential equations: brainpy.sdeint - Fractional differential equations: brainpy.fdeint - Delayed differential equations #### Modular and composable ä»Žå¾®è§‚åˆ°å®è§‚ **brainpy.DynamicalSystem** ![image-20230823151159786](/BrainPy-course-notes/master_content/Notes.assets/image-20230823151159786.png) #### JIT of object-oriented BrainPy provides object-oriented transformations: - `brainpy.math.jit` - `brainpy.math.grad` - `brainpy.math.for_loop` - `brainpy.math.ifelse` ## BrainPy Programming Basics ### Just-in-Time compilation Just In Time Compilation (JIT, or Dynamic Translation), is compilation that is being done during the execution of a program. JIT compilation attempts to use **the benefits of both**. While the interpreted program is being run, the JIT compiler determines the most frequently used code and compiles it to machine code. The advantages of a JIT are due to the fact that since the compilation takes place in run time, a JIT compiler has access to dynamic runtime information enabling it to make better optimizations (such as inlining functions). ```python def gelu(x): sqrt = bm.sqrt(2 / bm.pi) cdf = 0.5 * (1.0 + bm.tanh(sqrt * (x + 0.044715 * (x ** 3)))) y = x *cdf return y &amp;gt;&amp;gt;&amp;gt; gelu_jit = bm.jit(gelu) # ä½¿ç”¨JIT ``` ### Object-oriented JIT compilation - The class object must be inherited from brainpy.BrainPyObject, the base class of BrainPy, whose methods will be automatically JIT compiled. - All time-dependent variables must be defined as brainpy.math.Variable. ```python class LogisticRegression(bp.BrainPyObject): def __init__(self, dimension): super(LogisticRegression, self).__init__() # parameters self.dimension = dimension # variables self.w = bm.Variable(2.0 * bm.ones(dimension) - 1.3) def __call__(self, X, Y): u = bm.dot(((1.0 / (1.0 + bm.exp(-Y * bm.dot(X, self.w))) - 1.0) * Y), X) self.w.value = self.w - u # in-place update ``` **ExampleL Run a neuron model** ```python model = bp.neurons.HH(1000) #ä¸€å…±1000ä¸ªç¥žç»å…ƒ runner = bp.DSRunner(target=model, inputs=(&apos;input&apos;, 10.)) # jité»˜è®¤ä¸ºTrue runner(duration=1000, eval_time=True) #æ¨¡æ‹Ÿ 1000ms ``` ç¦ç”¨JITæ¥debug ### Data operations #### Array ç­‰ä»·äºŽ`numpy`çš„`array` #### BrainPy arrays &amp; JAX arrays ```python t1 = bm.arange(3) print(t1) print(t1.value) ``` &amp;gt; JaxArray([0, 1, 2], dtype=int32) &amp;gt; DeviceArray([0, 1, 2], dtype=int32) #### Variables Arrays that are not marked as dynamic variables will be JIT-compiled as static arrays, and modifications to static arrays will not be valid in the JIT compilation environment. ```python t = bm.arange(4) v = bm.Variable(t) print(v) print(v.value) ``` &amp;gt; Variable([0, 1, 2, 3], dtype=int32) &amp;gt; DeviceArray([0, 1, 2, 3], dtype=int32) ### Variables **In-place updating** å°±åœ°æ›´æ–° #### Indexing and slicing - Indexing: `v[i] = a` or `v[(1, 3)] = c` - Slicing: `v[i:j] = b` - Slicing all values `v[:] = d`, `v[...] = e` #### Augmented assignment - add - subtract - divide - multiply - floor divide - modulo - power - and - or - xor - left shift - right shift #### Value assignment ```python v.value = bm.arange(10) check_no_change(v) ``` #### Update assignment ```python v.update(bm.random.randint(0, 20, size=10)) ``` ### Control flows #### If-else `brainpy.math.where` ```python a = 1. bm.where(a DeviceArray(1., dtype=float32, weak_type=True) `brainpy.math.ifelse` ```python def ifelse(condition, branches, operands): true_fun, false_fun = branches if condition: return true_fun(operands) else: return false_fun(operands) ``` #### For loop ```python import brainpy.math hist_of_out_vars = brainpy.math.for_loop(body_fun, operands) ``` #### While loop ```python i = bm.Variable(bm.zeros(1)) counter = bm.Variable(bm.zeros(1)) def cond_f(): return i[0] $$ (2\pi a\Delta x)c_{\mathrm{M}}\frac{\partial V(x,t)}{\partial t}+(2\pi a\Delta x)i_{\mathrm{ion}}=\frac{\pi a^{2}}{\rho_{\mathrm{L}}}\frac{\partial V(x+\Delta x,t)}{\partial x}-\frac{\pi a^{2}}{\rho_{\mathrm{L}}}\frac{\partial V(x,t)}{\partial x} $$ **Cable Equation** $$ c_\mathrm{M}\frac{\partial V(x,t)}{\partial t}=\frac{a}{2\rho_\mathrm{L}}\frac{\partial^2V(x,t)}{\partial x^2}-i_\mathrm{ion} $$ ç”µæµåœ¨é€šè¿‡é•¿ç›´å¯¼ä½“æ—¶ä¼šæ³„éœ²ç”µæµï¼Œå¦‚ä½•è®°å½•è†œç”µä½ï¼Œå¯ä»¥ä½¿ç”¨æ­¤æ–¹ç¨‹æ¥æè¿° **Passive conduction:** ion currents are caused by leaky channels exclusively $$ i_{\mathrm{ion}}=V(x,t)/r_{\mathrm{M}} $$ -&amp;gt; $$ \begin{aligned}c_\mathrm{M}\frac{\partial V(x,t)}{\partial t}&amp;=\frac{a}{2\rho_\mathrm{L}}\frac{\partial^2V(x,t)}{\partial x^2}-\frac{V(x,t)}{r_\mathrm{M}}\\\\\tau\frac{\partial V(x,t)}{\partial t}&amp;=\lambda^2\frac{\partial^2V(x,t)}{\partial x^2}-V(x,t)\quad\lambda=\sqrt{0.5ar_\mathrm{M}/\rho_\mathrm{L}}\end{aligned} $$ æ²¡æœ‰åŠ¨ä½œç”µä½ï¼Œå•çº¯é€šè¿‡ç”µç¼†ä¼ è¾“ ![image-20230824102932665](/BrainPy-course-notes/master_content/Notes.assets/image-20230824102932665.png) If a constant external current is applied to ð‘¥ = 0 the steady-state membrane potential $ð‘‰_{ss}(ð‘¥)$ is $$ \lambda^2\frac{\mathrm{d}^2V_{\mathrm{ss}}(x)}{\mathrm{d}x^2}-V_{\mathrm{ss}}(x)=0\longrightarrow V_{\mathrm{ss}}(x)=\frac{\lambda\rho_{\mathrm{L}}}{\pi a^2}I_0e^{-x/\lambda} $$ ç”µä¿¡å·æ— è¡°å‡ä¼ æ’­: åŠ¨ä½œç”µä½ ## Action potential &amp; active transport Steps of an action potential: - Depolarization - Repolarization - Hyperpolarization - Resting Characteristics: - All-or-none - Fixed shape - Active electrical property ![image-20230824103322522](/BrainPy-course-notes/master_content/Notes.assets/image-20230824103322522.png) How to simulate an action potential? $$ \begin{aligned} \frac{I(t)}{A}&amp; =c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+i_{\mathrm{ion}} \\ \Rightarrow\quad c_{\mathrm{M}}\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}&amp; =-g_{\mathrm{Cl}}(V_{\mathrm{M}}-E_{\mathrm{Cl}})-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}})-g_{\mathrm{Na}}(V_{\mathrm{M}}-E_{\mathrm{Na}})+\frac{I(t)}{A} \end{aligned} $$ ç¦»å­é€šé“çš„å¼€é—­ä¼šéšç€ç”µåŽ‹è€Œå˜åŒ–ï¼Œç”µå¯¼ä¹Ÿéšç€ç”µåŽ‹è€Œå˜åŒ– Mechanism: voltage-gated ion channels **HHå»ºæ¨¡æ€è·¯ï¼šé€šè¿‡ç”µå¯¼** ### Nodes of Ranvier Saltatory conduction with a much higher speed and less energy consumption ä¸¤ä¸ªéƒŽé£žç»“ä¹‹é—´ä¼šæœ‰ç¦»å­é€šé“ï¼Œæ—¢æœ‰è¢«åŠ¨ä¼ å¯¼ï¼Œä¹Ÿæœ‰ä¸»åŠ¨çš„é˜²æ­¢è¡°å‡ ![image-20230824104220106](/BrainPy-course-notes/master_content/Notes.assets/image-20230824104220106.png) ## The Hodgkin-Huxley Model ### Modeling of each ion channel Modeling of each ion channel: $$ g_m=\bar{g}_mm^x $$ Modeling of each ion gate: $$ \mathcal{C}\underset{}{\operatorname*{\overset{\alpha(\mathrm{V})}{\underset{\beta(\mathrm{V})}{\operatorname*{\longrightarrow}}}}\mathcal{O}} \\ \Rightarrow \begin{aligned} \frac{\mathrm{d}m}{\mathrm{d}t}&amp; =\alpha(V)(1-m)-\beta(V)m \\ &amp;=\frac{m_{\infty}(V)-m}{\tau_{m}(V)} \end{aligned} \\ \\ \begin{aligned}m_\infty(V)&amp;=\frac{\alpha(V)}{\alpha(V)+\beta(V)}.\\\tau_m(V)&amp;=\frac{1}{\alpha(V)+\beta(V)}\end{aligned} $$ $$ \text{If}\ V\text{ is constant:}m(t)=m_\infty(V)+(m_0-m_\infty(V))\mathrm{e}^{-t/\tau_m(V)} $$ ### Voltage clamp $$ \begin{aligned} \frac{I(t)}{A}&amp; =c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+i_{\mathrm{ion}} \\ \Rightarrow\quad c_{\mathrm{M}}\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}&amp; =-g_{\mathrm{Cl}}(V_{\mathrm{M}}-E_{\mathrm{Cl}})-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}})-g_{\mathrm{Na}}(V_{\mathrm{M}}-E_{\mathrm{Na}})+\frac{I(t)}{A} \end{aligned} $$ - The membrane potential is kept constant - The current from capacitors is excluded - Currents must come from leaky/voltage-gated ion channels $$ \begin{aligned}I_{\mathrm{cap}}&amp;=c\frac{dV}{dt}=0\\I_{\mathrm{fb}}&amp;=\quad i_{\mathrm{ion}}=g_{\mathrm{Na}}(V-E_{\mathrm{Na}})+g_{\mathrm{K}}(V-E_{\mathrm{K}})+g_{\mathrm{L}}(V-E_{\mathrm{L}})\end{aligned} $$ åªæµ‹é‡ä¸€ä¸ªç¦»å­é€šé“å°±å¯ä»¥å¾ˆå®¹æ˜“å¾—åˆ°ç”µå¯¼ ![image-20230824111620056](/BrainPy-course-notes/master_content/Notes.assets/image-20230824111620056.png) ### Leaky channel Hyperpolarization â†’ the sodium and potassium channels are closed $$ I_{\mathrm{fb}}=g_{\mathrm{Na}}(V-E_{\mathrm{Na}})+g_{\mathrm{K}}(V-E_{\mathrm{K}})+g_{\mathrm{L}}(V-E_{\mathrm{L}}) $$ $$ \Rightarrow I_{\mathrm{fb}}=g_L(V-E_L) $$ $$ g_\mathrm{L}=0.3\mathrm{mS/cm}^2,E_\mathrm{L}=-54.4\mathrm{mV} $$ #### Potassium and sodium channels Potassium channels: Use choline to eliminate the inward current of Na + Na + current: $I_{fb} - I_{K}$ ![image-20230824112328953](/BrainPy-course-notes/master_content/Notes.assets/image-20230824112328953.png) ![image-20230824112333144](/BrainPy-course-notes/master_content/Notes.assets/image-20230824112333144.png) è½¬åŒ–é€ŸçŽ‡å’Œç”µå¯¼çŽ‡ä¸¤ä¸ªå› ç´  Potassium channels - Resting state (gate closed) - Activated state (gate open) â†’ Activation gate: $g_{\mathrm{K}}=\bar{g}_{K}n^{x}$ Sodium channels - Resting state (gate closed) - Activated state (gate open) - Inactivated state (gate blocked) â†’ Activation gate + inactivation gate: $g_{\mathrm{Na}}=\bar{g}_\text{Na}m^3h$ ![image-20230824113116329](/BrainPy-course-notes/master_content/Notes.assets/image-20230824113116329.png) The gates of sodium channels Modeling of each ion gate: $$ \begin{aligned} &amp;\text{gk}&amp;&amp; =\bar{g}_{K}n^{x} \\ &amp;\text{gNa}&amp;&amp; =\bar{g}_{\mathrm{Na}}m^{3}h \\ &amp;\frac{\mathrm{d}n}{\mathrm{d}t}&amp;&amp; =\alpha_{n}(V)(1-n)-\beta_{n}(V)n \\ &amp;\frac{\mathrm{d}m}{\mathrm{d}t}&amp;&amp; =\alpha_{m}(V)(1-m)-\beta_{m}(V)m \\ &amp;\frac{\mathrm{d}h}{\mathrm{d}t}&amp;&amp; =\alpha_{h}(V)(1-h)-\beta_{h}(V)h \end{aligned} $$ $$ \begin{aligned} \frac{\mathrm{d}m}{\mathrm{d}t}&amp; =\alpha(V)(1-m)-\beta(V)m \\ &amp;=\frac{m_{\infty}(V)-m}{\tau_{m}(V)} \end{aligned} $$ $$ \begin{aligned}m_\infty(V)&amp;=\frac{\alpha(V)}{\alpha(V)+\beta(V)}\\\tau_m(V)&amp;=\frac{1}{\alpha(V)+\beta(V)}\end{aligned}. $$ $$ m(t)=m_\infty(V)+(m_0-m_\infty(V))\mathrm{e}^{-t/\tau_m(V)} $$ ### The Hodgkin-Huxley(HH) Model $$ c_\mathrm{M}\frac{\mathrm{d}V_\mathrm{M}}{\mathrm{d}t}=-g_\mathrm{Cl}(V_\mathrm{M}-E_\mathrm{Cl})-g_\mathrm{K}(V_\mathrm{M}-E_\mathrm{K})-g_\mathrm{Na}(V_\mathrm{M}-E_\mathrm{Na})+\frac{I(t)}{A} $$ æœ¬è´¨æ˜¯4ä¸ªå¾®åˆ†æ–¹ç¨‹è”ç«‹åœ¨ä¸€èµ· $$ \left\{\begin{aligned}&amp;c\frac{\mathrm{d}V}{\mathrm{d}t}=-\bar{g}_\text{Na}m^3h(V-E_\text{Na})-\bar{g}_\text{K}n^4(V-E_\text{K})-\bar{g}_\text{L}(V-E_\text{L})+I_\text{ext},\\&amp;\frac{\mathrm{d}n}{\mathrm{d}t}=\phi\left[\alpha_n(V)(1-n)-\beta_n(V)n\right]\\&amp;\frac{\mathrm{d}m}{\mathrm{d}t}=\phi\left[\alpha_m(V)(1-m)-\beta_m(V)m\right],\\&amp;\frac{\mathrm{d}h}{\mathrm{d}t}=\phi\left[\alpha_h(V)(1-h)-\beta_h(V)h\right],\end{aligned}\right. $$ $$ \begin{aligned}\alpha_n(V)&amp;=\frac{0.01(V+55)}{1-\exp\left(-\frac{V+55}{10}\right)},\quad\beta_n(V)&amp;=0.125\exp\left(-\frac{V+65}{80}\right),\\\alpha_h(V)&amp;=0.07\exp\left(-\frac{V+65}{20}\right),\quad\beta_n(V)&amp;=\frac{1}{\left(\exp\left(-\frac{V+55}{10}\right)+1\right)},\\\alpha_m(V)&amp;=\frac{0.1(V+40)}{1-\exp\left(-(V+40)/10\right)},\quad\beta_m(V)&amp;=4\exp\left(-(V+65)/18\right).\end{aligned} $$ $$ \phi=Q_{10}^{(T-T_{\mathrm{base}})/10} $$ æ¯ä¸€æ­¥ç¬¦åˆç”Ÿç‰©å­¦ ![image-20230824113714178](/BrainPy-course-notes/master_content/Notes.assets/image-20230824113714178.png) #### How to fit each gating variable? **Fitting n:** $g_{\mathbf{K}}=\bar{g}_{K}n^{x}\quad m(t)=m_{\infty}(V)+(m_{0}-\color{red}{\boxed{m_{\infty}(V)}})\mathrm{e}^{-t/\pi_{m}(V)}$ â†’ $g_\mathrm{K}(V,t)=\bar{g}_\mathrm{K}\left[n_\infty(V)-(n_\infty(V)-n_0(V))\mathrm{e}^{-\frac{t}{\tau_n(V)}}\right]^x$ by $g_{\mathrm{K}\infty}=\bar{g}_{\mathrm{K}}n_{\infty}^{x},g_{\mathrm{K}0}=\bar{g}_{\mathrm{K}}n_{0}^{x}$ â†’ $g_{\mathrm{K}}(V,t)=\left[g_{\mathrm{K}\infty}^{1/x}-(g_{\mathrm{K}\infty}^{1/x}-g_{\mathrm{K}0}^{1/x})\mathrm{e}^{-\frac{t}{\tau_{n}(V)}}\right]^{x}$ ![image-20230824114623467](/BrainPy-course-notes/master_content/Notes.assets/image-20230824114623467.png) # Hodgkin-Huxley brain dynamics programming ## Dynamics Programming Basics ### Integrators å¾®åˆ†å™¨ ![image-20230824140806650](/BrainPy-course-notes/master_content/Notes.assets/image-20230824140806650.png) **example** FitzHugh-Nagumo equation $$ \begin{aligned}\tau\dot{w}&amp;=v+a-bw,\\\dot{v}&amp;=v-\frac{ u^3}{3}-w+I_{\mathrm{ext}}.\end{aligned} $$ ```python @bp.odeint(method=&apos;Euler&apos;, dt=0.01) def integral(V, w, t, Iext, a, b, tau): dw = (V + a - b * w) / tau dV = V - V * V * V / 3 - w + Iext return dV, dw ``` **JointEq** In a dynamical system, there may be multiple variables that change dynamically over time. Sometimes these variables are interrelated, and updating one variable requires other variables as inputs. For better integration accuracy, we recommend that you use `brainpy.JointEq` to jointly solve interrelated differential equations. ```python a, b = 0.02, 0.20 dV = lambda V, t, w, Iext: 0.04 * V * V + 5 * V + 140 - w + Iext # ç¬¬ä¸€ä¸ªæ–¹ç¨‹ dw = lambda w, t, V: a * (b * V - w) # ç¬¬äºŒä¸ªæ–¹ç¨‹ joint_eq = bp.JointEq(dV, dw) # è”åˆå¾®åˆ†æ–¹ç¨‹ integral2 = bp.odeint(joint_eq, method=&apos;rk2&apos;) # å®šä¹‰è¯¥è”åˆå¾®åˆ†æ–¹ç¨‹çš„æ•°å€¼ç§¯åˆ†æ–¹æ³• ``` ```python # å£°æ˜Žç§¯åˆ†è¿è¡Œå™¨ runner = bp.integrators.IntegratorRunner( integral, monitors=[&apos;V&apos;] inits=dict(V=0., w=0.) args=dict(a=a, b=b, tau=tau, Iext=Iext), dt=0.01 ) # ä½¿ç”¨ç§¯åˆ†è¿è¡Œå™¨æ¥è¿›è¡Œæ¨¡æ‹Ÿ100msï¼Œç»“åˆæ­¥é•¿dt=0.01 runner.run(100.) plt.plot(runner.mon.ts, runner.mon.V) plt.show() ``` ![image-20230824142019832](/BrainPy-course-notes/master_content/Notes.assets/image-20230824142019832.png) ### `DynamicalSystem` BrainPy provides a generic `SynamicalSystem` class to define various types of dynamical models. BrainPy supports modelings in brain simulation and brain-inspired computing. All these supports are based on one common concept: **Dynamical System** via `brainpy.DynamicalSystem`. #### What is `DynamicalSystem` A `DynamicalSystem` defines the updating rule of the model at single time step. 1. For models with state, `DynamicalSystem` defines the state transition from $t$ to $t + dt$, i.e., $S(t+dt)=F(S(t),x,t,dt)$, where $S$ is the state, $x$ is input, $t$ is the time, and $dt$ is the time step. This is the case for recurrent neural networks (like GRU, LSTM), neuron models (like HH, LIF), or synapse models which are widely used in brain simulation. 2. However, for models in deep learning, like convolution and fully-connected linear layers, `DynamicalSystem` defines the input-to-output mapping, i.e., $y=F(x,t)$. ![img](https://brainpy.readthedocs.io/en/latest/_images/dynamical_system.png) #### How to define `DynamicalSystem` ```python class YourDynamicalSystem(bp.DynamicalSystem): def update(self, x): ... ``` Instead of input x, there are shared arguments across all nodes/layers in the network: - the current time `t`, or - the current running index `i`, or - the current time step `dt`, or - the current phase of training or testing `fit=True/False`. Here, it is necessary to explain the usage of `bp.share`. - `bp.share.save( )`: The function saves shared arguments in the global context. User can save shared arguments in tow ways, for example, if user want to set the current time `t=100`, the current time step `dt=0.1`,the user can use `bp.share.save(&quot;t&quot;,100,&quot;dt&quot;,0.1)` or `bp.share.save(t=100,dt=0.1)`. - `bp.share.load( )`: The function gets the shared data by the `key`, for example, `bp.share.load(&quot;t&quot;)`. - `bp.share.clear_shargs( )`: The function clears the specific shared arguments in the global context, for example, `bp.share.clear_shargs(&quot;t&quot;)`. - `bp.share.clear( )`: The function clears all shared arguments in the global context. #### How to run `DynamicalSystem` As we have stated above that `DynamicalSystem` only defines the updating rule at single time step, to run a `DynamicalSystem` instance over time, we need a for loop mechanism. ![img](https://brainpy.readthedocs.io/en/latest/_images/dynamical_system_and_dsrunner.png) ##### `brainpy.math.for_loop` `for_loop` is a structural control flow API which runs a function with the looping over the inputs. Moreover, this API just-in-time compile the looping process into the machine code. ```python inputs = bp.inputs.section_input([0., 6.0, 0.], [100., 200., 100.]) indices = np.arange(inputs.size) def run(i, x): neu.step_run(i, x) return neu.V.value vs = bm.for_loop(run, (indices, inputs), progress_bar=True) ``` ##### `brainpy.LoopOverTime` Different from `for_loop`, `brainpy.LoopOverTime` is used for constructing a dynamical system that automatically loops the model over time when receiving an input. `for_loop` runs the model over time. While `brainpy.LoopOverTime` creates a model which will run the model over time when calling it. ```python net2.reset_state(batch_size=10) looper = bp.LoopOverTime(net2) out = looper(currents) ``` ##### `brainpy.DSRunner` **Initializing a `DSRunner`** Generally, we can initialize a runner for dynamical systems with the format of: ``` runner = DSRunner(target=instance_of_dynamical_system, inputs=inputs_for_target_DynamicalSystem, monitors=interested_variables_to_monitor, dyn_vars=dynamical_changed_variables, jit=enable_jit_or_not, progress_bar=report_the_running_progress, numpy_mon_after_run=transform_into_numpy_ndarray ) ``` - `target` specifies the model to be simulated. It must an instance of brainpy.DynamicalSystem. - `inputs` is used to define the input operations for specific variables. - It should be the format of `[(target, value, [type, operation])]`, where `target` is the input target, `value` is the input value, `type` is the input type (such as â€œfixâ€, â€œiterâ€, â€œfuncâ€), `operation` is the operation for inputs (such as â€œ+â€, â€œ-â€, â€œ*â€, â€œ/â€, â€œ=â€). Also, if you want to specify multiple inputs, just give multiple `(target, value, [type, operation])`, such as `[(target1, value1), (target2, value2)]`. - It can also be a function, which is used to manually specify the inputs for the target variables. This input function should receive one argument `tdi` which contains the shared arguments like time `t`, time step `dt`, and index `i`. - `monitors` is used to define target variables in the model. During the simulation, the history values of the monitored variables will be recorded. It can also to monitor variables by callable functions and it should be a `dict`. The `key` should be a string for later retrieval by `runner.mon[key]`. The `value` should be a callable function which receives an argument: `tdt`. - `dyn_vars` is used to specify all the dynamically changed [variables](https://brainpy.readthedocs.io/en/latest/tutorial_math/variables.html) used in the `target` model. - `jit` determines whether to use JIT compilation during the simulation. - `progress_bar` determines whether to use progress bar to report the running progress or not. - `numpy_mon_after_run` determines whether to transform the JAX arrays into numpy ndarray or not when the network finishes running. **Running a `DSRunner`** After initialization of the runner, users can call `.run()` function to run the simulation. The format of function `.run()` is showed as follows: ```python runner.run(duration=simulation_time_length, inputs=input_data, reset_state=whether_reset_the_model_states, shared_args=shared_arguments_across_different_layers, progress_bar=report_the_running_progress, eval_time=evaluate_the_running_time ) ``` - `duration` is the simulation time length. - `inputs` is the input data. If `inputs_are_batching=True`, `inputs` must be a PyTree of data with two dimensions: `(num_sample, num_time, ...)`. Otherwise, the `inputs` should be a PyTree of data with one dimension: `(num_time, ...)`. - `reset_state` determines whether to reset the model states. - `shared_args` is shared arguments across different layers. All the layers can access the elements in `shared_args`. - `progress_bar` determines whether to use progress bar to report the running progress or not. - `eval_time` determines whether to evaluate the running time. ### Monitors ```python # initialize monitor through a list of strings runner1 = bp.DSRunner(target=net, monitors=[&apos;E.spike&apos;, &apos;E.V&apos;, &apos;I.spike&apos;, &apos;I.V&apos;], # 4 elements in monitors inputs=[(&apos;E.input&apos;, 20.), (&apos;I.input&apos;, 20.)], jit=True) ``` Once we call the runner with a given time duration, the monitor will automatically record the variable evolutions in the corresponding models. Afterwards, users can access these variable trajectories by using .mon.[variable_name]. The default history times .mon.ts will also be generated after the model finishes its running. Letâ€™s see an example. ```python runner1.run(100.) bp.visualize.raster_plot(runner1.mon.ts, runner1.mon[&apos;E.spike&apos;], show=True) ``` **Initialization with index specification** ```python monitors=[(&apos;E.spike&apos;, [1, 2, 3]), # monitor values of Variable at index of [1, 2, 3] &apos;E.V&apos;], # monitor all values of Variable &apos;V&apos; ``` &amp;gt; The monitor shape of &quot;E.V&quot; is (run length, variable size) = (1000, 3200) &amp;gt; The monitor shape of &quot;E.spike&quot; is (run length, index size) = (1000, 3) **Explicit monitor target** ```python monitors={&apos;spike&apos;: net.E.spike, &apos;V&apos;: net.E.V}, ``` &amp;gt; The monitor shape of &quot;V&quot; is = (1000, 3200) &amp;gt; The monitor shape of &quot;spike&quot; is = (1000, 3200) **Explicit monitor target with index specification** ```python monitors={&apos;E.spike&apos;: (net.E.spike, [1, 2]), # monitor values of Variable at index of [1, 2] &apos;E.V&apos;: net.E.V}, # monitor all values of Variable &apos;V&apos; ``` &amp;gt; The monitor shape of &quot;E.V&quot; is = (1000, 3200) &amp;gt; The monitor shape of &quot;E.spike&quot; is = (1000, 2) ### Inputs In brain dynamics simulation, various inputs are usually given to different units of the dynamical system. In BrainPy, `inputs` can be specified to runners for dynamical systems. The aim of `inputs` is to mimic the input operations in experiments like Transcranial Magnetic Stimulation (TMS) and patch clamp recording. `inputs` should have the format like `(target, value, [type, operation])`, where - `target` is the target variable to inject the input. - `value` is the input value. It can be a scalar, a tensor, or a iterable object/function. - `type` is the type of the input value. It support two types of input: `fix` and `iter`. The first one means that the data is static; the second one denotes the data can be iterable, no matter whether the input value is a tensor or a function. The `iter` type must be explicitly stated. - `operation` is the input operation on the target variable. It should be set as one of `{ + , - , * , / , = }`, and if users do not provide this item explicitly, it will be set to â€˜+â€™ by default, which means that the target variable will be updated as `val = val + input`. #### Static inputs ```python runner6 = bp.DSRunner(target=net, monitors=[&apos;E.spike&apos;], inputs=[(&apos;E.input&apos;, 20.), (&apos;I.input&apos;, 20.)], # static inputs jit=True) runner6.run(100.) bp.visualize.raster_plot(runner6.mon.ts, runner6.mon[&apos;E.spike&apos;]) ``` #### Iterable inputs ```python I, length = bp.inputs.section_input(values=[0, 20., 0], durations=[100, 1000, 100], return_length=True, dt=0.1) runner7 = bp.DSRunner(target=net, monitors=[&apos;E.spike&apos;], inputs=[(&apos;E.input&apos;, I, &apos;iter&apos;), (&apos;I.input&apos;, I, &apos;iter&apos;)], # iterable inputs jit=True) runner7.run(length) bp.visualize.raster_plot(runner7.mon.ts, runner7.mon[&apos;E.spike&apos;]) ``` ## Run a built-in HH model [Using Built-in Models â€” BrainPy documentation](https://brainpy.readthedocs.io/en/latest/tutorial_building/overview_of_dynamic_model.html) ```python import brainpy as bp import brainpy.math as bm current, length = bp.inputs.section_input(values=[0., bm.asarray([1., 2., 4., 8., 10., 15.]), 0.], durations=[10, 2, 25], return_length=True) hh_neurons = bp.neurons.HH(current.shape[1]) runner = bp.DSRunner(hh_neurons, monitors=[&apos;V&apos;, &apos;m&apos;, &apos;h&apos;, &apos;n&apos;], inputs=(&apos;input&apos;, current, &apos;iter&apos;)) runner.run(length) ``` ## Run a HH model from scratch The mathematic expression of the HH model $$ \left\{\begin{aligned}&amp;c\frac{\mathrm{d}V}{\mathrm{d}t}=-\bar{g}_\text{Na}m^3h(V-E_\text{Na})-\bar{g}_\text{K}n^4(V-E_\text{K})-\bar{g}_\text{L}(V-E_\text{L})+I_\text{ext},\\&amp;\frac{\mathrm{d}n}{\mathrm{d}t}=\phi\left[\alpha_n(V)(1-n)-\beta_n(V)n\right]\\&amp;\frac{\mathrm{d}m}{\mathrm{d}t}=\phi\left[\alpha_m(V)(1-m)-\beta_m(V)m\right],\\&amp;\frac{\mathrm{d}h}{\mathrm{d}t}=\phi\left[\alpha_h(V)(1-h)-\beta_h(V)h\right],\end{aligned}\right. $$ $$ \begin{aligned}\alpha_n(V)&amp;=\frac{0.01(V+55)}{1-\exp\left(-\frac{V+55}{10}\right)},\quad\beta_n(V)&amp;=0.125\exp\left(-\frac{V+65}{80}\right),\\\alpha_h(V)&amp;=0.07\exp\left(-\frac{V+65}{20}\right),\quad\beta_n(V)&amp;=\frac{1}{\left(\exp\left(-\frac{V+55}{10}\right)+1\right)},\\\alpha_m(V)&amp;=\frac{0.1(V+40)}{1-\exp\left(-(V+40)/10\right)},\quad\beta_m(V)&amp;=4\exp\left(-(V+65)/18\right).\end{aligned} $$ $$ \phi=Q_{10}^{(T-T_{\mathrm{base}})/10} $$ V: the membrane potential n: activation variable of the Kt channel m: activation variable of the Nat channel h; inactivation variable of the Nat channe ### Define HH model `class` - Inherit `bp.dyn.NeuDyn` ```python import brainpy as bp import brainpy.math as bm class HH(bp.dyn.NeuDyn): def __init__(self, size, ENa=50., gNa=120., Ek=-77., gK=36., EL=-54.387, gL=0.03, V_th=0., C=1.0, T=6.3): super(HH, self).__init__(size=size) ``` ### Initialization ```python import brainpy as bp import brainpy.math as bm class HH(bp.dyn.NeuDyn): def __init__(self, size, ENa=50., gNa=120., Ek=-77., gK=36., EL=-54.387, gL=0.03, V_th=0., C=1.0, T=6.3): super(HH, self).__init__(size=size) # parameters self.ENa = ENa self.EK = EK self.EL = EL self.gNA = gNa self.gK = gK self.gL = gL self.C = C self.V_th = V_th self.T_base = 6.3 self.phi = 3.0 ** ((T - self.T_base) / 10.0) # variable self.V = bm.Variable(-70.68 * bm.ones(self.num)) self.m = bm.Variable(0.0266 * bm.ones(self.num)) self.h = bm.Variable(0.772 * bm.ones(self.num)) self.n = bm.Variable(0.235 * bm.ones(self.num)) self.input = bm.Variable(bm.zeros(self.num)) self.spike = bm.Variable(bm.zeros(self.num, dtype=bool)) self.t_last_spike = bm.Variable(bm.ones(self.num) * -1e7) # å®šä¹‰ç§¯åˆ†å‡½æ•° self.integral = bp.odeint(f=self.derivative, method=&apos;exp_auto&apos;) ``` ### Define the derivative function ```python @property def derivative(self): return bp.JointEq(self.dV, self.dm, self.dh, self.dn) def dV(self, V, t, m, h, n, Iext): I_Na = (self.gNa * m ** 3.0 * h) * (V - self.ENa) I_K = (self.gK * n ** 4.0) * (V - self.EK) I_leak = self.gL * (V - self.EL) dVdt = (- I_Na - I_K - I_leak + Iext) / self.C return dVdt def dm(self, m, t, V): alpha = 0.1 * (V + 40) / (1 - bm.exp(-(V + 40) / 10)) beta = 4.0 * bm.exp(-(V + 65) / 18) dmdt = alpha * (1 - m) - beta * m return self.phi * dmdt def dh(self, h, t, V): alpha = 0.07 * bm.exp(-(V + 65) / 20.) beta = 1 / (1 + bm.exp(-(V + 35) / 10)) dhdt = alpha * (1 - h) - beta * h return self.phi * dhdt def dn(self, n, t, V): alpha = 0.01 * (V + 55) / (1 - bm.exp(-(V + 55) / 10)) beta = 0.125 * bm.exp(-(V + 65) / 80) dndt = alpha * (1 - n) - beta * n return self.phi * dndt ``` ### Complete the `update()` function ```python def update(self, x=None): t = bp.share.load(&apos;t&apos;) dt = bp.share.load(&apos;dt&apos;) # TODO: æ›´æ–°å˜é‡V, m, h, n, æš‚å­˜åœ¨V, m, h, nä¸­ V, m, h, n = self.integral(self.V, self.m, self.h, self.n, t, self.input, dt=dt) #åˆ¤æ–­æ˜¯å¦å‘ç”ŸåŠ¨ä½œç”µä½ self.spike.value = bm.logical_and(self.V = self.V_th) # æ›´æ–°æœ€åŽä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´ self.t_last_spike.value = bm.where(self.spike, t, self.t_last_spike) # TODO: æ›´æ–°å˜é‡V, m, h, nçš„å€¼ self.V.value = V self.m.value = m self.h.value = h self.n.value = n #é‡ç½®è¾“å…¥ self.input[:] = 0 ``` ### Simulation ```python current, length = bp.inputs.section_input(values=[0., bm.asarray([1., 2., 4., 8., 10., 15.]), 0.], durations=[10, 2, 25], return_length=True) hh_neurons = HH(current.shape[1]) runner = bp.DSRunner(hh_neurons, monitors=[&apos;V&apos;, &apos;m&apos;, &apos;h&apos;, &apos;n&apos;], inputs=(&apos;input&apos;, current, &apos;iter&apos;)) runner.run(length) ``` ### Visualization ```python import numpy as np import matplotlib.pyplot as plt bp.visualize.line_plot(runner.mon.ts, runner.mon.V, ylabel=&apos;V (mV)&apos;, plot_ids=np.arange(current.shape[1])) plt.plot(runner.mon.ts, bm.where(current[:, -1]&amp;gt;0, 10, 0) - 90.) plt.figure() plt.plot(runner.mon.ts, runner.mon.m[:, -1]) plt.plot(runner.mon.ts, runner.mon.h[:, -1]) plt.plot(runner.mon.ts, runner.mon.n[:, -1]) plt.legend([&apos;m&apos;, &apos;h&apos;, &apos;n&apos;]) plt.xlabel(&apos;Time (ms)&apos;) ``` ## Customize a conductance-based model ç”µè·¯æ¨¡æ‹Ÿï¼Œå†™æˆç”µå¯¼å½¢å¼ ![image-20230824180831033](/BrainPy-course-notes/master_content/Notes.assets/image-20230824180831033.png) $$ \begin{aligned} \text{gK}&amp; =\bar{g}_\text{K}n^4, \\ \frac{\mathrm{d}n}{\mathrm{d}t}&amp; =\phi[\alpha_n(V)(1-n)-\beta_n(V)n], \end{aligned} $$ åŠ¨åŠ›å­¦å½¢å¼æè¿°ï¼Œå¼•å…¥é—¨æ¡†å˜é‡$n$ $$ \begin{aligned} &amp;\alpha_{n}(V) =\frac{0.01(V+55)}{1-\exp(-\frac{V+55}{10})}, \\ &amp;\beta_{n}(V) =0.125\exp\left(-\frac{V+65}{80}\right). \end{aligned} $$ ç”±æ­¤å¼æ¥å»ºæ¨¡é’¾ç¦»å­é€šé“ ### Programming an ion channel #### Three ion channel ```python import brainpy as bp import brainpy.math as bm class IK(bp.dyn.IonChannel): def __init__(self, size, E=-77., g_max=36., phi=1., method=&apos;exp_auto&apos;): super(IK, self).__init__(size) self.g_max = g_max self.E = E self.phi = phi self.n = bm.Variable(bm.zeros(size)) # variables should be packed with bm.Variable self.integral = bp.odeint(self.dn, method=method) def dn(self, n, t, V): alpha_n = 0.01 * (V + 55) / (1 - bm.exp(-(V + 55) / 10)) beta_n = 0.125 * bm.exp(-(V + 65) / 80) return self.phi * (alpha_n * (1. - n) - beta_n * n) def update(self, V): t = bp.share.load(&apos;t&apos;) dt = bp.share.load(&apos;dt&apos;) self.n.value = self.integral(self.n, t, V, dt=dt) def current(self, V): return self.g_max * self.n ** 4 * (self.E - V) ``` ```python class INa(bp.dyn.IonChannel): def __init__(self, size, E= 50., g_max=120., phi=1., method=&apos;exp_auto&apos;): super(INa, self).__init__(size) self.g_max = g_max self.E = E self.phi = phi self.m = bm.Variable(bm.zeros(size)) # variables should be packed with bm.Variable self.h = bm.Variable(bm.zeros(size)) self.integral_m = bp.odeint(self.dm, method=method) self.integral_h = bp.odeint(self.dh, method=method) def dm(self, m, t, V): # TODO: è®¡ç®—dm/dt alpha_m = 0.11 * (V + 40) / (1 - bm.exp(-(V + 40) / 10)) beta_m = 4 * bm.exp(-(V + 65) / 18) return self.phi * (alpha_m * (1. - m) - beta_m * m) def dh(self, h, t, V): # TODO: è®¡ç®—dh/dt alpha_h = 0.07 * bm.exp(-(V + 65) / 20) beta_h = 1. / (1 + bm.exp(-(V + 35) / 10)) return self.phi * (alpha_h * (1. - h) - beta_h * h) def update(self, V): t = bp.share.load(&apos;t&apos;) dt = bp.share.load(&apos;dt&apos;) # TODO: æ›´æ–°self.m, self.h self.m.value = self.integral_m(self.m, t, V, dt=dt) self.h.value = self.integral_h(self.h, t, V, dt=dt) def current(self, V): return self.g_max * self.m ** 3 * self.h * (self.E - V) ``` ```python class IL(bp.dyn.IonChannel): def __init__(self, size, E=-54.39, g_max=0.03): super(IL, self).__init__(size) self.g_max = g_max self.E = E def current(self, V): return self.g_max * (self.E - V) def update(self, V): pass ``` #### Build a HH model with ion channels **Using customized ion channels** ```python class HH(bp.dyn.CondNeuGroup): def __init__(self, size): super(HH, self).__init__(size, V_initializer=bp.init.Uniform(-80, -60.)) # TODO: åˆå§‹åŒ–ä¸‰ä¸ªç¦»å­é€šé“ self.IK = IK(size, E=-77., g_max=36.) self.INa = INa(size, E=50., g_max=120.) self.IL = IL(size, E=-54.39, g_max=0.03) ``` **Using built-in ion channels** ```python class HH(bp.dyn.CondNeuGroup): def __init__(self, size): super().__init__(size) self.INa = bp.channels.INa_HH1952(size) self.IK = bp.channels.IK_HH1952(size) self.IL = bp.cahnnels.IL(size, E=-54.387, g_max=0.03) ``` #### Simulation ```python neu = HH(1) runner = bp.DSRunner( neu, monitors=[&apos;V&apos;, &apos;IK.n&apos;, &apos;INa.m&apos;, &apos;INa.h&apos;], inputs=(&apos;input&apos;, 1.698) # near the threshold current ) runner.run(200) # the running time is 200 ms import matplotlib.pyplot as plt plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;V&apos;]) plt.xlabel(&apos;t (ms)&apos;) plt.ylabel(&apos;V (mV)&apos;) plt.savefig(&quot;HH.jpg&quot;) plt.show() plt.figure(figsize=(6, 2)) plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;IK.n&apos;], label=&apos;n&apos;) plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;INa.m&apos;], label=&apos;m&apos;) plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;INa.h&apos;], label=&apos;h&apos;) plt.xlabel(&apos;t (ms)&apos;) plt.legend() plt.savefig(&quot;HH_channels.jpg&quot;) plt.show() ``` ![image-20230824184016011](/BrainPy-course-notes/master_content/Notes.assets/image-20230824184016011.png) # Simple Neuron Modeling: Simplified Models ## The Leaky Integrate-and-Fire(LIF) Neuron Model ### The LIF neuron model $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-(V-V_{\mathrm{rest}})+RI(t)\\\\\mathrm{if}V&amp;&amp;gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\text{last}\ {t_{ref}}\end{aligned} $$ åªæœ‰ä¸€ä¸ªå¾®åˆ†æ–¹ç¨‹ï¼Œè¦åŠ ä¸€ä¸ªä¸åº”æœŸ(**t refractory period**)ï¼Œè†œç”µä½ä¸å‘ç”Ÿä»»ä½•æ”¹å˜ï¼Œè®¤ä¸ºç¦»å­é€šé“åªæœ‰æ³„éœ²é€šé“ ![image-20230825101057570](/BrainPy-course-notes/master_content/Notes.assets/image-20230825101057570.png) Given a constant current input: ![image-20230825101410745](/BrainPy-course-notes/master_content/Notes.assets/image-20230825101410745.png) æ²¡æœ‰å»ºæ¨¡å‡†ç¡®å˜åŒ–ï¼Œåªæä¾›ä»€ä¹ˆæ—¶å€™è†œç”µä½çš„å˜åŒ– ### The dynamic features of the LIF model **General solution (constant input):**$V(t)=V_{\text{reset}}+RI_{\text{c}}(1-\mathrm{e}^{-\frac{t-t_0}{\tau}})$ **Firing frequency:** $$ \begin{aligned}T&amp;=-\tau\ln\left(1-\frac{V_{\phi h}-V_{\mathrm{rest}}}{RI_{\varsigma}}\right)\\f&amp;=\frac{1}{T+t_{\mathrm{ref}}}=\frac{1}{t_{\mathrm{ref}}-\tau\ln\left(1-\frac{V_{0}-V_{\mathrm{rest}}}{RI_{\varsigma}}\right)}\end{aligned} $$ **Rheobase current (minimal current):** $$ I_{\theta}=\frac{V_{\mathrm{th}}-V_{\mathrm{reset}}}{R} $$ åŸºå¼ºç”µæµï¼Œå¦‚æžœå°äºŽå®ƒå°†æ— æ³•å‘æ”¾ ### Strengths &amp; weaknesses of the LIF model #### Strengths - Simple, high simulation efficiency - Intuitive - Fits well the subthreshold membrane potential #### Weaknesses - The shape of action potentials is over-simplified - Has no memory of the spiking history - Cannot reproduce diverse firing patterns ### Other Univariate neuron models #### The Quadratic Integrate-and-Fire (QOF) model: $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=a_{0}(V-V_{\mathrm{re}t})(V-V_{\mathrm{c}})+RI(t)\\&amp;\text{if }V&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{re}set}\quad\text{last}\quad t_{\mathrm{ref}}\end{aligned} $$ ![image-20230825103243039](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103243039.png) è†œç”µä½ä»éœ€è¦æ‰‹åŠ¨é‡ç½® #### The Theta neuron model $$ \frac{\mathrm{d}\theta}{\mathrm{d}t}=1-\cos\theta+(1+\cos\theta)(\beta+I(t)) $$ ![image-20230825103331170](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103331170.png) éšå¼è¡¨è¾¾ï¼Œä¸å…·æœ‰ç‰©ç†æ„ä¹‰ï¼Œä½†ä¹Ÿä¼šè¿›è¡Œæ•´åˆå‘æ”¾ #### The Exponential Integrate-and-Fire (ExpIF) model $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-\left(V-V_{\mathrm{res}t}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{3T}}+RI(t)\\\mathrm{if~}V&amp;&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{res}t}\mathrm{last}t_{\mathrm{ref}}\end{aligned} $$ ![image-20230825103501912](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103501912.png) ä»éœ€è¦æ‰‹åŠ¨é‡ç½®è†œç”µä½ ## The Adaptive Exponential Integrate-and-Fire(AdEx) Neuron Model ### The AdEx neuron model Two variables: - ð‘‰: membrane potential - ð‘¤: adaptation variable $$ \begin{aligned} \tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{S_{T}}}-Rw+RI(t) \\ \tau_{w}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{\mathrm{w}}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ \mathrm{if}V&amp; &amp;gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} \end{aligned} $$ ä¸ä¸ºé›¶ï¼Œå°±ä¼šè¡°å‡åˆ°$-w$ ![image-20230825103840880](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103840880.png) - A larger ð‘¤ suppresses ð‘‰ from increasing - ð‘¤ decays exponentially while having a sudden increase when the neuron fires **Firing patterns of the AdEx model** ![image-20230825104254936](/BrainPy-course-notes/master_content/Notes.assets/image-20230825104254936.png) **Categorization of firing patterns** According to the steady-state firing time intervals: - Tonic/regular spiking - Adapting - Bursting - Irregular spiking According to the initial-state features: - Tonic/classic spiking - Initial burst - Delayed spiking ### Other multivariate neuron models #### The Izhikevich model $$ \begin{aligned} &amp;\frac{dV}{dt} =0.04V^{2}+5V+140-u+I \\ &amp;\frac{\mathrm{d}u}{\mathrm{d}t} =a\left(bV-u\right) \\ &amp;\operatorname{if}V &amp;gt;\theta,\quad V\leftarrow c,u\leftarrow u+d\text{ last }t_{\mathrm{ref}} \end{aligned} $$ äºŒæ¬¡æ•´åˆå‘æ”¾å¤šåŠ äº†ä¸€ä¸ª$u$ ![image-20230825104832770](/BrainPy-course-notes/master_content/Notes.assets/image-20230825104832770.png) #### The FitzHughâ€“Nagumo (FHN) model $$ \begin{aligned}\dot{v}&amp;=v-\frac{v^3}3-w+RI_{\mathrm{ext}}\\\tau\dot{w}&amp;=v+a-bw.\end{aligned} $$ æ²¡æœ‰å¯¹è†œç”µä½è¿›è¡Œäººä¸ºçš„é‡ç½®ï¼Œå¯ä»¥æ›´å¥½çš„è¿›è¡ŒåŠ¨åŠ›å­¦åˆ†æžï¼Œæ²¡æœ‰æ‰“ç ´å¾®åˆ†æ–¹ç¨‹çš„è¿žç»­æ€§ ![image-20230825104922636](/BrainPy-course-notes/master_content/Notes.assets/image-20230825104922636.png) #### The Generalized Integrate-and-Fire (GIF) model n+2ä¸ªå˜é‡ $$ \begin{aligned} &amp;\tau{\frac{\mathrm{d}V}{\mathrm{d}t}} =-\left(V-V_{\mathrm{rest}}\right)+R\sum_{j}I_{j}+RI \\ &amp;\frac{\mathrm{d}\Theta}{\mathrm{d}t} =a\left(V-V_{\mathrm{rest}}\right)-b\left(\Theta-\Theta_{\infty}\right) \\ &amp;\frac{\mathrm{d}l_{j}}{\mathrm{d}t} =-k_{j}I_{j},\quad j=1,2,...,n \\ &amp;\operatorname{if}V &amp;gt;\Theta,\quad I_{j}\leftarrow R_{j}I_{j}+A_{j},V\leftarrow V_{\mathrm{reset}},\Theta\leftarrow max(\Theta_{\mathrm{reset}},\Theta) \end{aligned} $$ æ¯ä¸ªå˜é‡éƒ½æ˜¯çº¿æ€§çš„ï¼Œæ³›åŒ–æ€§ä½“çŽ°åœ¨é‡ç½®æ¡ä»¶ä¸Š ![image-20230825105035349](/BrainPy-course-notes/master_content/Notes.assets/image-20230825105035349.png) ## Dynamic analysis: phase-plane analysis ### Phase plane analysis å¯¹åŠ¨åŠ›å­¦ç³»ç»Ÿçš„è¡Œä¸ºæ¥åˆ†æžï¼Œæ™®éå¯¹ä¸¤ä¸ªå˜é‡æ¥è¿›è¡Œåˆ†æž Analyzes the behavior of a dynamical system with (usually two) variables described by ordinary differential equations $$ \begin{aligned} &amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp;&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{S_{T}}}-Rw+RI(t) \\ &amp;\tau_{W}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp;&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ &amp;\mathrm{if}V&amp;&amp; &amp;gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} \end{aligned} $$ **Elements:** - Nullclines: $\mathrm{d}V/\mathrm{d}t=0;\mathrm{d}w/\mathrm{d}t=0$ - Fixed points: $\mathrm{d}V/\mathrm{d}t=0\mathrm{~and~}\mathrm{d}w/\mathrm{d}t=0$ - The vector field - The trajectory of variables å‡è®¾å¤–éƒ¨ç”µæµæ’å®š ![image-20230825110708994](/BrainPy-course-notes/master_content/Notes.assets/image-20230825110708994.png) ### Phase plane analysis for the AdEx neuron model $$ \begin{aligned} &amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp;&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{\Lambda_{T}}}-Rw+RI(t) \\ &amp;\tau_{w}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp;&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ &amp;\text{ifV}&amp;&amp; &amp;gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} \end{aligned} $$ ![image-20230825110811399](/BrainPy-course-notes/master_content/Notes.assets/image-20230825110811399.png) #### Tonic ![image-20230825112857175](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112857175.png) #### Adaptation ![image-20230825112918815](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112918815.png) #### Bursting ![image-20230825112933938](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112933938.png) #### Transient spiking ![image-20230825112950297](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112950297.png) ## Dynamic analysis: bifurcation analysis ### Bifurcation analysis Quantitative analysis of the existence and the properties of fixed points in a dynamical system with a changing parameter æŸä¸ªå¤–ç•Œæ¡ä»¶å˜åŒ–æ—¶ï¼Œå›ºå®šç‚¹çš„å˜åŒ– Elements: - Lines of fixed points - Stability properties of fixed points ![image-20230825114510710](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114510710.png) ### Bifurcation analysis for the AdEx Neuron model bifurcation analysis for 2 variables Variables: ð‘‰ and ð‘¤ Parameters: $I_{ext}$ $$ \begin{aligned} &amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}=-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{{\frac{V-V_{T}}{ST}}}-Rw+RI(t) \\ &amp;\text{-} {\frac{\mathrm{d}w}{\mathrm{d}t}}=a(V-V_{\mathrm{rest}})-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ &amp;\mathrm{if}V&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{reset}}\ \mathrm{last}\ t_{\mathrm{ref}} \end{aligned} $$ ![image-20230825114801456](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114801456.png) ![image-20230825114742740](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114742740.png) **Subjects: two variables (ð‘‰ and ð‘¤)** ![image-20230825114856403](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114856403.png) ### Extended: The limit cycle The FitzHughâ€“Nagumo (FHN) model $$ \begin{aligned}\dot{v}&amp;=v-\frac{v^3}3-w+RI_\mathrm{ext}\\\tau\dot{w}&amp;=v+a-bw.\end{aligned} $$ This dynamical system, in certain conditions, exhibits a cyclic pattern of variable changes which can be visualized as a closed trajectory in the phase plane. å˜åŒ–é”å®šåˆ°çŽ¯ä¸­ ![image-20230825115348008](/BrainPy-course-notes/master_content/Notes.assets/image-20230825115348008.png) ![image-20230825115354146](/BrainPy-course-notes/master_content/Notes.assets/image-20230825115354146.png) # Reduced Models - brain dynamics programming ## LIF neuron models programming ### Define LIF `class` $$ \begin{aligned}&amp;\tau\frac{\mathrm{d}V}{\mathrm{d}t}=-(V-V_{\mathrm{rest}})+RI(t)\\&amp;\text{if }V&amp;gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\text{last}t_{\mathrm{ref}}\end{aligned} $$ ```python class LIF(bp.dyn.NeuDyn): def __init__(self, size, V_rest=0, V_reset=-5, V_th=20, R=1, tau=10, t_ref=5., **kwargs): # åˆå§‹åŒ–çˆ¶ç±» super(LIF, self).__init__(size=size, **kwargs) ``` ### Initialization ```python class LIF(bp.dyn.NeuDyn): def __init__(self, size, V_rest=0, V_reset=-5, V_th=20, R=1, tau=10, t_ref=5., **kwargs): # åˆå§‹åŒ–çˆ¶ç±» super(LIF, self).__init__(size=size, **kwargs) # åˆå§‹åŒ–å‚æ•° self.V_rest = V_rest self.V_reset = V_reset self.V_th = V_th self.R = R self.tau = tau self.t_ref = t_ref # ä¸åº”æœŸæ—¶é•¿ # åˆå§‹åŒ–å˜é‡ self.V = bm.Variable(bm.random.randn(self.num) + V_reset) self.input = bm.Variable(bm.zeros(self.num)) self.t_last_spike = bm.Variable(bm.ones(self.num) * -1e7) # ä¸Šä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´ self.refractory = bm.Variable(bm.zeros(self.num, dtype=bool)) # æ˜¯å¦å¤„äºŽä¸åº”æœŸ self.spike = bm.Variable(bm.zeros(self.num, dtype=bool)) # è„‰å†²å‘æ”¾çŠ¶æ€ # ä½¿ç”¨æŒ‡æ•°æ¬§æ‹‰æ–¹æ³•è¿›è¡Œç§¯åˆ† self.integral = bp.odeint(f=self.derivative, method=&apos;exponential_euler&apos;) ``` ### Define the derivative function ```python # å®šä¹‰è†œç”µä½å…³äºŽæ—¶é—´å˜åŒ–çš„å¾®åˆ†æ–¹ç¨‹ def derivative(self, V, t, Iext): dVdt = (-V + self.V_rest + self.R * Iext) / self.tau return dVdt ``` ### Complete the `update()` function ```python def update(self): t, dt = bp.share[&apos;t&apos;], bp.share[&apos;dt&apos;] # ä»¥æ•°ç»„çš„æ–¹å¼å¯¹ç¥žç»å…ƒè¿›è¡Œæ›´æ–° refractory = (t - self.t_last_spike) self.V_th # å°†å¤§äºŽé˜ˆå€¼çš„ç¥žç»å…ƒæ ‡è®°ä¸ºå‘æ”¾äº†è„‰å†² self.spike[:] = spike # æ›´æ–°ç¥žç»å…ƒè„‰å†²å‘æ”¾çŠ¶æ€ self.t_last_spike[:] = bm.where(spike, t, self.t_last_spike) # æ›´æ–°æœ€åŽä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´ self.V[:] = bm.where(spike, self.V_reset, V) # å°†å‘æ”¾äº†è„‰å†²çš„ç¥žç»å…ƒè†œç”µä½ç½®ä¸ºV_resetï¼Œå…¶ä½™ä¸å˜ self.refractory[:] = bm.logical_or(refractory, spike) # æ›´æ–°ç¥žç»å…ƒæ˜¯å¦å¤„äºŽä¸åº”æœŸ self.input[:] = 0. # é‡ç½®å¤–ç•Œè¾“å…¥ ``` ### Simulation ```python def run_LIF(): # è¿è¡ŒLIFæ¨¡åž‹ group = LIF(1) runner = bp.DSRunner(group, monitors=[&apos;V&apos;], inputs=(&apos;input&apos;, 22.)) runner(200) # è¿è¡Œæ—¶é•¿ä¸º200ms # ç»“æžœå¯è§†åŒ– fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) plt.plot(runner.mon.ts, runner.mon.V) plt.xlabel(r&apos;$t$ (ms)&apos;) plt.ylabel(r&apos;$V$ (mV)&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) plt.show() ``` ![image-20230825141201825](/BrainPy-course-notes/master_content/Notes.assets/image-20230825141201825.png) ### Input current &amp; firing frequency $$ \begin{gathered} V(t)=V_{\mathrm{reset}}+RI_{\mathrm{c}}(1-\mathrm{e}^{-\frac{t-t_{0}}{\tau}}). \\ T=-\tau\ln\left[1-\frac{V_{\mathrm{th}}-V_{\mathrm{rest}}}{RI_{\mathrm{c}}}\right] \\ f={\frac{1}{T+t_{\mathrm{ref}}}}={\frac{1}{t_{\mathrm{ref}}-\tau\ln\left[1-{\frac{V_{\mathrm{th}}-V_{\mathrm{rest}}}{RI_{c}}}\right]}} \end{gathered} $$ ```python # è¾“å…¥ä¸Žé¢‘çŽ‡çš„å…³ç³» current = bm.arange(0, 600, 2) duration = 1000 LIF_neuron = LIF(current.shape[0]) runner_2 = bp.dyn.DSRunner(LIF_neurons, monitors=[&apos;spike&apos;], inputs={&apos;input&apos;, current}, dt=0.01) runner_2.run(duration) freqs = runner_2.mon.spike.sum(axis=0) / (duration/1000) plt.figure() plt.plot(current, freqs) plt.xlabel(&apos;inputs&apos;) plt.ylabel(&apos;frequencies&apos;) ``` ![image-20230825143405952](/BrainPy-course-notes/master_content/Notes.assets/image-20230825143405952.png) ### Other Univariate neuron models **The Quadratic Integrate-and-Fire (QIF) model** $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=a_{0}(V-V_{\mathrm{res}t})(V-V_{c})+RI(t)\\\mathrm{if~}V&amp;&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{reset~last~}t_{\mathrm{ref}}}\end{aligned} $$ ```python def derivative(self, V, t, I): dVdt = (self.c * (V - self.V_reset) * (V - self.V_c) + self.R * I) / self.tau return dVdt ``` **The Exponential Integrate-and-Fire (ExpIF) model** $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{\delta_{T}}}+RI(t)\\&amp;\mathrm{if~}V&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{reset}}\mathrm{last}t_{\mathrm{ref}}\end{aligned} $$ ```python def derivative(self, V, t, I): exp_v = self.delta_T * bm.exp((V - self.V_T) / self.delta_T) dvdt = (- (V - self.V_rest) + exp_v + self.R * I) / self.tau return dvdt ``` ## AdEx neuron models programming $$ \begin{gathered} \tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}=-(V-V_{\mathrm{rest}})+\Delta_{T}\mathrm{e}^{{\frac{V-V_{T}}{\Delta T}}}-Rw+RI(t), \\ \tau_{w}\frac{\mathrm{d}w}{\mathrm{d}t}=a(V-V_{\mathrm{rest}})-w+b\tau_{w}\sum_{t^{(f)}}\delta(t-t^{(f)})), \\ \mathrm{if~}V&amp;gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\mathrm{last}t_{\mathrm{ref}}. \end{gathered} $$ ### Define AdEx `class` ```python class AdEx(bp.dyn.NeuDyn): def __init__(self, size, V_rest=-65, V_reset=-68, V_th=-30, V_T=-59.9, delta_T=3.48 a=1., b=1., R=1., tau=10., tau_w=30., tau_ref=0., **kwargs): # åˆå§‹åŒ–çˆ¶ç±» super(AdEx, self).__init__(size=size, **kwargs) ``` ### Initialization ```python class AdEx(bp.dyn.NeuDyn): def __init__(self, size, V_rest=-65, V_reset=-68, V_th=-30, V_T=-59.9, delta_T=3.48 a=1., b=1., R=1., tau=10., tau_w=30., tau_ref=0., **kwargs): # åˆå§‹åŒ–çˆ¶ç±» super(AdEx, self).__init__(size=size, **kwargs) # åˆå§‹åŒ–å‚æ•° self.V_rest = V_rest self.V_reset = V_reset self.V_th = V_th self.V_T = V_T self.delta_T = delta_T self.a = a self.b = b self.R = R self.tau = tau self.tau_w = tau_w self.tau_ref = tau_ref # åˆå§‹åŒ–å˜é‡ self.V = bm.Variable(bm.random.randn(self.num) - 65.) self.w = bm.Variable(bm.zeros(self.num)) self.input = bm.Variable(bm.zeros(self.num)) self.t_last_spike = bm.Variable(bm.ones(self.num) * -1e7) # ä¸Šä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´ self.refractory = bm.Variable(bm.zeros(self.num, dtype=bool)) # æ˜¯å¦å¤„äºŽä¸åº”æœŸ self.spike = bm.Variable(bm.zeros(self.num, dtype=bool)) # è„‰å†²å‘æ”¾çŠ¶æ€ # å®šä¹‰ç§¯åˆ†å™¨ self.integral = bp.odeint(f=self.derivative, method=&apos;exp_auto&apos;) ``` ### Define the derivative function ```python def dV(self, V, t, w, I): exp = self.delta_T * bm.exp((V - self.V_T) / self.delta_T) dVdt = (-V + self.V_rest + exp - self.R * w + self.R * I) / self.tau return dVdt def dw(self, w, t, V): dwdt = (self.a * (V - self.V_rest) - w) / self.tau_w return dwdt @property def derivative(self): return bp.JointEq([self.dV, self.dw]) ``` ### Complete the `update()` function ```python def update(self): t, dt = bp.share[&apos;t&apos;], bp.share[&apos;dt&apos;] V, w = self.integral(self.V.value, self.w.value, t, self.input, dt=dt) # ä»¥æ•°ç»„çš„æ–¹å¼å¯¹ç¥žç»å…ƒè¿›è¡Œæ›´æ–° refractory = (t - self.t_last_spike) self.V_th # å°†å¤§äºŽé˜ˆå€¼çš„ç¥žç»å…ƒæ ‡è®°ä¸ºå‘æ”¾äº†è„‰å†² self.spike[:] = spike # æ›´æ–°ç¥žç»å…ƒè„‰å†²å‘æ”¾çŠ¶æ€ self.t_last_spike[:] = bm.where(spike, t, self.t_last_spike) # æ›´æ–°æœ€åŽä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´ self.V[:] = bm.where(spike, self.V_reset, V) # å°†å‘æ”¾äº†è„‰å†²çš„ç¥žç»å…ƒè†œç”µä½ç½®ä¸ºV_resetï¼Œå…¶ä½™ä¸å˜ self.w[:] = bm.where(spike, w + self.b, w) #æ›´æ–°è‡ªé€‚åº”ç”µæµ self.refractory[:] = bm.logical_or(refractory, spike) # æ›´æ–°ç¥žç»å…ƒæ˜¯å¦å¤„äºŽä¸åº”æœŸ self.input[:] = 0. # é‡ç½®å¤–ç•Œè¾“å…¥ ``` ### Simulation ![image-20230825145518709](/BrainPy-course-notes/master_content/Notes.assets/image-20230825145518709.png) ### Other multivariate neuron models **The Izhikevich model** $$ \begin{aligned} &amp;\frac{dV}{dt} =0.04V^{2}+5V+140-u+I \\ &amp;\frac{\mathrm{d}u}{\mathrm{d}t} =a\left(bV-u\right) \\ &amp;\operatorname{if}V &amp;gt;\theta,\quad V\leftarrow c,u\leftarrow u+d\mathrm{last}t_{\mathrm{ref}} \end{aligned} $$ ```python def dV(self, V, t, u, I): dVdt = 0.04 * V * V + 5 * V + 140 - u + I return dVdt def du(self, u, t, V): dudt = self.a * (self.b * V - u) return dudt ``` **The Generalized Integrate-and-Fire (GIF) model** $$ \begin{aligned} &amp;\tau{\frac{\mathrm{d}V}{\mathrm{d}t}} =-\left(V-V_{\mathrm{rest}}\right)+R\sum_{j}I_{j}+RI \\ &amp;\frac{\mathrm{d}\Theta}{\mathrm{d}t} =a\left(V-V_{\mathrm{est}}\right)-b\left(\Theta-\Theta_{\infty}\right) \\ &amp;\frac{\mathrm{d}I_j}{\mathrm{d}r} =-k_jI_j,\quad j=1,2,\ldots,n \\ &amp;\text{if V} &amp;gt;\Theta,\quad I_{j}\leftarrow R_{j}I_{j}+A_{j},V\leftarrow V_{\mathrm{reset}},\Theta\leftarrow max\left(\Theta_{\mathrm{reset}},\Theta\right) \end{aligned} $$ ```python def dI1(self, I1, t): return - self.k1 * I1 def dI2(self, I2, t): return - self.k2 * I2 def dVth(self, V_th, t, V): return self.a * (V - self.v_rest) - self.b * (V_th - self.V_th_inf) def dV(self, V, t, I1, I2, I): return (- (V - self.V_rest) + self.R * (I + I1 + I2)) / self.tau ``` **Built-in reduced neuron models** ![image-20230825145947800](/BrainPy-course-notes/master_content/Notes.assets/image-20230825145947800.png) ## Dynamic analysis: phase-plane analysis ### Simple case $$ \frac{dx}{dt}=\sin(x)+I, $$ ```python @bp.odeint def int_x(x, t, Iext): return bp.math.sin(x) + Iext ``` ```python pp = bp.analysis.PhasePlane1D( model=int_x, target_vars={&apos;x&apos;: [-10, 10]}, pars_update={&apos;Iext&apos;: 0.}, resolutions={&apos;x&apos;: 0.01} ) pp.plot_vector_field() pp.plot_fixed_point(show=True) ``` ![image-20230825152003373](/BrainPy-course-notes/master_content/Notes.assets/image-20230825152003373.png) - Nullcline: The zero-growth isoclines, such as $f(x,y) = 0$ and $g(x,y) = 0$ - Fixed points: The equilibrium points of the system, which are located at all the nullclines intersect. - Vector field: The vector field of the system. - Limit cycles: The limit cycles. - Trajectories: A simulation trajectory with the given initial values ### Phase plane analysis for AdEx ```python def ppa_AdEx(group): bm.enable_x64() v_range = [-70., -40.] w_range = [-10., 50.] phase_plane_analyzer = bp.analysis.PhasePlane2D( model=group, target_vars={&apos;V&apos;: v_range, &apos;w&apos;: w_range, }, # å¾…åˆ†æžå˜é‡ pars_update={&apos;I&apos;: Iext}, # éœ€è¦æ›´æ–°çš„å˜é‡ resolutions=0.05 ) # ç”»å‡ºV, wçš„é›¶å¢žé•¿æ›²çº¿ phase_plane_analyzer.plot_nullcline() # ç”»å‡ºå¥‡ç‚¹ phase_plane_analyzer.plot_fixed_point() # ç”»å‡ºå‘é‡åœº phase_plane_analyzer.plot_vector_field() # åˆ†æ®µç”»å‡ºV, wçš„å˜åŒ–è½¨è¿¹ group.V[:], group.w[:] = group.V_reset, 0 runner = bp.DSRunner(group, monitors=[&apos;V&apos;, &apos;w&apos;, &apos;spike&apos;], inputs=(&apos;input&apos;, Iext)) runner(500) spike = runner.mon.spike.squeeze() s_idx = np.where(spike)[0] # æ‰¾åˆ°æ‰€æœ‰å‘æ”¾åŠ¨ä½œç”µä½å¯¹åº”çš„index s_idx = np.concatenate(([0], s_idx, [len(spike) - 1])) # åŠ ä¸Šèµ·å§‹ç‚¹å’Œç»ˆæ­¢ç‚¹çš„index for i in range(len(s_idx) - 1): vs = runner.mon.V[s_idx[i]: s_idx[i + 1]] ws = runner.mon.w[s_idx[i]: s_idx[i + 1]] plt.plot(vs, ws, color=&apos;darkslateblue&apos;) # ç”»å‡ºè™šçº¿ x = V_reset plt.plot([group.V_reset, group.V_reset], w_range, &apos;--&apos;, color=&apos;grey&apos;, zorder=-1) plt.show() ``` ![image-20230825152925463](/BrainPy-course-notes/master_content/Notes.assets/image-20230825152925463.png) ## Dynamic analysis: bifurcation analysis ### Simple case $$ \frac{dx}{dt}=\sin(x)+I, $$ ```python bif = bp.analysis.Bifurcation1D( model=int_x, target_vars={&apos;x&apos;: [-10, 10]}, target_pars={&apos;Iext&apos;: [0., 1.5]}, resolutions={&apos;Iext&apos;: 0.005, &apos;x&apos;: 0.05} ) bif.plot_bifurcation(show=True) ``` ![image-20230825154227567](/BrainPy-course-notes/master_content/Notes.assets/image-20230825154227567.png) # Synapse models and their programming ## The biology of synapses ### Neurotransmitter &amp; Synapse When the action potential invades the axon terminals, it causes voltage-gated ð¶ð¶ð‘Žð‘Ž 2+ channels to open (1), which triggers vesicles to bind to the presynaptic membrane (2). Neurotransmitter is released into the synaptic cleft by exocytosis and diffuses across the cleft (3). Binding of the neurotransmitter to receptor molecules in the postsynaptic membrane completes the process of transmission (4). åŽ»æžåŒ–æ—¶é’™ç¦»å­å†…æµï¼Œä¸Žå›Šæ³¡ç›¸ç»“åˆï¼Œ...ï¼Œä¸Žå—ä½“ç»“åˆï¼Œæ‰“å¼€ç¦»å­é€šé“ï¼Œè¶…æžåŒ–ã€åŽ»æžåŒ–çŽ°è±¡ ![image-20230826100321307](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100321307.png) ![image-20230826100418911](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100418911.png) **Neurotransmitter leading to postsynaptic potential.** The binding of neurotransmitter to the postsynaptic membrane receptors changes the membrane potential ($V_m$). These postsynaptic potentials can be either excitatory (depolarizing the membrane), as shown here, or inhibitory (hyperpolarizing the membrane). ![image-20230826100531535](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100531535.png) ### Neurotransmitters å…´å¥‹æ€§ç¥žç»é€’è´¨ï¼š - ä¹™é…°èƒ†ç¢± (ACh) - å„¿èŒ¶é…šèƒº (catecholamines) - è°·æ°¨é…¸ (glutamate) - ç»„èƒº (histamine) - 5-ç¾Ÿè‰²èƒº (serotonin) - æŸäº›ç¥žç»è‚½ç±» (some of neuropeptides) æŠ‘åˆ¶æ€§ç¥žç»é€’è´¨ï¼š - GABA - ç”˜æ°¨é…¸ (glycine) - æŸäº›ç¥žç»è‚½ç±» (some of peptides) ![image-20230826100609904](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100609904.png) ### The postsynaptic response The aim of a synapse model is to describe accurately the postsynaptic response generated by the arrival of an action potential at a presynaptic terminal. 1. The fundamental quantity to be modelled is the time course of the postsynaptic receptor conductance 2. The models: - Simple phenomenological waveforms - More complex kinetic schemes that are analogous to the models of membrane- bound ion channels ![image-20230826100701580](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100701580.png) å»ºæ¨¡è¿™ç§å“åº”æ¨¡å¼ï¼Œæ‰“å¼€å…³é—­çš„æ¦‚çŽ‡... ## Phenomenological synapse models ### Exponential Model ![image-20230826100738460](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100738460.png) **Assumption**: - The release of neurotransmitter, its diffusion across the cleft, the receptor binding, and channel opening all happen very quickly, so that the channels instantaneously jump from the closed to the open state. channelä¼šçž¬é—´å¢žåŠ ç„¶åŽé€æ¸å…³é—­ $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}e^{-(t-t_{0})/\tau} \\ \begin{matrix}\bullet&amp;\tau \ \text{is the time constant}\\\bullet&amp;t_0 \ \text{is the time of the pre-synaptic spike}\\\bullet&amp;\bar{g_{syn}}\ \text{is the maximal conductance}\end{matrix} $$ -&amp;gt; corresponding differential equation $$ \tau\frac{dg_{\mathrm{syn}}(t)}{dt}=-g_{\mathrm{syn}}(t)+\bar{g}_{\mathrm{syn}}\delta\left(t_{0}-t\right) $$ - Can fit with experimental data. - A good approximation for GABA A and AMPA, because the rising phase is much shorter than their decay phase. ### Dual Exponential Model ![image-20230826101203059](/BrainPy-course-notes/master_content/Notes.assets/image-20230826101203059.png) exponential modelä¸Šå‡çš„å¤ªå¿«ï¼Œä¸å¤ªç¬¦åˆæŸäº›synapse Dual exponential synapse provides a general way to describe the synaptic conductance with different rising and decay time constants. $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}\frac{\tau_{1}\tau_{2}}{\tau_{1}-\tau_{2}}\left(\exp\left(-\frac{t-t_{0}}{\tau_{1}}\right)-\exp\left(-\frac{t-t_{0}}{\tau_{2}}\right)\right) \\ \begin{matrix} \bullet &amp;t_1\ \text{is the decay synaptic time constant} \\ \bullet &amp;\tau_2\ \text{is the rise synaptic time constant} \\ \bullet &amp;t_0\ \text{is the time of the pre-synaptic spike} \\ \bullet &amp;\bar{g}_{syn}\ \text{is the maximal conductance} \end{matrix} $$ -&amp;gt;corresponding differential equation $$ \begin{aligned} &amp;g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}g \\ &amp;\frac{dg}{dt}=-\frac{g}{\tau_{\mathrm{decay}}}+h \\ &amp;\frac{dh}{dt}&amp; =-\frac{h}{\tau_{\mathrm{rise}}}+\delta\left(t_{0}-t\right), \end{aligned} $$ The time course of most synaptic conductance can be well described by this sum of two exponentials. ### Synaptic time constants ![image-20230826101544786](/BrainPy-course-notes/master_content/Notes.assets/image-20230826101544786.png) http://compneuro.uwaterloo.ca/research/constants-constraints/neurotransmitter-time-constants-pscs.html #### AMPA synapse - $t_{decay}$ = 0.18 ms in the auditory system of the chick nucleus magnocellularis (Trussell, 1999). - $t_{rise}$ 25 ms and $\tau_{decay}$ =0.77 ms in dentate gyrus basket cells (Geiger et al., 1997). - $t_{rise}$ = 0.2 ms and $\tau_{decay}$ =1.7 ms in in neocortical layer 5 pyramidal neurons (Hausser and Roth, 1997b). - Reversal potential is nearly 0 mV. #### NMDA synapse - The decay time constants (at near-physiological temperature): - 19 ms in dentate gyrus basket cells (Geiger et al., 1997), - 26 ms in neocortical layer 2/3 pyramidal neurons (Feldmeyer et al., 2002), - 89 ms in CA1 pyramidal cells (Diamond, 2001). - The rise time constants are about 2 ms (Feldmeyer et al., 2002). - Reversal potential is nearly 0 mV. #### GABA$_A$ synapse - GABAergic synapses from dentate gyrus basket cells onto other basket cells are faster: $t_{rise}$ = 0.3 ms and $t_{decay}$ = 2.5 ms (Bartos et al., 2001) than synapses from basket cells to granule cells: $t_{rise}$ = 0.26 ms and $t_{decay}$ = 6.5 ms (Kraushaar and Jonas, 2000). - Reversal potential is nearly -80 mV. #### GABA$_B$ synapse - Common models use models with a rise time of about 25-50 ms, a fast decay time in the range of 100-300ms and a slow decay time of 500-1000 ms. - Reversal potential is nearly -90 mV. ### General property of synaptic time constants - The time constants of synaptic conductance vary widely among synapse types. - The synaptic kinetics tends to accelerate during development (T. Takahashi, Neuroscience Research, 2005) . - The synaptic kinetics becomes substantially faster with increasing temperature. ![image-20230826102033433](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102033433.png) ### Current- and Conductance-based Response ![image-20230826102042614](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102042614.png) #### Conductance-based Response Most synaptic ion channels, such as AMPA and GABA, display an approximately linear current-voltage relationship when they open. ![image-20230826102113670](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102113670.png) **For example**: The synapse is located on a thin dendrite, because the local membrane potential V changes considerably when the synapse is activated. #### Current-based Response In some case, we can also approximate the synapses as sources of current and not a conductance. ![image-20230826102150487](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102150487.png) **For example**: The excitatory synapse on a large compartment, because the depolarization of the membrane is small. ## Programming of phenomenological synapse models ### `ProjAlignPostMg2` ![Image Name](https://cdn.kesci.com/upload/rzz4o4uyar.png?imageView2/0/w/960/h/960) ```python brainpy.dyn.ProjAlignPostMg2( pre, delay, comm, syn, out, post ) ``` - ``pre (JointType[DynamicalSystem, AutoDelaySupp])``: The pre-synaptic neuron group. - ``delay (Union[None, int, float])``: The synaptic delay. - ``comm (DynamicalSystem)``: The synaptic communication. - ``syn (ParamDescInit)``: The synaptic dynamics. - ``out (ParamDescInit)``: The synaptic output. - ``post (DynamicalSystem)`` The post-synaptic neuron group. åªéœ€è¦å»ºæ¨¡æ‰€æœ‰postçš„neurons ### CSR matrix ![Image Name](https://cdn.kesci.com/upload/rzz4on32hr.png?imageView2/0/w/960/h/960) ### Exponential Model The single exponential decay synapse model assumes the release of neurotransmitter, its diffusion across the cleft, the receptor binding, and channel opening all happen very quickly, so that the channels instantaneously jump from the closed to the open state. Therefore, its expression is given by $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} e^{-\left(t-t_{0}\right) / \tau} $$ where $\tau$ is the time constant, $t_0$ is the time of the pre-synaptic spike, $\bar{g}_{\mathrm{syn}}$ is the maximal conductance. The corresponding differential equation: $$ \frac{d g}{d t} = -\frac{g}{\tau_{decay}}+\sum_{k} \delta(t-t_{j}^{k}). $$ #### COBA Given the synaptic conductance, the COBA model outputs the post-synaptic current with $$ I_{syn}(t) = g_{\mathrm{syn}}(t) (E - V(t)) $$ ```python class ExponSparseCOBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.COBA.desc(E=E), post=post, ) ``` ```python class SimpleNet(bp.DynSysGroup): def __init__(self, E=0.): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = ExponSparseCOBA(self.pre, self.post, delay=None, prob=1., g_max=1., tau=5., E=E) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` ```python def run_a_net(net): indices = np.arange(1000) # 100 ms conductances, currents, potentials = bm.for_loop(net.step_run, indices, progress_bar=True) ts = indices * bm.get_dt() # --- similar to: # runner = bp.DSRunner(net) # conductances, currents, potentials = runner.run(100.) fig, gs = bp.visualize.get_figure(1, 3, 3.5, 4) fig.add_subplot(gs[0, 0]) plt.plot(ts, conductances) plt.title(&apos;Syn conductance&apos;) fig.add_subplot(gs[0, 1]) plt.plot(ts, currents) plt.title(&apos;Syn current&apos;) fig.add_subplot(gs[0, 2]) plt.plot(ts, potentials) plt.title(&apos;Post V&apos;) plt.show() ``` #### CUBA Given the conductance, this model outputs the post-synaptic current with a identity function: $$ I_{\mathrm{syn}}(t) = g_{\mathrm{syn}}(t) $$ ```python class ExponSparseCUBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.CUBA.desc(), post=post, ) ``` ```python class SimpleNet2(bp.DynSysGroup): def __init__(self, g_max=1.): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = ExponSparseCUBA(self.pre, self.post, delay=None, prob=1., g_max=g_max, tau=5.) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` #### Dense connections Exponential synapse model with the conductance-based (COBA) output current and dense connections. ```python class ExponDenseCOBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.MaskedLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.COBA.desc(E=E), post=post, ) ``` ![Image Name](https://cdn.kesci.com/upload/rzz4p7x6dl.png?imageView2/0/w/960/h/960) Exponential synapse model with the current-based (COBA) output current and dense connections. ```python class ExponDenseCUBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.MaskedLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.CUBA.desc(), post=post, ) ``` ### `ProjAlignPreMg2` Synaptic projection which defines the synaptic computation with the dimension of presynaptic neuron group. ![Image Name](https://cdn.kesci.com/upload/rzz4pj1qmk.png?imageView2/0/w/960/h/960) ```python brainpy.dyn.ProjAlignPreMg2( pre, delay, syn, comm, out, post ) ``` - ``pre (JointType[DynamicalSystem, AutoDelaySupp])``: The pre-synaptic neuron group. - ``delay (Union[None, int, float])``: The synaptic delay. - ``syn (ParamDescInit)``: The synaptic dynamics. - ``comm (DynamicalSystem)``: The synaptic communication. - ``out (ParamDescInit)``: The synaptic output. - ``post (DynamicalSystem)`` The post-synaptic neuron group. ### Dual Exponential Model The dual exponential synapse model, also named as **difference of two exponentials model**, is given by: $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} \frac{\tau_{1} \tau_{2}}{\tau_{1}-\tau_{2}}\left(\exp \left(-\frac{t-t_{0}}{\tau_{1}}\right)-\exp \left(-\frac{t-t_{0}}{\tau_{2}}\right)\right) $$ where $\tau_1$ is the time constant of the decay phase, $\tau_2$ is the time constant of the rise phase, $t_0$ is the time of the pre-synaptic spike, $\bar{g}_{\mathrm{syn}}$ is the maximal conductance. The corresponding differential equation: $$ \begin{aligned} &amp;g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} g \\ &amp;\frac{d g}{d t}=-\frac{g}{\tau_{\mathrm{decay}}}+h \\ &amp;\frac{d h}{d t}=-\frac{h}{\tau_{\text {rise }}}+ \delta\left(t_{0}-t\right), \end{aligned} $$ The alpha function is retrieved in the limit when both time constants are equal. ```python class DualExpSparseCOBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau_decay, tau_rise, E): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.DualExpon.desc(pre.num, tau_decay=tau_decay, tau_rise=tau_rise), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.COBA(E=E), post=post, ) ``` ```python class SimpleNet4(bp.DynSysGroup): def __init__(self, E=0.): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = DualExpSparseCOBA(self.pre, self.post, delay=None, prob=1., g_max=1., tau_decay=5., tau_rise=1., E=E) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` ## Biophysical synapse models ### Limitations of phenomenological models æ‰“å¼€çš„æ•°é‡æ˜¯æœ‰é™çš„ï¼Œè€Œä¸”æœ‰é¥±å’ŒæœŸ 1. Saturation of postsynaptic receptors by previously released transmitter. 2. Certain receptor types also exhibit desensitization that prevents them (re)opening for a period after transmitter-binding, like sodium channels underlying action potential. ![image-20230826111443117](/BrainPy-course-notes/master_content/Notes.assets/image-20230826111443117.png) ### Linetic/Markov models ![image-20230826111733654](/BrainPy-course-notes/master_content/Notes.assets/image-20230826111733654.png) - The simplest kinetic model is a two-state scheme in which receptors can be either closed, ð¶, or open, ð‘‚, and the transition between states depends on transmitter concentration, [ð‘‡], in the synaptic cleft: - ð›¼ and ð›½ are voltage-independent forward and backward rate constants. - ð¶ and ð‘‚ can range from 0 to 1, and describe the fraction of receptors in the closed and open states, respectively. - The synaptic conductance is: $g_{syn}(t)=\bar{g}_{max}g(t)$ ### AMPA/GABA$_A$ synapse model $$ \begin{aligned}\frac{ds}{dt}&amp;=\alpha[T](1-s)-\beta s\\I&amp;=\tilde{g}s(V-E)\end{aligned} $$ - ð›¼[ð‘‡] denotes the transition probability from state (1âˆ’ð‘ ) to state (ð‘ ) - ð›½ represents the transition probability of the other direction - ð¸ is a reverse potential, which can determine whether the direction of ð¼ is inhibition or excitation. - ð¸ = 0 ð‘šð‘šð‘‰ð‘‰ =&amp;gt; Excitatory synapse [AMPA] - ð¸ = âˆ’80 ð‘šð‘šð‘‰ð‘‰ =&amp;gt; Inhibitory synapse [GABA A ] ### Comparison ![image-20230826111950713](/BrainPy-course-notes/master_content/Notes.assets/image-20230826111950713.png) ### NMDA synapse model ![image-20230826112027689](/BrainPy-course-notes/master_content/Notes.assets/image-20230826112027689.png) ![image-20230826112034481](/BrainPy-course-notes/master_content/Notes.assets/image-20230826112034481.png) $$ \begin{aligned} &amp;\frac{ds}{dt} =\alpha[T](1-s)-\beta s \\ &amp;I=\tilde{g}sB(V)(V-E) \\ &amp;B(V )=\frac{1}{1+\exp(-0.062V)[Mg^{2+}]_{o}/3.57} \end{aligned} $$ The magnesium block of the NMDA receptor channel is an extremely fast process compared to the other kinetics of the receptor (Jahr and Stevens 1990a, 1990b). The block can therefore be accurately modeled as an instantaneous function of voltage(Jahr and Stevens 1990b). where $[Mg^{2+}]$ is the external magnesium concentration (1 to 2mM inphysiological conditions) ## Programming of biophysical synapse models ### AMPA synapse model ```python class AMPA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, E=0.): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.AMPA.desc(pre.num, alpha=0.98, beta=0.18, T=0.5, T_dur=0.5), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.COBA(E=E), post=post, ) ``` ```python class SimpleNet(bp.DynSysGroup): def __init__(self, syn_cls): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = syn_cls(self.pre, self.post, delay=None, prob=1., g_max=1.) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` ```python def run_a_net(net, duration=100): indices = np.arange(int(duration/bm.get_dt())) # duration ms conductances, currents, potentials = bm.for_loop(net.step_run, indices, progress_bar=True) ts = indices * bm.get_dt() # --- similar to: # runner = bp.DSRunner(net) # conductances, currents, potentials = runner.run(100.) fig, gs = bp.visualize.get_figure(1, 3, 3.5, 4) fig.add_subplot(gs[0, 0]) plt.plot(ts, conductances) plt.title(&apos;Syn conductance&apos;) fig.add_subplot(gs[0, 1]) plt.plot(ts, currents) plt.title(&apos;Syn current&apos;) fig.add_subplot(gs[0, 2]) plt.plot(ts, potentials) plt.title(&apos;Post V&apos;) plt.show() ``` ### $\text{GABA}_A$ synapse model ```python class GABAa(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, E=-80.): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.GABAa.desc(pre.num, alpha=0.53, beta=0.18, T=1.0, T_dur=1.0), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.COBA(E=E), post=post, ) ``` ```python run_a_net(SimpleNet(syn_cls=GABAa)) ``` ### NMDA synapse model ```python class NMDA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, E=0.0): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.NMDA.desc(pre.num, a=0.5, tau_decay=100., tau_rise=2.), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.MgBlock(E=E), post=post, ) ``` ```python run_a_net(SimpleNet(NMDA)) ``` ### Kinetic synapse models are more realistic ```python class SimpleNet5(bp.DynSysGroup): def __init__(self, freqs=10.): super().__init__() self.pre = bp.dyn.PoissonGroup(1, freqs=freqs) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = NMDA(self.pre, self.post, delay=None, prob=1., g_max=1., E=0.) def update(self): self.pre() self.syn() self.post() # monitor the following variables return self.syn.proj.refs[&apos;syn&apos;].g, self.post.V ``` ```python def compare_freqs(freqs): fig, _ = bp.visualize.get_figure(1, 1, 4.5, 6.) for freq in freqs: net = SimpleNet5(freqs=freq) indices = np.arange(1000) # 100 ms conductances, potentials = bm.for_loop(net.step_run, indices, progress_bar=True) ts = indices * bm.get_dt() plt.plot(ts, conductances, label=f&apos;{freq} Hz&apos;) plt.legend() plt.ylabel(&apos;g&apos;) plt.show() ``` ```python compare_freqs([10., 100., 1000., 10000.]) ``` ### How to customize a synapse #### Preparations `ProjAlignPostMg2` and `ProjAlignPreMg2` #### Exponential Model ```python class Exponen(bp.dyn.SynDyn, bp.mixin.AlignPost): def __init__(self, size, tau): super().__init__(size) # parameters self.tau = tau # variables self.g = bm.Variable(bm.zeros(self.num)) # integral self.integral = bp.odeint(lambda g, t: -g/tau, method=&apos;exp_auto&apos;) def update(self, pre_spike=None): self.g.value = self.integral(g=self.g.value, t=bp.share[&apos;t&apos;], dt=bp.share[&apos;dt&apos;]) if pre_spike is not None: self.add_current(pre_spike) return self.g.value def add_current(self, x): # specical for bp.mixin.AlignPost self.g += x def return_info(self): return self.g ``` #### AMPA Model ```python class AMPA(bp.dyn.SynDyn): def __init__(self, size, alpha= 0.98, beta=0.18, T=0.5, T_dur=0.5): super().__init__(size=size) # parameters self.alpha = alpha self.beta = beta self.T = T self.T_duration = T_dur # functions self.integral = bp.odeint(method=&apos;exp_auto&apos;, f=self.dg) # variables self.g = bm.Variable(bm.zeros(self.num)) self.spike_arrival_time = bm.Variable(bm.ones(self.num) * -1e7) def dg(self, g, t, TT): return self.alpha * TT * (1 - g) - self.beta * g def update(self, pre_spike): self.spike_arrival_time.value = bm.where(pre_spike, bp.share[&apos;t&apos;], self.spike_arrival_time) TT = ((bp.share[&apos;t&apos;] - self.spike_arrival_time) åšæ—¶é—´å¹³å‡ STP based on firing rate $$ \begin{gathered} \frac{du(t)}{dt}=\frac{-u(t)}{\tau_{f}}+U_{sE}(1-u^{-})\delta\big(t-t_{sp}\big), \\ \frac{dx(t)}{dt}=\frac{1-x(t)}{\tau_{d}}-u^{+}x^{-}\delta\big(t-t_{sp}\big), \\ \frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{s}}+Au^{+}x^{-}\delta\big(t-t_{sp}\big), \\ u^{+}=\lim_{t-t_{sp\rightarrow0^{+}}}u(t), \end{gathered} $$ ![image-20230826155016657](/BrainPy-course-notes/master_content/Notes.assets/image-20230826155016657.png) ä¸¢æŽ‰æ—¶é—´å˜åŒ–çš„å…·ä½“ç»†èŠ‚ï¼ŒæŠ“ä½äº†é‡è¦è¶‹åŠ¿ ### Theoretical analysis of the rate model Suppose the pre-synaptic firing rate keeps as constant, we can calculate the stationary response $$ u_{st}=\frac{U_{SE}R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}},\quad u_{st}^{+}=U_{SE}\frac{1+R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}},\quad x_{st}=\frac{1}{1+u_{st}^{+}\tau_{d}R_{0}}, $$ $$ EPSC_{st}=Au_{st}^{+}x_{st}=A\frac{u_{st}^{+}}{1+u_{st}^{+}\tau_{d}R_{0}},\quad PSV_{st}\propto g_{st}=\tau_{s}Au_{st}^{+}x_{st}R_{0}=A\frac{u_{st}^{+}R_{0}}{1+u_{st}^{+}\tau_{d}R_{0}}, $$ ![image-20230826155234134](/BrainPy-course-notes/master_content/Notes.assets/image-20230826155234134.png) ### Frequency-dependent Gain control of spike information $$ \begin{gathered} u_{st}^{+}=U_{SE}\frac{1+R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}}, \\ x_{st}=\frac{1}{1+u_{st}^{+}\tau_{d}R_{0}}, \\ EPSC_{st}=Au_{st}^{+}x_{st}=A\frac{u_{st}^{+}}{1+u_{st}^{+}\tau_{d}R_{0}}, \end{gathered} $$ Peak frequency: $\theta\sim\frac{1}{\sqrt{U\tau_{f}\tau_{d}}}$ ### Simulation of Frequency-dependent Gain control ![image-20230826155715445](/BrainPy-course-notes/master_content/Notes.assets/image-20230826155715445.png) ## Effects on network dynamics ### STP modeling Working memory ![image-20230826160102332](/BrainPy-course-notes/master_content/Notes.assets/image-20230826160102332.png) ![image-20230826160113456](/BrainPy-course-notes/master_content/Notes.assets/image-20230826160113456.png) # E-I Balanced Neural Network ## Irregular Spiking of Neurons ### Signal process of single neuron External Stimulus -&amp;gt; Single neuron model $$ \begin{aligned}\tau&amp;\frac{\mathrm{d}V}{\mathrm{d}t}=-(V-V_\text{rest })+RI(t)\\\\&amp;\text{if}V&amp;gt;V_\text{th},\quad V\leftarrow V_\text{reset }\text{last}t_\text{ref}\end{aligned} $$ -&amp;gt; ... -&amp;gt; Perception or action çœŸæ­£çš„ç¥žç»å…ƒå¹¶ä¸æ˜¯LIF modelçš„è¾“å‡º ![image-20230827100647851](/BrainPy-course-notes/master_content/Notes.assets/image-20230827100647851.png) Simulation ![image-20230827100706310](/BrainPy-course-notes/master_content/Notes.assets/image-20230827100706310.png) Neuron recorded in vivo ### Irregular Spiking of Neurons ![image-20230827092807270](/BrainPy-course-notes/master_content/Notes.assets/image-20230827092807270.png) #### Statistical Description of Spikes ç”¨ä»¥ä¸‹çš„å˜é‡æ¥è¿›è¡Œç»Ÿè®¡æè¿° - Firing Rate Rate = average over time(single neuron, single run) Spike count $v=\frac{n_{sp}}{T}$ - ISI(Interspike interval distributions) average ISI $\overline{\Delta t}=\frac{1}{n_{sp}-1}\sum_{i=1}^{n_{sp}-1}\Delta t_{i}$ standard deviation ISI: $\sigma_{\Delta t}^{2}=\frac{1}{n_{sp}-1}\sum_{i=1}^{n_{sp}-1}(\Delta t_{i}-\overline{\Delta t})^{2}$ - $C_V$(Coefficient of variation, Fano factor) **çª„è¿˜æ˜¯å®½çš„åˆ†å¸ƒ** ä¿¡æ¯è¡¨å¾æœ‰å¤šå¼ºçš„ä¸ç¨³å®šæ€§ $C_{V}=\sigma_{\Delta t}^{2}/\overline{\Delta t}$ #### Poisson Process In probability theory and statistics, the Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known **constant mean rate** and **independently** of the time since the last eventã€‚ $$ \begin{aligned} &amp;P(X=k\mathrm{~events~in~interval~}t)=e^{-rt}\frac{(rt)^{k}}{k!} \\ &amp;\mathrm{mean:}\quad\overline{X}=rt \\ &amp;\mathrm{variance}:\quad\sigma^{2}=rt\\ &amp;\mathrm{Fano factor:}\quad\frac{\sigma^{2}}{X}=1 \end{aligned} $$ Fano factor -&amp;gt; noise-to-signal ratio #### Irregular Spiking of Neurons LIFåœ¨å•ä¸ªç¥žç»å…ƒçš„æƒ…å†µä¸‹æ˜¯åŸºæœ¬æ²¡æœ‰å¤ªå¤§é—®é¢˜çš„ï¼Œåœ¨æ•´ä¸ªç½‘ç»œä¸­ä¼šå—ç½‘ç»œä¿¡æ¯è°ƒæŽ§ ![image-20230827093614607](/BrainPy-course-notes/master_content/Notes.assets/image-20230827093614607.png) #### Why Irregular? - ä¸å®Œå…¨æ˜¯inputå½±å“çš„ - ä¸èƒ½ç®€å•æ¥è¡¡é‡ On average, a cortical neuron receives inputs from 1000~10000 connected neurons. -&amp;gt; averaged noise ~ 0 ## E-I Balanced Network $$ \begin{gathered} \tau\frac{du_{i}^{E}}{dt}=-u_{i}^{E}+\sum_{j=1}^{K_{E}}J_{EE}r_{j}^{E}+\sum_{j=1}^{K_{I}}J_{EI}r_{j}^{I}+I_{i}^{E} \\ \tau\frac{du_{i}^{I}}{dt}=-u_{i}^{I}+\sum_{j=1}^{K_{I}}J_{II}r_{j}^{I}+\sum_{j=1}^{K_{E}}J_{IE}r_{j}^{E}+I_{i}^{I} \end{gathered} $$ ![image-20230827093708220](/BrainPy-course-notes/master_content/Notes.assets/image-20230827093708220.png) Sparse &amp; random connections:$1\ll K_{\mathrm{E}},K_{1}\ll N_{\mathrm{E}},N_{\mathrm{I}}$ . Neurons fire largely independently to each other. $$ \begin{gathered} \text{Single neuron fires irregularly } r_j^E, r_j^{\prime} \text{with mean rate } \mu \text{and variance } \sigma^2.\\ \text{The mean of recurrent input received by E neuron:} \\ \sim K_{E}J_{EE}\mu-K_{I}J_{EI}\mu \\ \text{The variance of recurrent input received by E neuron:} \\ \sim K_{E}(J_{EE})^{2}\sigma^{2}+K_{I}(J_{EI})^{2}\sigma^{2} \\ \begin{gathered} \\ \text{The balanced condition:} \\ K_{E}J_{EE}-K_{l}J_{El}{\sim}0(1) \\ J_{EE}=\frac{1}{\sqrt{K_{E}}},J_{EI}=\frac{1}{\sqrt{K_{I}}},K_{E}(J_{EE})^{2}\sigma^{2}+K_{I}(J_{EI})^{2}\sigma^{2}\sim O(1) \end{gathered} \end{gathered} $$ $$ \begin{aligned}\frac{I_E}{I_I}&amp;&amp;gt;\frac{J_E}{J_I}&amp;&amp;gt;1\\\\J_E&amp;&amp;gt;1\\\\\text{r not too big}\end{aligned} $$ $$ \overline{I_a}=\overline{F_a}+\overline{R_a}=\sqrt{N}(f_a\mu_0+w_{aE}r_E+w_{aI}r_I),\quad a=E,I,\\ \begin{gathered} w_{ab}~=~p_{ab}j_{ab}q_{b} \\ J_{ij}^{ab}~=~j_{ab}/\sqrt{N}; \\ \frac{f_{E}}{f_{I}}&amp;gt;\frac{w_{EI}}{w_{II}}&amp;gt;\frac{w_{EE}}{w_{IE}}. \end{gathered} $$ ## BrainPy Simulation ### Simulation LIF neuron 4000 (E/I=4/1, P=0.02) ðœ = 20 ms ð‘‰ð‘Ÿð‘’ð‘ ð‘¡ = -60 mV Spiking threshold: -50 mV Refractory period: 5 ms $$ \begin{gathered} \tau\frac{dV}{dt}=(V_{\mathrm{rest}}-V)+I \\ I=g_{exc}(E_{exc}-V)+g_{inh}(E_{inh}-V)+I_{\mathrm{ext}} \end{gathered} \ \ \ \ \ \ \begin{aligned}\tau_{exc}&amp;\frac{dg_{exc}}{dt}=-g_{exc}\\\tau_{inh}&amp;\frac{dg_{inh}}{dt}=-g_{inh}\end{aligned} $$ $$ \begin{array}{l}E_\mathrm{exc}=0\text{mV}\mathrm{and}E_\mathrm{inh}=-80\text{mV},I_\mathrm{ext}=20.\\\tau_\mathrm{exc}=5\text{ ms},\tau_\mathrm{inh}=10\text{ ms},\Delta g_\mathrm{exc}=0.6\text{ and}\Delta g_\mathrm{inh}=6.7.\end{array} $$ ![image-20230827094502860](/BrainPy-course-notes/master_content/Notes.assets/image-20230827094502860.png) ### Synaptic Computation ```python # åŸºäºŽ align post Exponential synaptic computation class Exponential(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E, label=None): super().__init__() self.pron = bp.dyn.ProjAlignPost2( pre=pre, delay=delay, comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), # éšæœºè¿žæŽ¥ syn=bp.dyn.Expon(size=post.num, tau=tau), # Exponential synapse out=bp.dyn.COBA(E=E), # COBA network post=post, out_label=label ) ``` ### E-I Balanced Network ```python # æž„å»º E-I Balanced Network class EINet(bp.DynamicalSystem): def __init__(self, ne=3200, ni=800): super().__init__() # bp.neurons.LIF() self.E = bp.dyn.LifRef(ne, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Normal(-55., 2.)) self.I = bp.dyn.LifRef(ni, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Normal(-55., 2.)) #### E2E, E2I, I2E, I2I Exponential synaptic computation # delay=0, prob=0.02, g_max_E=0.6, g_max_I=6.7, tau_E=5, tau_I=10, # reversal potentials E_E=0, E_E=-80, label=EE,EI,IE,II self.E2E = Exponential(self.E, self.E, 0., 0.02, 0.6, 5., 0., &apos;EE&apos;) self.E2I = Exponential(self.E, self.I, 0., 0.02, 0.6, 5., 0., &apos;EI&apos;) self.I2E = Exponential(self.I, self.E, 0., 0.02, 6.7, 5., -80., &apos;IE&apos;) self.I2I = Exponential(self.I, self.I, 0., 0.02, 6.7, 5., -80., &apos;II&apos;) ``` ```python def update(self, inp=0.): # æ›´æ–°çªè§¦ä¼ å…¥ç”µæµ self.E2E() self.E2I() self.I2E() self.I2I() # æ›´æ–°ç¥žç»å…ƒç¾¤ä½“ self.E(inp) self.I(inp) # è®°å½•éœ€è¦ monitorçš„å˜é‡ E_E_inp = self.E.sum_inputs(self.E.V, label=&apos;EE&apos;) #E2Eçš„è¾“å…¥ I_E_inp = self.E.sum_inputs(self.E.V, label=&apos;IE&apos;) # I2Eçš„è¾“å…¥ return self.E.spike, self.I.spike, E_E_inp, I_E_inp ``` ![image-20230827110737553](/BrainPy-course-notes/master_content/Notes.assets/image-20230827110737553.png) ![image-20230827110746410](/BrainPy-course-notes/master_content/Notes.assets/image-20230827110746410.png) ## Properties of E-I Balanced Network - Linear encoding External input strength is â€œlinearlyâ€ encoded by the mean firing rate of the neural population - Fast Response The network responds rapidly to abrupt changes of the input ### Noise speeds up computation å¿«é€Ÿç›¸åº”çš„åŽŸç†ï¼Œå‡åŒ€åˆ†å¸ƒåœ¨é˜ˆå€¼ä¸‹é¢çš„ç©ºé—´ - A neural ensemble jointly encodes stimulus information; - Noise randomizes the distribution of neuronal membrane potentials; - Those neurons (red circle) whose potentials are close to the threshold will fire rapidly; - If the noisy environment is proper, even for a small input, a certain number of neurons will fire instantly to report the presence of a stimulus. ![image-20230827113451626](/BrainPy-course-notes/master_content/Notes.assets/image-20230827113451626.png) # Continuous Attractor Neural Network ## Attractor Models ### The concept of attractor dynamics Different types of attractors: Point attractors, Line attractors, Ring attractors, Plane attractors, Cyclic attractors, Chaotic attractors ![image-20230827140250173](/BrainPy-course-notes/master_content/Notes.assets/image-20230827140250173.png) ç¨³æ€ï¼Œèƒ½é‡æ¢¯åº¦å¸å¼•åˆ°attractor ### Discrete attractor Network Model: Hopfield Model $S_i=\pm1$: the neuronal state $W_{ij}$ : the neuronal connection The network dynamics: $$ S_{i}=\mathrm{sign}\bigg(\sum_{j}w_{ij}S_{j}-\theta\bigg),\quad\mathrm{sign}(x)=1,\mathrm{for}x&amp;gt;0;-1,\mathrm{otherwise} $$ Updating rule: synchronous or asynchronous Consider the network stores $p$ pattern, $\xi_{i}^{\mu},\mathrm{for}\mu=1,\ldots p;i=1,\ldots N$ Setting $w_{ij}=\frac{1}{N}\sum_{\mu=1}^{p}\xi_{i}^{\mu}\xi_{j}^{\mu}$ ![image-20230827140827784](/BrainPy-course-notes/master_content/Notes.assets/image-20230827140827784.png) #### Energy space of Hopfield network $$ \begin{aligned} &amp;\text{Energy function: }E=-\frac{1}{2}\sum_{i,j}w_{ij}S_{i}S_{j}+\theta\sum_{i}S_{i} \\ &amp;\mathrm{Consider}S_{i}\mathrm{~is~updated},S_{i}(t+1)=sign[\sum_{j}w_{ij}S_{j}(t)-\theta] \\ &amp;\Delta E=E(t+1)-E(t)\\ &amp;=-[S_{i}(t+1)-S_{i}(t)]\sum_{j}w_{ij}S_{j}(t)+\theta\left[S_{i}(t+1)-S_{i}(t)\right] \\ &amp;=-[S_{i}(t+1)-S_{i}(t)][\sum_{j}w_{ij}S_{j}(t)-\theta] \\ &amp;\leq0 \end{aligned} $$ åŒæ ·æ¿€æ´»åŒæ ·patternçš„ç¥žç»å…ƒï¼Œ~å¸å¼•å­ #### Auto-associative memory in Hopfield Network A partial/noisy input can retrieve the related memory pattern ![image-20230827141253326](/BrainPy-course-notes/master_content/Notes.assets/image-20230827141253326.png) #### Persistent activity in working memory After the removal of external input, the neurons in the network encoding the stimulus continue to fire persistently. ![image-20230827141421796](/BrainPy-course-notes/master_content/Notes.assets/image-20230827141421796.png) ## Continuous Attractor Neural Network ### Neural coding #### Low-dimensional continuous feature ![image-20230827142520189](/BrainPy-course-notes/master_content/Notes.assets/image-20230827142520189.png) #### Continuous Attractor neural network ![image-20230827142606695](/BrainPy-course-notes/master_content/Notes.assets/image-20230827142606695.png) ### CANN: A rate-based recurrent circuit model $$ \begin{aligned}\tau\frac{\partial U(x,t)}{\partial t}&amp;=-U(x,t)+\rho\int f(x,x&apos;)r(x&apos;,t)dx&apos;+l^{ext}(1)\\r(x,t)&amp;=\frac{U^2(x,t)}{1+k\rho\int U^2(x,t)dx}\quad(2)\\J(x,x&apos;)&amp;=\frac{J_0}{\sqrt{2\pi}a}\exp\left[-\frac{(x-x&apos;)^2}{2a^2}\right](3)\end{aligned} $$ ré¢‘çŽ‡ï¼ŒJå¼ºåº¦ï¼ŒU decay #### A Continuous family of attractor states åšå¹³ç§»çš„æ”¹å˜ï¼Œå˜åŒ–ä¼šè¢«ä¿ç•™ï¼Œline attractorï¼Œå—åˆ°ç¼–ç è¿žç»­åˆºæ¿€ ![image-20230827143707784](/BrainPy-course-notes/master_content/Notes.assets/image-20230827143707784.png) #### Stability analysis derive continuous attractor dynamics åªéœ€è¦çœ‹åœ¨åŽŸå§‹çŠ¶æ€åŠ å…¥ä¸€ä¸ªå°é‡é¡¹ï¼Œå†ä»£å…¥å›ž Consider small fluctuations around a stationary state at z: Projecting $\delta U$ on the $i$th right eigenvector of $F(\delta U)_i(t)=(\delta U)_i(0)e^{-(1-\lambda _i)t/\tau}$ Two cases: - If $\lambda _i ## Computation with CANN ### Persistent activity for working memory When the global inhibition is not too strong, the network spontaneously hold bump activity: $$ k\frac{\tau}{\tau _v}$, Travelling wave ![image-20230827150543244](/BrainPy-course-notes/master_content/Notes.assets/image-20230827150543244.png) #### Levy flights vs. Brownian motion ![image-20230827150851309](/BrainPy-course-notes/master_content/Notes.assets/image-20230827150851309.png) #### LÃ©vy flights in ecology and human cogniDve behaviors ç”Ÿç‰©å­¦å¤§å¤šè¿åŠ¨æœä»Žlevy flights ### Noisy adaptation generates Levy flight in CANN ![image-20230827151343126](/BrainPy-course-notes/master_content/Notes.assets/image-20230827151343126.png) ### Time Delay in Neural Signal Transmission ![image-20230827151622032](/BrainPy-course-notes/master_content/Notes.assets/image-20230827151622032.png) ### Anticipatory Head Direction Signals in Anterior Thalamus æœ‰é¢„æµ‹ç­–ç•¥ï¼Œå®žçŽ°æŠµæ¶ˆä¿¡æ¯ä¼ é€’çš„delay CANNåŠ å…¥è´Ÿåé¦ˆæœºåˆ¶æ˜¯å¯ä»¥å®žçŽ°é¢„æµ‹çš„ ![image-20230827152731895](/BrainPy-course-notes/master_content/Notes.assets/image-20230827152731895.png) ### CANN with STP $$ \begin{gathered} \tau{\frac{\mathrm{d}U(x,t)}{\mathrm{d}t}} {\cal O}=-U(x,t)+\rho\int g^{+}(x)h(x^{\prime},t)J(x,x^{\prime})r(x^{\prime},t)dx^{\prime}+I^{ext}(x,t)(1) \\ \frac{dg(x,t)}{dt}=-\frac{g(x,t)}{\tau_{f}}+G(1-g^{-}(x))r(x^{\prime},t)\quad(2) \\ \frac{dh(x,t)}{dt}=\frac{1-h(x,t)}{\tau_{d}}-g^{+}(x)h(x,t)r(x^{\prime},t)\quad(3) \\ r(x,t)={\frac{U^{2}(x,t)}{1+k\rho\int U^{2}(x,t)dx}}\quad(4) \end{gathered} $$ ## Programming in BrainPy ### Customize a ring CANN in brainpy In simulations, we can not simulate a CANN encoding features ranging $(-\inf, \inf)$. Instead, we simulate a ring attractor network which encodes features ranging $(-\pi, \pi)$. Note that the distance on a ring should be: $$ dist_{ring}(x,x&apos;) = min(|x-x&apos;|,2\pi-|x-x&apos;|) $$ ![Image Name](https://cdn.kesci.com/upload/s01apgi89t.png?imageView2/0/w/320/h/320) ```python class CANN1D(bp.NeuGroupNS): def __init__(self, num, tau=1., k=8.1, a=0.5, A=10., J0=4., z_min=-bm.pi, z_max=bm.pi, **kwargs): super(CANN1D, self).__init__(size=num, **kwargs) # åˆå§‹åŒ–å‚æ•° self.tau = tau self.k = k self.a = a self.A = A self.J0 = J0 # åˆå§‹åŒ–ç‰¹å¾ç©ºé—´ç›¸å…³å‚æ•° self.z_min = z_min self.z_max = z_max self.z_range = z_max - z_min self.x = bm.linspace(z_min, z_max, num) self.rho = num / self.z_range self.dx = self.z_range / num # åˆå§‹åŒ–å˜é‡ self.u = bm.Variable(bm.zeros(num)) self.input = bm.Variable(bm.zeros(num)) self.conn_mat = self.make_conn(self.x) # è¿žæŽ¥çŸ©é˜µ # å®šä¹‰ç§¯åˆ†å‡½æ•° self.integral = bp.odeint(self.derivative) # å¾®åˆ†æ–¹ç¨‹ @property def derivative(self): du = lambda u, t, Irec, Iext: (-u + Irec + Iext) / self.tau return du # å°†è·ç¦»è½¬æ¢åˆ°[-z_range/2, z_range/2)ä¹‹é—´ def dist(self, d): d = bm.remainder(d, self.z_range) d = bm.where(d &amp;gt; 0.5 * self.z_range, d - self.z_range, d) return d # è®¡ç®—è¿žæŽ¥çŸ©é˜µ def make_conn(self, x): assert bm.ndim(x) == 1 d = self.dist(x - x[:, None]) # è·ç¦»çŸ©é˜µ Jxx = self.J0 * bm.exp( -0.5 * bm.square(d / self.a)) / (bm.sqrt(2 * bm.pi) * self.a) return Jxx # èŽ·å–å„ä¸ªç¥žç»å…ƒåˆ°poså¤„ç¥žç»å…ƒçš„è¾“å…¥ def get_stimulus_by_pos(self, pos): return self.A * bm.exp(-0.25 * bm.square(self.dist(self.x - pos) / self.a)) def update(self, x=None): _t = bp.share[&apos;t&apos;] u2 = bm.square(self.u) r = u2 / (1.0 + self.k * bm.sum(u2)) Irec = bm.dot(self.conn_mat, r) self.u[:] = self.integral(self.u, _t,Irec, self.input) self.input[:] = 0. # é‡ç½®å¤–éƒ¨ç”µæµ ``` ### Simulate the persistent activity of CANN after the removal of external input ```python def Persistent_Activity(k=0.1,J0=1.): # ç”ŸæˆCANN cann = CANN1D(num=512, k=k,J0=J0) # ç”Ÿæˆå¤–éƒ¨åˆºæ¿€ï¼Œä»Žç¬¬2åˆ°12msï¼ŒæŒç»­10ms dur1, dur2, dur3 = 2., 10., 10. I1 = cann.get_stimulus_by_pos(0.) Iext, duration = bp.inputs.section_input(values=[0., I1, 0.], durations=[dur1, dur2, dur3], return_length=True) noise_level = 0.1 noise = bm.random.normal(0., noise_level, (int(duration / bm.get_dt()), len(I1))) Iext += noise # è¿è¡Œæ•°å€¼æ¨¡æ‹Ÿ runner = bp.DSRunner(cann, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;]) runner.run(duration) # å¯è§†åŒ– def plot_response(t): fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann.x, I, label=&apos;Iext&apos;) ax.plot(cann.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() # plt.savefig(f&apos;CANN_t={t}.pdf&apos;, transparent=True, dpi=500) plot_response(t=10.) plot_response(t=20.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=1, frame_delay=40, show=True, ) plt.show() Persistent_Activity(k=0.1) ``` ### Simulate the tracking behavior of CANN ```python def smooth_tracking(): cann = CANN1D(num=512, k=8.1) # å®šä¹‰éšæ—¶é—´å˜åŒ–çš„å¤–éƒ¨åˆºæ¿€ v_ext = 1e-3 dur1, dur2, dur3 = 10., 10., 20 num1 = int(dur1 / bm.get_dt()) num2 = int(dur2 / bm.get_dt()) num3 = int(dur3 / bm.get_dt()) position = bm.zeros(num1 + num2 + num3) position[num1: num1 + num2] = bm.linspace(0., 1.5 * bm.pi, num2) position[num1 + num2: ] = 1.5 * bm.pi position = position.reshape((-1, 1)) Iext = cann.get_stimulus_by_pos(position) # è¿è¡Œæ¨¡æ‹Ÿ runner = bp.DSRunner(cann, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;]) runner.run(dur1 + dur2 + dur3) # å¯è§†åŒ– def plot_response(t, extra_fun=None): fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann.x, I, label=&apos;Iext&apos;) ax.plot(cann.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() if extra_fun: extra_fun() # plt.savefig(f&apos;CANN_tracking_t={t}.pdf&apos;, transparent=True, dpi=500) plot_response(t=10.) def f(): plt.annotate(&apos;&apos;, xy=(1.5, 10), xytext=(0.5, 10), arrowprops=dict(arrowstyle=&quot;-&amp;gt;&quot;)) plot_response(t=15., extra_fun=f) def f(): plt.annotate(&apos;&apos;, xy=(-2, 10), xytext=(-3, 10), arrowprops=dict(arrowstyle=&quot;-&amp;gt;&quot;)) plot_response(t=20., extra_fun=f) plot_response(t=30.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=5, frame_delay=50, show=True, ) plt.show() smooth_tracking() ``` ### Customize a CANN with SFASimulate the spontaneous traveling wave ```python class CANN1D_SFA(bp.NeuGroupNS): def __init__(self, num, m = 0.1, tau=1., tau_v=10., k=8.1, a=0.5, A=10., J0=4., z_min=-bm.pi, z_max=bm.pi, **kwargs): super(CANN1D_SFA, self).__init__(size=num, **kwargs) # åˆå§‹åŒ–å‚æ•° self.tau = tau self.tau_v = tau_v #time constant of SFA self.k = k self.a = a self.A = A self.J0 = J0 self.m = m #SFA strength # åˆå§‹åŒ–ç‰¹å¾ç©ºé—´ç›¸å…³å‚æ•° self.z_min = z_min self.z_max = z_max self.z_range = z_max - z_min self.x = bm.linspace(z_min, z_max, num) self.rho = num / self.z_range self.dx = self.z_range / num # åˆå§‹åŒ–å˜é‡ self.u = bm.Variable(bm.zeros(num)) self.v = bm.Variable(bm.zeros(num)) #SFA current self.input = bm.Variable(bm.zeros(num)) self.conn_mat = self.make_conn(self.x) # è¿žæŽ¥çŸ©é˜µ # å®šä¹‰ç§¯åˆ†å‡½æ•° self.integral = bp.odeint(self.derivative) # å¾®åˆ†æ–¹ç¨‹ @property def derivative(self): du = lambda u, t, v, Irec, Iext: (-u + Irec + Iext-v) / self.tau dv = lambda v, t, u: (-v + self.m*u) / self.tau_v return bp.JointEq([du, dv]) # å°†è·ç¦»è½¬æ¢åˆ°[-z_range/2, z_range/2)ä¹‹é—´ def dist(self, d): d = bm.remainder(d, self.z_range) d = bm.where(d &amp;gt; 0.5 * self.z_range, d - self.z_range, d) return d # è®¡ç®—è¿žæŽ¥çŸ©é˜µ def make_conn(self, x): assert bm.ndim(x) == 1 d = self.dist(x - x[:, None]) # è·ç¦»çŸ©é˜µ Jxx = self.J0 * bm.exp( -0.5 * bm.square(d / self.a)) / (bm.sqrt(2 * bm.pi) * self.a) return Jxx # èŽ·å–å„ä¸ªç¥žç»å…ƒåˆ°poså¤„ç¥žç»å…ƒçš„è¾“å…¥ def get_stimulus_by_pos(self, pos): return self.A * bm.exp(-0.25 * bm.square(self.dist(self.x - pos) / self.a)) def update(self, x=None): u2 = bm.square(self.u) r = u2 / (1.0 + self.k * bm.sum(u2)) Irec = bm.dot(self.conn_mat, r) u, v = self.integral(self.u, self.v, bp.share[&apos;t&apos;],Irec, self.input) self.u[:] = bm.where(u&amp;gt;0,u,0) self.v[:] = v self.input[:] = 0. # é‡ç½®å¤–éƒ¨ç”µæµ ``` ### Simulate the spontaneous traveling wave ```python def traveling_wave(num=512,m=0.1,k=0.1): # ç”ŸæˆCANN cann_sfa = CANN1D_SFA(num=num, m=m,k=k) # ç”Ÿæˆå¤–éƒ¨åˆºæ¿€ dur = 1000. noise_level = 0.1 Iext = bm.random.normal(0., noise_level, (int(dur / bm.get_dt()), num)) duration = dur # è¿è¡Œæ•°å€¼æ¨¡æ‹Ÿ runner = bp.DSRunner(cann_sfa, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;]) runner.run(duration) # å¯è§†åŒ– def plot_response(t): fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann_sfa.x, I, label=&apos;Iext&apos;) ax.plot(cann_sfa.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() # plt.savefig(f&apos;CANN_t={t}.pdf&apos;, transparent=True, dpi=500) plot_response(t=100.) plot_response(t=150.) plot_response(t=200.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann_sfa.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann_sfa.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=1, frame_delay=40, show=True, ) plt.show() traveling_wave(num=512,m=0.5,k=0.1) ``` ### Simulate the anticipative tracking ```python def anticipative_tracking(m=10,v_ext=6*1e-3): cann_sfa = CANN1D_SFA(num=512, m=m) # å®šä¹‰éšæ—¶é—´å˜åŒ–çš„å¤–éƒ¨åˆºæ¿€ v_ext = v_ext dur1, dur2, = 10., 1000. num1 = int(dur1 / bm.get_dt()) num2 = int(dur2 / bm.get_dt()) position = np.zeros(num1 + num2) for i in range(num2): pos = position[i+num1-1]+v_ext*bm.dt # the periodical boundary pos = np.where(pos&amp;gt;np.pi, pos-2*np.pi, pos) pos = np.where(pos 0.5 * self.z_range, d - self.z_range, d) return d # è®¡ç®—è¿žæŽ¥çŸ©é˜µ def make_conn(self, x): assert bm.ndim(x) == 1 d = self.dist(x - x[:, None]) # è·ç¦»çŸ©é˜µ Jxx = self.J0 * bm.exp( -0.5 * bm.square(d / self.a)) / (bm.sqrt(2 * bm.pi) * self.a) return Jxx # èŽ·å–å„ä¸ªç¥žç»å…ƒåˆ°poså¤„ç¥žç»å…ƒçš„è¾“å…¥ def get_stimulus_by_pos(self, pos): return self.A * bm.exp(-0.25 * bm.square(self.dist(self.x - pos) / self.a)) def update(self, x=None): u2 = bm.square(self.u) r = u2 / (1.0 + self.k * bm.sum(u2)) Irec = bm.dot(self.conn_mat, (self.g + self.G * (1 - self.g))*self.h*r) u, g, h = self.integral(u=self.u, g=self.g, h=self.h, t=bp.share[&apos;t&apos;], Irec=Irec, Iext=self.input, r=r, dt=bm.dt) self.u[:] = bm.where(u&amp;gt;0,u,0) self.g.value = g self.h.value = h self.input[:] = 0. # é‡ç½®å¤–éƒ¨ç”µæµ ``` ### Simulate traveling wave in CANN with STP ```python def traveling_wave_STP(num=512,k=0.1,J0=12.,tau_d=1000,tau_f=1.,G=0.9): # ç”ŸæˆCANN cann_stp = CANN1D_STP(num=num, k=k,tau_d=tau_d,tau_f=tau_f,G=G, J0=J0) # ç”Ÿæˆå¤–éƒ¨åˆºæ¿€ dur = 1000. noise_level = 0.1 Iext = bm.random.normal(0., noise_level, (int(dur / bm.get_dt()), num)) duration = dur # è¿è¡Œæ•°å€¼æ¨¡æ‹Ÿ runner = bp.DSRunner(cann_stp, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;,&apos;g&apos;,&apos;h&apos;]) runner.run(duration) fig,ax = plt.subplots(figsize=(3,3)) u = bm.as_numpy(runner.mon.u) max_index = np.argmax(u[1000,:]) print(max_index) ax.plot(runner.mon.g[:,max_index],label=&apos;g&apos;) ax.plot(runner.mon.h[:,max_index],label=&apos;h&apos;) ax.legend() # å¯è§†åŒ– def plot_response(t): fig, gs = bp.visualize.get_figure(1, 1, 3, 3) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann_stp.x, I, label=&apos;Iext&apos;) ax.plot(cann_stp.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() plot_response(t=100.) plot_response(t=200.) plot_response(t=300.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann_stp.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann_stp.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=1, frame_delay=40, show=True, ) plt.show() traveling_wave_STP(G=0.5,tau_d=50) ``` # Decision-Making Network ## LIP -&amp;gt; Decision-Making ### Coherent motion task åˆ¤æ–­éšæœºç‚¹(å¤§éƒ¨åˆ†ç‚¹)çš„è¿åŠ¨æœå‘ ![image-20230828100425871](/BrainPy-course-notes/master_content/Notes.assets/image-20230828100425871.png) coherenceå½±å“ä»»åŠ¡çš„éš¾åº¦ 0%éš¾ï¼Œ100%ç®€å• ![image-20230828100516123](/BrainPy-course-notes/master_content/Notes.assets/image-20230828100516123.png) ç¼–ç å†³ç­–çš„å“åº”ï¼Œä¸æ˜¯è¿åŠ¨ ### Reaction Time vs. Fixed Duration coherenceè¶Šé«˜ï¼Œååº”æ—¶é—´è¶ŠçŸ­ Fixed Durationå¤šäº†Delay time ![image-20230828100658772](/BrainPy-course-notes/master_content/Notes.assets/image-20230828100658772.png) å®žéªŒè®¾è®¡çº¯ç²¹æŠŠdecision-makingç»™æå–å‡ºæ¥ #### Effect of Difficulty coherenceè¶Šå¤§ï¼Œååº”æ—¶é—´æ˜¯è¶ŠçŸ­ï¼Œsingle neuronå¾ˆéš¾åšåˆ°è¿™ä¹ˆçŸ­çš„decision-makingï¼Œè€ƒè™‘è¦å»ºæ¨¡çš„å› ç´  ![image-20230828101103008](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101103008.png) ![image-20230828101058059](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101058059.png) #### Response of MT Neurons è®°å½•MTçš„ç¥žç»å…ƒï¼Œå¯¹è¿™ç§è¿åŠ¨çš„æœå‘åˆºæ¿€è¿›è¡Œç¼–ç  çº¿æ€§ç¼–ç coherenceè¿åŠ¨å¼ºåº¦çš„æ–¹å‘ åšå†³ç­–åœ¨å®ƒçš„ä¸‹æ¸¸ ![image-20230828101303674](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101303674.png) #### Response of LIP Neurons MTçš„ä¸‹æ¸¸æ‰¾åˆ°LIPçš„ç¥žç»å…ƒ çˆ¬å‡åˆ°ä¸€å®šé«˜åº¦å†åšé€‰æ‹© coherenceä¸Žçˆ¬å‡çš„æ–œçŽ‡ä¹Ÿä¼šæœ‰å½±å“ï¼Œä»»åŠ¡è¶Šéš¾ï¼Œçˆ¬å‡æ–œçŽ‡è¶Šå° ![image-20230828101609881](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101609881.png) ### Ramping-to-threshold(perfect integrator) Model $$ \begin{aligned}\frac{dR}{dt}=I_A-I_B+\text{noise},\quad R(t)&amp;=(I_A-I_B)t+\int_0^tdt\text{noise}.\\\tau_\text{network}&amp;=\infty!\end{aligned} $$ ä¸¤ç§é€‰æ‹©ç§¯åˆ†æ±‚å’Œåšç§¯ç´¯ï¼Œç­‰åˆ°é˜ˆå€¼åšå†³ç­– Accumulates information (evidence) -&amp;gt; Ramping ç›´æŽ¥ä¿å­˜ä¿¡æ¯ï¼Œæ²¡æœ‰ç‰¹åˆ«å¥½çš„ç”Ÿç‰©å¯¹åº” ## A Spiking Network of DM ### A cortical microcircuit model ![image-20230828103055151](Notes.assets/image-20230828103055151.png) A=Upward motion B=Downward motion 2-population excitatory neurons (integrate-and-fire neurons driven by Poisson input) Slow reverberatory excitation mediated by the NMDA receptors at recurrent synapses AMPA receptors ($\tau _{syn}=$1 - 3 ms) NMDA receptors ($\tau _{syn}=$ 50 - 100 ms). ä¸¤ç¾¤ç¥žç»å…ƒåˆ†åˆ«åšä¸åŒçš„é€‰æ‹©ï¼Œä¸Žè‡ªå·±å¯¹æ–¹éƒ½æœ‰è¿žæŽ¥ NMDA ç¼“æ…¢çš„ä¿¡å·ä½¿å¾—æœ‰æ…¢æ…¢å¢žé•¿çš„rampingçš„è¿‡ç¨‹ interneuronsçš„backwardæœ‰æŠ‘åˆ¶ä½œç”¨ #### Coherence-Dependent Input çº¿æ€§ç¼–ç è¿åŠ¨æœå‘çš„ä¿¡æ¯ï¼Œcoherenceå¼ºåº¦å½±å“firing rateï¼Œä¸€ç³»åˆ—æ³Šæ¾è¿‡ç¨‹ï¼ŒåŒæ—¶è¿˜æœ‰noiseã€‚ æœ¬èº«ä¸¤ç§ä¿¡æ¯è¿˜æ˜¯æœ‰å·®å¼‚ ![image-20230828104054275](/BrainPy-course-notes/master_content/Notes.assets/image-20230828104054275.png) #### Duality of this model ä¸åŒcoherenceçš„ç¥žç»å…ƒå“åº” ![image-20230828104432061](/BrainPy-course-notes/master_content/Notes.assets/image-20230828104432061.png) ä¸¤ä¸ªgroupä¼šç«žäº‰ï¼Œå½“æœ‰ä¸€ä¸ªgroupè¾¾åˆ°20%ï¼Œè¿›å…¥è¿™ä¸ªçª—å£ï¼Œå°±ä¼šç›´æŽ¥å‘æ”¾ä¸ŠåŽ» Spontaneous symmetry breaking and stochastic decision making ![image-20230828104600840](/BrainPy-course-notes/master_content/Notes.assets/image-20230828104600840.png) ## Simulation of Spiking DM ### A Cortical Microcircuit Model ç”¨ä¸¤ä¸ªcoherenceç”Ÿæˆå‡ºæ¥çš„åºåˆ— ![image-20230828110300576](/BrainPy-course-notes/master_content/Notes.assets/image-20230828110300576.png) $$ \begin{gathered}C_m\frac{dV(t)}{dt}=-g_L(V(t)-V_L)-I_{syn}(t)\\I_{syn}(t)=I_{\mathrm{ext},\mathrm{AMPA}}\left(t\right)+I_{\mathrm{rec},AMPA}(t)+I_{\mathrm{rec},NMDA}(t)+I_{\mathrm{rec},\mathrm{GABA}}(t)\end{gathered} $$ $$ \begin{gathered} I_{\mathrm{ext},\mathrm{AMPA}}\left(t\right)=g_{\mathrm{ext},\mathrm{AMPA}}\left(V(t)-V_{E}\right)s^{\mathrm{ext},\mathrm{AMPA}}\left(t\right) \\ I_{\mathrm{rec},\mathrm{AMP}\Lambda}\left(t\right)=g_{\mathrm{rec},\mathrm{AMP}\Lambda}\left(V(t)-V_{E}\right)\sum_{j=1}^{Ce}w_{j}s_{j}^{AMPA}(t) \\ I_{\mathrm{rec},\mathrm{NMDA}}\left(t\right)=\frac{g_{\mathrm{NMDA}}(V(t)-V_{E})}{\left(1+\left[\mathrm{Mg}^{2+}\right]\exp(-0.062V(t))/3.57\right)}\sum_{j=1}^{\mathrm{C_E}}w_{j}s_{j}^{\mathrm{NMDA}}\left(t\right) \\ I_\mathrm{rec,GABA}(t)=g_\mathrm{GABA}(V(t)-V_l)\sum_{j=1}^{C_1}s_j^\mathrm{GABA}(t) \end{gathered} $$ $$ w_j=\left\{\begin{matrix}w_+&amp;gt;1,\\w_-E/I conn self.I2B = AMPA(self.I, self.B, &apos;all2all&apos;, 0.5, g_I2E_GABAa, tau=5., E=-70.) self.I2A = AMPA(self.I, self.A, &apos;all2all&apos;, 0.5, g_I2E_GABAa, tau=5., E=-70.) self.I2N = AMPA(self.I, self.N, &apos;all2all&apos;, 0.5, g_I2E_GABAa, tau=5., E=-70.) self.I2I = AMPA(self.I, self.I, &apos;all2all&apos;, 0.5, g_I2I_GABAa, tau=5., E=-70.) # define external projections #### TO DO!!!! self.noise2B = AMPA(self.noise_B, self.B, &apos;one2one&apos;, None, g_ext2E_AMPA, tau=2., E=0.) self.noise2A = AMPA(self.noise_A, self.A, &apos;one2one&apos;, None, g_ext2E_AMPA, tau=2., E=0.) self.noise2N = AMPA(self.noise_N, self.N, &apos;one2one&apos;, None, g_ext2E_AMPA, tau=2., E=0.) self.noise2I = AMPA(self.noise_I, self.I, &apos;one2one&apos;, None, g_ext2I_AMPA, tau=2., E=0.) ``` ```python class Tool: def __init__(self, pre_stimulus_period=100., stimulus_period=1000., delay_period=500.): self.pre_stimulus_period = pre_stimulus_period self.stimulus_period = stimulus_period self.delay_period = delay_period self.freq_variance = 10. self.freq_interval = 50. self.total_period = pre_stimulus_period + stimulus_period + delay_period def generate_freqs(self, mean): # stimulus period n_stim = int(self.stimulus_period / self.freq_interval) n_interval = int(self.freq_interval / bm.get_dt()) freqs_stim = np.random.normal(mean, self.freq_variance, (n_stim, 1)) freqs_stim = np.tile(freqs_stim, (1, n_interval)).flatten() # pre stimulus period freqs_pre = np.zeros(int(self.pre_stimulus_period / bm.get_dt())) # post stimulus period freqs_delay = np.zeros(int(self.delay_period / bm.get_dt())) all_freqs = np.concatenate([freqs_pre, freqs_stim, freqs_delay], axis=0) return bm.asarray(all_freqs) def visualize_results(self, mon, IA_freqs, IB_freqs, t_start=0., title=None): fig, gs = bp.visualize.get_figure(4, 1, 3, 10) axes = [fig.add_subplot(gs[i, 0]) for i in range(4)] ax = axes[0] bp.visualize.raster_plot(mon[&apos;ts&apos;], mon[&apos;A.spike&apos;], markersize=1, ax=ax) if title: ax.set_title(title) ax.set_ylabel(&quot;Group A&quot;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax = axes[1] bp.visualize.raster_plot(mon[&apos;ts&apos;], mon[&apos;B.spike&apos;], markersize=1, ax=ax) ax.set_ylabel(&quot;Group B&quot;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax = axes[2] rateA = bp.measure.firing_rate(mon[&apos;A.spike&apos;], width=10.) rateB = bp.measure.firing_rate(mon[&apos;B.spike&apos;], width=10.) ax.plot(mon[&apos;ts&apos;], rateA, label=&quot;Group A&quot;) ax.plot(mon[&apos;ts&apos;], rateB, label=&quot;Group B&quot;) ax.set_ylabel(&apos;Population activity [Hz]&apos;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax.legend() ax = axes[3] ax.plot(mon[&apos;ts&apos;], IA_freqs, label=&quot;group A&quot;) ax.plot(mon[&apos;ts&apos;], IB_freqs, label=&quot;group B&quot;) ax.set_ylabel(&quot;Input activity [Hz]&quot;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax.legend() ax.set_xlabel(&quot;Time [ms]&quot;) plt.show() ``` ```python tool = Tool() net = DecisionMakingNet() mu0 = 40. coherence = 25.6 IA_freqs = tool.generate_freqs(mu0 + mu0 / 100. * coherence) IB_freqs = tool.generate_freqs(mu0 - mu0 / 100. * coherence) def give_input(): i = bp.share[&apos;i&apos;] net.IA.freqs[0] = IA_freqs[i] net.IB.freqs[0] = IB_freqs[i] runner = bp.DSRunner(net, inputs=give_input, monitors=[&apos;A.spike&apos;, &apos;B.spike&apos;]) runner.run(tool.total_period) tool.visualize_results(runner.mon, IA_freqs, IB_freqs) ``` ### Results ![image-20230828112245950](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112245950.png) #### Stochastic Decision Making ![image-20230828112253619](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112253619.png) ## A Rate Network of DM ### Reduced Model åŒ–ç®€åˆ°åªæœ‰ä¸¤ç¾¤ç¥žç»å…ƒï¼ŒåªæŽ¥å—å¤–ç•Œè¾“å…¥ä¿¡å·ï¼Œäº’ç›¸å½±å“å¯¹æ–¹ ![image-20230828112326267](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112326267.png) Synaptic variables $$ \begin{gathered} \frac{dS_{1}}{dt} =F(x_1)\gamma(1-S_1)-S_1/\tau_s \\ \frac{dS_2}{dt} =F(x_2)\gamma(1-S_2)-S_2/\tau_s \end{gathered} $$ Input current to each population $$ \begin{gathered} x_{1} =J_{E}S_{1}+J_{I}S_{2}+I_{0}+I_{noise1}+J_{\text{ext }\mu_{1}} \\ x_{2} =J_{E}S_{2}+J_{I}S_{1}+I_{0}+I_{noise2}+J_{\mathrm{ext}}\mu_{2} \end{gathered} $$ Background input $$ I_0+I_{noise}\\ \begin{gathered} dI_{noise1} =-I_{noise1}\frac{dt}{\tau_{0}}+\sigma dW \\ dI_{noise2} =-I_{noise2}\frac{dt}{\tau_{0}}+\sigma dW \end{gathered} $$ Firing rates $$ r_i=F(x_i)=\frac{ax_i-b}{1-\exp(-d(ax_i-b))} $$ Coherence-dependent inputs $$ \begin{array}{l}\mu_1=\mu_0\big(1+c&apos;/100\big)\\\mu_2=\mu_0\big(1-c&apos;/100\big)\end{array} $$ $$ \begin{aligned}&amp;\gamma,a,b,d,J_E,J_I,J_{\mathrm{ext}},I_0,\mu_0,\tau_{\mathrm{AMPA}},\sigma_{\mathrm{noise}}\\&amp;\text{are fixed parameters.}\end{aligned} $$ ```python class DecisionMakingRateModel(bp.dyn.NeuGroup): def __init__(self, size, coherence, JE=0.2609, JI=0.0497, Jext=5.2e-4, I0=0.3255, gamma=6.41e-4, tau=100., tau_n=2., sigma_n=0.02, a=270., b=108., d=0.154, noise_freq=2400., method=&apos;exp_auto&apos;, **kwargs): super(DecisionMakingRateModel, self).__init__(size, **kwargs) # åˆå§‹åŒ–å‚æ•° self.coherence = coherence self.JE = JE self.JI = JI self.Jext = Jext self.I0 = I0 self.gamma = gamma self.tau = tau self.tau_n = tau_n self.sigma_n = sigma_n self.a = a self.b = b self.d = d # åˆå§‹åŒ–å˜é‡ self.s1 = bm.Variable(bm.zeros(self.num) + 0.15) self.s2 = bm.Variable(bm.zeros(self.num) + 0.15) self.r1 = bm.Variable(bm.zeros(self.num)) self.r2 = bm.Variable(bm.zeros(self.num)) self.mu0 = bm.Variable(bm.zeros(self.num)) self.I1_noise = bm.Variable(bm.zeros(self.num)) self.I2_noise = bm.Variable(bm.zeros(self.num)) # å™ªå£°è¾“å…¥çš„ç¥žç»å…ƒ self.noise1 = bp.dyn.PoissonGroup(self.num, freqs=noise_freq) self.noise2 = bp.dyn.PoissonGroup(self.num, freqs=noise_freq) # å®šä¹‰ç§¯åˆ†å‡½æ•° self.integral = bp.odeint(self.derivative, method=method) @property def derivative(self): return bp.JointEq([self.ds1, self.ds2, self.dI1noise, self.dI2noise]) def ds1(self, s1, t, s2, mu0): I1 = self.Jext * mu0 * (1. + self.coherence / 100.) x1 = self.JE * s1 - self.JI * s2 + self.I0 + I1 + self.I1_noise r1 = (self.a * x1 - self.b) / (1. - bm.exp(-self.d * (self.a * x1 - self.b))) return - s1 / self.tau + (1. - s1) * self.gamma * r1 def ds2(self, s2, t, s1, mu0): I2=self.Jext*mu0*(1.- self.coherence / 100.) x2 = self.JE * s2 - self.JI * s1 + self.I0 + I2 + self.I2_noise r2 = (self.a * x2 - self.b) / (1. - bm.exp(-self.d * (self.a * x2 - self.b))) return - s2 / self.tau + (1. - s2) * self.gamma * r2 def dI1noise(self, I1_noise, t, noise1): return (- I1_noise + noise1.spike * bm.sqrt(self.tau_n * self.sigma_n * self.sigma_n)) / self.tau_n def dI2noise(self, I2_noise, t, noise2): return (- I2_noise + noise2.spike * bm.sqrt(self.tau_n * self.sigma_n * self.sigma_n)) / self.tau_n def update(self, tdi): # æ›´æ–°å™ªå£°ç¥žç»å…ƒä»¥äº§ç”Ÿæ–°çš„éšæœºå‘æ”¾ self.noise1.update(tdi) self.noise2.update(tdi) # æ›´æ–°s1ã€s2ã€I1_noiseã€I2_noise integral = self.integral(self.s1, self.s2, self.I1_noise, self.I2_noise, tdi.t, mu0=self.mu0, noise1=self.noise1, noise2=self.noise2, dt=tdi.dt) self.s1.value, self.s2.value, self.I1_noise.value, self.I2_noise.value = integral # ç”¨æ›´æ–°åŽçš„s1ã€s2è®¡ç®—r1ã€r2 I1 = self.Jext * self.mu0 * (1. + self.coherence / 100.) x1 = self.JE * self.s1 + self.JI * self.s2 + self.I0 + I1 + self.I1_noise self.r1.value = (self.a * x1 - self.b) / (1. - bm.exp(-self.d * (self.a * x1 - self.b))) I2 = self.Jext * self.mu0 * (1. - self.coherence / 100.) x2 = self.JE * self.s2 + self.JI * self.s1 + self.I0 + I2 + self.I2_noise self.r2.value = (self.a * x2 - self.b) / (1. - bm.exp(-self.d * (self.a * x2 - self.b))) # é‡ç½®å¤–éƒ¨è¾“å…¥ self.mu0[:] = 0. ``` ```python # å®šä¹‰å„ä¸ªé˜¶æ®µçš„æ—¶é•¿ pre_stimulus_period, stimulus_period, delay_period = 100., 2000., 500. # ç”Ÿæˆæ¨¡åž‹ dmnet = DecisionMakingRateModel(1, coherence=25.6, noise_freq=2400.) # å®šä¹‰ç”µæµéšæ—¶é—´çš„å˜åŒ– inputs, total_period = bp.inputs.constant_input([(0., pre_stimulus_period), (20., stimulus_period), (0., delay_period)]) # è¿è¡Œæ•°å€¼æ¨¡æ‹Ÿ runner = bp.DSRunner(dmnet, monitors=[&apos;s1&apos;, &apos;s2&apos;, &apos;r1&apos;, &apos;r2&apos;], inputs=(&apos;mu0&apos;, inputs, &apos;iter&apos;)) runner.run(total_period) # å¯è§†åŒ– fig, gs = plt.subplots(2, 1, figsize=(6, 6), sharex=&apos;all&apos;) gs[0].plot(runner.mon.ts, runner.mon.s1, label=&apos;s1&apos;) gs[0].plot(runner.mon.ts, runner.mon.s2, label=&apos;s2&apos;) gs[0].axvline(pre_stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[0].axvline(pre_stimulus_period + stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[0].set_ylabel(&apos;gating variable $s$&apos;) gs[0].legend() gs[1].plot(runner.mon.ts, runner.mon.r1, label=&apos;r1&apos;) gs[1].plot(runner.mon.ts, runner.mon.r2, label=&apos;r2&apos;) gs[1].axvline(pre_stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[1].axvline(pre_stimulus_period + stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[1].set_xlabel(&apos;t (ms)&apos;) gs[1].set_ylabel(&apos;firing rate $r$&apos;) gs[1].legend() plt.subplots_adjust(hspace=0.1) plt.show() ``` ### Results ![image-20230828112555018](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112555018.png) ## Phase Plane Analysis å› ä¸ºåªæœ‰ä¸¤ä¸ªvariable ### Model implementation ```python @bp.odeint def int_s1(s1, t, s2, coh=0.5, mu=20.): x1 = JE * s1 + JI * s2 + Ib + JAext * mu * (1. + coh/100) r1 = (a * x1 - b) / (1. - bm.exp(-d * (a * x1 - b))) return - s1 / tau + (1. - s1) * gamma * r1 @bp.odeint def int_s2(s2, t, s1, coh=0.5, mu=20.): x2 = JE * s2 + JI * s1 + Ib + JAext * mu * (1. - coh/100) r2 = (a * x2 - b) / (1. - bm.exp(-d * (a * x2 - b))) return - s2 / tau + (1. - s2) * gamma * r2 ``` ### Without / with input ![image-20230828112709355](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112709355.png) åªå—æ‰°åŠ¨å½±å“ï¼Œæœ‰inputåŽä¸­é—´å˜å¾—ä¸ç¨³å®šï¼Œä½†å¦‚æžœå·²ç»é€‰æ‹©ï¼Œç½‘ç»œä»ç»´æŒä¹‹å‰é€‰æ‹©çš„ç»“æžœ ![image-20230828112811394](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112811394.png) ### Coherence ç¨³å®šç‚¹å¯¹ç½‘ç»œçš„æ‹‰ä¼¸æ›´å¼º ![image-20230828113031946](/BrainPy-course-notes/master_content/Notes.assets/image-20230828113031946.png) ![image-20230828113009219](/BrainPy-course-notes/master_content/Notes.assets/image-20230828113009219.png) # Reservoir Computing å¼•å…¥è®­ç»ƒ å€¾å‘äºŽä½¿ç”¨RNN ![image-20230828140305956](/BrainPy-course-notes/master_content/Notes.assets/image-20230828140305956.png) Connecting different units $$ \begin{aligned} &amp;\textsf{Input to unit i from unit j:} \\ &amp;&amp;&amp;I_{j\rightarrow i}=J_{ij}r_{j}(t) \\ &amp;\textsf{Total input to unit i:} \\ &amp;&amp;&amp;I_{i}^{(tot)}=\sum_{j=1}^{N}J_{ij}r_{j}(t)+I_{i}^{(ext)} \end{aligned} $$ $$ \textsf{Activation of unit i:} \\ \tau\frac{dx_{i}}{dt}=-x_{i}+\sum_{j=1}^{N}J_{ij}\frac{\phi(x_{j})}{1}+I_{i}^{(ext)}(t) $$ è®­ç»ƒèŒƒå¼ ![image-20230828140707998](/BrainPy-course-notes/master_content/Notes.assets/image-20230828140707998.png) ## Echo state machine ### Echo state machine ç±»ä¼¼äººå·¥ç¥žç»ç½‘ç»œRNNï¼Œå¯ä»¥å¤„ç†temporalä¿¡æ¯ ![image-20230828140937455](/BrainPy-course-notes/master_content/Notes.assets/image-20230828140937455.png) $$ \begin{aligned} &amp;\mathbf{x}(n+1) =f(\mathbf{W}^{\mathrm{in}}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n)+\mathbf{W}^{\mathrm{back}}\mathbf{y}(n)) \\ &amp;\mathbf{y}(n+1) =\mathbf{W}^{\mathrm{out}}(\mathbf{u}(n+1),\mathbf{x}(n+1),\mathbf{y}(n)) \end{aligned} $$ For an RNN, the state of its internal neurons reflects the historical information of the external inputs. åæ˜ çš„echoçš„åŽ†å²ä¿¡æ¯ï¼Œå”¯ä¸€ä¾èµ–åŽ†å²ä¿¡æ¯ Assuming that the updates of the network are discrete, the external input at the ð‘›th moment is u(ð‘›) and the neuron state is x(ð‘›), then x(ð‘›) should be determined by u(ð‘›), u(ð‘› - 1), ... uniquely determined. At this point, x(ð‘›) can be regarded as an &quot;echo&quot; of the historical input signals. ä¸éœ€è¦è®­ç»ƒconnection ### Echo state machine with leaky integrator æœ‰ä¸€ä¸ªleakyé¡¹ï¼Œå¼•å…¥decay ### $$ \begin{aligned}\hat{h}(n)=\tanh(W^{in}x(n)+W^{rec}h(n-1)+W^{fb}y(n-1)+b^{rec})\\h(n)=(1-\alpha)x(n-1)+\alpha\hat{h}(n)\end{aligned} $$ where $h(n)$ is a vector of reservoir neuron activations, $W^{in}$ and $W^{rec}$ are the input and recurrent weight matrices respectively, and $\alpha\in(0,1]$ is the leaking rate. The model is also sometimes used without the leaky integration, which is a special case of $\alpha=1$ The linear readout layer is defined as $$ y(n)=W^{out}h(n)+b^{out} $$ where $y(n)$ is network output, $W^{out}$ the output weight matrix, and $b^out$ is the output bias ## Constraints of echo state machine ### Echo state property #### Theorem 1 For the echo state network defined above, the network will be echoey as long as the maximum singular value $\sigma_{max} Provement: &amp;gt; $$ &amp;gt; \begin{aligned} &amp;gt; d(\mathbf{x}(n+1),\mathbf{x}^{\prime}(n+1))&amp; =d(T(\mathbf{x}(n),\mathbf{u}(n+1)),T(\mathbf{x}&apos;(n),\mathbf{u}(n+1))) \\ &amp;gt; &amp;=d(f(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n)),f(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}&apos;(n))) \\ &amp;gt; &amp;\leq d(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n),\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}^{\prime}(n)) \\ &amp;gt; &amp;=d(\mathbf{W}\mathbf{x}(n),\mathbf{W}\mathbf{x}&apos;(n)) \\ &amp;gt; &amp;=||\mathbf{W}(\mathbf{x}(n)-\mathbf{x}^{\prime}(n))|| \\ &amp;gt; &amp;\leq\sigma_{\max}(\mathbf{W})d(\mathbf{x}(n),\mathbf{x}&apos;(n)) &amp;gt; \end{aligned} &amp;gt; $$ #### Theorem 2 For the echo state network defined above, as long as the spectral radius $|\lambda_{max}|$ of the recurrent connection matrix W &amp;gt; 1, then the network must not be echogenic. The spectral radius of the matrix is the absolute value of the largest eigenvalue $\lambda_{max}$. #### How to initialize Using these two theorems, how should we initialize W so that the network has an echo property? If we scale W, i.e., multiply it by a scaling factor $\alpha$, then $\sigma_{max}\alpha_{max}\text{,the network will not have the echo state.}\\\bullet&amp;\text{if}\alpha_{min}\le\alpha\le\alpha_{max}\text{,the network may have the echo state.}\end{array} $$ **$\alpha$è®¾çš„ç•¥å°äºŽ1** ![image-20230828142516052](/BrainPy-course-notes/master_content/Notes.assets/image-20230828142516052.png) ### Global parameters of reservoir è¿™äº›è¶…å‚ä¼šå½±å“reservoir networkçš„æ€§èƒ½ï¼Œéœ€è¦æ‰‹åŠ¨è°ƒå‚ï¼Œå¾ˆéš¾è‡ªåŠ¨åŽ»è°ƒæ•´ - The size $N_x$ - General wisdom: the bigger the reservoir, the better the obtainable performance - Select global parameters with smaller reservoirs, then scale to bigger ones. - Sparsity - Distribution of nonzero elements: - Normal distribution - Uniform distribution - The width of the distributions does not matter - spectral radius of $W$ - scales the width of the distribution of its nonzero elements - determines how fast the influence of an input dies out in a reservoir with time, and how stable the reservoir activations are - The spectral radius should be larger in tasks requiring longer memory of the input - Scaling(-s) to $W^{in}$: - For uniform distributed $W^{in}$, $\alpha$ in the range of the interval $[-a;a]$. - For normal distributed $W^{in}$, one may take the standard deviation as a scaling measure. The leaking rate $\alpha$ ## Training of echo state machine ### Offline learning The advantage of the echo state network is that it does not train recurrent connections within the reservoir, but only the readout layer from the reservoir to the output. çº¿æ€§å±‚çš„ä¼˜åŒ–æ–¹æ³•æ˜¯ç®€å•çš„ **Ridge regression** $$ \begin{aligned}\epsilon_{\mathrm{train}}(n)&amp;=\mathbf{y}(n)-\mathbf{\hat{y}}(n) \\&amp;=\mathbf{y}(n)-\mathbf{W}^{\mathrm{out}}\mathbf{x}(n) \\&amp;L_{\mathrm{ridge}}=\frac{1}{N}\sum_{i=1}^{N}\epsilon_{\mathrm{train}}^{2}(i)+\alpha||\mathbf{W^{out}}||^{2} \\\\W^{out}&amp;=Y^{target}X^T(XX^T+\beta I)^{-1}\end{aligned} $$ ```python trainer = bp.OfflineTrainer(model, fit_method=bp.algorithms.RidgeRegression(1e-7), dt=dt) ``` ### Online learning æ¥ä¸€ä¸ªsampleï¼Œè¿›è¡Œä¸€æ¬¡trainingï¼Œå¯¹è®­ç»ƒèµ„æºå¯ä»¥é¿å…ç“¶é¢ˆ The training data is passed to the trainer in a certain sequence (e.g., time series), and the trainer continuously learns based on the new incoming data. **Recursive Least Squares (RLS) algorithm** $$ E(\mathbf{y},\mathbf{y}^\mathrm{target},n)=\frac{1}{N_\mathrm{y}}\sum_{i=1}^{N_\mathrm{y}}\sum_{j=1}^{n}\lambda^{n-j}\left(y_i(j)-y_i^\mathrm{target}(j)\right)^2, $$ ```python trainer = bp.OnlineTrainer(model, fit_method=bp.algorithms.RLS(), dt=dt) ``` ### Dataset ç»™å®štime sequenceï¼Œå¯ä»¥è®©ç½‘ç»œåŽ»é¢„æµ‹regression ![image-20230828144309742](/BrainPy-course-notes/master_content/Notes.assets/image-20230828144309742.png) ç”¨åˆ°BrainPyé›†æˆçš„`Neuromorphic and Cognitive Datasets` ### Other tasks `MNIST dataset` or `Fashion MNIST` Two aspect: - Running time - Memory Usage ## Echo state machine programming ```python import brainpy as bp import brainpy.math as bm import brainpy_datasets as bd import matplotlib.pyplot as plt # enable x64 computation bm.set_environment(x64=True, mode=bm.batching_mode) bm.set_platform(&apos;cpu&apos;) ``` ### Dataset ```python def plot_mackey_glass_series(ts, x_series, x_tau_series, num_sample): plt.figure(figsize=(13, 5)) plt.subplot(121) plt.title(f&quot;Timeserie - {num_sample} timesteps&quot;) plt.plot(ts[:num_sample], x_series[:num_sample], lw=2, color=&quot;lightgrey&quot;, zorder=0) plt.scatter(ts[:num_sample], x_series[:num_sample], c=ts[:num_sample], cmap=&quot;viridis&quot;, s=6) plt.xlabel(&quot;$t$&quot;) plt.ylabel(&quot;$P(t)$&quot;) ax = plt.subplot(122) ax.margins(0.05) plt.title(f&quot;Phase diagram: $P(t) = f(P(t-\\tau))$&quot;) plt.plot(x_tau_series[: num_sample], x_series[: num_sample], lw=1, color=&quot;lightgrey&quot;, zorder=0) plt.scatter(x_tau_series[:num_sample], x_series[: num_sample], lw=0.5, c=ts[:num_sample], cmap=&quot;viridis&quot;, s=6) plt.xlabel(&quot;$P(t-\\tau)$&quot;) plt.ylabel(&quot;$P(t)$&quot;) cbar = plt.colorbar() cbar.ax.set_ylabel(&apos;$t$&apos;) plt.tight_layout() plt.show() ``` ```python dt = 0.1 mg_data = bd.chaos.MackeyGlassEq(25000, dt=dt, tau=17, beta=0.2, gamma=0.1, n=10) ts = mg_data.ts xs = mg_data.xs ys = mg_data.ys plot_mackey_glass_series(ts, xs, ys, num_sample=int(1000 / dt)) ``` ![image-20230828151451523](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151451523.png) ### Prediction of Mackey-Glass timeseries #### Prepare the data ```python def get_data(t_warm, t_forcast, t_train, sample_rate=1): warmup = int(t_warm / dt) # warmup the reservoir forecast = int(t_forcast / dt) # predict 10 ms ahead train_length = int(t_train / dt) X_warm = xs[:warmup:sample_rate] X_warm = bm.expand_dims(X_warm, 0) X_train = xs[warmup: warmup+train_length: sample_rate] X_train = bm.expand_dims(X_train, 0) Y_train = xs[warmup+forecast: warmup+train_length+forecast: sample_rate] Y_train = bm.expand_dims(Y_train, 0) X_test = xs[warmup + train_length: -forecast: sample_rate] X_test = bm.expand_dims(X_test, 0) Y_test = xs[warmup + train_length + forecast::sample_rate] Y_test = bm.expand_dims(Y_test, 0) return X_warm, X_train, Y_train, X_test, Y_test ``` ```python # First warmup the reservoir using the first 100 ms # Then, train the network in 20000 ms to predict 1 ms chaotic series ahead x_warm, x_train, y_train, x_test, y_test = get_data(100, 1, 20000) ``` ```python sample = 3000 fig = plt.figure(figsize=(15, 5)) plt.plot(x_train[0, :sample], label=&quot;Training data&quot;) plt.plot(y_train[0, :sample], label=&quot;True prediction&quot;) plt.legend() plt.show() ``` ![image-20230828151606545](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151606545.png) #### Prepare the ESN ```python class ESN(bp.DynamicalSystemNS): def __init__(self, num_in, num_hidden, num_out, sr=1., leaky_rate=0.3, Win_initializer=bp.init.Uniform(0, 0.2)): super(ESN, self).__init__() self.r = bp.layers.Reservoir( num_in, num_hidden, Win_initializer=Win_initializer, spectral_radius=sr, leaky_rate=leaky_rate, ) self.o = bp.layers.Dense(num_hidden, num_out, mode=bm.training_mode) def update(self, x): return x &amp;gt;&amp;gt; self.r &amp;gt;&amp;gt; self.o ``` #### Train and test ```python model = ESN(1, 100, 1) model.reset_state(1) trainer = bp.RidgeTrainer(model, alpha=1e-6) ``` ```python # warmup _ = trainer.predict(x_warm) ``` ```python # train _ = trainer.fit([x_train, y_train]) ``` #### Test the training data ```python ys_predict = trainer.predict(x_train) ``` ```python start, end = 1000, 6000 plt.figure(figsize=(15, 7)) plt.subplot(211) plt.plot(bm.as_numpy(ys_predict)[0, start:end, 0], lw=3, label=&quot;ESN prediction&quot;) plt.plot(bm.as_numpy(y_train)[0, start:end, 0], linestyle=&quot;--&quot;, lw=2, label=&quot;True value&quot;) plt.title(f&apos;Mean Square Error: {bp.losses.mean_squared_error(ys_predict, y_train)}&apos;) plt.legend() plt.show() ``` ![image-20230828151747954](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151747954.png) #### Test the testing data ```python ys_predict = trainer.predict(x_test) start, end = 1000, 6000 plt.figure(figsize=(15, 7)) plt.subplot(211) plt.plot(bm.as_numpy(ys_predict)[0, start:end, 0], lw=3, label=&quot;ESN prediction&quot;) plt.plot(bm.as_numpy(y_test)[0,start:end, 0], linestyle=&quot;--&quot;, lw=2, label=&quot;True value&quot;) plt.title(f&apos;Mean Square Error: {bp.losses.mean_squared_error(ys_predict, y_test)}&apos;) plt.legend() plt.show() ``` ![image-20230828151824907](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151824907.png) ### JIT connection operators - Just-in-time randomly generated matrix. - Support for Mat@Vec and Mat@Mat. - Support different random generation methods.(homogenous, uniform, normal) ```python import math, random def jitconn_prob_homo(events, prob, weight, seed, outs): random.seed(seed) max_cdist= math.ceil(2/prob -1) for event in events: if event: post_i = random.randint(1, max_cdist) outs[post_i] += weight ``` ![image-20230828153353131](/BrainPy-course-notes/master_content/Notes.assets/image-20230828153353131.png) ## Applications ### From the perspective of kernel methods ç»´åº¦æ‰©å¼ æ€æƒ³ Non-linear SVMs: Kernel Mapping ![image-20230828153621843](/BrainPy-course-notes/master_content/Notes.assets/image-20230828153621843.png) Kernel methods in neural system? **ä¸Žç»´åº¦æ‰©å¼ çš„æ€æƒ³ç›¸ä¼¼** ![image-20230828153801285](/BrainPy-course-notes/master_content/Notes.assets/image-20230828153801285.png) ### Subcortical pathway for rapid motion processing The first two stages of subcortical visual pathway: Retina -&amp;gt; superior colliculus The first two stages of primary auditory pathway: Inner Ear -&amp;gt; Cochlear Nuclei ç»´åº¦æ‰©å¼ åœ¨subcortical pathwayä¸­ä½“çŽ°ï¼Œreservoir èƒ½å¤Ÿé«˜ç»´å¤„ç†çš„æ›´ç®€å• ### Spatial-temporal tasks ![image-20230828154155803](/BrainPy-course-notes/master_content/Notes.assets/image-20230828154155803.png) æ—¢æœ‰æ—¶é—´ä¿¡æ¯ï¼Œåˆæœ‰ç©ºé—´ä¿¡æ¯çš„datasetï¼Œä½¿ç”¨reservoiræ¥å¤„ç†é«˜ç»´ä¿¡æ¯ï¼Œåä½ Dimension expansion ### Gait recognition inputæ¥äº†å†åšè®¡ç®— ![image-20230828154352087](/BrainPy-course-notes/master_content/Notes.assets/image-20230828154352087.png) ### Spatial-temporal tasks large-scaleï¼Œéšsizeå¢žå¤§ï¼Œaccuracyå¢žå¤§ ![image-20230828154428762](/BrainPy-course-notes/master_content/Notes.assets/image-20230828154428762.png) ### Liquid state machine A liquid state machine (LSM) is a type of reservoir computer that uses a spiking neural network. ä¸ŽESNä¸€æ ·çš„èŒƒå¼ï¼Œéƒ½æ˜¯åŽ»åšdimension expansion å¾ˆéš¾åŽ»åˆ†æžæ€Žä¹ˆworkçš„">
<meta name="keywords" content="Jekyll, NexT">
<meta property="og:type" content="website">
<meta property="og:title" content="BrainPy course notes">
<meta property="og:url" content="routhleck.github.io/BrainPy-course-notes/master_content/Notes/">
<meta property="og:site_name" content="BrainPy course notes">
<meta property="og:description" content="[TOC] # ç¥žç»è®¡ç®—å»ºæ¨¡ç®€ä»‹ ## è®¡ç®—ç¥žç»ç§‘å­¦çš„èƒŒæ™¯ä¸Žä½¿å‘½ è®¡ç®—ç¥žç»ç§‘å­¦æ˜¯**è„‘ç§‘å­¦**å¯¹**ç±»è„‘æ™ºèƒ½**çš„**æ¡¥æ¢** ### ä¸¤å¤§ç›®æ ‡ - ç”¨è®¡ç®—å»ºæ¨¡çš„æ–¹æ³•æ¥é˜æ˜Žå¤§è„‘åŠŸèƒ½çš„è®¡ç®—åŽŸç† - å‘å±•ç±»è„‘æ™ºèƒ½çš„æ¨¡åž‹å’Œç®—æ³• ### Prehistory - 1907 LIF model ç¥žç»è®¡ç®—çš„æœ¬è´¨ - 1950s HH model ç”µä½å®šé‡åŒ–æ¨¡åž‹ æœ€fundamentalçš„ - 1960s Roll&apos;s cable equation æè¿°ä¿¡å·åœ¨è½´çªå’Œæ ‘çªæ€Žä¹ˆä¼ é€’ - 1970s Amari, Wilson, Cowan et al. çŽ°ä»Šå»ºæ¨¡çš„åŸºç¡€ - 1982 Hopfield model(Amari-Hopfield model) å¼•å…¥ç‰©ç†å­¦æŠ€æœ¯ï¼Œå¸å¼•å­æ¨¡åž‹ - 1988 Sejnowski et al. &quot;Computational Neuroscience&quot;(science) æå‡ºè®¡ç®—ç¥žç»ç§‘å­¦æ¦‚å¿µ **çŽ°åœ¨çš„è®¡ç®—ç¥žç»ç§‘å­¦å¯¹åº”äºŽç‰©ç†å­¦çš„ç¬¬è°·-ä¼½åˆ©ç•¥æ—¶ä»£ï¼Œå¯¹å¤§è„‘å·¥ä½œåŽŸç†è¿˜ç¼ºä¹æ¸…æ™°çš„ç†è®º** ### Three levels of Brain Science ![image-20230823105226568](/BrainPy-course-notes/master_content/Notes.assets/image-20230823105226568.png) - å¤§è„‘åšä»€ä¹ˆ Computational theory -&amp;gt; Psychology &amp; Cognitive Science -&amp;gt; Human-like Cognitive function - å¤§è„‘æ€Žä¹ˆåš Representation &amp; Algorithm -&amp;gt; Computational Neuroscience -&amp;gt; Brain-inspired model &amp; algorithm - å¤§è„‘æ€Žä¹ˆå®žçŽ° Implementation -&amp;gt; Neuroscience -&amp;gt; Neuromorphic computing ### Mission of Computational Neuroscience &amp;gt; What I can not build a computational model, I do not understand ## ç¥žç»è®¡ç®—å»ºæ¨¡çš„ç›®æ ‡ä¸ŽæŒ‘æˆ˜ ### Limitation of Deep Learning - ä¸æ“…é•¿å¯¹æŠ—æ ·æœ¬ - å¯¹å›¾åƒçš„ç†è§£æœ‰é™ ![image-20230823105836259](/BrainPy-course-notes/master_content/Notes.assets/image-20230823105836259.png) ### Brain is for Processing Dynamical Information **We never &quot;see&quot; a static image** ![image-20230823105918336](/BrainPy-course-notes/master_content/Notes.assets/image-20230823105918336.png) ### The missing link a computational model of higher cognitive functior ![image-20230823110617639](/BrainPy-course-notes/master_content/Notes.assets/image-20230823110617639.png) çŽ°åœ¨åªæ˜¯åšçš„**å±€éƒ¨**çš„ç½‘ç»œï¼Œæ²¡æœ‰ä¸€ä¸ªæˆåŠŸçš„æ¨¡åž‹ï¼Œèƒ½**ä»Žç¥žç»å…ƒå‡ºå‘æž„å»ºç½‘ç»œï¼Œåˆ°ç³»ç»Ÿå±‚é¢ä¸Š** **åŽŸå› **: å› ä¸ºç¥žç»ç§‘å­¦åº•å±‚æ•°æ®çš„ç¼ºå¤±ï¼Œå¯ä»¥è€ƒè™‘æ•°æ®é©±åŠ¨ã€å¤§æ•°æ®çš„æ–¹å¼æ¥åŠ å¿«å‘å±• ## ç¥žç»è®¡ç®—å»ºæ¨¡çš„å·¥å…· &amp;gt; å·¥æ¬²è¡Œå…¶äº‹ï¼Œå¿…å…ˆåˆ©å…¶å™¨ &amp;gt; We need &quot;PyTorch/TensorFlow&quot; in Computational Neuroscience! ### Challenges in neural modelling æœ‰ä¸åŒçš„å°ºåº¦ - Mutiple-scale - Large-scale - Multiple purposes ![image-20230823111212460](/BrainPy-course-notes/master_content/Notes.assets/image-20230823111212460.png) &amp;gt; The modeling targets and methods are extremely complex, and we need a general framework. ### Limitations of Existing Brain Simulators çŽ°ä»Šçš„æ¡†æž¶ä¸èƒ½æ»¡è¶³ä»¥ä¸Š ![image-20230823111509523](/BrainPy-course-notes/master_content/Notes.assets/image-20230823111509523.png) ### What are needed for a brain simulator 1. Efficiency High-speed simulation on parallel computing devices, etc. 2. Integration Integrated modeling of simulation, training, and analysis 3. Flexibility New models at all scales can be accommodated 4. Extensibility Extensible to new modeling methods(machine learning) éœ€è¦æ–°çš„èŒƒå¼ ### Our solution: BrainPy 4 levels ![image-20230823111903456](/BrainPy-course-notes/master_content/Notes.assets/image-20230823111903456.png) ## ç¥žç»è®¡ç®—å»ºæ¨¡ä¸¾ä¾‹ ### Image understanding: an ill-posed problem Image Understanding = image segmentation + image object recognition &amp;gt; Chicken vs. Egg dilemma &amp;gt; &amp;gt; - Without segmentation, how to recognize &amp;gt; - Without recognition, how to segment **The solution of brain:** Analysis-by-synthesis çŒœæµ‹ä¸ŽéªŒè¯æ–¹æ³• ### Reverse Hierarchy Theory äººçš„æ„ŸçŸ¥æ˜¯æ•´ä½“åˆ°å±€éƒ¨ ### Two pathways for visual information processing ![image-20230823114517888](/BrainPy-course-notes/master_content/Notes.assets/image-20230823114517888.png) ### Key Computational Issues for Global-to-local Neural Information Processing - What are global and local features - How to rapidly extract global features - How to generate global hypotheses - How to implement from global to local processing - The interplay between global and local features - Others #### How to extract global features **Global first = Topology first**(å¤§èŒƒå›´é¦–å…ˆï¼Œé™ˆéœ–) è§†è§‰ç³»ç»Ÿæ›´æ•æ„ŸäºŽæ‹“æ‰‘æ€§è´¨çš„å·®å¼‚ &amp;gt; DNNs has difficulty to recognize topology **A retina-SC network for topology detection** è§†ç½‘è†œåˆ°ä¸Šä¸˜çš„æ£€æµ‹ï¼ŒGap junction coupling ... ### A Model for Motion Pattern Recognition Reservoir Module Decision-making Module ### How to generate &quot;global&quot; hypotheses in the representation space Attractor neural network ![image-20230823115853980](/BrainPy-course-notes/master_content/Notes.assets/image-20230823115853980.png) Levy Flight in Animal Behaviors ![image-20230823120000911](/BrainPy-course-notes/master_content/Notes.assets/image-20230823120000911.png) ### How to process information from global to local Push-pull Feedback A hierarchical Hopfield Model ### Interplay between global and local features A two-pathway model for object recognition ![image-20230823120750349](/BrainPy-course-notes/master_content/Notes.assets/image-20230823120750349.png) Modeling visual masking å¯ä»¥ç”¨two-pathwayå¾ˆå¥½è§£é‡Š # Programming basics ## Python Basics ### Values - Boolean - String - Integer - Float - ... ### Keywords Not allowed to use keywords, they define structure and rules of a language. ```python help(&quot;keywords&quot;) ``` ### Operators æ•°æ®ä¹‹é—´çš„æ“ä½œ #### For Integers and Floats ```python a=5 b=3 # addition + print(&quot;a+b=&quot;,atb) # subtraction - print(&quot;a-b=&quot;,a-b) # multiplication * print(&quot;axb=&quot;a*b) # division / print(&quot;a/b=&quot;,a/b) # power ** print(&quot;a**b=&quot;,a**b) ``` #### Booleans ```python #Boolean experssions # equals: == print(&quot;5==5&quot;,5==5) # do not equal: != print(&quot;5!-5&quot;,5!=5) # greater than: &amp;gt; print(&quot;5&amp;gt;5&quot;,5&amp;gt;5) # greater than or equal: &amp;gt;= print(&quot;5&amp;gt;=5â€5&amp;gt;=5) ``` ```python # logica operators print(&quot;True and False:&quot;, True and False) print(&quot;True or False:&quot;, True or False) print(&quot;not False:&quot;, not False) ``` ### Modules Not all functionality available comes automatically when starting python. ```python import match import numpy as np print(math.pi) print(np.pi) from numpy import pi print(pi) from numpy import * print(pi) ``` ### Control statements #### If ```python a = 5 # In Python, blocks of code are defined using indentation. if a == 5: print(&quot;ok&quot;) ``` &amp;gt; ok #### For ```python # range(5) means a list with integers, 0, 1, 2, 3, 4 for i in range(5): print(i) ``` &amp;gt; 0 &amp;gt; 1 &amp;gt; 2 &amp;gt; 3 &amp;gt; 4 #### While ```python i = 1 while i 1 &amp;gt; 8 &amp;gt; 1000 ### Functions - Functions are used to abstract components of a program. - Much like a mathematical function, they take some input and then find the result. start a function definition with a keyword def - Then comes the function name, with arguments in braces, and then a colon. ```python def func(args1, args2): pass ``` ### Data types #### List - Group variables together - Specific order - Access item with brankets: [ ] - List can be sliced - List can be multiplied - List can be added - Lists are mutable - Copying a list ```python myList = [0, 1, 2, 0,&quot;name&quot;] print(&quot;myList[0]:&quot;, myList[0]) print(&quot;myList[1]:&quot;, myList[1]) print(&quot;myList[3]:&quot;, myList[3]) print(&quot;myList[-1]:&quot;, myList[-1]) print(&quot;myList[-2]:&quot;, myList[-2]) ``` &amp;gt; myList[0]: 0 &amp;gt; myList[1]: 1 &amp;gt; myList[3]: name &amp;gt; myList[-1]: name &amp;gt; myList[-2]: 2.0 ```python myList = [0, 1.0, &quot;hello&quot;] print(&quot;myList[0:2]:&quot;, mylist[0:2]) print(&quot;myList*2:&quot;, myList*2) myList2 = [2,&quot;yes&quot;] print(&quot;myList+myList2:&quot;, myList+myList2) ``` &amp;gt; myList[0:2]: [0ï¼Œ1.0] &amp;gt; myList*2: [0ï¼Œ1.0ï¼Œ hello&apos;ï¼Œ0ï¼Œ1.0ï¼Œ hello&apos;] &amp;gt; myList+myList2: [0ï¼Œ1.0ï¼Œ&apos;hello&apos;ï¼Œ2ï¼Œyes&apos;] #### tuple Tuples are immutable. #### dictionary A dictionary is a collection of key-value pairs ```python d = {} d[1] = 2 d[&quot;a&quot;] = 3 print(&quot;d: &quot;, d) c = {1:2, &quot;a&quot;:3} print(&quot;c: &quot;, c) print(&quot;c[1]: &quot;, c[1]) ``` &amp;gt; d: {1: 2, &apos;a&apos;: 3} &amp;gt; c: {1: 2, &apos;a&apos;: 3} &amp;gt; c[1]: 2 ### Class In Python, everything is an object. Classes are objects, instances of classes are objects, modules are objects, and functions are objects. 1. a **type** 2. an internal **data representation** (primitive or composite) 3. a set of procedures for **interaction** with the object **a simple example** ```python # define class class Linear(): pass # instantiate object layer1 = Linear() print(layer1) ``` &amp;gt; `` #### Initializing an object ```python # define class class Linear(): # It refers to the object (instance) itself def __init__(self, n_input): self.n_input = n_input layer1 = Linear(100) layer2 = Linear(1000) print(&quot;layer1 : &quot;, layer1.n_input) print(&quot;layer2 : &quot;, layer2.n_input) ``` &amp;gt; layer1 : 100 &amp;gt; layer2 : 1000 #### Class has methods (similar to functions) ```python # define class class Linear(): ### It refers to the the object (instance) itself def __init__(self, n_input, n_output): self.n_input = n_input self.n_output = n_output def compute n params(self): num_params = self.n_input * self.n_output return num_params layerl = Linear(10,100) print(layerl.compute_n_params()) ``` &amp;gt; 1000 ## NumPy Basic ### Numpy Introduction - Fundamental package for scientific computing with Python - N-dimensional array object - Linear algebra, frontier transform, random number capacities - Building block for other packages (e.g. Scipy) ### Array - Arrays are mutable - Arrays attributes - ... ```python A = np.zeros((2, 2)) print(A) ``` &amp;gt; [[0. 0.] &amp;gt; [0. 0.]] ```python a.ndim # 2 dimension a.shape # (2, 5) shape of array a.size # 10 $ of elements a.T # transpose a.dtype # data type ``` #### Array broadcasting When operating on two arrays, numpy compares shapes. Two dimensions are compatible when 1. They are of equal size 2. One of them is 1 ![image-20230823143622229](/BrainPy-course-notes/master_content/Notes.assets/image-20230823143622229.png) ### Vector operations - Inner product - Outer product - Dot product (matrix multiplication) ```python u = [1, 2, 3] v = [1, 1, 1] np.inner(u, v) np.outer(u, v) np.dot(u, v) ``` &amp;gt; 6 &amp;gt; array([[1, 1, 1], &amp;gt; [2, 2, 2], &amp;gt; [3, 3, 3]]) &amp;gt; 6 ### Matrix operations - `np.ones` - `.T` - `np.dot` - `np.eye` - `np.trace` - `np.row_stack` - `np.column_stack` ### Operations along axes ```python a = np.ones((2, 3)) print(a) a.sum() a.sum(axis=0) a.cumsum() a.cumsum(axis=0) ``` ### Slicing arrays ```python a = np.random.random((2, 3)) print(a) a[0,:] # first row, all columns a[0:2] # first and second rows, al columns a[:,1:3]# all rows, second and third columns ``` ### Reshape ```python a = np.ones((10,1)) a.reshape(2,5) ``` ### Linear algebra ```python qr # Computes the QR decomposition cholesky # Computes the Cholesky decomposition inv(A) # Inverse solve(A,b) # Solves Ax = b for A full rank lstsq(A,b) # Solves arg minx //Ax - b//2 eig(A) # Eigenvalue decomposition eigvals(A) # Computes eigenvalues svd(Aï¼Œfull) # Sinqular value decomposition pinv(A) # Computes pseudo-inverse of A ``` ### Fourier transform ```python import numpy.fft fft # 1-dimensional DFT fft2 # 2-dimensional DFT fftn # N-dimensional DFT ifft # 1-dimensional inverse DFT (etc.) rfft # Real DFT (1-dim) ``` ### Random sampling ```python import numpy.random rand(d0, d1, ..., dn) # Random values in a given shape randn(d0, d1, ..., dn) # Random standard normal randint(lo, hi, size) # Random integers [lo hi) choice(a, size, repl, p) # Sample from a shuffle(a) # Permutation (in-place) permutation(a) # Permutation (new array) ``` ### Distributions in random ```python import numpy.random beta binomial chisquare exponential dirichlet gamma laplace lognormal ... ``` ### Scipy - `SciPy` is a library of algorithms and mathematical tools built to work with `NumPy ` arrays. - `scipy.linalg linear algebra` - `scipy.stats statistics` - `scipy.optimize optimization` - `scipy.sparse sparse matrices` - `scipy.signal signal processing` - etc. ## BrainPy introduction ### Modeling demands - Large-scale - Multi-scale - Methods ### BrainPy Architecture - Infrastructure - Functions - Just-in-time compilation - Devices ![image-20230823145349681](/BrainPy-course-notes/master_content/Notes.assets/image-20230823145349681.png) ### Main features #### Dense operators - Compatible with `NumPy`, `TensorFlow`, `PyTorch` and other dense matrix operator syntax. - Users do not need to learn and get started programming directly. #### Dedicated operatorsq - Applies brain dynamics sparse connectivity properties with event-driven computational features. - Reduce the complexity of brain dynamics simulations by several orders of magnitude. #### Numerical Integrators - Ordinary differential equations: brainpy.odeint - Stochastic differential equations: brainpy.sdeint - Fractional differential equations: brainpy.fdeint - Delayed differential equations #### Modular and composable ä»Žå¾®è§‚åˆ°å®è§‚ **brainpy.DynamicalSystem** ![image-20230823151159786](/BrainPy-course-notes/master_content/Notes.assets/image-20230823151159786.png) #### JIT of object-oriented BrainPy provides object-oriented transformations: - `brainpy.math.jit` - `brainpy.math.grad` - `brainpy.math.for_loop` - `brainpy.math.ifelse` ## BrainPy Programming Basics ### Just-in-Time compilation Just In Time Compilation (JIT, or Dynamic Translation), is compilation that is being done during the execution of a program. JIT compilation attempts to use **the benefits of both**. While the interpreted program is being run, the JIT compiler determines the most frequently used code and compiles it to machine code. The advantages of a JIT are due to the fact that since the compilation takes place in run time, a JIT compiler has access to dynamic runtime information enabling it to make better optimizations (such as inlining functions). ```python def gelu(x): sqrt = bm.sqrt(2 / bm.pi) cdf = 0.5 * (1.0 + bm.tanh(sqrt * (x + 0.044715 * (x ** 3)))) y = x *cdf return y &amp;gt;&amp;gt;&amp;gt; gelu_jit = bm.jit(gelu) # ä½¿ç”¨JIT ``` ### Object-oriented JIT compilation - The class object must be inherited from brainpy.BrainPyObject, the base class of BrainPy, whose methods will be automatically JIT compiled. - All time-dependent variables must be defined as brainpy.math.Variable. ```python class LogisticRegression(bp.BrainPyObject): def __init__(self, dimension): super(LogisticRegression, self).__init__() # parameters self.dimension = dimension # variables self.w = bm.Variable(2.0 * bm.ones(dimension) - 1.3) def __call__(self, X, Y): u = bm.dot(((1.0 / (1.0 + bm.exp(-Y * bm.dot(X, self.w))) - 1.0) * Y), X) self.w.value = self.w - u # in-place update ``` **ExampleL Run a neuron model** ```python model = bp.neurons.HH(1000) #ä¸€å…±1000ä¸ªç¥žç»å…ƒ runner = bp.DSRunner(target=model, inputs=(&apos;input&apos;, 10.)) # jité»˜è®¤ä¸ºTrue runner(duration=1000, eval_time=True) #æ¨¡æ‹Ÿ 1000ms ``` ç¦ç”¨JITæ¥debug ### Data operations #### Array ç­‰ä»·äºŽ`numpy`çš„`array` #### BrainPy arrays &amp; JAX arrays ```python t1 = bm.arange(3) print(t1) print(t1.value) ``` &amp;gt; JaxArray([0, 1, 2], dtype=int32) &amp;gt; DeviceArray([0, 1, 2], dtype=int32) #### Variables Arrays that are not marked as dynamic variables will be JIT-compiled as static arrays, and modifications to static arrays will not be valid in the JIT compilation environment. ```python t = bm.arange(4) v = bm.Variable(t) print(v) print(v.value) ``` &amp;gt; Variable([0, 1, 2, 3], dtype=int32) &amp;gt; DeviceArray([0, 1, 2, 3], dtype=int32) ### Variables **In-place updating** å°±åœ°æ›´æ–° #### Indexing and slicing - Indexing: `v[i] = a` or `v[(1, 3)] = c` - Slicing: `v[i:j] = b` - Slicing all values `v[:] = d`, `v[...] = e` #### Augmented assignment - add - subtract - divide - multiply - floor divide - modulo - power - and - or - xor - left shift - right shift #### Value assignment ```python v.value = bm.arange(10) check_no_change(v) ``` #### Update assignment ```python v.update(bm.random.randint(0, 20, size=10)) ``` ### Control flows #### If-else `brainpy.math.where` ```python a = 1. bm.where(a DeviceArray(1., dtype=float32, weak_type=True) `brainpy.math.ifelse` ```python def ifelse(condition, branches, operands): true_fun, false_fun = branches if condition: return true_fun(operands) else: return false_fun(operands) ``` #### For loop ```python import brainpy.math hist_of_out_vars = brainpy.math.for_loop(body_fun, operands) ``` #### While loop ```python i = bm.Variable(bm.zeros(1)) counter = bm.Variable(bm.zeros(1)) def cond_f(): return i[0] $$ (2\pi a\Delta x)c_{\mathrm{M}}\frac{\partial V(x,t)}{\partial t}+(2\pi a\Delta x)i_{\mathrm{ion}}=\frac{\pi a^{2}}{\rho_{\mathrm{L}}}\frac{\partial V(x+\Delta x,t)}{\partial x}-\frac{\pi a^{2}}{\rho_{\mathrm{L}}}\frac{\partial V(x,t)}{\partial x} $$ **Cable Equation** $$ c_\mathrm{M}\frac{\partial V(x,t)}{\partial t}=\frac{a}{2\rho_\mathrm{L}}\frac{\partial^2V(x,t)}{\partial x^2}-i_\mathrm{ion} $$ ç”µæµåœ¨é€šè¿‡é•¿ç›´å¯¼ä½“æ—¶ä¼šæ³„éœ²ç”µæµï¼Œå¦‚ä½•è®°å½•è†œç”µä½ï¼Œå¯ä»¥ä½¿ç”¨æ­¤æ–¹ç¨‹æ¥æè¿° **Passive conduction:** ion currents are caused by leaky channels exclusively $$ i_{\mathrm{ion}}=V(x,t)/r_{\mathrm{M}} $$ -&amp;gt; $$ \begin{aligned}c_\mathrm{M}\frac{\partial V(x,t)}{\partial t}&amp;=\frac{a}{2\rho_\mathrm{L}}\frac{\partial^2V(x,t)}{\partial x^2}-\frac{V(x,t)}{r_\mathrm{M}}\\\\\tau\frac{\partial V(x,t)}{\partial t}&amp;=\lambda^2\frac{\partial^2V(x,t)}{\partial x^2}-V(x,t)\quad\lambda=\sqrt{0.5ar_\mathrm{M}/\rho_\mathrm{L}}\end{aligned} $$ æ²¡æœ‰åŠ¨ä½œç”µä½ï¼Œå•çº¯é€šè¿‡ç”µç¼†ä¼ è¾“ ![image-20230824102932665](/BrainPy-course-notes/master_content/Notes.assets/image-20230824102932665.png) If a constant external current is applied to ð‘¥ = 0 the steady-state membrane potential $ð‘‰_{ss}(ð‘¥)$ is $$ \lambda^2\frac{\mathrm{d}^2V_{\mathrm{ss}}(x)}{\mathrm{d}x^2}-V_{\mathrm{ss}}(x)=0\longrightarrow V_{\mathrm{ss}}(x)=\frac{\lambda\rho_{\mathrm{L}}}{\pi a^2}I_0e^{-x/\lambda} $$ ç”µä¿¡å·æ— è¡°å‡ä¼ æ’­: åŠ¨ä½œç”µä½ ## Action potential &amp; active transport Steps of an action potential: - Depolarization - Repolarization - Hyperpolarization - Resting Characteristics: - All-or-none - Fixed shape - Active electrical property ![image-20230824103322522](/BrainPy-course-notes/master_content/Notes.assets/image-20230824103322522.png) How to simulate an action potential? $$ \begin{aligned} \frac{I(t)}{A}&amp; =c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+i_{\mathrm{ion}} \\ \Rightarrow\quad c_{\mathrm{M}}\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}&amp; =-g_{\mathrm{Cl}}(V_{\mathrm{M}}-E_{\mathrm{Cl}})-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}})-g_{\mathrm{Na}}(V_{\mathrm{M}}-E_{\mathrm{Na}})+\frac{I(t)}{A} \end{aligned} $$ ç¦»å­é€šé“çš„å¼€é—­ä¼šéšç€ç”µåŽ‹è€Œå˜åŒ–ï¼Œç”µå¯¼ä¹Ÿéšç€ç”µåŽ‹è€Œå˜åŒ– Mechanism: voltage-gated ion channels **HHå»ºæ¨¡æ€è·¯ï¼šé€šè¿‡ç”µå¯¼** ### Nodes of Ranvier Saltatory conduction with a much higher speed and less energy consumption ä¸¤ä¸ªéƒŽé£žç»“ä¹‹é—´ä¼šæœ‰ç¦»å­é€šé“ï¼Œæ—¢æœ‰è¢«åŠ¨ä¼ å¯¼ï¼Œä¹Ÿæœ‰ä¸»åŠ¨çš„é˜²æ­¢è¡°å‡ ![image-20230824104220106](/BrainPy-course-notes/master_content/Notes.assets/image-20230824104220106.png) ## The Hodgkin-Huxley Model ### Modeling of each ion channel Modeling of each ion channel: $$ g_m=\bar{g}_mm^x $$ Modeling of each ion gate: $$ \mathcal{C}\underset{}{\operatorname*{\overset{\alpha(\mathrm{V})}{\underset{\beta(\mathrm{V})}{\operatorname*{\longrightarrow}}}}\mathcal{O}} \\ \Rightarrow \begin{aligned} \frac{\mathrm{d}m}{\mathrm{d}t}&amp; =\alpha(V)(1-m)-\beta(V)m \\ &amp;=\frac{m_{\infty}(V)-m}{\tau_{m}(V)} \end{aligned} \\ \\ \begin{aligned}m_\infty(V)&amp;=\frac{\alpha(V)}{\alpha(V)+\beta(V)}.\\\tau_m(V)&amp;=\frac{1}{\alpha(V)+\beta(V)}\end{aligned} $$ $$ \text{If}\ V\text{ is constant:}m(t)=m_\infty(V)+(m_0-m_\infty(V))\mathrm{e}^{-t/\tau_m(V)} $$ ### Voltage clamp $$ \begin{aligned} \frac{I(t)}{A}&amp; =c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+i_{\mathrm{ion}} \\ \Rightarrow\quad c_{\mathrm{M}}\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}&amp; =-g_{\mathrm{Cl}}(V_{\mathrm{M}}-E_{\mathrm{Cl}})-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}})-g_{\mathrm{Na}}(V_{\mathrm{M}}-E_{\mathrm{Na}})+\frac{I(t)}{A} \end{aligned} $$ - The membrane potential is kept constant - The current from capacitors is excluded - Currents must come from leaky/voltage-gated ion channels $$ \begin{aligned}I_{\mathrm{cap}}&amp;=c\frac{dV}{dt}=0\\I_{\mathrm{fb}}&amp;=\quad i_{\mathrm{ion}}=g_{\mathrm{Na}}(V-E_{\mathrm{Na}})+g_{\mathrm{K}}(V-E_{\mathrm{K}})+g_{\mathrm{L}}(V-E_{\mathrm{L}})\end{aligned} $$ åªæµ‹é‡ä¸€ä¸ªç¦»å­é€šé“å°±å¯ä»¥å¾ˆå®¹æ˜“å¾—åˆ°ç”µå¯¼ ![image-20230824111620056](/BrainPy-course-notes/master_content/Notes.assets/image-20230824111620056.png) ### Leaky channel Hyperpolarization â†’ the sodium and potassium channels are closed $$ I_{\mathrm{fb}}=g_{\mathrm{Na}}(V-E_{\mathrm{Na}})+g_{\mathrm{K}}(V-E_{\mathrm{K}})+g_{\mathrm{L}}(V-E_{\mathrm{L}}) $$ $$ \Rightarrow I_{\mathrm{fb}}=g_L(V-E_L) $$ $$ g_\mathrm{L}=0.3\mathrm{mS/cm}^2,E_\mathrm{L}=-54.4\mathrm{mV} $$ #### Potassium and sodium channels Potassium channels: Use choline to eliminate the inward current of Na + Na + current: $I_{fb} - I_{K}$ ![image-20230824112328953](/BrainPy-course-notes/master_content/Notes.assets/image-20230824112328953.png) ![image-20230824112333144](/BrainPy-course-notes/master_content/Notes.assets/image-20230824112333144.png) è½¬åŒ–é€ŸçŽ‡å’Œç”µå¯¼çŽ‡ä¸¤ä¸ªå› ç´  Potassium channels - Resting state (gate closed) - Activated state (gate open) â†’ Activation gate: $g_{\mathrm{K}}=\bar{g}_{K}n^{x}$ Sodium channels - Resting state (gate closed) - Activated state (gate open) - Inactivated state (gate blocked) â†’ Activation gate + inactivation gate: $g_{\mathrm{Na}}=\bar{g}_\text{Na}m^3h$ ![image-20230824113116329](/BrainPy-course-notes/master_content/Notes.assets/image-20230824113116329.png) The gates of sodium channels Modeling of each ion gate: $$ \begin{aligned} &amp;\text{gk}&amp;&amp; =\bar{g}_{K}n^{x} \\ &amp;\text{gNa}&amp;&amp; =\bar{g}_{\mathrm{Na}}m^{3}h \\ &amp;\frac{\mathrm{d}n}{\mathrm{d}t}&amp;&amp; =\alpha_{n}(V)(1-n)-\beta_{n}(V)n \\ &amp;\frac{\mathrm{d}m}{\mathrm{d}t}&amp;&amp; =\alpha_{m}(V)(1-m)-\beta_{m}(V)m \\ &amp;\frac{\mathrm{d}h}{\mathrm{d}t}&amp;&amp; =\alpha_{h}(V)(1-h)-\beta_{h}(V)h \end{aligned} $$ $$ \begin{aligned} \frac{\mathrm{d}m}{\mathrm{d}t}&amp; =\alpha(V)(1-m)-\beta(V)m \\ &amp;=\frac{m_{\infty}(V)-m}{\tau_{m}(V)} \end{aligned} $$ $$ \begin{aligned}m_\infty(V)&amp;=\frac{\alpha(V)}{\alpha(V)+\beta(V)}\\\tau_m(V)&amp;=\frac{1}{\alpha(V)+\beta(V)}\end{aligned}. $$ $$ m(t)=m_\infty(V)+(m_0-m_\infty(V))\mathrm{e}^{-t/\tau_m(V)} $$ ### The Hodgkin-Huxley(HH) Model $$ c_\mathrm{M}\frac{\mathrm{d}V_\mathrm{M}}{\mathrm{d}t}=-g_\mathrm{Cl}(V_\mathrm{M}-E_\mathrm{Cl})-g_\mathrm{K}(V_\mathrm{M}-E_\mathrm{K})-g_\mathrm{Na}(V_\mathrm{M}-E_\mathrm{Na})+\frac{I(t)}{A} $$ æœ¬è´¨æ˜¯4ä¸ªå¾®åˆ†æ–¹ç¨‹è”ç«‹åœ¨ä¸€èµ· $$ \left\{\begin{aligned}&amp;c\frac{\mathrm{d}V}{\mathrm{d}t}=-\bar{g}_\text{Na}m^3h(V-E_\text{Na})-\bar{g}_\text{K}n^4(V-E_\text{K})-\bar{g}_\text{L}(V-E_\text{L})+I_\text{ext},\\&amp;\frac{\mathrm{d}n}{\mathrm{d}t}=\phi\left[\alpha_n(V)(1-n)-\beta_n(V)n\right]\\&amp;\frac{\mathrm{d}m}{\mathrm{d}t}=\phi\left[\alpha_m(V)(1-m)-\beta_m(V)m\right],\\&amp;\frac{\mathrm{d}h}{\mathrm{d}t}=\phi\left[\alpha_h(V)(1-h)-\beta_h(V)h\right],\end{aligned}\right. $$ $$ \begin{aligned}\alpha_n(V)&amp;=\frac{0.01(V+55)}{1-\exp\left(-\frac{V+55}{10}\right)},\quad\beta_n(V)&amp;=0.125\exp\left(-\frac{V+65}{80}\right),\\\alpha_h(V)&amp;=0.07\exp\left(-\frac{V+65}{20}\right),\quad\beta_n(V)&amp;=\frac{1}{\left(\exp\left(-\frac{V+55}{10}\right)+1\right)},\\\alpha_m(V)&amp;=\frac{0.1(V+40)}{1-\exp\left(-(V+40)/10\right)},\quad\beta_m(V)&amp;=4\exp\left(-(V+65)/18\right).\end{aligned} $$ $$ \phi=Q_{10}^{(T-T_{\mathrm{base}})/10} $$ æ¯ä¸€æ­¥ç¬¦åˆç”Ÿç‰©å­¦ ![image-20230824113714178](/BrainPy-course-notes/master_content/Notes.assets/image-20230824113714178.png) #### How to fit each gating variable? **Fitting n:** $g_{\mathbf{K}}=\bar{g}_{K}n^{x}\quad m(t)=m_{\infty}(V)+(m_{0}-\color{red}{\boxed{m_{\infty}(V)}})\mathrm{e}^{-t/\pi_{m}(V)}$ â†’ $g_\mathrm{K}(V,t)=\bar{g}_\mathrm{K}\left[n_\infty(V)-(n_\infty(V)-n_0(V))\mathrm{e}^{-\frac{t}{\tau_n(V)}}\right]^x$ by $g_{\mathrm{K}\infty}=\bar{g}_{\mathrm{K}}n_{\infty}^{x},g_{\mathrm{K}0}=\bar{g}_{\mathrm{K}}n_{0}^{x}$ â†’ $g_{\mathrm{K}}(V,t)=\left[g_{\mathrm{K}\infty}^{1/x}-(g_{\mathrm{K}\infty}^{1/x}-g_{\mathrm{K}0}^{1/x})\mathrm{e}^{-\frac{t}{\tau_{n}(V)}}\right]^{x}$ ![image-20230824114623467](/BrainPy-course-notes/master_content/Notes.assets/image-20230824114623467.png) # Hodgkin-Huxley brain dynamics programming ## Dynamics Programming Basics ### Integrators å¾®åˆ†å™¨ ![image-20230824140806650](/BrainPy-course-notes/master_content/Notes.assets/image-20230824140806650.png) **example** FitzHugh-Nagumo equation $$ \begin{aligned}\tau\dot{w}&amp;=v+a-bw,\\\dot{v}&amp;=v-\frac{ u^3}{3}-w+I_{\mathrm{ext}}.\end{aligned} $$ ```python @bp.odeint(method=&apos;Euler&apos;, dt=0.01) def integral(V, w, t, Iext, a, b, tau): dw = (V + a - b * w) / tau dV = V - V * V * V / 3 - w + Iext return dV, dw ``` **JointEq** In a dynamical system, there may be multiple variables that change dynamically over time. Sometimes these variables are interrelated, and updating one variable requires other variables as inputs. For better integration accuracy, we recommend that you use `brainpy.JointEq` to jointly solve interrelated differential equations. ```python a, b = 0.02, 0.20 dV = lambda V, t, w, Iext: 0.04 * V * V + 5 * V + 140 - w + Iext # ç¬¬ä¸€ä¸ªæ–¹ç¨‹ dw = lambda w, t, V: a * (b * V - w) # ç¬¬äºŒä¸ªæ–¹ç¨‹ joint_eq = bp.JointEq(dV, dw) # è”åˆå¾®åˆ†æ–¹ç¨‹ integral2 = bp.odeint(joint_eq, method=&apos;rk2&apos;) # å®šä¹‰è¯¥è”åˆå¾®åˆ†æ–¹ç¨‹çš„æ•°å€¼ç§¯åˆ†æ–¹æ³• ``` ```python # å£°æ˜Žç§¯åˆ†è¿è¡Œå™¨ runner = bp.integrators.IntegratorRunner( integral, monitors=[&apos;V&apos;] inits=dict(V=0., w=0.) args=dict(a=a, b=b, tau=tau, Iext=Iext), dt=0.01 ) # ä½¿ç”¨ç§¯åˆ†è¿è¡Œå™¨æ¥è¿›è¡Œæ¨¡æ‹Ÿ100msï¼Œç»“åˆæ­¥é•¿dt=0.01 runner.run(100.) plt.plot(runner.mon.ts, runner.mon.V) plt.show() ``` ![image-20230824142019832](/BrainPy-course-notes/master_content/Notes.assets/image-20230824142019832.png) ### `DynamicalSystem` BrainPy provides a generic `SynamicalSystem` class to define various types of dynamical models. BrainPy supports modelings in brain simulation and brain-inspired computing. All these supports are based on one common concept: **Dynamical System** via `brainpy.DynamicalSystem`. #### What is `DynamicalSystem` A `DynamicalSystem` defines the updating rule of the model at single time step. 1. For models with state, `DynamicalSystem` defines the state transition from $t$ to $t + dt$, i.e., $S(t+dt)=F(S(t),x,t,dt)$, where $S$ is the state, $x$ is input, $t$ is the time, and $dt$ is the time step. This is the case for recurrent neural networks (like GRU, LSTM), neuron models (like HH, LIF), or synapse models which are widely used in brain simulation. 2. However, for models in deep learning, like convolution and fully-connected linear layers, `DynamicalSystem` defines the input-to-output mapping, i.e., $y=F(x,t)$. ![img](https://brainpy.readthedocs.io/en/latest/_images/dynamical_system.png) #### How to define `DynamicalSystem` ```python class YourDynamicalSystem(bp.DynamicalSystem): def update(self, x): ... ``` Instead of input x, there are shared arguments across all nodes/layers in the network: - the current time `t`, or - the current running index `i`, or - the current time step `dt`, or - the current phase of training or testing `fit=True/False`. Here, it is necessary to explain the usage of `bp.share`. - `bp.share.save( )`: The function saves shared arguments in the global context. User can save shared arguments in tow ways, for example, if user want to set the current time `t=100`, the current time step `dt=0.1`,the user can use `bp.share.save(&quot;t&quot;,100,&quot;dt&quot;,0.1)` or `bp.share.save(t=100,dt=0.1)`. - `bp.share.load( )`: The function gets the shared data by the `key`, for example, `bp.share.load(&quot;t&quot;)`. - `bp.share.clear_shargs( )`: The function clears the specific shared arguments in the global context, for example, `bp.share.clear_shargs(&quot;t&quot;)`. - `bp.share.clear( )`: The function clears all shared arguments in the global context. #### How to run `DynamicalSystem` As we have stated above that `DynamicalSystem` only defines the updating rule at single time step, to run a `DynamicalSystem` instance over time, we need a for loop mechanism. ![img](https://brainpy.readthedocs.io/en/latest/_images/dynamical_system_and_dsrunner.png) ##### `brainpy.math.for_loop` `for_loop` is a structural control flow API which runs a function with the looping over the inputs. Moreover, this API just-in-time compile the looping process into the machine code. ```python inputs = bp.inputs.section_input([0., 6.0, 0.], [100., 200., 100.]) indices = np.arange(inputs.size) def run(i, x): neu.step_run(i, x) return neu.V.value vs = bm.for_loop(run, (indices, inputs), progress_bar=True) ``` ##### `brainpy.LoopOverTime` Different from `for_loop`, `brainpy.LoopOverTime` is used for constructing a dynamical system that automatically loops the model over time when receiving an input. `for_loop` runs the model over time. While `brainpy.LoopOverTime` creates a model which will run the model over time when calling it. ```python net2.reset_state(batch_size=10) looper = bp.LoopOverTime(net2) out = looper(currents) ``` ##### `brainpy.DSRunner` **Initializing a `DSRunner`** Generally, we can initialize a runner for dynamical systems with the format of: ``` runner = DSRunner(target=instance_of_dynamical_system, inputs=inputs_for_target_DynamicalSystem, monitors=interested_variables_to_monitor, dyn_vars=dynamical_changed_variables, jit=enable_jit_or_not, progress_bar=report_the_running_progress, numpy_mon_after_run=transform_into_numpy_ndarray ) ``` - `target` specifies the model to be simulated. It must an instance of brainpy.DynamicalSystem. - `inputs` is used to define the input operations for specific variables. - It should be the format of `[(target, value, [type, operation])]`, where `target` is the input target, `value` is the input value, `type` is the input type (such as â€œfixâ€, â€œiterâ€, â€œfuncâ€), `operation` is the operation for inputs (such as â€œ+â€, â€œ-â€, â€œ*â€, â€œ/â€, â€œ=â€). Also, if you want to specify multiple inputs, just give multiple `(target, value, [type, operation])`, such as `[(target1, value1), (target2, value2)]`. - It can also be a function, which is used to manually specify the inputs for the target variables. This input function should receive one argument `tdi` which contains the shared arguments like time `t`, time step `dt`, and index `i`. - `monitors` is used to define target variables in the model. During the simulation, the history values of the monitored variables will be recorded. It can also to monitor variables by callable functions and it should be a `dict`. The `key` should be a string for later retrieval by `runner.mon[key]`. The `value` should be a callable function which receives an argument: `tdt`. - `dyn_vars` is used to specify all the dynamically changed [variables](https://brainpy.readthedocs.io/en/latest/tutorial_math/variables.html) used in the `target` model. - `jit` determines whether to use JIT compilation during the simulation. - `progress_bar` determines whether to use progress bar to report the running progress or not. - `numpy_mon_after_run` determines whether to transform the JAX arrays into numpy ndarray or not when the network finishes running. **Running a `DSRunner`** After initialization of the runner, users can call `.run()` function to run the simulation. The format of function `.run()` is showed as follows: ```python runner.run(duration=simulation_time_length, inputs=input_data, reset_state=whether_reset_the_model_states, shared_args=shared_arguments_across_different_layers, progress_bar=report_the_running_progress, eval_time=evaluate_the_running_time ) ``` - `duration` is the simulation time length. - `inputs` is the input data. If `inputs_are_batching=True`, `inputs` must be a PyTree of data with two dimensions: `(num_sample, num_time, ...)`. Otherwise, the `inputs` should be a PyTree of data with one dimension: `(num_time, ...)`. - `reset_state` determines whether to reset the model states. - `shared_args` is shared arguments across different layers. All the layers can access the elements in `shared_args`. - `progress_bar` determines whether to use progress bar to report the running progress or not. - `eval_time` determines whether to evaluate the running time. ### Monitors ```python # initialize monitor through a list of strings runner1 = bp.DSRunner(target=net, monitors=[&apos;E.spike&apos;, &apos;E.V&apos;, &apos;I.spike&apos;, &apos;I.V&apos;], # 4 elements in monitors inputs=[(&apos;E.input&apos;, 20.), (&apos;I.input&apos;, 20.)], jit=True) ``` Once we call the runner with a given time duration, the monitor will automatically record the variable evolutions in the corresponding models. Afterwards, users can access these variable trajectories by using .mon.[variable_name]. The default history times .mon.ts will also be generated after the model finishes its running. Letâ€™s see an example. ```python runner1.run(100.) bp.visualize.raster_plot(runner1.mon.ts, runner1.mon[&apos;E.spike&apos;], show=True) ``` **Initialization with index specification** ```python monitors=[(&apos;E.spike&apos;, [1, 2, 3]), # monitor values of Variable at index of [1, 2, 3] &apos;E.V&apos;], # monitor all values of Variable &apos;V&apos; ``` &amp;gt; The monitor shape of &quot;E.V&quot; is (run length, variable size) = (1000, 3200) &amp;gt; The monitor shape of &quot;E.spike&quot; is (run length, index size) = (1000, 3) **Explicit monitor target** ```python monitors={&apos;spike&apos;: net.E.spike, &apos;V&apos;: net.E.V}, ``` &amp;gt; The monitor shape of &quot;V&quot; is = (1000, 3200) &amp;gt; The monitor shape of &quot;spike&quot; is = (1000, 3200) **Explicit monitor target with index specification** ```python monitors={&apos;E.spike&apos;: (net.E.spike, [1, 2]), # monitor values of Variable at index of [1, 2] &apos;E.V&apos;: net.E.V}, # monitor all values of Variable &apos;V&apos; ``` &amp;gt; The monitor shape of &quot;E.V&quot; is = (1000, 3200) &amp;gt; The monitor shape of &quot;E.spike&quot; is = (1000, 2) ### Inputs In brain dynamics simulation, various inputs are usually given to different units of the dynamical system. In BrainPy, `inputs` can be specified to runners for dynamical systems. The aim of `inputs` is to mimic the input operations in experiments like Transcranial Magnetic Stimulation (TMS) and patch clamp recording. `inputs` should have the format like `(target, value, [type, operation])`, where - `target` is the target variable to inject the input. - `value` is the input value. It can be a scalar, a tensor, or a iterable object/function. - `type` is the type of the input value. It support two types of input: `fix` and `iter`. The first one means that the data is static; the second one denotes the data can be iterable, no matter whether the input value is a tensor or a function. The `iter` type must be explicitly stated. - `operation` is the input operation on the target variable. It should be set as one of `{ + , - , * , / , = }`, and if users do not provide this item explicitly, it will be set to â€˜+â€™ by default, which means that the target variable will be updated as `val = val + input`. #### Static inputs ```python runner6 = bp.DSRunner(target=net, monitors=[&apos;E.spike&apos;], inputs=[(&apos;E.input&apos;, 20.), (&apos;I.input&apos;, 20.)], # static inputs jit=True) runner6.run(100.) bp.visualize.raster_plot(runner6.mon.ts, runner6.mon[&apos;E.spike&apos;]) ``` #### Iterable inputs ```python I, length = bp.inputs.section_input(values=[0, 20., 0], durations=[100, 1000, 100], return_length=True, dt=0.1) runner7 = bp.DSRunner(target=net, monitors=[&apos;E.spike&apos;], inputs=[(&apos;E.input&apos;, I, &apos;iter&apos;), (&apos;I.input&apos;, I, &apos;iter&apos;)], # iterable inputs jit=True) runner7.run(length) bp.visualize.raster_plot(runner7.mon.ts, runner7.mon[&apos;E.spike&apos;]) ``` ## Run a built-in HH model [Using Built-in Models â€” BrainPy documentation](https://brainpy.readthedocs.io/en/latest/tutorial_building/overview_of_dynamic_model.html) ```python import brainpy as bp import brainpy.math as bm current, length = bp.inputs.section_input(values=[0., bm.asarray([1., 2., 4., 8., 10., 15.]), 0.], durations=[10, 2, 25], return_length=True) hh_neurons = bp.neurons.HH(current.shape[1]) runner = bp.DSRunner(hh_neurons, monitors=[&apos;V&apos;, &apos;m&apos;, &apos;h&apos;, &apos;n&apos;], inputs=(&apos;input&apos;, current, &apos;iter&apos;)) runner.run(length) ``` ## Run a HH model from scratch The mathematic expression of the HH model $$ \left\{\begin{aligned}&amp;c\frac{\mathrm{d}V}{\mathrm{d}t}=-\bar{g}_\text{Na}m^3h(V-E_\text{Na})-\bar{g}_\text{K}n^4(V-E_\text{K})-\bar{g}_\text{L}(V-E_\text{L})+I_\text{ext},\\&amp;\frac{\mathrm{d}n}{\mathrm{d}t}=\phi\left[\alpha_n(V)(1-n)-\beta_n(V)n\right]\\&amp;\frac{\mathrm{d}m}{\mathrm{d}t}=\phi\left[\alpha_m(V)(1-m)-\beta_m(V)m\right],\\&amp;\frac{\mathrm{d}h}{\mathrm{d}t}=\phi\left[\alpha_h(V)(1-h)-\beta_h(V)h\right],\end{aligned}\right. $$ $$ \begin{aligned}\alpha_n(V)&amp;=\frac{0.01(V+55)}{1-\exp\left(-\frac{V+55}{10}\right)},\quad\beta_n(V)&amp;=0.125\exp\left(-\frac{V+65}{80}\right),\\\alpha_h(V)&amp;=0.07\exp\left(-\frac{V+65}{20}\right),\quad\beta_n(V)&amp;=\frac{1}{\left(\exp\left(-\frac{V+55}{10}\right)+1\right)},\\\alpha_m(V)&amp;=\frac{0.1(V+40)}{1-\exp\left(-(V+40)/10\right)},\quad\beta_m(V)&amp;=4\exp\left(-(V+65)/18\right).\end{aligned} $$ $$ \phi=Q_{10}^{(T-T_{\mathrm{base}})/10} $$ V: the membrane potential n: activation variable of the Kt channel m: activation variable of the Nat channel h; inactivation variable of the Nat channe ### Define HH model `class` - Inherit `bp.dyn.NeuDyn` ```python import brainpy as bp import brainpy.math as bm class HH(bp.dyn.NeuDyn): def __init__(self, size, ENa=50., gNa=120., Ek=-77., gK=36., EL=-54.387, gL=0.03, V_th=0., C=1.0, T=6.3): super(HH, self).__init__(size=size) ``` ### Initialization ```python import brainpy as bp import brainpy.math as bm class HH(bp.dyn.NeuDyn): def __init__(self, size, ENa=50., gNa=120., Ek=-77., gK=36., EL=-54.387, gL=0.03, V_th=0., C=1.0, T=6.3): super(HH, self).__init__(size=size) # parameters self.ENa = ENa self.EK = EK self.EL = EL self.gNA = gNa self.gK = gK self.gL = gL self.C = C self.V_th = V_th self.T_base = 6.3 self.phi = 3.0 ** ((T - self.T_base) / 10.0) # variable self.V = bm.Variable(-70.68 * bm.ones(self.num)) self.m = bm.Variable(0.0266 * bm.ones(self.num)) self.h = bm.Variable(0.772 * bm.ones(self.num)) self.n = bm.Variable(0.235 * bm.ones(self.num)) self.input = bm.Variable(bm.zeros(self.num)) self.spike = bm.Variable(bm.zeros(self.num, dtype=bool)) self.t_last_spike = bm.Variable(bm.ones(self.num) * -1e7) # å®šä¹‰ç§¯åˆ†å‡½æ•° self.integral = bp.odeint(f=self.derivative, method=&apos;exp_auto&apos;) ``` ### Define the derivative function ```python @property def derivative(self): return bp.JointEq(self.dV, self.dm, self.dh, self.dn) def dV(self, V, t, m, h, n, Iext): I_Na = (self.gNa * m ** 3.0 * h) * (V - self.ENa) I_K = (self.gK * n ** 4.0) * (V - self.EK) I_leak = self.gL * (V - self.EL) dVdt = (- I_Na - I_K - I_leak + Iext) / self.C return dVdt def dm(self, m, t, V): alpha = 0.1 * (V + 40) / (1 - bm.exp(-(V + 40) / 10)) beta = 4.0 * bm.exp(-(V + 65) / 18) dmdt = alpha * (1 - m) - beta * m return self.phi * dmdt def dh(self, h, t, V): alpha = 0.07 * bm.exp(-(V + 65) / 20.) beta = 1 / (1 + bm.exp(-(V + 35) / 10)) dhdt = alpha * (1 - h) - beta * h return self.phi * dhdt def dn(self, n, t, V): alpha = 0.01 * (V + 55) / (1 - bm.exp(-(V + 55) / 10)) beta = 0.125 * bm.exp(-(V + 65) / 80) dndt = alpha * (1 - n) - beta * n return self.phi * dndt ``` ### Complete the `update()` function ```python def update(self, x=None): t = bp.share.load(&apos;t&apos;) dt = bp.share.load(&apos;dt&apos;) # TODO: æ›´æ–°å˜é‡V, m, h, n, æš‚å­˜åœ¨V, m, h, nä¸­ V, m, h, n = self.integral(self.V, self.m, self.h, self.n, t, self.input, dt=dt) #åˆ¤æ–­æ˜¯å¦å‘ç”ŸåŠ¨ä½œç”µä½ self.spike.value = bm.logical_and(self.V = self.V_th) # æ›´æ–°æœ€åŽä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´ self.t_last_spike.value = bm.where(self.spike, t, self.t_last_spike) # TODO: æ›´æ–°å˜é‡V, m, h, nçš„å€¼ self.V.value = V self.m.value = m self.h.value = h self.n.value = n #é‡ç½®è¾“å…¥ self.input[:] = 0 ``` ### Simulation ```python current, length = bp.inputs.section_input(values=[0., bm.asarray([1., 2., 4., 8., 10., 15.]), 0.], durations=[10, 2, 25], return_length=True) hh_neurons = HH(current.shape[1]) runner = bp.DSRunner(hh_neurons, monitors=[&apos;V&apos;, &apos;m&apos;, &apos;h&apos;, &apos;n&apos;], inputs=(&apos;input&apos;, current, &apos;iter&apos;)) runner.run(length) ``` ### Visualization ```python import numpy as np import matplotlib.pyplot as plt bp.visualize.line_plot(runner.mon.ts, runner.mon.V, ylabel=&apos;V (mV)&apos;, plot_ids=np.arange(current.shape[1])) plt.plot(runner.mon.ts, bm.where(current[:, -1]&amp;gt;0, 10, 0) - 90.) plt.figure() plt.plot(runner.mon.ts, runner.mon.m[:, -1]) plt.plot(runner.mon.ts, runner.mon.h[:, -1]) plt.plot(runner.mon.ts, runner.mon.n[:, -1]) plt.legend([&apos;m&apos;, &apos;h&apos;, &apos;n&apos;]) plt.xlabel(&apos;Time (ms)&apos;) ``` ## Customize a conductance-based model ç”µè·¯æ¨¡æ‹Ÿï¼Œå†™æˆç”µå¯¼å½¢å¼ ![image-20230824180831033](/BrainPy-course-notes/master_content/Notes.assets/image-20230824180831033.png) $$ \begin{aligned} \text{gK}&amp; =\bar{g}_\text{K}n^4, \\ \frac{\mathrm{d}n}{\mathrm{d}t}&amp; =\phi[\alpha_n(V)(1-n)-\beta_n(V)n], \end{aligned} $$ åŠ¨åŠ›å­¦å½¢å¼æè¿°ï¼Œå¼•å…¥é—¨æ¡†å˜é‡$n$ $$ \begin{aligned} &amp;\alpha_{n}(V) =\frac{0.01(V+55)}{1-\exp(-\frac{V+55}{10})}, \\ &amp;\beta_{n}(V) =0.125\exp\left(-\frac{V+65}{80}\right). \end{aligned} $$ ç”±æ­¤å¼æ¥å»ºæ¨¡é’¾ç¦»å­é€šé“ ### Programming an ion channel #### Three ion channel ```python import brainpy as bp import brainpy.math as bm class IK(bp.dyn.IonChannel): def __init__(self, size, E=-77., g_max=36., phi=1., method=&apos;exp_auto&apos;): super(IK, self).__init__(size) self.g_max = g_max self.E = E self.phi = phi self.n = bm.Variable(bm.zeros(size)) # variables should be packed with bm.Variable self.integral = bp.odeint(self.dn, method=method) def dn(self, n, t, V): alpha_n = 0.01 * (V + 55) / (1 - bm.exp(-(V + 55) / 10)) beta_n = 0.125 * bm.exp(-(V + 65) / 80) return self.phi * (alpha_n * (1. - n) - beta_n * n) def update(self, V): t = bp.share.load(&apos;t&apos;) dt = bp.share.load(&apos;dt&apos;) self.n.value = self.integral(self.n, t, V, dt=dt) def current(self, V): return self.g_max * self.n ** 4 * (self.E - V) ``` ```python class INa(bp.dyn.IonChannel): def __init__(self, size, E= 50., g_max=120., phi=1., method=&apos;exp_auto&apos;): super(INa, self).__init__(size) self.g_max = g_max self.E = E self.phi = phi self.m = bm.Variable(bm.zeros(size)) # variables should be packed with bm.Variable self.h = bm.Variable(bm.zeros(size)) self.integral_m = bp.odeint(self.dm, method=method) self.integral_h = bp.odeint(self.dh, method=method) def dm(self, m, t, V): # TODO: è®¡ç®—dm/dt alpha_m = 0.11 * (V + 40) / (1 - bm.exp(-(V + 40) / 10)) beta_m = 4 * bm.exp(-(V + 65) / 18) return self.phi * (alpha_m * (1. - m) - beta_m * m) def dh(self, h, t, V): # TODO: è®¡ç®—dh/dt alpha_h = 0.07 * bm.exp(-(V + 65) / 20) beta_h = 1. / (1 + bm.exp(-(V + 35) / 10)) return self.phi * (alpha_h * (1. - h) - beta_h * h) def update(self, V): t = bp.share.load(&apos;t&apos;) dt = bp.share.load(&apos;dt&apos;) # TODO: æ›´æ–°self.m, self.h self.m.value = self.integral_m(self.m, t, V, dt=dt) self.h.value = self.integral_h(self.h, t, V, dt=dt) def current(self, V): return self.g_max * self.m ** 3 * self.h * (self.E - V) ``` ```python class IL(bp.dyn.IonChannel): def __init__(self, size, E=-54.39, g_max=0.03): super(IL, self).__init__(size) self.g_max = g_max self.E = E def current(self, V): return self.g_max * (self.E - V) def update(self, V): pass ``` #### Build a HH model with ion channels **Using customized ion channels** ```python class HH(bp.dyn.CondNeuGroup): def __init__(self, size): super(HH, self).__init__(size, V_initializer=bp.init.Uniform(-80, -60.)) # TODO: åˆå§‹åŒ–ä¸‰ä¸ªç¦»å­é€šé“ self.IK = IK(size, E=-77., g_max=36.) self.INa = INa(size, E=50., g_max=120.) self.IL = IL(size, E=-54.39, g_max=0.03) ``` **Using built-in ion channels** ```python class HH(bp.dyn.CondNeuGroup): def __init__(self, size): super().__init__(size) self.INa = bp.channels.INa_HH1952(size) self.IK = bp.channels.IK_HH1952(size) self.IL = bp.cahnnels.IL(size, E=-54.387, g_max=0.03) ``` #### Simulation ```python neu = HH(1) runner = bp.DSRunner( neu, monitors=[&apos;V&apos;, &apos;IK.n&apos;, &apos;INa.m&apos;, &apos;INa.h&apos;], inputs=(&apos;input&apos;, 1.698) # near the threshold current ) runner.run(200) # the running time is 200 ms import matplotlib.pyplot as plt plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;V&apos;]) plt.xlabel(&apos;t (ms)&apos;) plt.ylabel(&apos;V (mV)&apos;) plt.savefig(&quot;HH.jpg&quot;) plt.show() plt.figure(figsize=(6, 2)) plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;IK.n&apos;], label=&apos;n&apos;) plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;INa.m&apos;], label=&apos;m&apos;) plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;INa.h&apos;], label=&apos;h&apos;) plt.xlabel(&apos;t (ms)&apos;) plt.legend() plt.savefig(&quot;HH_channels.jpg&quot;) plt.show() ``` ![image-20230824184016011](/BrainPy-course-notes/master_content/Notes.assets/image-20230824184016011.png) # Simple Neuron Modeling: Simplified Models ## The Leaky Integrate-and-Fire(LIF) Neuron Model ### The LIF neuron model $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-(V-V_{\mathrm{rest}})+RI(t)\\\\\mathrm{if}V&amp;&amp;gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\text{last}\ {t_{ref}}\end{aligned} $$ åªæœ‰ä¸€ä¸ªå¾®åˆ†æ–¹ç¨‹ï¼Œè¦åŠ ä¸€ä¸ªä¸åº”æœŸ(**t refractory period**)ï¼Œè†œç”µä½ä¸å‘ç”Ÿä»»ä½•æ”¹å˜ï¼Œè®¤ä¸ºç¦»å­é€šé“åªæœ‰æ³„éœ²é€šé“ ![image-20230825101057570](/BrainPy-course-notes/master_content/Notes.assets/image-20230825101057570.png) Given a constant current input: ![image-20230825101410745](/BrainPy-course-notes/master_content/Notes.assets/image-20230825101410745.png) æ²¡æœ‰å»ºæ¨¡å‡†ç¡®å˜åŒ–ï¼Œåªæä¾›ä»€ä¹ˆæ—¶å€™è†œç”µä½çš„å˜åŒ– ### The dynamic features of the LIF model **General solution (constant input):**$V(t)=V_{\text{reset}}+RI_{\text{c}}(1-\mathrm{e}^{-\frac{t-t_0}{\tau}})$ **Firing frequency:** $$ \begin{aligned}T&amp;=-\tau\ln\left(1-\frac{V_{\phi h}-V_{\mathrm{rest}}}{RI_{\varsigma}}\right)\\f&amp;=\frac{1}{T+t_{\mathrm{ref}}}=\frac{1}{t_{\mathrm{ref}}-\tau\ln\left(1-\frac{V_{0}-V_{\mathrm{rest}}}{RI_{\varsigma}}\right)}\end{aligned} $$ **Rheobase current (minimal current):** $$ I_{\theta}=\frac{V_{\mathrm{th}}-V_{\mathrm{reset}}}{R} $$ åŸºå¼ºç”µæµï¼Œå¦‚æžœå°äºŽå®ƒå°†æ— æ³•å‘æ”¾ ### Strengths &amp; weaknesses of the LIF model #### Strengths - Simple, high simulation efficiency - Intuitive - Fits well the subthreshold membrane potential #### Weaknesses - The shape of action potentials is over-simplified - Has no memory of the spiking history - Cannot reproduce diverse firing patterns ### Other Univariate neuron models #### The Quadratic Integrate-and-Fire (QOF) model: $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=a_{0}(V-V_{\mathrm{re}t})(V-V_{\mathrm{c}})+RI(t)\\&amp;\text{if }V&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{re}set}\quad\text{last}\quad t_{\mathrm{ref}}\end{aligned} $$ ![image-20230825103243039](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103243039.png) è†œç”µä½ä»éœ€è¦æ‰‹åŠ¨é‡ç½® #### The Theta neuron model $$ \frac{\mathrm{d}\theta}{\mathrm{d}t}=1-\cos\theta+(1+\cos\theta)(\beta+I(t)) $$ ![image-20230825103331170](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103331170.png) éšå¼è¡¨è¾¾ï¼Œä¸å…·æœ‰ç‰©ç†æ„ä¹‰ï¼Œä½†ä¹Ÿä¼šè¿›è¡Œæ•´åˆå‘æ”¾ #### The Exponential Integrate-and-Fire (ExpIF) model $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-\left(V-V_{\mathrm{res}t}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{3T}}+RI(t)\\\mathrm{if~}V&amp;&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{res}t}\mathrm{last}t_{\mathrm{ref}}\end{aligned} $$ ![image-20230825103501912](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103501912.png) ä»éœ€è¦æ‰‹åŠ¨é‡ç½®è†œç”µä½ ## The Adaptive Exponential Integrate-and-Fire(AdEx) Neuron Model ### The AdEx neuron model Two variables: - ð‘‰: membrane potential - ð‘¤: adaptation variable $$ \begin{aligned} \tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{S_{T}}}-Rw+RI(t) \\ \tau_{w}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{\mathrm{w}}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ \mathrm{if}V&amp; &amp;gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} \end{aligned} $$ ä¸ä¸ºé›¶ï¼Œå°±ä¼šè¡°å‡åˆ°$-w$ ![image-20230825103840880](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103840880.png) - A larger ð‘¤ suppresses ð‘‰ from increasing - ð‘¤ decays exponentially while having a sudden increase when the neuron fires **Firing patterns of the AdEx model** ![image-20230825104254936](/BrainPy-course-notes/master_content/Notes.assets/image-20230825104254936.png) **Categorization of firing patterns** According to the steady-state firing time intervals: - Tonic/regular spiking - Adapting - Bursting - Irregular spiking According to the initial-state features: - Tonic/classic spiking - Initial burst - Delayed spiking ### Other multivariate neuron models #### The Izhikevich model $$ \begin{aligned} &amp;\frac{dV}{dt} =0.04V^{2}+5V+140-u+I \\ &amp;\frac{\mathrm{d}u}{\mathrm{d}t} =a\left(bV-u\right) \\ &amp;\operatorname{if}V &amp;gt;\theta,\quad V\leftarrow c,u\leftarrow u+d\text{ last }t_{\mathrm{ref}} \end{aligned} $$ äºŒæ¬¡æ•´åˆå‘æ”¾å¤šåŠ äº†ä¸€ä¸ª$u$ ![image-20230825104832770](/BrainPy-course-notes/master_content/Notes.assets/image-20230825104832770.png) #### The FitzHughâ€“Nagumo (FHN) model $$ \begin{aligned}\dot{v}&amp;=v-\frac{v^3}3-w+RI_{\mathrm{ext}}\\\tau\dot{w}&amp;=v+a-bw.\end{aligned} $$ æ²¡æœ‰å¯¹è†œç”µä½è¿›è¡Œäººä¸ºçš„é‡ç½®ï¼Œå¯ä»¥æ›´å¥½çš„è¿›è¡ŒåŠ¨åŠ›å­¦åˆ†æžï¼Œæ²¡æœ‰æ‰“ç ´å¾®åˆ†æ–¹ç¨‹çš„è¿žç»­æ€§ ![image-20230825104922636](/BrainPy-course-notes/master_content/Notes.assets/image-20230825104922636.png) #### The Generalized Integrate-and-Fire (GIF) model n+2ä¸ªå˜é‡ $$ \begin{aligned} &amp;\tau{\frac{\mathrm{d}V}{\mathrm{d}t}} =-\left(V-V_{\mathrm{rest}}\right)+R\sum_{j}I_{j}+RI \\ &amp;\frac{\mathrm{d}\Theta}{\mathrm{d}t} =a\left(V-V_{\mathrm{rest}}\right)-b\left(\Theta-\Theta_{\infty}\right) \\ &amp;\frac{\mathrm{d}l_{j}}{\mathrm{d}t} =-k_{j}I_{j},\quad j=1,2,...,n \\ &amp;\operatorname{if}V &amp;gt;\Theta,\quad I_{j}\leftarrow R_{j}I_{j}+A_{j},V\leftarrow V_{\mathrm{reset}},\Theta\leftarrow max(\Theta_{\mathrm{reset}},\Theta) \end{aligned} $$ æ¯ä¸ªå˜é‡éƒ½æ˜¯çº¿æ€§çš„ï¼Œæ³›åŒ–æ€§ä½“çŽ°åœ¨é‡ç½®æ¡ä»¶ä¸Š ![image-20230825105035349](/BrainPy-course-notes/master_content/Notes.assets/image-20230825105035349.png) ## Dynamic analysis: phase-plane analysis ### Phase plane analysis å¯¹åŠ¨åŠ›å­¦ç³»ç»Ÿçš„è¡Œä¸ºæ¥åˆ†æžï¼Œæ™®éå¯¹ä¸¤ä¸ªå˜é‡æ¥è¿›è¡Œåˆ†æž Analyzes the behavior of a dynamical system with (usually two) variables described by ordinary differential equations $$ \begin{aligned} &amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp;&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{S_{T}}}-Rw+RI(t) \\ &amp;\tau_{W}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp;&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ &amp;\mathrm{if}V&amp;&amp; &amp;gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} \end{aligned} $$ **Elements:** - Nullclines: $\mathrm{d}V/\mathrm{d}t=0;\mathrm{d}w/\mathrm{d}t=0$ - Fixed points: $\mathrm{d}V/\mathrm{d}t=0\mathrm{~and~}\mathrm{d}w/\mathrm{d}t=0$ - The vector field - The trajectory of variables å‡è®¾å¤–éƒ¨ç”µæµæ’å®š ![image-20230825110708994](/BrainPy-course-notes/master_content/Notes.assets/image-20230825110708994.png) ### Phase plane analysis for the AdEx neuron model $$ \begin{aligned} &amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp;&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{\Lambda_{T}}}-Rw+RI(t) \\ &amp;\tau_{w}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp;&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ &amp;\text{ifV}&amp;&amp; &amp;gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} \end{aligned} $$ ![image-20230825110811399](/BrainPy-course-notes/master_content/Notes.assets/image-20230825110811399.png) #### Tonic ![image-20230825112857175](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112857175.png) #### Adaptation ![image-20230825112918815](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112918815.png) #### Bursting ![image-20230825112933938](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112933938.png) #### Transient spiking ![image-20230825112950297](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112950297.png) ## Dynamic analysis: bifurcation analysis ### Bifurcation analysis Quantitative analysis of the existence and the properties of fixed points in a dynamical system with a changing parameter æŸä¸ªå¤–ç•Œæ¡ä»¶å˜åŒ–æ—¶ï¼Œå›ºå®šç‚¹çš„å˜åŒ– Elements: - Lines of fixed points - Stability properties of fixed points ![image-20230825114510710](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114510710.png) ### Bifurcation analysis for the AdEx Neuron model bifurcation analysis for 2 variables Variables: ð‘‰ and ð‘¤ Parameters: $I_{ext}$ $$ \begin{aligned} &amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}=-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{{\frac{V-V_{T}}{ST}}}-Rw+RI(t) \\ &amp;\text{-} {\frac{\mathrm{d}w}{\mathrm{d}t}}=a(V-V_{\mathrm{rest}})-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ &amp;\mathrm{if}V&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{reset}}\ \mathrm{last}\ t_{\mathrm{ref}} \end{aligned} $$ ![image-20230825114801456](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114801456.png) ![image-20230825114742740](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114742740.png) **Subjects: two variables (ð‘‰ and ð‘¤)** ![image-20230825114856403](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114856403.png) ### Extended: The limit cycle The FitzHughâ€“Nagumo (FHN) model $$ \begin{aligned}\dot{v}&amp;=v-\frac{v^3}3-w+RI_\mathrm{ext}\\\tau\dot{w}&amp;=v+a-bw.\end{aligned} $$ This dynamical system, in certain conditions, exhibits a cyclic pattern of variable changes which can be visualized as a closed trajectory in the phase plane. å˜åŒ–é”å®šåˆ°çŽ¯ä¸­ ![image-20230825115348008](/BrainPy-course-notes/master_content/Notes.assets/image-20230825115348008.png) ![image-20230825115354146](/BrainPy-course-notes/master_content/Notes.assets/image-20230825115354146.png) # Reduced Models - brain dynamics programming ## LIF neuron models programming ### Define LIF `class` $$ \begin{aligned}&amp;\tau\frac{\mathrm{d}V}{\mathrm{d}t}=-(V-V_{\mathrm{rest}})+RI(t)\\&amp;\text{if }V&amp;gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\text{last}t_{\mathrm{ref}}\end{aligned} $$ ```python class LIF(bp.dyn.NeuDyn): def __init__(self, size, V_rest=0, V_reset=-5, V_th=20, R=1, tau=10, t_ref=5., **kwargs): # åˆå§‹åŒ–çˆ¶ç±» super(LIF, self).__init__(size=size, **kwargs) ``` ### Initialization ```python class LIF(bp.dyn.NeuDyn): def __init__(self, size, V_rest=0, V_reset=-5, V_th=20, R=1, tau=10, t_ref=5., **kwargs): # åˆå§‹åŒ–çˆ¶ç±» super(LIF, self).__init__(size=size, **kwargs) # åˆå§‹åŒ–å‚æ•° self.V_rest = V_rest self.V_reset = V_reset self.V_th = V_th self.R = R self.tau = tau self.t_ref = t_ref # ä¸åº”æœŸæ—¶é•¿ # åˆå§‹åŒ–å˜é‡ self.V = bm.Variable(bm.random.randn(self.num) + V_reset) self.input = bm.Variable(bm.zeros(self.num)) self.t_last_spike = bm.Variable(bm.ones(self.num) * -1e7) # ä¸Šä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´ self.refractory = bm.Variable(bm.zeros(self.num, dtype=bool)) # æ˜¯å¦å¤„äºŽä¸åº”æœŸ self.spike = bm.Variable(bm.zeros(self.num, dtype=bool)) # è„‰å†²å‘æ”¾çŠ¶æ€ # ä½¿ç”¨æŒ‡æ•°æ¬§æ‹‰æ–¹æ³•è¿›è¡Œç§¯åˆ† self.integral = bp.odeint(f=self.derivative, method=&apos;exponential_euler&apos;) ``` ### Define the derivative function ```python # å®šä¹‰è†œç”µä½å…³äºŽæ—¶é—´å˜åŒ–çš„å¾®åˆ†æ–¹ç¨‹ def derivative(self, V, t, Iext): dVdt = (-V + self.V_rest + self.R * Iext) / self.tau return dVdt ``` ### Complete the `update()` function ```python def update(self): t, dt = bp.share[&apos;t&apos;], bp.share[&apos;dt&apos;] # ä»¥æ•°ç»„çš„æ–¹å¼å¯¹ç¥žç»å…ƒè¿›è¡Œæ›´æ–° refractory = (t - self.t_last_spike) self.V_th # å°†å¤§äºŽé˜ˆå€¼çš„ç¥žç»å…ƒæ ‡è®°ä¸ºå‘æ”¾äº†è„‰å†² self.spike[:] = spike # æ›´æ–°ç¥žç»å…ƒè„‰å†²å‘æ”¾çŠ¶æ€ self.t_last_spike[:] = bm.where(spike, t, self.t_last_spike) # æ›´æ–°æœ€åŽä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´ self.V[:] = bm.where(spike, self.V_reset, V) # å°†å‘æ”¾äº†è„‰å†²çš„ç¥žç»å…ƒè†œç”µä½ç½®ä¸ºV_resetï¼Œå…¶ä½™ä¸å˜ self.refractory[:] = bm.logical_or(refractory, spike) # æ›´æ–°ç¥žç»å…ƒæ˜¯å¦å¤„äºŽä¸åº”æœŸ self.input[:] = 0. # é‡ç½®å¤–ç•Œè¾“å…¥ ``` ### Simulation ```python def run_LIF(): # è¿è¡ŒLIFæ¨¡åž‹ group = LIF(1) runner = bp.DSRunner(group, monitors=[&apos;V&apos;], inputs=(&apos;input&apos;, 22.)) runner(200) # è¿è¡Œæ—¶é•¿ä¸º200ms # ç»“æžœå¯è§†åŒ– fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) plt.plot(runner.mon.ts, runner.mon.V) plt.xlabel(r&apos;$t$ (ms)&apos;) plt.ylabel(r&apos;$V$ (mV)&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) plt.show() ``` ![image-20230825141201825](/BrainPy-course-notes/master_content/Notes.assets/image-20230825141201825.png) ### Input current &amp; firing frequency $$ \begin{gathered} V(t)=V_{\mathrm{reset}}+RI_{\mathrm{c}}(1-\mathrm{e}^{-\frac{t-t_{0}}{\tau}}). \\ T=-\tau\ln\left[1-\frac{V_{\mathrm{th}}-V_{\mathrm{rest}}}{RI_{\mathrm{c}}}\right] \\ f={\frac{1}{T+t_{\mathrm{ref}}}}={\frac{1}{t_{\mathrm{ref}}-\tau\ln\left[1-{\frac{V_{\mathrm{th}}-V_{\mathrm{rest}}}{RI_{c}}}\right]}} \end{gathered} $$ ```python # è¾“å…¥ä¸Žé¢‘çŽ‡çš„å…³ç³» current = bm.arange(0, 600, 2) duration = 1000 LIF_neuron = LIF(current.shape[0]) runner_2 = bp.dyn.DSRunner(LIF_neurons, monitors=[&apos;spike&apos;], inputs={&apos;input&apos;, current}, dt=0.01) runner_2.run(duration) freqs = runner_2.mon.spike.sum(axis=0) / (duration/1000) plt.figure() plt.plot(current, freqs) plt.xlabel(&apos;inputs&apos;) plt.ylabel(&apos;frequencies&apos;) ``` ![image-20230825143405952](/BrainPy-course-notes/master_content/Notes.assets/image-20230825143405952.png) ### Other Univariate neuron models **The Quadratic Integrate-and-Fire (QIF) model** $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=a_{0}(V-V_{\mathrm{res}t})(V-V_{c})+RI(t)\\\mathrm{if~}V&amp;&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{reset~last~}t_{\mathrm{ref}}}\end{aligned} $$ ```python def derivative(self, V, t, I): dVdt = (self.c * (V - self.V_reset) * (V - self.V_c) + self.R * I) / self.tau return dVdt ``` **The Exponential Integrate-and-Fire (ExpIF) model** $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{\delta_{T}}}+RI(t)\\&amp;\mathrm{if~}V&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{reset}}\mathrm{last}t_{\mathrm{ref}}\end{aligned} $$ ```python def derivative(self, V, t, I): exp_v = self.delta_T * bm.exp((V - self.V_T) / self.delta_T) dvdt = (- (V - self.V_rest) + exp_v + self.R * I) / self.tau return dvdt ``` ## AdEx neuron models programming $$ \begin{gathered} \tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}=-(V-V_{\mathrm{rest}})+\Delta_{T}\mathrm{e}^{{\frac{V-V_{T}}{\Delta T}}}-Rw+RI(t), \\ \tau_{w}\frac{\mathrm{d}w}{\mathrm{d}t}=a(V-V_{\mathrm{rest}})-w+b\tau_{w}\sum_{t^{(f)}}\delta(t-t^{(f)})), \\ \mathrm{if~}V&amp;gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\mathrm{last}t_{\mathrm{ref}}. \end{gathered} $$ ### Define AdEx `class` ```python class AdEx(bp.dyn.NeuDyn): def __init__(self, size, V_rest=-65, V_reset=-68, V_th=-30, V_T=-59.9, delta_T=3.48 a=1., b=1., R=1., tau=10., tau_w=30., tau_ref=0., **kwargs): # åˆå§‹åŒ–çˆ¶ç±» super(AdEx, self).__init__(size=size, **kwargs) ``` ### Initialization ```python class AdEx(bp.dyn.NeuDyn): def __init__(self, size, V_rest=-65, V_reset=-68, V_th=-30, V_T=-59.9, delta_T=3.48 a=1., b=1., R=1., tau=10., tau_w=30., tau_ref=0., **kwargs): # åˆå§‹åŒ–çˆ¶ç±» super(AdEx, self).__init__(size=size, **kwargs) # åˆå§‹åŒ–å‚æ•° self.V_rest = V_rest self.V_reset = V_reset self.V_th = V_th self.V_T = V_T self.delta_T = delta_T self.a = a self.b = b self.R = R self.tau = tau self.tau_w = tau_w self.tau_ref = tau_ref # åˆå§‹åŒ–å˜é‡ self.V = bm.Variable(bm.random.randn(self.num) - 65.) self.w = bm.Variable(bm.zeros(self.num)) self.input = bm.Variable(bm.zeros(self.num)) self.t_last_spike = bm.Variable(bm.ones(self.num) * -1e7) # ä¸Šä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´ self.refractory = bm.Variable(bm.zeros(self.num, dtype=bool)) # æ˜¯å¦å¤„äºŽä¸åº”æœŸ self.spike = bm.Variable(bm.zeros(self.num, dtype=bool)) # è„‰å†²å‘æ”¾çŠ¶æ€ # å®šä¹‰ç§¯åˆ†å™¨ self.integral = bp.odeint(f=self.derivative, method=&apos;exp_auto&apos;) ``` ### Define the derivative function ```python def dV(self, V, t, w, I): exp = self.delta_T * bm.exp((V - self.V_T) / self.delta_T) dVdt = (-V + self.V_rest + exp - self.R * w + self.R * I) / self.tau return dVdt def dw(self, w, t, V): dwdt = (self.a * (V - self.V_rest) - w) / self.tau_w return dwdt @property def derivative(self): return bp.JointEq([self.dV, self.dw]) ``` ### Complete the `update()` function ```python def update(self): t, dt = bp.share[&apos;t&apos;], bp.share[&apos;dt&apos;] V, w = self.integral(self.V.value, self.w.value, t, self.input, dt=dt) # ä»¥æ•°ç»„çš„æ–¹å¼å¯¹ç¥žç»å…ƒè¿›è¡Œæ›´æ–° refractory = (t - self.t_last_spike) self.V_th # å°†å¤§äºŽé˜ˆå€¼çš„ç¥žç»å…ƒæ ‡è®°ä¸ºå‘æ”¾äº†è„‰å†² self.spike[:] = spike # æ›´æ–°ç¥žç»å…ƒè„‰å†²å‘æ”¾çŠ¶æ€ self.t_last_spike[:] = bm.where(spike, t, self.t_last_spike) # æ›´æ–°æœ€åŽä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´ self.V[:] = bm.where(spike, self.V_reset, V) # å°†å‘æ”¾äº†è„‰å†²çš„ç¥žç»å…ƒè†œç”µä½ç½®ä¸ºV_resetï¼Œå…¶ä½™ä¸å˜ self.w[:] = bm.where(spike, w + self.b, w) #æ›´æ–°è‡ªé€‚åº”ç”µæµ self.refractory[:] = bm.logical_or(refractory, spike) # æ›´æ–°ç¥žç»å…ƒæ˜¯å¦å¤„äºŽä¸åº”æœŸ self.input[:] = 0. # é‡ç½®å¤–ç•Œè¾“å…¥ ``` ### Simulation ![image-20230825145518709](/BrainPy-course-notes/master_content/Notes.assets/image-20230825145518709.png) ### Other multivariate neuron models **The Izhikevich model** $$ \begin{aligned} &amp;\frac{dV}{dt} =0.04V^{2}+5V+140-u+I \\ &amp;\frac{\mathrm{d}u}{\mathrm{d}t} =a\left(bV-u\right) \\ &amp;\operatorname{if}V &amp;gt;\theta,\quad V\leftarrow c,u\leftarrow u+d\mathrm{last}t_{\mathrm{ref}} \end{aligned} $$ ```python def dV(self, V, t, u, I): dVdt = 0.04 * V * V + 5 * V + 140 - u + I return dVdt def du(self, u, t, V): dudt = self.a * (self.b * V - u) return dudt ``` **The Generalized Integrate-and-Fire (GIF) model** $$ \begin{aligned} &amp;\tau{\frac{\mathrm{d}V}{\mathrm{d}t}} =-\left(V-V_{\mathrm{rest}}\right)+R\sum_{j}I_{j}+RI \\ &amp;\frac{\mathrm{d}\Theta}{\mathrm{d}t} =a\left(V-V_{\mathrm{est}}\right)-b\left(\Theta-\Theta_{\infty}\right) \\ &amp;\frac{\mathrm{d}I_j}{\mathrm{d}r} =-k_jI_j,\quad j=1,2,\ldots,n \\ &amp;\text{if V} &amp;gt;\Theta,\quad I_{j}\leftarrow R_{j}I_{j}+A_{j},V\leftarrow V_{\mathrm{reset}},\Theta\leftarrow max\left(\Theta_{\mathrm{reset}},\Theta\right) \end{aligned} $$ ```python def dI1(self, I1, t): return - self.k1 * I1 def dI2(self, I2, t): return - self.k2 * I2 def dVth(self, V_th, t, V): return self.a * (V - self.v_rest) - self.b * (V_th - self.V_th_inf) def dV(self, V, t, I1, I2, I): return (- (V - self.V_rest) + self.R * (I + I1 + I2)) / self.tau ``` **Built-in reduced neuron models** ![image-20230825145947800](/BrainPy-course-notes/master_content/Notes.assets/image-20230825145947800.png) ## Dynamic analysis: phase-plane analysis ### Simple case $$ \frac{dx}{dt}=\sin(x)+I, $$ ```python @bp.odeint def int_x(x, t, Iext): return bp.math.sin(x) + Iext ``` ```python pp = bp.analysis.PhasePlane1D( model=int_x, target_vars={&apos;x&apos;: [-10, 10]}, pars_update={&apos;Iext&apos;: 0.}, resolutions={&apos;x&apos;: 0.01} ) pp.plot_vector_field() pp.plot_fixed_point(show=True) ``` ![image-20230825152003373](/BrainPy-course-notes/master_content/Notes.assets/image-20230825152003373.png) - Nullcline: The zero-growth isoclines, such as $f(x,y) = 0$ and $g(x,y) = 0$ - Fixed points: The equilibrium points of the system, which are located at all the nullclines intersect. - Vector field: The vector field of the system. - Limit cycles: The limit cycles. - Trajectories: A simulation trajectory with the given initial values ### Phase plane analysis for AdEx ```python def ppa_AdEx(group): bm.enable_x64() v_range = [-70., -40.] w_range = [-10., 50.] phase_plane_analyzer = bp.analysis.PhasePlane2D( model=group, target_vars={&apos;V&apos;: v_range, &apos;w&apos;: w_range, }, # å¾…åˆ†æžå˜é‡ pars_update={&apos;I&apos;: Iext}, # éœ€è¦æ›´æ–°çš„å˜é‡ resolutions=0.05 ) # ç”»å‡ºV, wçš„é›¶å¢žé•¿æ›²çº¿ phase_plane_analyzer.plot_nullcline() # ç”»å‡ºå¥‡ç‚¹ phase_plane_analyzer.plot_fixed_point() # ç”»å‡ºå‘é‡åœº phase_plane_analyzer.plot_vector_field() # åˆ†æ®µç”»å‡ºV, wçš„å˜åŒ–è½¨è¿¹ group.V[:], group.w[:] = group.V_reset, 0 runner = bp.DSRunner(group, monitors=[&apos;V&apos;, &apos;w&apos;, &apos;spike&apos;], inputs=(&apos;input&apos;, Iext)) runner(500) spike = runner.mon.spike.squeeze() s_idx = np.where(spike)[0] # æ‰¾åˆ°æ‰€æœ‰å‘æ”¾åŠ¨ä½œç”µä½å¯¹åº”çš„index s_idx = np.concatenate(([0], s_idx, [len(spike) - 1])) # åŠ ä¸Šèµ·å§‹ç‚¹å’Œç»ˆæ­¢ç‚¹çš„index for i in range(len(s_idx) - 1): vs = runner.mon.V[s_idx[i]: s_idx[i + 1]] ws = runner.mon.w[s_idx[i]: s_idx[i + 1]] plt.plot(vs, ws, color=&apos;darkslateblue&apos;) # ç”»å‡ºè™šçº¿ x = V_reset plt.plot([group.V_reset, group.V_reset], w_range, &apos;--&apos;, color=&apos;grey&apos;, zorder=-1) plt.show() ``` ![image-20230825152925463](/BrainPy-course-notes/master_content/Notes.assets/image-20230825152925463.png) ## Dynamic analysis: bifurcation analysis ### Simple case $$ \frac{dx}{dt}=\sin(x)+I, $$ ```python bif = bp.analysis.Bifurcation1D( model=int_x, target_vars={&apos;x&apos;: [-10, 10]}, target_pars={&apos;Iext&apos;: [0., 1.5]}, resolutions={&apos;Iext&apos;: 0.005, &apos;x&apos;: 0.05} ) bif.plot_bifurcation(show=True) ``` ![image-20230825154227567](/BrainPy-course-notes/master_content/Notes.assets/image-20230825154227567.png) # Synapse models and their programming ## The biology of synapses ### Neurotransmitter &amp; Synapse When the action potential invades the axon terminals, it causes voltage-gated ð¶ð¶ð‘Žð‘Ž 2+ channels to open (1), which triggers vesicles to bind to the presynaptic membrane (2). Neurotransmitter is released into the synaptic cleft by exocytosis and diffuses across the cleft (3). Binding of the neurotransmitter to receptor molecules in the postsynaptic membrane completes the process of transmission (4). åŽ»æžåŒ–æ—¶é’™ç¦»å­å†…æµï¼Œä¸Žå›Šæ³¡ç›¸ç»“åˆï¼Œ...ï¼Œä¸Žå—ä½“ç»“åˆï¼Œæ‰“å¼€ç¦»å­é€šé“ï¼Œè¶…æžåŒ–ã€åŽ»æžåŒ–çŽ°è±¡ ![image-20230826100321307](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100321307.png) ![image-20230826100418911](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100418911.png) **Neurotransmitter leading to postsynaptic potential.** The binding of neurotransmitter to the postsynaptic membrane receptors changes the membrane potential ($V_m$). These postsynaptic potentials can be either excitatory (depolarizing the membrane), as shown here, or inhibitory (hyperpolarizing the membrane). ![image-20230826100531535](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100531535.png) ### Neurotransmitters å…´å¥‹æ€§ç¥žç»é€’è´¨ï¼š - ä¹™é…°èƒ†ç¢± (ACh) - å„¿èŒ¶é…šèƒº (catecholamines) - è°·æ°¨é…¸ (glutamate) - ç»„èƒº (histamine) - 5-ç¾Ÿè‰²èƒº (serotonin) - æŸäº›ç¥žç»è‚½ç±» (some of neuropeptides) æŠ‘åˆ¶æ€§ç¥žç»é€’è´¨ï¼š - GABA - ç”˜æ°¨é…¸ (glycine) - æŸäº›ç¥žç»è‚½ç±» (some of peptides) ![image-20230826100609904](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100609904.png) ### The postsynaptic response The aim of a synapse model is to describe accurately the postsynaptic response generated by the arrival of an action potential at a presynaptic terminal. 1. The fundamental quantity to be modelled is the time course of the postsynaptic receptor conductance 2. The models: - Simple phenomenological waveforms - More complex kinetic schemes that are analogous to the models of membrane- bound ion channels ![image-20230826100701580](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100701580.png) å»ºæ¨¡è¿™ç§å“åº”æ¨¡å¼ï¼Œæ‰“å¼€å…³é—­çš„æ¦‚çŽ‡... ## Phenomenological synapse models ### Exponential Model ![image-20230826100738460](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100738460.png) **Assumption**: - The release of neurotransmitter, its diffusion across the cleft, the receptor binding, and channel opening all happen very quickly, so that the channels instantaneously jump from the closed to the open state. channelä¼šçž¬é—´å¢žåŠ ç„¶åŽé€æ¸å…³é—­ $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}e^{-(t-t_{0})/\tau} \\ \begin{matrix}\bullet&amp;\tau \ \text{is the time constant}\\\bullet&amp;t_0 \ \text{is the time of the pre-synaptic spike}\\\bullet&amp;\bar{g_{syn}}\ \text{is the maximal conductance}\end{matrix} $$ -&amp;gt; corresponding differential equation $$ \tau\frac{dg_{\mathrm{syn}}(t)}{dt}=-g_{\mathrm{syn}}(t)+\bar{g}_{\mathrm{syn}}\delta\left(t_{0}-t\right) $$ - Can fit with experimental data. - A good approximation for GABA A and AMPA, because the rising phase is much shorter than their decay phase. ### Dual Exponential Model ![image-20230826101203059](/BrainPy-course-notes/master_content/Notes.assets/image-20230826101203059.png) exponential modelä¸Šå‡çš„å¤ªå¿«ï¼Œä¸å¤ªç¬¦åˆæŸäº›synapse Dual exponential synapse provides a general way to describe the synaptic conductance with different rising and decay time constants. $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}\frac{\tau_{1}\tau_{2}}{\tau_{1}-\tau_{2}}\left(\exp\left(-\frac{t-t_{0}}{\tau_{1}}\right)-\exp\left(-\frac{t-t_{0}}{\tau_{2}}\right)\right) \\ \begin{matrix} \bullet &amp;t_1\ \text{is the decay synaptic time constant} \\ \bullet &amp;\tau_2\ \text{is the rise synaptic time constant} \\ \bullet &amp;t_0\ \text{is the time of the pre-synaptic spike} \\ \bullet &amp;\bar{g}_{syn}\ \text{is the maximal conductance} \end{matrix} $$ -&amp;gt;corresponding differential equation $$ \begin{aligned} &amp;g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}g \\ &amp;\frac{dg}{dt}=-\frac{g}{\tau_{\mathrm{decay}}}+h \\ &amp;\frac{dh}{dt}&amp; =-\frac{h}{\tau_{\mathrm{rise}}}+\delta\left(t_{0}-t\right), \end{aligned} $$ The time course of most synaptic conductance can be well described by this sum of two exponentials. ### Synaptic time constants ![image-20230826101544786](/BrainPy-course-notes/master_content/Notes.assets/image-20230826101544786.png) http://compneuro.uwaterloo.ca/research/constants-constraints/neurotransmitter-time-constants-pscs.html #### AMPA synapse - $t_{decay}$ = 0.18 ms in the auditory system of the chick nucleus magnocellularis (Trussell, 1999). - $t_{rise}$ 25 ms and $\tau_{decay}$ =0.77 ms in dentate gyrus basket cells (Geiger et al., 1997). - $t_{rise}$ = 0.2 ms and $\tau_{decay}$ =1.7 ms in in neocortical layer 5 pyramidal neurons (Hausser and Roth, 1997b). - Reversal potential is nearly 0 mV. #### NMDA synapse - The decay time constants (at near-physiological temperature): - 19 ms in dentate gyrus basket cells (Geiger et al., 1997), - 26 ms in neocortical layer 2/3 pyramidal neurons (Feldmeyer et al., 2002), - 89 ms in CA1 pyramidal cells (Diamond, 2001). - The rise time constants are about 2 ms (Feldmeyer et al., 2002). - Reversal potential is nearly 0 mV. #### GABA$_A$ synapse - GABAergic synapses from dentate gyrus basket cells onto other basket cells are faster: $t_{rise}$ = 0.3 ms and $t_{decay}$ = 2.5 ms (Bartos et al., 2001) than synapses from basket cells to granule cells: $t_{rise}$ = 0.26 ms and $t_{decay}$ = 6.5 ms (Kraushaar and Jonas, 2000). - Reversal potential is nearly -80 mV. #### GABA$_B$ synapse - Common models use models with a rise time of about 25-50 ms, a fast decay time in the range of 100-300ms and a slow decay time of 500-1000 ms. - Reversal potential is nearly -90 mV. ### General property of synaptic time constants - The time constants of synaptic conductance vary widely among synapse types. - The synaptic kinetics tends to accelerate during development (T. Takahashi, Neuroscience Research, 2005) . - The synaptic kinetics becomes substantially faster with increasing temperature. ![image-20230826102033433](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102033433.png) ### Current- and Conductance-based Response ![image-20230826102042614](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102042614.png) #### Conductance-based Response Most synaptic ion channels, such as AMPA and GABA, display an approximately linear current-voltage relationship when they open. ![image-20230826102113670](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102113670.png) **For example**: The synapse is located on a thin dendrite, because the local membrane potential V changes considerably when the synapse is activated. #### Current-based Response In some case, we can also approximate the synapses as sources of current and not a conductance. ![image-20230826102150487](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102150487.png) **For example**: The excitatory synapse on a large compartment, because the depolarization of the membrane is small. ## Programming of phenomenological synapse models ### `ProjAlignPostMg2` ![Image Name](https://cdn.kesci.com/upload/rzz4o4uyar.png?imageView2/0/w/960/h/960) ```python brainpy.dyn.ProjAlignPostMg2( pre, delay, comm, syn, out, post ) ``` - ``pre (JointType[DynamicalSystem, AutoDelaySupp])``: The pre-synaptic neuron group. - ``delay (Union[None, int, float])``: The synaptic delay. - ``comm (DynamicalSystem)``: The synaptic communication. - ``syn (ParamDescInit)``: The synaptic dynamics. - ``out (ParamDescInit)``: The synaptic output. - ``post (DynamicalSystem)`` The post-synaptic neuron group. åªéœ€è¦å»ºæ¨¡æ‰€æœ‰postçš„neurons ### CSR matrix ![Image Name](https://cdn.kesci.com/upload/rzz4on32hr.png?imageView2/0/w/960/h/960) ### Exponential Model The single exponential decay synapse model assumes the release of neurotransmitter, its diffusion across the cleft, the receptor binding, and channel opening all happen very quickly, so that the channels instantaneously jump from the closed to the open state. Therefore, its expression is given by $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} e^{-\left(t-t_{0}\right) / \tau} $$ where $\tau$ is the time constant, $t_0$ is the time of the pre-synaptic spike, $\bar{g}_{\mathrm{syn}}$ is the maximal conductance. The corresponding differential equation: $$ \frac{d g}{d t} = -\frac{g}{\tau_{decay}}+\sum_{k} \delta(t-t_{j}^{k}). $$ #### COBA Given the synaptic conductance, the COBA model outputs the post-synaptic current with $$ I_{syn}(t) = g_{\mathrm{syn}}(t) (E - V(t)) $$ ```python class ExponSparseCOBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.COBA.desc(E=E), post=post, ) ``` ```python class SimpleNet(bp.DynSysGroup): def __init__(self, E=0.): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = ExponSparseCOBA(self.pre, self.post, delay=None, prob=1., g_max=1., tau=5., E=E) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` ```python def run_a_net(net): indices = np.arange(1000) # 100 ms conductances, currents, potentials = bm.for_loop(net.step_run, indices, progress_bar=True) ts = indices * bm.get_dt() # --- similar to: # runner = bp.DSRunner(net) # conductances, currents, potentials = runner.run(100.) fig, gs = bp.visualize.get_figure(1, 3, 3.5, 4) fig.add_subplot(gs[0, 0]) plt.plot(ts, conductances) plt.title(&apos;Syn conductance&apos;) fig.add_subplot(gs[0, 1]) plt.plot(ts, currents) plt.title(&apos;Syn current&apos;) fig.add_subplot(gs[0, 2]) plt.plot(ts, potentials) plt.title(&apos;Post V&apos;) plt.show() ``` #### CUBA Given the conductance, this model outputs the post-synaptic current with a identity function: $$ I_{\mathrm{syn}}(t) = g_{\mathrm{syn}}(t) $$ ```python class ExponSparseCUBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.CUBA.desc(), post=post, ) ``` ```python class SimpleNet2(bp.DynSysGroup): def __init__(self, g_max=1.): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = ExponSparseCUBA(self.pre, self.post, delay=None, prob=1., g_max=g_max, tau=5.) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` #### Dense connections Exponential synapse model with the conductance-based (COBA) output current and dense connections. ```python class ExponDenseCOBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.MaskedLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.COBA.desc(E=E), post=post, ) ``` ![Image Name](https://cdn.kesci.com/upload/rzz4p7x6dl.png?imageView2/0/w/960/h/960) Exponential synapse model with the current-based (COBA) output current and dense connections. ```python class ExponDenseCUBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.MaskedLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.CUBA.desc(), post=post, ) ``` ### `ProjAlignPreMg2` Synaptic projection which defines the synaptic computation with the dimension of presynaptic neuron group. ![Image Name](https://cdn.kesci.com/upload/rzz4pj1qmk.png?imageView2/0/w/960/h/960) ```python brainpy.dyn.ProjAlignPreMg2( pre, delay, syn, comm, out, post ) ``` - ``pre (JointType[DynamicalSystem, AutoDelaySupp])``: The pre-synaptic neuron group. - ``delay (Union[None, int, float])``: The synaptic delay. - ``syn (ParamDescInit)``: The synaptic dynamics. - ``comm (DynamicalSystem)``: The synaptic communication. - ``out (ParamDescInit)``: The synaptic output. - ``post (DynamicalSystem)`` The post-synaptic neuron group. ### Dual Exponential Model The dual exponential synapse model, also named as **difference of two exponentials model**, is given by: $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} \frac{\tau_{1} \tau_{2}}{\tau_{1}-\tau_{2}}\left(\exp \left(-\frac{t-t_{0}}{\tau_{1}}\right)-\exp \left(-\frac{t-t_{0}}{\tau_{2}}\right)\right) $$ where $\tau_1$ is the time constant of the decay phase, $\tau_2$ is the time constant of the rise phase, $t_0$ is the time of the pre-synaptic spike, $\bar{g}_{\mathrm{syn}}$ is the maximal conductance. The corresponding differential equation: $$ \begin{aligned} &amp;g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} g \\ &amp;\frac{d g}{d t}=-\frac{g}{\tau_{\mathrm{decay}}}+h \\ &amp;\frac{d h}{d t}=-\frac{h}{\tau_{\text {rise }}}+ \delta\left(t_{0}-t\right), \end{aligned} $$ The alpha function is retrieved in the limit when both time constants are equal. ```python class DualExpSparseCOBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau_decay, tau_rise, E): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.DualExpon.desc(pre.num, tau_decay=tau_decay, tau_rise=tau_rise), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.COBA(E=E), post=post, ) ``` ```python class SimpleNet4(bp.DynSysGroup): def __init__(self, E=0.): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = DualExpSparseCOBA(self.pre, self.post, delay=None, prob=1., g_max=1., tau_decay=5., tau_rise=1., E=E) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` ## Biophysical synapse models ### Limitations of phenomenological models æ‰“å¼€çš„æ•°é‡æ˜¯æœ‰é™çš„ï¼Œè€Œä¸”æœ‰é¥±å’ŒæœŸ 1. Saturation of postsynaptic receptors by previously released transmitter. 2. Certain receptor types also exhibit desensitization that prevents them (re)opening for a period after transmitter-binding, like sodium channels underlying action potential. ![image-20230826111443117](/BrainPy-course-notes/master_content/Notes.assets/image-20230826111443117.png) ### Linetic/Markov models ![image-20230826111733654](/BrainPy-course-notes/master_content/Notes.assets/image-20230826111733654.png) - The simplest kinetic model is a two-state scheme in which receptors can be either closed, ð¶, or open, ð‘‚, and the transition between states depends on transmitter concentration, [ð‘‡], in the synaptic cleft: - ð›¼ and ð›½ are voltage-independent forward and backward rate constants. - ð¶ and ð‘‚ can range from 0 to 1, and describe the fraction of receptors in the closed and open states, respectively. - The synaptic conductance is: $g_{syn}(t)=\bar{g}_{max}g(t)$ ### AMPA/GABA$_A$ synapse model $$ \begin{aligned}\frac{ds}{dt}&amp;=\alpha[T](1-s)-\beta s\\I&amp;=\tilde{g}s(V-E)\end{aligned} $$ - ð›¼[ð‘‡] denotes the transition probability from state (1âˆ’ð‘ ) to state (ð‘ ) - ð›½ represents the transition probability of the other direction - ð¸ is a reverse potential, which can determine whether the direction of ð¼ is inhibition or excitation. - ð¸ = 0 ð‘šð‘šð‘‰ð‘‰ =&amp;gt; Excitatory synapse [AMPA] - ð¸ = âˆ’80 ð‘šð‘šð‘‰ð‘‰ =&amp;gt; Inhibitory synapse [GABA A ] ### Comparison ![image-20230826111950713](/BrainPy-course-notes/master_content/Notes.assets/image-20230826111950713.png) ### NMDA synapse model ![image-20230826112027689](/BrainPy-course-notes/master_content/Notes.assets/image-20230826112027689.png) ![image-20230826112034481](/BrainPy-course-notes/master_content/Notes.assets/image-20230826112034481.png) $$ \begin{aligned} &amp;\frac{ds}{dt} =\alpha[T](1-s)-\beta s \\ &amp;I=\tilde{g}sB(V)(V-E) \\ &amp;B(V )=\frac{1}{1+\exp(-0.062V)[Mg^{2+}]_{o}/3.57} \end{aligned} $$ The magnesium block of the NMDA receptor channel is an extremely fast process compared to the other kinetics of the receptor (Jahr and Stevens 1990a, 1990b). The block can therefore be accurately modeled as an instantaneous function of voltage(Jahr and Stevens 1990b). where $[Mg^{2+}]$ is the external magnesium concentration (1 to 2mM inphysiological conditions) ## Programming of biophysical synapse models ### AMPA synapse model ```python class AMPA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, E=0.): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.AMPA.desc(pre.num, alpha=0.98, beta=0.18, T=0.5, T_dur=0.5), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.COBA(E=E), post=post, ) ``` ```python class SimpleNet(bp.DynSysGroup): def __init__(self, syn_cls): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = syn_cls(self.pre, self.post, delay=None, prob=1., g_max=1.) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` ```python def run_a_net(net, duration=100): indices = np.arange(int(duration/bm.get_dt())) # duration ms conductances, currents, potentials = bm.for_loop(net.step_run, indices, progress_bar=True) ts = indices * bm.get_dt() # --- similar to: # runner = bp.DSRunner(net) # conductances, currents, potentials = runner.run(100.) fig, gs = bp.visualize.get_figure(1, 3, 3.5, 4) fig.add_subplot(gs[0, 0]) plt.plot(ts, conductances) plt.title(&apos;Syn conductance&apos;) fig.add_subplot(gs[0, 1]) plt.plot(ts, currents) plt.title(&apos;Syn current&apos;) fig.add_subplot(gs[0, 2]) plt.plot(ts, potentials) plt.title(&apos;Post V&apos;) plt.show() ``` ### $\text{GABA}_A$ synapse model ```python class GABAa(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, E=-80.): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.GABAa.desc(pre.num, alpha=0.53, beta=0.18, T=1.0, T_dur=1.0), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.COBA(E=E), post=post, ) ``` ```python run_a_net(SimpleNet(syn_cls=GABAa)) ``` ### NMDA synapse model ```python class NMDA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, E=0.0): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.NMDA.desc(pre.num, a=0.5, tau_decay=100., tau_rise=2.), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.MgBlock(E=E), post=post, ) ``` ```python run_a_net(SimpleNet(NMDA)) ``` ### Kinetic synapse models are more realistic ```python class SimpleNet5(bp.DynSysGroup): def __init__(self, freqs=10.): super().__init__() self.pre = bp.dyn.PoissonGroup(1, freqs=freqs) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = NMDA(self.pre, self.post, delay=None, prob=1., g_max=1., E=0.) def update(self): self.pre() self.syn() self.post() # monitor the following variables return self.syn.proj.refs[&apos;syn&apos;].g, self.post.V ``` ```python def compare_freqs(freqs): fig, _ = bp.visualize.get_figure(1, 1, 4.5, 6.) for freq in freqs: net = SimpleNet5(freqs=freq) indices = np.arange(1000) # 100 ms conductances, potentials = bm.for_loop(net.step_run, indices, progress_bar=True) ts = indices * bm.get_dt() plt.plot(ts, conductances, label=f&apos;{freq} Hz&apos;) plt.legend() plt.ylabel(&apos;g&apos;) plt.show() ``` ```python compare_freqs([10., 100., 1000., 10000.]) ``` ### How to customize a synapse #### Preparations `ProjAlignPostMg2` and `ProjAlignPreMg2` #### Exponential Model ```python class Exponen(bp.dyn.SynDyn, bp.mixin.AlignPost): def __init__(self, size, tau): super().__init__(size) # parameters self.tau = tau # variables self.g = bm.Variable(bm.zeros(self.num)) # integral self.integral = bp.odeint(lambda g, t: -g/tau, method=&apos;exp_auto&apos;) def update(self, pre_spike=None): self.g.value = self.integral(g=self.g.value, t=bp.share[&apos;t&apos;], dt=bp.share[&apos;dt&apos;]) if pre_spike is not None: self.add_current(pre_spike) return self.g.value def add_current(self, x): # specical for bp.mixin.AlignPost self.g += x def return_info(self): return self.g ``` #### AMPA Model ```python class AMPA(bp.dyn.SynDyn): def __init__(self, size, alpha= 0.98, beta=0.18, T=0.5, T_dur=0.5): super().__init__(size=size) # parameters self.alpha = alpha self.beta = beta self.T = T self.T_duration = T_dur # functions self.integral = bp.odeint(method=&apos;exp_auto&apos;, f=self.dg) # variables self.g = bm.Variable(bm.zeros(self.num)) self.spike_arrival_time = bm.Variable(bm.ones(self.num) * -1e7) def dg(self, g, t, TT): return self.alpha * TT * (1 - g) - self.beta * g def update(self, pre_spike): self.spike_arrival_time.value = bm.where(pre_spike, bp.share[&apos;t&apos;], self.spike_arrival_time) TT = ((bp.share[&apos;t&apos;] - self.spike_arrival_time) åšæ—¶é—´å¹³å‡ STP based on firing rate $$ \begin{gathered} \frac{du(t)}{dt}=\frac{-u(t)}{\tau_{f}}+U_{sE}(1-u^{-})\delta\big(t-t_{sp}\big), \\ \frac{dx(t)}{dt}=\frac{1-x(t)}{\tau_{d}}-u^{+}x^{-}\delta\big(t-t_{sp}\big), \\ \frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{s}}+Au^{+}x^{-}\delta\big(t-t_{sp}\big), \\ u^{+}=\lim_{t-t_{sp\rightarrow0^{+}}}u(t), \end{gathered} $$ ![image-20230826155016657](/BrainPy-course-notes/master_content/Notes.assets/image-20230826155016657.png) ä¸¢æŽ‰æ—¶é—´å˜åŒ–çš„å…·ä½“ç»†èŠ‚ï¼ŒæŠ“ä½äº†é‡è¦è¶‹åŠ¿ ### Theoretical analysis of the rate model Suppose the pre-synaptic firing rate keeps as constant, we can calculate the stationary response $$ u_{st}=\frac{U_{SE}R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}},\quad u_{st}^{+}=U_{SE}\frac{1+R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}},\quad x_{st}=\frac{1}{1+u_{st}^{+}\tau_{d}R_{0}}, $$ $$ EPSC_{st}=Au_{st}^{+}x_{st}=A\frac{u_{st}^{+}}{1+u_{st}^{+}\tau_{d}R_{0}},\quad PSV_{st}\propto g_{st}=\tau_{s}Au_{st}^{+}x_{st}R_{0}=A\frac{u_{st}^{+}R_{0}}{1+u_{st}^{+}\tau_{d}R_{0}}, $$ ![image-20230826155234134](/BrainPy-course-notes/master_content/Notes.assets/image-20230826155234134.png) ### Frequency-dependent Gain control of spike information $$ \begin{gathered} u_{st}^{+}=U_{SE}\frac{1+R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}}, \\ x_{st}=\frac{1}{1+u_{st}^{+}\tau_{d}R_{0}}, \\ EPSC_{st}=Au_{st}^{+}x_{st}=A\frac{u_{st}^{+}}{1+u_{st}^{+}\tau_{d}R_{0}}, \end{gathered} $$ Peak frequency: $\theta\sim\frac{1}{\sqrt{U\tau_{f}\tau_{d}}}$ ### Simulation of Frequency-dependent Gain control ![image-20230826155715445](/BrainPy-course-notes/master_content/Notes.assets/image-20230826155715445.png) ## Effects on network dynamics ### STP modeling Working memory ![image-20230826160102332](/BrainPy-course-notes/master_content/Notes.assets/image-20230826160102332.png) ![image-20230826160113456](/BrainPy-course-notes/master_content/Notes.assets/image-20230826160113456.png) # E-I Balanced Neural Network ## Irregular Spiking of Neurons ### Signal process of single neuron External Stimulus -&amp;gt; Single neuron model $$ \begin{aligned}\tau&amp;\frac{\mathrm{d}V}{\mathrm{d}t}=-(V-V_\text{rest })+RI(t)\\\\&amp;\text{if}V&amp;gt;V_\text{th},\quad V\leftarrow V_\text{reset }\text{last}t_\text{ref}\end{aligned} $$ -&amp;gt; ... -&amp;gt; Perception or action çœŸæ­£çš„ç¥žç»å…ƒå¹¶ä¸æ˜¯LIF modelçš„è¾“å‡º ![image-20230827100647851](/BrainPy-course-notes/master_content/Notes.assets/image-20230827100647851.png) Simulation ![image-20230827100706310](/BrainPy-course-notes/master_content/Notes.assets/image-20230827100706310.png) Neuron recorded in vivo ### Irregular Spiking of Neurons ![image-20230827092807270](/BrainPy-course-notes/master_content/Notes.assets/image-20230827092807270.png) #### Statistical Description of Spikes ç”¨ä»¥ä¸‹çš„å˜é‡æ¥è¿›è¡Œç»Ÿè®¡æè¿° - Firing Rate Rate = average over time(single neuron, single run) Spike count $v=\frac{n_{sp}}{T}$ - ISI(Interspike interval distributions) average ISI $\overline{\Delta t}=\frac{1}{n_{sp}-1}\sum_{i=1}^{n_{sp}-1}\Delta t_{i}$ standard deviation ISI: $\sigma_{\Delta t}^{2}=\frac{1}{n_{sp}-1}\sum_{i=1}^{n_{sp}-1}(\Delta t_{i}-\overline{\Delta t})^{2}$ - $C_V$(Coefficient of variation, Fano factor) **çª„è¿˜æ˜¯å®½çš„åˆ†å¸ƒ** ä¿¡æ¯è¡¨å¾æœ‰å¤šå¼ºçš„ä¸ç¨³å®šæ€§ $C_{V}=\sigma_{\Delta t}^{2}/\overline{\Delta t}$ #### Poisson Process In probability theory and statistics, the Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known **constant mean rate** and **independently** of the time since the last eventã€‚ $$ \begin{aligned} &amp;P(X=k\mathrm{~events~in~interval~}t)=e^{-rt}\frac{(rt)^{k}}{k!} \\ &amp;\mathrm{mean:}\quad\overline{X}=rt \\ &amp;\mathrm{variance}:\quad\sigma^{2}=rt\\ &amp;\mathrm{Fano factor:}\quad\frac{\sigma^{2}}{X}=1 \end{aligned} $$ Fano factor -&amp;gt; noise-to-signal ratio #### Irregular Spiking of Neurons LIFåœ¨å•ä¸ªç¥žç»å…ƒçš„æƒ…å†µä¸‹æ˜¯åŸºæœ¬æ²¡æœ‰å¤ªå¤§é—®é¢˜çš„ï¼Œåœ¨æ•´ä¸ªç½‘ç»œä¸­ä¼šå—ç½‘ç»œä¿¡æ¯è°ƒæŽ§ ![image-20230827093614607](/BrainPy-course-notes/master_content/Notes.assets/image-20230827093614607.png) #### Why Irregular? - ä¸å®Œå…¨æ˜¯inputå½±å“çš„ - ä¸èƒ½ç®€å•æ¥è¡¡é‡ On average, a cortical neuron receives inputs from 1000~10000 connected neurons. -&amp;gt; averaged noise ~ 0 ## E-I Balanced Network $$ \begin{gathered} \tau\frac{du_{i}^{E}}{dt}=-u_{i}^{E}+\sum_{j=1}^{K_{E}}J_{EE}r_{j}^{E}+\sum_{j=1}^{K_{I}}J_{EI}r_{j}^{I}+I_{i}^{E} \\ \tau\frac{du_{i}^{I}}{dt}=-u_{i}^{I}+\sum_{j=1}^{K_{I}}J_{II}r_{j}^{I}+\sum_{j=1}^{K_{E}}J_{IE}r_{j}^{E}+I_{i}^{I} \end{gathered} $$ ![image-20230827093708220](/BrainPy-course-notes/master_content/Notes.assets/image-20230827093708220.png) Sparse &amp; random connections:$1\ll K_{\mathrm{E}},K_{1}\ll N_{\mathrm{E}},N_{\mathrm{I}}$ . Neurons fire largely independently to each other. $$ \begin{gathered} \text{Single neuron fires irregularly } r_j^E, r_j^{\prime} \text{with mean rate } \mu \text{and variance } \sigma^2.\\ \text{The mean of recurrent input received by E neuron:} \\ \sim K_{E}J_{EE}\mu-K_{I}J_{EI}\mu \\ \text{The variance of recurrent input received by E neuron:} \\ \sim K_{E}(J_{EE})^{2}\sigma^{2}+K_{I}(J_{EI})^{2}\sigma^{2} \\ \begin{gathered} \\ \text{The balanced condition:} \\ K_{E}J_{EE}-K_{l}J_{El}{\sim}0(1) \\ J_{EE}=\frac{1}{\sqrt{K_{E}}},J_{EI}=\frac{1}{\sqrt{K_{I}}},K_{E}(J_{EE})^{2}\sigma^{2}+K_{I}(J_{EI})^{2}\sigma^{2}\sim O(1) \end{gathered} \end{gathered} $$ $$ \begin{aligned}\frac{I_E}{I_I}&amp;&amp;gt;\frac{J_E}{J_I}&amp;&amp;gt;1\\\\J_E&amp;&amp;gt;1\\\\\text{r not too big}\end{aligned} $$ $$ \overline{I_a}=\overline{F_a}+\overline{R_a}=\sqrt{N}(f_a\mu_0+w_{aE}r_E+w_{aI}r_I),\quad a=E,I,\\ \begin{gathered} w_{ab}~=~p_{ab}j_{ab}q_{b} \\ J_{ij}^{ab}~=~j_{ab}/\sqrt{N}; \\ \frac{f_{E}}{f_{I}}&amp;gt;\frac{w_{EI}}{w_{II}}&amp;gt;\frac{w_{EE}}{w_{IE}}. \end{gathered} $$ ## BrainPy Simulation ### Simulation LIF neuron 4000 (E/I=4/1, P=0.02) ðœ = 20 ms ð‘‰ð‘Ÿð‘’ð‘ ð‘¡ = -60 mV Spiking threshold: -50 mV Refractory period: 5 ms $$ \begin{gathered} \tau\frac{dV}{dt}=(V_{\mathrm{rest}}-V)+I \\ I=g_{exc}(E_{exc}-V)+g_{inh}(E_{inh}-V)+I_{\mathrm{ext}} \end{gathered} \ \ \ \ \ \ \begin{aligned}\tau_{exc}&amp;\frac{dg_{exc}}{dt}=-g_{exc}\\\tau_{inh}&amp;\frac{dg_{inh}}{dt}=-g_{inh}\end{aligned} $$ $$ \begin{array}{l}E_\mathrm{exc}=0\text{mV}\mathrm{and}E_\mathrm{inh}=-80\text{mV},I_\mathrm{ext}=20.\\\tau_\mathrm{exc}=5\text{ ms},\tau_\mathrm{inh}=10\text{ ms},\Delta g_\mathrm{exc}=0.6\text{ and}\Delta g_\mathrm{inh}=6.7.\end{array} $$ ![image-20230827094502860](/BrainPy-course-notes/master_content/Notes.assets/image-20230827094502860.png) ### Synaptic Computation ```python # åŸºäºŽ align post Exponential synaptic computation class Exponential(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E, label=None): super().__init__() self.pron = bp.dyn.ProjAlignPost2( pre=pre, delay=delay, comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), # éšæœºè¿žæŽ¥ syn=bp.dyn.Expon(size=post.num, tau=tau), # Exponential synapse out=bp.dyn.COBA(E=E), # COBA network post=post, out_label=label ) ``` ### E-I Balanced Network ```python # æž„å»º E-I Balanced Network class EINet(bp.DynamicalSystem): def __init__(self, ne=3200, ni=800): super().__init__() # bp.neurons.LIF() self.E = bp.dyn.LifRef(ne, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Normal(-55., 2.)) self.I = bp.dyn.LifRef(ni, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Normal(-55., 2.)) #### E2E, E2I, I2E, I2I Exponential synaptic computation # delay=0, prob=0.02, g_max_E=0.6, g_max_I=6.7, tau_E=5, tau_I=10, # reversal potentials E_E=0, E_E=-80, label=EE,EI,IE,II self.E2E = Exponential(self.E, self.E, 0., 0.02, 0.6, 5., 0., &apos;EE&apos;) self.E2I = Exponential(self.E, self.I, 0., 0.02, 0.6, 5., 0., &apos;EI&apos;) self.I2E = Exponential(self.I, self.E, 0., 0.02, 6.7, 5., -80., &apos;IE&apos;) self.I2I = Exponential(self.I, self.I, 0., 0.02, 6.7, 5., -80., &apos;II&apos;) ``` ```python def update(self, inp=0.): # æ›´æ–°çªè§¦ä¼ å…¥ç”µæµ self.E2E() self.E2I() self.I2E() self.I2I() # æ›´æ–°ç¥žç»å…ƒç¾¤ä½“ self.E(inp) self.I(inp) # è®°å½•éœ€è¦ monitorçš„å˜é‡ E_E_inp = self.E.sum_inputs(self.E.V, label=&apos;EE&apos;) #E2Eçš„è¾“å…¥ I_E_inp = self.E.sum_inputs(self.E.V, label=&apos;IE&apos;) # I2Eçš„è¾“å…¥ return self.E.spike, self.I.spike, E_E_inp, I_E_inp ``` ![image-20230827110737553](/BrainPy-course-notes/master_content/Notes.assets/image-20230827110737553.png) ![image-20230827110746410](/BrainPy-course-notes/master_content/Notes.assets/image-20230827110746410.png) ## Properties of E-I Balanced Network - Linear encoding External input strength is â€œlinearlyâ€ encoded by the mean firing rate of the neural population - Fast Response The network responds rapidly to abrupt changes of the input ### Noise speeds up computation å¿«é€Ÿç›¸åº”çš„åŽŸç†ï¼Œå‡åŒ€åˆ†å¸ƒåœ¨é˜ˆå€¼ä¸‹é¢çš„ç©ºé—´ - A neural ensemble jointly encodes stimulus information; - Noise randomizes the distribution of neuronal membrane potentials; - Those neurons (red circle) whose potentials are close to the threshold will fire rapidly; - If the noisy environment is proper, even for a small input, a certain number of neurons will fire instantly to report the presence of a stimulus. ![image-20230827113451626](/BrainPy-course-notes/master_content/Notes.assets/image-20230827113451626.png) # Continuous Attractor Neural Network ## Attractor Models ### The concept of attractor dynamics Different types of attractors: Point attractors, Line attractors, Ring attractors, Plane attractors, Cyclic attractors, Chaotic attractors ![image-20230827140250173](/BrainPy-course-notes/master_content/Notes.assets/image-20230827140250173.png) ç¨³æ€ï¼Œèƒ½é‡æ¢¯åº¦å¸å¼•åˆ°attractor ### Discrete attractor Network Model: Hopfield Model $S_i=\pm1$: the neuronal state $W_{ij}$ : the neuronal connection The network dynamics: $$ S_{i}=\mathrm{sign}\bigg(\sum_{j}w_{ij}S_{j}-\theta\bigg),\quad\mathrm{sign}(x)=1,\mathrm{for}x&amp;gt;0;-1,\mathrm{otherwise} $$ Updating rule: synchronous or asynchronous Consider the network stores $p$ pattern, $\xi_{i}^{\mu},\mathrm{for}\mu=1,\ldots p;i=1,\ldots N$ Setting $w_{ij}=\frac{1}{N}\sum_{\mu=1}^{p}\xi_{i}^{\mu}\xi_{j}^{\mu}$ ![image-20230827140827784](/BrainPy-course-notes/master_content/Notes.assets/image-20230827140827784.png) #### Energy space of Hopfield network $$ \begin{aligned} &amp;\text{Energy function: }E=-\frac{1}{2}\sum_{i,j}w_{ij}S_{i}S_{j}+\theta\sum_{i}S_{i} \\ &amp;\mathrm{Consider}S_{i}\mathrm{~is~updated},S_{i}(t+1)=sign[\sum_{j}w_{ij}S_{j}(t)-\theta] \\ &amp;\Delta E=E(t+1)-E(t)\\ &amp;=-[S_{i}(t+1)-S_{i}(t)]\sum_{j}w_{ij}S_{j}(t)+\theta\left[S_{i}(t+1)-S_{i}(t)\right] \\ &amp;=-[S_{i}(t+1)-S_{i}(t)][\sum_{j}w_{ij}S_{j}(t)-\theta] \\ &amp;\leq0 \end{aligned} $$ åŒæ ·æ¿€æ´»åŒæ ·patternçš„ç¥žç»å…ƒï¼Œ~å¸å¼•å­ #### Auto-associative memory in Hopfield Network A partial/noisy input can retrieve the related memory pattern ![image-20230827141253326](/BrainPy-course-notes/master_content/Notes.assets/image-20230827141253326.png) #### Persistent activity in working memory After the removal of external input, the neurons in the network encoding the stimulus continue to fire persistently. ![image-20230827141421796](/BrainPy-course-notes/master_content/Notes.assets/image-20230827141421796.png) ## Continuous Attractor Neural Network ### Neural coding #### Low-dimensional continuous feature ![image-20230827142520189](/BrainPy-course-notes/master_content/Notes.assets/image-20230827142520189.png) #### Continuous Attractor neural network ![image-20230827142606695](/BrainPy-course-notes/master_content/Notes.assets/image-20230827142606695.png) ### CANN: A rate-based recurrent circuit model $$ \begin{aligned}\tau\frac{\partial U(x,t)}{\partial t}&amp;=-U(x,t)+\rho\int f(x,x&apos;)r(x&apos;,t)dx&apos;+l^{ext}(1)\\r(x,t)&amp;=\frac{U^2(x,t)}{1+k\rho\int U^2(x,t)dx}\quad(2)\\J(x,x&apos;)&amp;=\frac{J_0}{\sqrt{2\pi}a}\exp\left[-\frac{(x-x&apos;)^2}{2a^2}\right](3)\end{aligned} $$ ré¢‘çŽ‡ï¼ŒJå¼ºåº¦ï¼ŒU decay #### A Continuous family of attractor states åšå¹³ç§»çš„æ”¹å˜ï¼Œå˜åŒ–ä¼šè¢«ä¿ç•™ï¼Œline attractorï¼Œå—åˆ°ç¼–ç è¿žç»­åˆºæ¿€ ![image-20230827143707784](/BrainPy-course-notes/master_content/Notes.assets/image-20230827143707784.png) #### Stability analysis derive continuous attractor dynamics åªéœ€è¦çœ‹åœ¨åŽŸå§‹çŠ¶æ€åŠ å…¥ä¸€ä¸ªå°é‡é¡¹ï¼Œå†ä»£å…¥å›ž Consider small fluctuations around a stationary state at z: Projecting $\delta U$ on the $i$th right eigenvector of $F(\delta U)_i(t)=(\delta U)_i(0)e^{-(1-\lambda _i)t/\tau}$ Two cases: - If $\lambda _i ## Computation with CANN ### Persistent activity for working memory When the global inhibition is not too strong, the network spontaneously hold bump activity: $$ k\frac{\tau}{\tau _v}$, Travelling wave ![image-20230827150543244](/BrainPy-course-notes/master_content/Notes.assets/image-20230827150543244.png) #### Levy flights vs. Brownian motion ![image-20230827150851309](/BrainPy-course-notes/master_content/Notes.assets/image-20230827150851309.png) #### LÃ©vy flights in ecology and human cogniDve behaviors ç”Ÿç‰©å­¦å¤§å¤šè¿åŠ¨æœä»Žlevy flights ### Noisy adaptation generates Levy flight in CANN ![image-20230827151343126](/BrainPy-course-notes/master_content/Notes.assets/image-20230827151343126.png) ### Time Delay in Neural Signal Transmission ![image-20230827151622032](/BrainPy-course-notes/master_content/Notes.assets/image-20230827151622032.png) ### Anticipatory Head Direction Signals in Anterior Thalamus æœ‰é¢„æµ‹ç­–ç•¥ï¼Œå®žçŽ°æŠµæ¶ˆä¿¡æ¯ä¼ é€’çš„delay CANNåŠ å…¥è´Ÿåé¦ˆæœºåˆ¶æ˜¯å¯ä»¥å®žçŽ°é¢„æµ‹çš„ ![image-20230827152731895](/BrainPy-course-notes/master_content/Notes.assets/image-20230827152731895.png) ### CANN with STP $$ \begin{gathered} \tau{\frac{\mathrm{d}U(x,t)}{\mathrm{d}t}} {\cal O}=-U(x,t)+\rho\int g^{+}(x)h(x^{\prime},t)J(x,x^{\prime})r(x^{\prime},t)dx^{\prime}+I^{ext}(x,t)(1) \\ \frac{dg(x,t)}{dt}=-\frac{g(x,t)}{\tau_{f}}+G(1-g^{-}(x))r(x^{\prime},t)\quad(2) \\ \frac{dh(x,t)}{dt}=\frac{1-h(x,t)}{\tau_{d}}-g^{+}(x)h(x,t)r(x^{\prime},t)\quad(3) \\ r(x,t)={\frac{U^{2}(x,t)}{1+k\rho\int U^{2}(x,t)dx}}\quad(4) \end{gathered} $$ ## Programming in BrainPy ### Customize a ring CANN in brainpy In simulations, we can not simulate a CANN encoding features ranging $(-\inf, \inf)$. Instead, we simulate a ring attractor network which encodes features ranging $(-\pi, \pi)$. Note that the distance on a ring should be: $$ dist_{ring}(x,x&apos;) = min(|x-x&apos;|,2\pi-|x-x&apos;|) $$ ![Image Name](https://cdn.kesci.com/upload/s01apgi89t.png?imageView2/0/w/320/h/320) ```python class CANN1D(bp.NeuGroupNS): def __init__(self, num, tau=1., k=8.1, a=0.5, A=10., J0=4., z_min=-bm.pi, z_max=bm.pi, **kwargs): super(CANN1D, self).__init__(size=num, **kwargs) # åˆå§‹åŒ–å‚æ•° self.tau = tau self.k = k self.a = a self.A = A self.J0 = J0 # åˆå§‹åŒ–ç‰¹å¾ç©ºé—´ç›¸å…³å‚æ•° self.z_min = z_min self.z_max = z_max self.z_range = z_max - z_min self.x = bm.linspace(z_min, z_max, num) self.rho = num / self.z_range self.dx = self.z_range / num # åˆå§‹åŒ–å˜é‡ self.u = bm.Variable(bm.zeros(num)) self.input = bm.Variable(bm.zeros(num)) self.conn_mat = self.make_conn(self.x) # è¿žæŽ¥çŸ©é˜µ # å®šä¹‰ç§¯åˆ†å‡½æ•° self.integral = bp.odeint(self.derivative) # å¾®åˆ†æ–¹ç¨‹ @property def derivative(self): du = lambda u, t, Irec, Iext: (-u + Irec + Iext) / self.tau return du # å°†è·ç¦»è½¬æ¢åˆ°[-z_range/2, z_range/2)ä¹‹é—´ def dist(self, d): d = bm.remainder(d, self.z_range) d = bm.where(d &amp;gt; 0.5 * self.z_range, d - self.z_range, d) return d # è®¡ç®—è¿žæŽ¥çŸ©é˜µ def make_conn(self, x): assert bm.ndim(x) == 1 d = self.dist(x - x[:, None]) # è·ç¦»çŸ©é˜µ Jxx = self.J0 * bm.exp( -0.5 * bm.square(d / self.a)) / (bm.sqrt(2 * bm.pi) * self.a) return Jxx # èŽ·å–å„ä¸ªç¥žç»å…ƒåˆ°poså¤„ç¥žç»å…ƒçš„è¾“å…¥ def get_stimulus_by_pos(self, pos): return self.A * bm.exp(-0.25 * bm.square(self.dist(self.x - pos) / self.a)) def update(self, x=None): _t = bp.share[&apos;t&apos;] u2 = bm.square(self.u) r = u2 / (1.0 + self.k * bm.sum(u2)) Irec = bm.dot(self.conn_mat, r) self.u[:] = self.integral(self.u, _t,Irec, self.input) self.input[:] = 0. # é‡ç½®å¤–éƒ¨ç”µæµ ``` ### Simulate the persistent activity of CANN after the removal of external input ```python def Persistent_Activity(k=0.1,J0=1.): # ç”ŸæˆCANN cann = CANN1D(num=512, k=k,J0=J0) # ç”Ÿæˆå¤–éƒ¨åˆºæ¿€ï¼Œä»Žç¬¬2åˆ°12msï¼ŒæŒç»­10ms dur1, dur2, dur3 = 2., 10., 10. I1 = cann.get_stimulus_by_pos(0.) Iext, duration = bp.inputs.section_input(values=[0., I1, 0.], durations=[dur1, dur2, dur3], return_length=True) noise_level = 0.1 noise = bm.random.normal(0., noise_level, (int(duration / bm.get_dt()), len(I1))) Iext += noise # è¿è¡Œæ•°å€¼æ¨¡æ‹Ÿ runner = bp.DSRunner(cann, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;]) runner.run(duration) # å¯è§†åŒ– def plot_response(t): fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann.x, I, label=&apos;Iext&apos;) ax.plot(cann.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() # plt.savefig(f&apos;CANN_t={t}.pdf&apos;, transparent=True, dpi=500) plot_response(t=10.) plot_response(t=20.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=1, frame_delay=40, show=True, ) plt.show() Persistent_Activity(k=0.1) ``` ### Simulate the tracking behavior of CANN ```python def smooth_tracking(): cann = CANN1D(num=512, k=8.1) # å®šä¹‰éšæ—¶é—´å˜åŒ–çš„å¤–éƒ¨åˆºæ¿€ v_ext = 1e-3 dur1, dur2, dur3 = 10., 10., 20 num1 = int(dur1 / bm.get_dt()) num2 = int(dur2 / bm.get_dt()) num3 = int(dur3 / bm.get_dt()) position = bm.zeros(num1 + num2 + num3) position[num1: num1 + num2] = bm.linspace(0., 1.5 * bm.pi, num2) position[num1 + num2: ] = 1.5 * bm.pi position = position.reshape((-1, 1)) Iext = cann.get_stimulus_by_pos(position) # è¿è¡Œæ¨¡æ‹Ÿ runner = bp.DSRunner(cann, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;]) runner.run(dur1 + dur2 + dur3) # å¯è§†åŒ– def plot_response(t, extra_fun=None): fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann.x, I, label=&apos;Iext&apos;) ax.plot(cann.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() if extra_fun: extra_fun() # plt.savefig(f&apos;CANN_tracking_t={t}.pdf&apos;, transparent=True, dpi=500) plot_response(t=10.) def f(): plt.annotate(&apos;&apos;, xy=(1.5, 10), xytext=(0.5, 10), arrowprops=dict(arrowstyle=&quot;-&amp;gt;&quot;)) plot_response(t=15., extra_fun=f) def f(): plt.annotate(&apos;&apos;, xy=(-2, 10), xytext=(-3, 10), arrowprops=dict(arrowstyle=&quot;-&amp;gt;&quot;)) plot_response(t=20., extra_fun=f) plot_response(t=30.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=5, frame_delay=50, show=True, ) plt.show() smooth_tracking() ``` ### Customize a CANN with SFASimulate the spontaneous traveling wave ```python class CANN1D_SFA(bp.NeuGroupNS): def __init__(self, num, m = 0.1, tau=1., tau_v=10., k=8.1, a=0.5, A=10., J0=4., z_min=-bm.pi, z_max=bm.pi, **kwargs): super(CANN1D_SFA, self).__init__(size=num, **kwargs) # åˆå§‹åŒ–å‚æ•° self.tau = tau self.tau_v = tau_v #time constant of SFA self.k = k self.a = a self.A = A self.J0 = J0 self.m = m #SFA strength # åˆå§‹åŒ–ç‰¹å¾ç©ºé—´ç›¸å…³å‚æ•° self.z_min = z_min self.z_max = z_max self.z_range = z_max - z_min self.x = bm.linspace(z_min, z_max, num) self.rho = num / self.z_range self.dx = self.z_range / num # åˆå§‹åŒ–å˜é‡ self.u = bm.Variable(bm.zeros(num)) self.v = bm.Variable(bm.zeros(num)) #SFA current self.input = bm.Variable(bm.zeros(num)) self.conn_mat = self.make_conn(self.x) # è¿žæŽ¥çŸ©é˜µ # å®šä¹‰ç§¯åˆ†å‡½æ•° self.integral = bp.odeint(self.derivative) # å¾®åˆ†æ–¹ç¨‹ @property def derivative(self): du = lambda u, t, v, Irec, Iext: (-u + Irec + Iext-v) / self.tau dv = lambda v, t, u: (-v + self.m*u) / self.tau_v return bp.JointEq([du, dv]) # å°†è·ç¦»è½¬æ¢åˆ°[-z_range/2, z_range/2)ä¹‹é—´ def dist(self, d): d = bm.remainder(d, self.z_range) d = bm.where(d &amp;gt; 0.5 * self.z_range, d - self.z_range, d) return d # è®¡ç®—è¿žæŽ¥çŸ©é˜µ def make_conn(self, x): assert bm.ndim(x) == 1 d = self.dist(x - x[:, None]) # è·ç¦»çŸ©é˜µ Jxx = self.J0 * bm.exp( -0.5 * bm.square(d / self.a)) / (bm.sqrt(2 * bm.pi) * self.a) return Jxx # èŽ·å–å„ä¸ªç¥žç»å…ƒåˆ°poså¤„ç¥žç»å…ƒçš„è¾“å…¥ def get_stimulus_by_pos(self, pos): return self.A * bm.exp(-0.25 * bm.square(self.dist(self.x - pos) / self.a)) def update(self, x=None): u2 = bm.square(self.u) r = u2 / (1.0 + self.k * bm.sum(u2)) Irec = bm.dot(self.conn_mat, r) u, v = self.integral(self.u, self.v, bp.share[&apos;t&apos;],Irec, self.input) self.u[:] = bm.where(u&amp;gt;0,u,0) self.v[:] = v self.input[:] = 0. # é‡ç½®å¤–éƒ¨ç”µæµ ``` ### Simulate the spontaneous traveling wave ```python def traveling_wave(num=512,m=0.1,k=0.1): # ç”ŸæˆCANN cann_sfa = CANN1D_SFA(num=num, m=m,k=k) # ç”Ÿæˆå¤–éƒ¨åˆºæ¿€ dur = 1000. noise_level = 0.1 Iext = bm.random.normal(0., noise_level, (int(dur / bm.get_dt()), num)) duration = dur # è¿è¡Œæ•°å€¼æ¨¡æ‹Ÿ runner = bp.DSRunner(cann_sfa, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;]) runner.run(duration) # å¯è§†åŒ– def plot_response(t): fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann_sfa.x, I, label=&apos;Iext&apos;) ax.plot(cann_sfa.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() # plt.savefig(f&apos;CANN_t={t}.pdf&apos;, transparent=True, dpi=500) plot_response(t=100.) plot_response(t=150.) plot_response(t=200.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann_sfa.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann_sfa.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=1, frame_delay=40, show=True, ) plt.show() traveling_wave(num=512,m=0.5,k=0.1) ``` ### Simulate the anticipative tracking ```python def anticipative_tracking(m=10,v_ext=6*1e-3): cann_sfa = CANN1D_SFA(num=512, m=m) # å®šä¹‰éšæ—¶é—´å˜åŒ–çš„å¤–éƒ¨åˆºæ¿€ v_ext = v_ext dur1, dur2, = 10., 1000. num1 = int(dur1 / bm.get_dt()) num2 = int(dur2 / bm.get_dt()) position = np.zeros(num1 + num2) for i in range(num2): pos = position[i+num1-1]+v_ext*bm.dt # the periodical boundary pos = np.where(pos&amp;gt;np.pi, pos-2*np.pi, pos) pos = np.where(pos 0.5 * self.z_range, d - self.z_range, d) return d # è®¡ç®—è¿žæŽ¥çŸ©é˜µ def make_conn(self, x): assert bm.ndim(x) == 1 d = self.dist(x - x[:, None]) # è·ç¦»çŸ©é˜µ Jxx = self.J0 * bm.exp( -0.5 * bm.square(d / self.a)) / (bm.sqrt(2 * bm.pi) * self.a) return Jxx # èŽ·å–å„ä¸ªç¥žç»å…ƒåˆ°poså¤„ç¥žç»å…ƒçš„è¾“å…¥ def get_stimulus_by_pos(self, pos): return self.A * bm.exp(-0.25 * bm.square(self.dist(self.x - pos) / self.a)) def update(self, x=None): u2 = bm.square(self.u) r = u2 / (1.0 + self.k * bm.sum(u2)) Irec = bm.dot(self.conn_mat, (self.g + self.G * (1 - self.g))*self.h*r) u, g, h = self.integral(u=self.u, g=self.g, h=self.h, t=bp.share[&apos;t&apos;], Irec=Irec, Iext=self.input, r=r, dt=bm.dt) self.u[:] = bm.where(u&amp;gt;0,u,0) self.g.value = g self.h.value = h self.input[:] = 0. # é‡ç½®å¤–éƒ¨ç”µæµ ``` ### Simulate traveling wave in CANN with STP ```python def traveling_wave_STP(num=512,k=0.1,J0=12.,tau_d=1000,tau_f=1.,G=0.9): # ç”ŸæˆCANN cann_stp = CANN1D_STP(num=num, k=k,tau_d=tau_d,tau_f=tau_f,G=G, J0=J0) # ç”Ÿæˆå¤–éƒ¨åˆºæ¿€ dur = 1000. noise_level = 0.1 Iext = bm.random.normal(0., noise_level, (int(dur / bm.get_dt()), num)) duration = dur # è¿è¡Œæ•°å€¼æ¨¡æ‹Ÿ runner = bp.DSRunner(cann_stp, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;,&apos;g&apos;,&apos;h&apos;]) runner.run(duration) fig,ax = plt.subplots(figsize=(3,3)) u = bm.as_numpy(runner.mon.u) max_index = np.argmax(u[1000,:]) print(max_index) ax.plot(runner.mon.g[:,max_index],label=&apos;g&apos;) ax.plot(runner.mon.h[:,max_index],label=&apos;h&apos;) ax.legend() # å¯è§†åŒ– def plot_response(t): fig, gs = bp.visualize.get_figure(1, 1, 3, 3) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann_stp.x, I, label=&apos;Iext&apos;) ax.plot(cann_stp.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() plot_response(t=100.) plot_response(t=200.) plot_response(t=300.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann_stp.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann_stp.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=1, frame_delay=40, show=True, ) plt.show() traveling_wave_STP(G=0.5,tau_d=50) ``` # Decision-Making Network ## LIP -&amp;gt; Decision-Making ### Coherent motion task åˆ¤æ–­éšæœºç‚¹(å¤§éƒ¨åˆ†ç‚¹)çš„è¿åŠ¨æœå‘ ![image-20230828100425871](/BrainPy-course-notes/master_content/Notes.assets/image-20230828100425871.png) coherenceå½±å“ä»»åŠ¡çš„éš¾åº¦ 0%éš¾ï¼Œ100%ç®€å• ![image-20230828100516123](/BrainPy-course-notes/master_content/Notes.assets/image-20230828100516123.png) ç¼–ç å†³ç­–çš„å“åº”ï¼Œä¸æ˜¯è¿åŠ¨ ### Reaction Time vs. Fixed Duration coherenceè¶Šé«˜ï¼Œååº”æ—¶é—´è¶ŠçŸ­ Fixed Durationå¤šäº†Delay time ![image-20230828100658772](/BrainPy-course-notes/master_content/Notes.assets/image-20230828100658772.png) å®žéªŒè®¾è®¡çº¯ç²¹æŠŠdecision-makingç»™æå–å‡ºæ¥ #### Effect of Difficulty coherenceè¶Šå¤§ï¼Œååº”æ—¶é—´æ˜¯è¶ŠçŸ­ï¼Œsingle neuronå¾ˆéš¾åšåˆ°è¿™ä¹ˆçŸ­çš„decision-makingï¼Œè€ƒè™‘è¦å»ºæ¨¡çš„å› ç´  ![image-20230828101103008](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101103008.png) ![image-20230828101058059](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101058059.png) #### Response of MT Neurons è®°å½•MTçš„ç¥žç»å…ƒï¼Œå¯¹è¿™ç§è¿åŠ¨çš„æœå‘åˆºæ¿€è¿›è¡Œç¼–ç  çº¿æ€§ç¼–ç coherenceè¿åŠ¨å¼ºåº¦çš„æ–¹å‘ åšå†³ç­–åœ¨å®ƒçš„ä¸‹æ¸¸ ![image-20230828101303674](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101303674.png) #### Response of LIP Neurons MTçš„ä¸‹æ¸¸æ‰¾åˆ°LIPçš„ç¥žç»å…ƒ çˆ¬å‡åˆ°ä¸€å®šé«˜åº¦å†åšé€‰æ‹© coherenceä¸Žçˆ¬å‡çš„æ–œçŽ‡ä¹Ÿä¼šæœ‰å½±å“ï¼Œä»»åŠ¡è¶Šéš¾ï¼Œçˆ¬å‡æ–œçŽ‡è¶Šå° ![image-20230828101609881](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101609881.png) ### Ramping-to-threshold(perfect integrator) Model $$ \begin{aligned}\frac{dR}{dt}=I_A-I_B+\text{noise},\quad R(t)&amp;=(I_A-I_B)t+\int_0^tdt\text{noise}.\\\tau_\text{network}&amp;=\infty!\end{aligned} $$ ä¸¤ç§é€‰æ‹©ç§¯åˆ†æ±‚å’Œåšç§¯ç´¯ï¼Œç­‰åˆ°é˜ˆå€¼åšå†³ç­– Accumulates information (evidence) -&amp;gt; Ramping ç›´æŽ¥ä¿å­˜ä¿¡æ¯ï¼Œæ²¡æœ‰ç‰¹åˆ«å¥½çš„ç”Ÿç‰©å¯¹åº” ## A Spiking Network of DM ### A cortical microcircuit model ![image-20230828103055151](Notes.assets/image-20230828103055151.png) A=Upward motion B=Downward motion 2-population excitatory neurons (integrate-and-fire neurons driven by Poisson input) Slow reverberatory excitation mediated by the NMDA receptors at recurrent synapses AMPA receptors ($\tau _{syn}=$1 - 3 ms) NMDA receptors ($\tau _{syn}=$ 50 - 100 ms). ä¸¤ç¾¤ç¥žç»å…ƒåˆ†åˆ«åšä¸åŒçš„é€‰æ‹©ï¼Œä¸Žè‡ªå·±å¯¹æ–¹éƒ½æœ‰è¿žæŽ¥ NMDA ç¼“æ…¢çš„ä¿¡å·ä½¿å¾—æœ‰æ…¢æ…¢å¢žé•¿çš„rampingçš„è¿‡ç¨‹ interneuronsçš„backwardæœ‰æŠ‘åˆ¶ä½œç”¨ #### Coherence-Dependent Input çº¿æ€§ç¼–ç è¿åŠ¨æœå‘çš„ä¿¡æ¯ï¼Œcoherenceå¼ºåº¦å½±å“firing rateï¼Œä¸€ç³»åˆ—æ³Šæ¾è¿‡ç¨‹ï¼ŒåŒæ—¶è¿˜æœ‰noiseã€‚ æœ¬èº«ä¸¤ç§ä¿¡æ¯è¿˜æ˜¯æœ‰å·®å¼‚ ![image-20230828104054275](/BrainPy-course-notes/master_content/Notes.assets/image-20230828104054275.png) #### Duality of this model ä¸åŒcoherenceçš„ç¥žç»å…ƒå“åº” ![image-20230828104432061](/BrainPy-course-notes/master_content/Notes.assets/image-20230828104432061.png) ä¸¤ä¸ªgroupä¼šç«žäº‰ï¼Œå½“æœ‰ä¸€ä¸ªgroupè¾¾åˆ°20%ï¼Œè¿›å…¥è¿™ä¸ªçª—å£ï¼Œå°±ä¼šç›´æŽ¥å‘æ”¾ä¸ŠåŽ» Spontaneous symmetry breaking and stochastic decision making ![image-20230828104600840](/BrainPy-course-notes/master_content/Notes.assets/image-20230828104600840.png) ## Simulation of Spiking DM ### A Cortical Microcircuit Model ç”¨ä¸¤ä¸ªcoherenceç”Ÿæˆå‡ºæ¥çš„åºåˆ— ![image-20230828110300576](/BrainPy-course-notes/master_content/Notes.assets/image-20230828110300576.png) $$ \begin{gathered}C_m\frac{dV(t)}{dt}=-g_L(V(t)-V_L)-I_{syn}(t)\\I_{syn}(t)=I_{\mathrm{ext},\mathrm{AMPA}}\left(t\right)+I_{\mathrm{rec},AMPA}(t)+I_{\mathrm{rec},NMDA}(t)+I_{\mathrm{rec},\mathrm{GABA}}(t)\end{gathered} $$ $$ \begin{gathered} I_{\mathrm{ext},\mathrm{AMPA}}\left(t\right)=g_{\mathrm{ext},\mathrm{AMPA}}\left(V(t)-V_{E}\right)s^{\mathrm{ext},\mathrm{AMPA}}\left(t\right) \\ I_{\mathrm{rec},\mathrm{AMP}\Lambda}\left(t\right)=g_{\mathrm{rec},\mathrm{AMP}\Lambda}\left(V(t)-V_{E}\right)\sum_{j=1}^{Ce}w_{j}s_{j}^{AMPA}(t) \\ I_{\mathrm{rec},\mathrm{NMDA}}\left(t\right)=\frac{g_{\mathrm{NMDA}}(V(t)-V_{E})}{\left(1+\left[\mathrm{Mg}^{2+}\right]\exp(-0.062V(t))/3.57\right)}\sum_{j=1}^{\mathrm{C_E}}w_{j}s_{j}^{\mathrm{NMDA}}\left(t\right) \\ I_\mathrm{rec,GABA}(t)=g_\mathrm{GABA}(V(t)-V_l)\sum_{j=1}^{C_1}s_j^\mathrm{GABA}(t) \end{gathered} $$ $$ w_j=\left\{\begin{matrix}w_+&amp;gt;1,\\w_-E/I conn self.I2B = AMPA(self.I, self.B, &apos;all2all&apos;, 0.5, g_I2E_GABAa, tau=5., E=-70.) self.I2A = AMPA(self.I, self.A, &apos;all2all&apos;, 0.5, g_I2E_GABAa, tau=5., E=-70.) self.I2N = AMPA(self.I, self.N, &apos;all2all&apos;, 0.5, g_I2E_GABAa, tau=5., E=-70.) self.I2I = AMPA(self.I, self.I, &apos;all2all&apos;, 0.5, g_I2I_GABAa, tau=5., E=-70.) # define external projections #### TO DO!!!! self.noise2B = AMPA(self.noise_B, self.B, &apos;one2one&apos;, None, g_ext2E_AMPA, tau=2., E=0.) self.noise2A = AMPA(self.noise_A, self.A, &apos;one2one&apos;, None, g_ext2E_AMPA, tau=2., E=0.) self.noise2N = AMPA(self.noise_N, self.N, &apos;one2one&apos;, None, g_ext2E_AMPA, tau=2., E=0.) self.noise2I = AMPA(self.noise_I, self.I, &apos;one2one&apos;, None, g_ext2I_AMPA, tau=2., E=0.) ``` ```python class Tool: def __init__(self, pre_stimulus_period=100., stimulus_period=1000., delay_period=500.): self.pre_stimulus_period = pre_stimulus_period self.stimulus_period = stimulus_period self.delay_period = delay_period self.freq_variance = 10. self.freq_interval = 50. self.total_period = pre_stimulus_period + stimulus_period + delay_period def generate_freqs(self, mean): # stimulus period n_stim = int(self.stimulus_period / self.freq_interval) n_interval = int(self.freq_interval / bm.get_dt()) freqs_stim = np.random.normal(mean, self.freq_variance, (n_stim, 1)) freqs_stim = np.tile(freqs_stim, (1, n_interval)).flatten() # pre stimulus period freqs_pre = np.zeros(int(self.pre_stimulus_period / bm.get_dt())) # post stimulus period freqs_delay = np.zeros(int(self.delay_period / bm.get_dt())) all_freqs = np.concatenate([freqs_pre, freqs_stim, freqs_delay], axis=0) return bm.asarray(all_freqs) def visualize_results(self, mon, IA_freqs, IB_freqs, t_start=0., title=None): fig, gs = bp.visualize.get_figure(4, 1, 3, 10) axes = [fig.add_subplot(gs[i, 0]) for i in range(4)] ax = axes[0] bp.visualize.raster_plot(mon[&apos;ts&apos;], mon[&apos;A.spike&apos;], markersize=1, ax=ax) if title: ax.set_title(title) ax.set_ylabel(&quot;Group A&quot;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax = axes[1] bp.visualize.raster_plot(mon[&apos;ts&apos;], mon[&apos;B.spike&apos;], markersize=1, ax=ax) ax.set_ylabel(&quot;Group B&quot;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax = axes[2] rateA = bp.measure.firing_rate(mon[&apos;A.spike&apos;], width=10.) rateB = bp.measure.firing_rate(mon[&apos;B.spike&apos;], width=10.) ax.plot(mon[&apos;ts&apos;], rateA, label=&quot;Group A&quot;) ax.plot(mon[&apos;ts&apos;], rateB, label=&quot;Group B&quot;) ax.set_ylabel(&apos;Population activity [Hz]&apos;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax.legend() ax = axes[3] ax.plot(mon[&apos;ts&apos;], IA_freqs, label=&quot;group A&quot;) ax.plot(mon[&apos;ts&apos;], IB_freqs, label=&quot;group B&quot;) ax.set_ylabel(&quot;Input activity [Hz]&quot;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax.legend() ax.set_xlabel(&quot;Time [ms]&quot;) plt.show() ``` ```python tool = Tool() net = DecisionMakingNet() mu0 = 40. coherence = 25.6 IA_freqs = tool.generate_freqs(mu0 + mu0 / 100. * coherence) IB_freqs = tool.generate_freqs(mu0 - mu0 / 100. * coherence) def give_input(): i = bp.share[&apos;i&apos;] net.IA.freqs[0] = IA_freqs[i] net.IB.freqs[0] = IB_freqs[i] runner = bp.DSRunner(net, inputs=give_input, monitors=[&apos;A.spike&apos;, &apos;B.spike&apos;]) runner.run(tool.total_period) tool.visualize_results(runner.mon, IA_freqs, IB_freqs) ``` ### Results ![image-20230828112245950](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112245950.png) #### Stochastic Decision Making ![image-20230828112253619](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112253619.png) ## A Rate Network of DM ### Reduced Model åŒ–ç®€åˆ°åªæœ‰ä¸¤ç¾¤ç¥žç»å…ƒï¼ŒåªæŽ¥å—å¤–ç•Œè¾“å…¥ä¿¡å·ï¼Œäº’ç›¸å½±å“å¯¹æ–¹ ![image-20230828112326267](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112326267.png) Synaptic variables $$ \begin{gathered} \frac{dS_{1}}{dt} =F(x_1)\gamma(1-S_1)-S_1/\tau_s \\ \frac{dS_2}{dt} =F(x_2)\gamma(1-S_2)-S_2/\tau_s \end{gathered} $$ Input current to each population $$ \begin{gathered} x_{1} =J_{E}S_{1}+J_{I}S_{2}+I_{0}+I_{noise1}+J_{\text{ext }\mu_{1}} \\ x_{2} =J_{E}S_{2}+J_{I}S_{1}+I_{0}+I_{noise2}+J_{\mathrm{ext}}\mu_{2} \end{gathered} $$ Background input $$ I_0+I_{noise}\\ \begin{gathered} dI_{noise1} =-I_{noise1}\frac{dt}{\tau_{0}}+\sigma dW \\ dI_{noise2} =-I_{noise2}\frac{dt}{\tau_{0}}+\sigma dW \end{gathered} $$ Firing rates $$ r_i=F(x_i)=\frac{ax_i-b}{1-\exp(-d(ax_i-b))} $$ Coherence-dependent inputs $$ \begin{array}{l}\mu_1=\mu_0\big(1+c&apos;/100\big)\\\mu_2=\mu_0\big(1-c&apos;/100\big)\end{array} $$ $$ \begin{aligned}&amp;\gamma,a,b,d,J_E,J_I,J_{\mathrm{ext}},I_0,\mu_0,\tau_{\mathrm{AMPA}},\sigma_{\mathrm{noise}}\\&amp;\text{are fixed parameters.}\end{aligned} $$ ```python class DecisionMakingRateModel(bp.dyn.NeuGroup): def __init__(self, size, coherence, JE=0.2609, JI=0.0497, Jext=5.2e-4, I0=0.3255, gamma=6.41e-4, tau=100., tau_n=2., sigma_n=0.02, a=270., b=108., d=0.154, noise_freq=2400., method=&apos;exp_auto&apos;, **kwargs): super(DecisionMakingRateModel, self).__init__(size, **kwargs) # åˆå§‹åŒ–å‚æ•° self.coherence = coherence self.JE = JE self.JI = JI self.Jext = Jext self.I0 = I0 self.gamma = gamma self.tau = tau self.tau_n = tau_n self.sigma_n = sigma_n self.a = a self.b = b self.d = d # åˆå§‹åŒ–å˜é‡ self.s1 = bm.Variable(bm.zeros(self.num) + 0.15) self.s2 = bm.Variable(bm.zeros(self.num) + 0.15) self.r1 = bm.Variable(bm.zeros(self.num)) self.r2 = bm.Variable(bm.zeros(self.num)) self.mu0 = bm.Variable(bm.zeros(self.num)) self.I1_noise = bm.Variable(bm.zeros(self.num)) self.I2_noise = bm.Variable(bm.zeros(self.num)) # å™ªå£°è¾“å…¥çš„ç¥žç»å…ƒ self.noise1 = bp.dyn.PoissonGroup(self.num, freqs=noise_freq) self.noise2 = bp.dyn.PoissonGroup(self.num, freqs=noise_freq) # å®šä¹‰ç§¯åˆ†å‡½æ•° self.integral = bp.odeint(self.derivative, method=method) @property def derivative(self): return bp.JointEq([self.ds1, self.ds2, self.dI1noise, self.dI2noise]) def ds1(self, s1, t, s2, mu0): I1 = self.Jext * mu0 * (1. + self.coherence / 100.) x1 = self.JE * s1 - self.JI * s2 + self.I0 + I1 + self.I1_noise r1 = (self.a * x1 - self.b) / (1. - bm.exp(-self.d * (self.a * x1 - self.b))) return - s1 / self.tau + (1. - s1) * self.gamma * r1 def ds2(self, s2, t, s1, mu0): I2=self.Jext*mu0*(1.- self.coherence / 100.) x2 = self.JE * s2 - self.JI * s1 + self.I0 + I2 + self.I2_noise r2 = (self.a * x2 - self.b) / (1. - bm.exp(-self.d * (self.a * x2 - self.b))) return - s2 / self.tau + (1. - s2) * self.gamma * r2 def dI1noise(self, I1_noise, t, noise1): return (- I1_noise + noise1.spike * bm.sqrt(self.tau_n * self.sigma_n * self.sigma_n)) / self.tau_n def dI2noise(self, I2_noise, t, noise2): return (- I2_noise + noise2.spike * bm.sqrt(self.tau_n * self.sigma_n * self.sigma_n)) / self.tau_n def update(self, tdi): # æ›´æ–°å™ªå£°ç¥žç»å…ƒä»¥äº§ç”Ÿæ–°çš„éšæœºå‘æ”¾ self.noise1.update(tdi) self.noise2.update(tdi) # æ›´æ–°s1ã€s2ã€I1_noiseã€I2_noise integral = self.integral(self.s1, self.s2, self.I1_noise, self.I2_noise, tdi.t, mu0=self.mu0, noise1=self.noise1, noise2=self.noise2, dt=tdi.dt) self.s1.value, self.s2.value, self.I1_noise.value, self.I2_noise.value = integral # ç”¨æ›´æ–°åŽçš„s1ã€s2è®¡ç®—r1ã€r2 I1 = self.Jext * self.mu0 * (1. + self.coherence / 100.) x1 = self.JE * self.s1 + self.JI * self.s2 + self.I0 + I1 + self.I1_noise self.r1.value = (self.a * x1 - self.b) / (1. - bm.exp(-self.d * (self.a * x1 - self.b))) I2 = self.Jext * self.mu0 * (1. - self.coherence / 100.) x2 = self.JE * self.s2 + self.JI * self.s1 + self.I0 + I2 + self.I2_noise self.r2.value = (self.a * x2 - self.b) / (1. - bm.exp(-self.d * (self.a * x2 - self.b))) # é‡ç½®å¤–éƒ¨è¾“å…¥ self.mu0[:] = 0. ``` ```python # å®šä¹‰å„ä¸ªé˜¶æ®µçš„æ—¶é•¿ pre_stimulus_period, stimulus_period, delay_period = 100., 2000., 500. # ç”Ÿæˆæ¨¡åž‹ dmnet = DecisionMakingRateModel(1, coherence=25.6, noise_freq=2400.) # å®šä¹‰ç”µæµéšæ—¶é—´çš„å˜åŒ– inputs, total_period = bp.inputs.constant_input([(0., pre_stimulus_period), (20., stimulus_period), (0., delay_period)]) # è¿è¡Œæ•°å€¼æ¨¡æ‹Ÿ runner = bp.DSRunner(dmnet, monitors=[&apos;s1&apos;, &apos;s2&apos;, &apos;r1&apos;, &apos;r2&apos;], inputs=(&apos;mu0&apos;, inputs, &apos;iter&apos;)) runner.run(total_period) # å¯è§†åŒ– fig, gs = plt.subplots(2, 1, figsize=(6, 6), sharex=&apos;all&apos;) gs[0].plot(runner.mon.ts, runner.mon.s1, label=&apos;s1&apos;) gs[0].plot(runner.mon.ts, runner.mon.s2, label=&apos;s2&apos;) gs[0].axvline(pre_stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[0].axvline(pre_stimulus_period + stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[0].set_ylabel(&apos;gating variable $s$&apos;) gs[0].legend() gs[1].plot(runner.mon.ts, runner.mon.r1, label=&apos;r1&apos;) gs[1].plot(runner.mon.ts, runner.mon.r2, label=&apos;r2&apos;) gs[1].axvline(pre_stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[1].axvline(pre_stimulus_period + stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[1].set_xlabel(&apos;t (ms)&apos;) gs[1].set_ylabel(&apos;firing rate $r$&apos;) gs[1].legend() plt.subplots_adjust(hspace=0.1) plt.show() ``` ### Results ![image-20230828112555018](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112555018.png) ## Phase Plane Analysis å› ä¸ºåªæœ‰ä¸¤ä¸ªvariable ### Model implementation ```python @bp.odeint def int_s1(s1, t, s2, coh=0.5, mu=20.): x1 = JE * s1 + JI * s2 + Ib + JAext * mu * (1. + coh/100) r1 = (a * x1 - b) / (1. - bm.exp(-d * (a * x1 - b))) return - s1 / tau + (1. - s1) * gamma * r1 @bp.odeint def int_s2(s2, t, s1, coh=0.5, mu=20.): x2 = JE * s2 + JI * s1 + Ib + JAext * mu * (1. - coh/100) r2 = (a * x2 - b) / (1. - bm.exp(-d * (a * x2 - b))) return - s2 / tau + (1. - s2) * gamma * r2 ``` ### Without / with input ![image-20230828112709355](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112709355.png) åªå—æ‰°åŠ¨å½±å“ï¼Œæœ‰inputåŽä¸­é—´å˜å¾—ä¸ç¨³å®šï¼Œä½†å¦‚æžœå·²ç»é€‰æ‹©ï¼Œç½‘ç»œä»ç»´æŒä¹‹å‰é€‰æ‹©çš„ç»“æžœ ![image-20230828112811394](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112811394.png) ### Coherence ç¨³å®šç‚¹å¯¹ç½‘ç»œçš„æ‹‰ä¼¸æ›´å¼º ![image-20230828113031946](/BrainPy-course-notes/master_content/Notes.assets/image-20230828113031946.png) ![image-20230828113009219](/BrainPy-course-notes/master_content/Notes.assets/image-20230828113009219.png) # Reservoir Computing å¼•å…¥è®­ç»ƒ å€¾å‘äºŽä½¿ç”¨RNN ![image-20230828140305956](/BrainPy-course-notes/master_content/Notes.assets/image-20230828140305956.png) Connecting different units $$ \begin{aligned} &amp;\textsf{Input to unit i from unit j:} \\ &amp;&amp;&amp;I_{j\rightarrow i}=J_{ij}r_{j}(t) \\ &amp;\textsf{Total input to unit i:} \\ &amp;&amp;&amp;I_{i}^{(tot)}=\sum_{j=1}^{N}J_{ij}r_{j}(t)+I_{i}^{(ext)} \end{aligned} $$ $$ \textsf{Activation of unit i:} \\ \tau\frac{dx_{i}}{dt}=-x_{i}+\sum_{j=1}^{N}J_{ij}\frac{\phi(x_{j})}{1}+I_{i}^{(ext)}(t) $$ è®­ç»ƒèŒƒå¼ ![image-20230828140707998](/BrainPy-course-notes/master_content/Notes.assets/image-20230828140707998.png) ## Echo state machine ### Echo state machine ç±»ä¼¼äººå·¥ç¥žç»ç½‘ç»œRNNï¼Œå¯ä»¥å¤„ç†temporalä¿¡æ¯ ![image-20230828140937455](/BrainPy-course-notes/master_content/Notes.assets/image-20230828140937455.png) $$ \begin{aligned} &amp;\mathbf{x}(n+1) =f(\mathbf{W}^{\mathrm{in}}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n)+\mathbf{W}^{\mathrm{back}}\mathbf{y}(n)) \\ &amp;\mathbf{y}(n+1) =\mathbf{W}^{\mathrm{out}}(\mathbf{u}(n+1),\mathbf{x}(n+1),\mathbf{y}(n)) \end{aligned} $$ For an RNN, the state of its internal neurons reflects the historical information of the external inputs. åæ˜ çš„echoçš„åŽ†å²ä¿¡æ¯ï¼Œå”¯ä¸€ä¾èµ–åŽ†å²ä¿¡æ¯ Assuming that the updates of the network are discrete, the external input at the ð‘›th moment is u(ð‘›) and the neuron state is x(ð‘›), then x(ð‘›) should be determined by u(ð‘›), u(ð‘› - 1), ... uniquely determined. At this point, x(ð‘›) can be regarded as an &quot;echo&quot; of the historical input signals. ä¸éœ€è¦è®­ç»ƒconnection ### Echo state machine with leaky integrator æœ‰ä¸€ä¸ªleakyé¡¹ï¼Œå¼•å…¥decay ### $$ \begin{aligned}\hat{h}(n)=\tanh(W^{in}x(n)+W^{rec}h(n-1)+W^{fb}y(n-1)+b^{rec})\\h(n)=(1-\alpha)x(n-1)+\alpha\hat{h}(n)\end{aligned} $$ where $h(n)$ is a vector of reservoir neuron activations, $W^{in}$ and $W^{rec}$ are the input and recurrent weight matrices respectively, and $\alpha\in(0,1]$ is the leaking rate. The model is also sometimes used without the leaky integration, which is a special case of $\alpha=1$ The linear readout layer is defined as $$ y(n)=W^{out}h(n)+b^{out} $$ where $y(n)$ is network output, $W^{out}$ the output weight matrix, and $b^out$ is the output bias ## Constraints of echo state machine ### Echo state property #### Theorem 1 For the echo state network defined above, the network will be echoey as long as the maximum singular value $\sigma_{max} Provement: &amp;gt; $$ &amp;gt; \begin{aligned} &amp;gt; d(\mathbf{x}(n+1),\mathbf{x}^{\prime}(n+1))&amp; =d(T(\mathbf{x}(n),\mathbf{u}(n+1)),T(\mathbf{x}&apos;(n),\mathbf{u}(n+1))) \\ &amp;gt; &amp;=d(f(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n)),f(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}&apos;(n))) \\ &amp;gt; &amp;\leq d(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n),\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}^{\prime}(n)) \\ &amp;gt; &amp;=d(\mathbf{W}\mathbf{x}(n),\mathbf{W}\mathbf{x}&apos;(n)) \\ &amp;gt; &amp;=||\mathbf{W}(\mathbf{x}(n)-\mathbf{x}^{\prime}(n))|| \\ &amp;gt; &amp;\leq\sigma_{\max}(\mathbf{W})d(\mathbf{x}(n),\mathbf{x}&apos;(n)) &amp;gt; \end{aligned} &amp;gt; $$ #### Theorem 2 For the echo state network defined above, as long as the spectral radius $|\lambda_{max}|$ of the recurrent connection matrix W &amp;gt; 1, then the network must not be echogenic. The spectral radius of the matrix is the absolute value of the largest eigenvalue $\lambda_{max}$. #### How to initialize Using these two theorems, how should we initialize W so that the network has an echo property? If we scale W, i.e., multiply it by a scaling factor $\alpha$, then $\sigma_{max}\alpha_{max}\text{,the network will not have the echo state.}\\\bullet&amp;\text{if}\alpha_{min}\le\alpha\le\alpha_{max}\text{,the network may have the echo state.}\end{array} $$ **$\alpha$è®¾çš„ç•¥å°äºŽ1** ![image-20230828142516052](/BrainPy-course-notes/master_content/Notes.assets/image-20230828142516052.png) ### Global parameters of reservoir è¿™äº›è¶…å‚ä¼šå½±å“reservoir networkçš„æ€§èƒ½ï¼Œéœ€è¦æ‰‹åŠ¨è°ƒå‚ï¼Œå¾ˆéš¾è‡ªåŠ¨åŽ»è°ƒæ•´ - The size $N_x$ - General wisdom: the bigger the reservoir, the better the obtainable performance - Select global parameters with smaller reservoirs, then scale to bigger ones. - Sparsity - Distribution of nonzero elements: - Normal distribution - Uniform distribution - The width of the distributions does not matter - spectral radius of $W$ - scales the width of the distribution of its nonzero elements - determines how fast the influence of an input dies out in a reservoir with time, and how stable the reservoir activations are - The spectral radius should be larger in tasks requiring longer memory of the input - Scaling(-s) to $W^{in}$: - For uniform distributed $W^{in}$, $\alpha$ in the range of the interval $[-a;a]$. - For normal distributed $W^{in}$, one may take the standard deviation as a scaling measure. The leaking rate $\alpha$ ## Training of echo state machine ### Offline learning The advantage of the echo state network is that it does not train recurrent connections within the reservoir, but only the readout layer from the reservoir to the output. çº¿æ€§å±‚çš„ä¼˜åŒ–æ–¹æ³•æ˜¯ç®€å•çš„ **Ridge regression** $$ \begin{aligned}\epsilon_{\mathrm{train}}(n)&amp;=\mathbf{y}(n)-\mathbf{\hat{y}}(n) \\&amp;=\mathbf{y}(n)-\mathbf{W}^{\mathrm{out}}\mathbf{x}(n) \\&amp;L_{\mathrm{ridge}}=\frac{1}{N}\sum_{i=1}^{N}\epsilon_{\mathrm{train}}^{2}(i)+\alpha||\mathbf{W^{out}}||^{2} \\\\W^{out}&amp;=Y^{target}X^T(XX^T+\beta I)^{-1}\end{aligned} $$ ```python trainer = bp.OfflineTrainer(model, fit_method=bp.algorithms.RidgeRegression(1e-7), dt=dt) ``` ### Online learning æ¥ä¸€ä¸ªsampleï¼Œè¿›è¡Œä¸€æ¬¡trainingï¼Œå¯¹è®­ç»ƒèµ„æºå¯ä»¥é¿å…ç“¶é¢ˆ The training data is passed to the trainer in a certain sequence (e.g., time series), and the trainer continuously learns based on the new incoming data. **Recursive Least Squares (RLS) algorithm** $$ E(\mathbf{y},\mathbf{y}^\mathrm{target},n)=\frac{1}{N_\mathrm{y}}\sum_{i=1}^{N_\mathrm{y}}\sum_{j=1}^{n}\lambda^{n-j}\left(y_i(j)-y_i^\mathrm{target}(j)\right)^2, $$ ```python trainer = bp.OnlineTrainer(model, fit_method=bp.algorithms.RLS(), dt=dt) ``` ### Dataset ç»™å®štime sequenceï¼Œå¯ä»¥è®©ç½‘ç»œåŽ»é¢„æµ‹regression ![image-20230828144309742](/BrainPy-course-notes/master_content/Notes.assets/image-20230828144309742.png) ç”¨åˆ°BrainPyé›†æˆçš„`Neuromorphic and Cognitive Datasets` ### Other tasks `MNIST dataset` or `Fashion MNIST` Two aspect: - Running time - Memory Usage ## Echo state machine programming ```python import brainpy as bp import brainpy.math as bm import brainpy_datasets as bd import matplotlib.pyplot as plt # enable x64 computation bm.set_environment(x64=True, mode=bm.batching_mode) bm.set_platform(&apos;cpu&apos;) ``` ### Dataset ```python def plot_mackey_glass_series(ts, x_series, x_tau_series, num_sample): plt.figure(figsize=(13, 5)) plt.subplot(121) plt.title(f&quot;Timeserie - {num_sample} timesteps&quot;) plt.plot(ts[:num_sample], x_series[:num_sample], lw=2, color=&quot;lightgrey&quot;, zorder=0) plt.scatter(ts[:num_sample], x_series[:num_sample], c=ts[:num_sample], cmap=&quot;viridis&quot;, s=6) plt.xlabel(&quot;$t$&quot;) plt.ylabel(&quot;$P(t)$&quot;) ax = plt.subplot(122) ax.margins(0.05) plt.title(f&quot;Phase diagram: $P(t) = f(P(t-\\tau))$&quot;) plt.plot(x_tau_series[: num_sample], x_series[: num_sample], lw=1, color=&quot;lightgrey&quot;, zorder=0) plt.scatter(x_tau_series[:num_sample], x_series[: num_sample], lw=0.5, c=ts[:num_sample], cmap=&quot;viridis&quot;, s=6) plt.xlabel(&quot;$P(t-\\tau)$&quot;) plt.ylabel(&quot;$P(t)$&quot;) cbar = plt.colorbar() cbar.ax.set_ylabel(&apos;$t$&apos;) plt.tight_layout() plt.show() ``` ```python dt = 0.1 mg_data = bd.chaos.MackeyGlassEq(25000, dt=dt, tau=17, beta=0.2, gamma=0.1, n=10) ts = mg_data.ts xs = mg_data.xs ys = mg_data.ys plot_mackey_glass_series(ts, xs, ys, num_sample=int(1000 / dt)) ``` ![image-20230828151451523](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151451523.png) ### Prediction of Mackey-Glass timeseries #### Prepare the data ```python def get_data(t_warm, t_forcast, t_train, sample_rate=1): warmup = int(t_warm / dt) # warmup the reservoir forecast = int(t_forcast / dt) # predict 10 ms ahead train_length = int(t_train / dt) X_warm = xs[:warmup:sample_rate] X_warm = bm.expand_dims(X_warm, 0) X_train = xs[warmup: warmup+train_length: sample_rate] X_train = bm.expand_dims(X_train, 0) Y_train = xs[warmup+forecast: warmup+train_length+forecast: sample_rate] Y_train = bm.expand_dims(Y_train, 0) X_test = xs[warmup + train_length: -forecast: sample_rate] X_test = bm.expand_dims(X_test, 0) Y_test = xs[warmup + train_length + forecast::sample_rate] Y_test = bm.expand_dims(Y_test, 0) return X_warm, X_train, Y_train, X_test, Y_test ``` ```python # First warmup the reservoir using the first 100 ms # Then, train the network in 20000 ms to predict 1 ms chaotic series ahead x_warm, x_train, y_train, x_test, y_test = get_data(100, 1, 20000) ``` ```python sample = 3000 fig = plt.figure(figsize=(15, 5)) plt.plot(x_train[0, :sample], label=&quot;Training data&quot;) plt.plot(y_train[0, :sample], label=&quot;True prediction&quot;) plt.legend() plt.show() ``` ![image-20230828151606545](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151606545.png) #### Prepare the ESN ```python class ESN(bp.DynamicalSystemNS): def __init__(self, num_in, num_hidden, num_out, sr=1., leaky_rate=0.3, Win_initializer=bp.init.Uniform(0, 0.2)): super(ESN, self).__init__() self.r = bp.layers.Reservoir( num_in, num_hidden, Win_initializer=Win_initializer, spectral_radius=sr, leaky_rate=leaky_rate, ) self.o = bp.layers.Dense(num_hidden, num_out, mode=bm.training_mode) def update(self, x): return x &amp;gt;&amp;gt; self.r &amp;gt;&amp;gt; self.o ``` #### Train and test ```python model = ESN(1, 100, 1) model.reset_state(1) trainer = bp.RidgeTrainer(model, alpha=1e-6) ``` ```python # warmup _ = trainer.predict(x_warm) ``` ```python # train _ = trainer.fit([x_train, y_train]) ``` #### Test the training data ```python ys_predict = trainer.predict(x_train) ``` ```python start, end = 1000, 6000 plt.figure(figsize=(15, 7)) plt.subplot(211) plt.plot(bm.as_numpy(ys_predict)[0, start:end, 0], lw=3, label=&quot;ESN prediction&quot;) plt.plot(bm.as_numpy(y_train)[0, start:end, 0], linestyle=&quot;--&quot;, lw=2, label=&quot;True value&quot;) plt.title(f&apos;Mean Square Error: {bp.losses.mean_squared_error(ys_predict, y_train)}&apos;) plt.legend() plt.show() ``` ![image-20230828151747954](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151747954.png) #### Test the testing data ```python ys_predict = trainer.predict(x_test) start, end = 1000, 6000 plt.figure(figsize=(15, 7)) plt.subplot(211) plt.plot(bm.as_numpy(ys_predict)[0, start:end, 0], lw=3, label=&quot;ESN prediction&quot;) plt.plot(bm.as_numpy(y_test)[0,start:end, 0], linestyle=&quot;--&quot;, lw=2, label=&quot;True value&quot;) plt.title(f&apos;Mean Square Error: {bp.losses.mean_squared_error(ys_predict, y_test)}&apos;) plt.legend() plt.show() ``` ![image-20230828151824907](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151824907.png) ### JIT connection operators - Just-in-time randomly generated matrix. - Support for Mat@Vec and Mat@Mat. - Support different random generation methods.(homogenous, uniform, normal) ```python import math, random def jitconn_prob_homo(events, prob, weight, seed, outs): random.seed(seed) max_cdist= math.ceil(2/prob -1) for event in events: if event: post_i = random.randint(1, max_cdist) outs[post_i] += weight ``` ![image-20230828153353131](/BrainPy-course-notes/master_content/Notes.assets/image-20230828153353131.png) ## Applications ### From the perspective of kernel methods ç»´åº¦æ‰©å¼ æ€æƒ³ Non-linear SVMs: Kernel Mapping ![image-20230828153621843](/BrainPy-course-notes/master_content/Notes.assets/image-20230828153621843.png) Kernel methods in neural system? **ä¸Žç»´åº¦æ‰©å¼ çš„æ€æƒ³ç›¸ä¼¼** ![image-20230828153801285](/BrainPy-course-notes/master_content/Notes.assets/image-20230828153801285.png) ### Subcortical pathway for rapid motion processing The first two stages of subcortical visual pathway: Retina -&amp;gt; superior colliculus The first two stages of primary auditory pathway: Inner Ear -&amp;gt; Cochlear Nuclei ç»´åº¦æ‰©å¼ åœ¨subcortical pathwayä¸­ä½“çŽ°ï¼Œreservoir èƒ½å¤Ÿé«˜ç»´å¤„ç†çš„æ›´ç®€å• ### Spatial-temporal tasks ![image-20230828154155803](/BrainPy-course-notes/master_content/Notes.assets/image-20230828154155803.png) æ—¢æœ‰æ—¶é—´ä¿¡æ¯ï¼Œåˆæœ‰ç©ºé—´ä¿¡æ¯çš„datasetï¼Œä½¿ç”¨reservoiræ¥å¤„ç†é«˜ç»´ä¿¡æ¯ï¼Œåä½ Dimension expansion ### Gait recognition inputæ¥äº†å†åšè®¡ç®— ![image-20230828154352087](/BrainPy-course-notes/master_content/Notes.assets/image-20230828154352087.png) ### Spatial-temporal tasks large-scaleï¼Œéšsizeå¢žå¤§ï¼Œaccuracyå¢žå¤§ ![image-20230828154428762](/BrainPy-course-notes/master_content/Notes.assets/image-20230828154428762.png) ### Liquid state machine A liquid state machine (LSM) is a type of reservoir computer that uses a spiking neural network. ä¸ŽESNä¸€æ ·çš„èŒƒå¼ï¼Œéƒ½æ˜¯åŽ»åšdimension expansion å¾ˆéš¾åŽ»åˆ†æžæ€Žä¹ˆworkçš„">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="Notes.assets/image-20230826111831637.png">
<meta property="og:image" content="Notes.assets/image-20230826111841073.png">
<meta property="og:image" content="Notes.assets/image-20230827142918529.png">
<meta property="og:image" content="Notes.assets/image-20230827143435002.png">
<meta property="og:image" content="Notes.assets/image-20230827144557359.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="BrainPy course notes">
<meta name="twitter:description" content="[TOC] # ç¥žç»è®¡ç®—å»ºæ¨¡ç®€ä»‹ ## è®¡ç®—ç¥žç»ç§‘å­¦çš„èƒŒæ™¯ä¸Žä½¿å‘½ è®¡ç®—ç¥žç»ç§‘å­¦æ˜¯**è„‘ç§‘å­¦**å¯¹**ç±»è„‘æ™ºèƒ½**çš„**æ¡¥æ¢** ### ä¸¤å¤§ç›®æ ‡ - ç”¨è®¡ç®—å»ºæ¨¡çš„æ–¹æ³•æ¥é˜æ˜Žå¤§è„‘åŠŸèƒ½çš„è®¡ç®—åŽŸç† - å‘å±•ç±»è„‘æ™ºèƒ½çš„æ¨¡åž‹å’Œç®—æ³• ### Prehistory - 1907 LIF model ç¥žç»è®¡ç®—çš„æœ¬è´¨ - 1950s HH model ç”µä½å®šé‡åŒ–æ¨¡åž‹ æœ€fundamentalçš„ - 1960s Roll&apos;s cable equation æè¿°ä¿¡å·åœ¨è½´çªå’Œæ ‘çªæ€Žä¹ˆä¼ é€’ - 1970s Amari, Wilson, Cowan et al. çŽ°ä»Šå»ºæ¨¡çš„åŸºç¡€ - 1982 Hopfield model(Amari-Hopfield model) å¼•å…¥ç‰©ç†å­¦æŠ€æœ¯ï¼Œå¸å¼•å­æ¨¡åž‹ - 1988 Sejnowski et al. &quot;Computational Neuroscience&quot;(science) æå‡ºè®¡ç®—ç¥žç»ç§‘å­¦æ¦‚å¿µ **çŽ°åœ¨çš„è®¡ç®—ç¥žç»ç§‘å­¦å¯¹åº”äºŽç‰©ç†å­¦çš„ç¬¬è°·-ä¼½åˆ©ç•¥æ—¶ä»£ï¼Œå¯¹å¤§è„‘å·¥ä½œåŽŸç†è¿˜ç¼ºä¹æ¸…æ™°çš„ç†è®º** ### Three levels of Brain Science ![image-20230823105226568](/BrainPy-course-notes/master_content/Notes.assets/image-20230823105226568.png) - å¤§è„‘åšä»€ä¹ˆ Computational theory -&amp;gt; Psychology &amp; Cognitive Science -&amp;gt; Human-like Cognitive function - å¤§è„‘æ€Žä¹ˆåš Representation &amp; Algorithm -&amp;gt; Computational Neuroscience -&amp;gt; Brain-inspired model &amp; algorithm - å¤§è„‘æ€Žä¹ˆå®žçŽ° Implementation -&amp;gt; Neuroscience -&amp;gt; Neuromorphic computing ### Mission of Computational Neuroscience &amp;gt; What I can not build a computational model, I do not understand ## ç¥žç»è®¡ç®—å»ºæ¨¡çš„ç›®æ ‡ä¸ŽæŒ‘æˆ˜ ### Limitation of Deep Learning - ä¸æ“…é•¿å¯¹æŠ—æ ·æœ¬ - å¯¹å›¾åƒçš„ç†è§£æœ‰é™ ![image-20230823105836259](/BrainPy-course-notes/master_content/Notes.assets/image-20230823105836259.png) ### Brain is for Processing Dynamical Information **We never &quot;see&quot; a static image** ![image-20230823105918336](/BrainPy-course-notes/master_content/Notes.assets/image-20230823105918336.png) ### The missing link a computational model of higher cognitive functior ![image-20230823110617639](/BrainPy-course-notes/master_content/Notes.assets/image-20230823110617639.png) çŽ°åœ¨åªæ˜¯åšçš„**å±€éƒ¨**çš„ç½‘ç»œï¼Œæ²¡æœ‰ä¸€ä¸ªæˆåŠŸçš„æ¨¡åž‹ï¼Œèƒ½**ä»Žç¥žç»å…ƒå‡ºå‘æž„å»ºç½‘ç»œï¼Œåˆ°ç³»ç»Ÿå±‚é¢ä¸Š** **åŽŸå› **: å› ä¸ºç¥žç»ç§‘å­¦åº•å±‚æ•°æ®çš„ç¼ºå¤±ï¼Œå¯ä»¥è€ƒè™‘æ•°æ®é©±åŠ¨ã€å¤§æ•°æ®çš„æ–¹å¼æ¥åŠ å¿«å‘å±• ## ç¥žç»è®¡ç®—å»ºæ¨¡çš„å·¥å…· &amp;gt; å·¥æ¬²è¡Œå…¶äº‹ï¼Œå¿…å…ˆåˆ©å…¶å™¨ &amp;gt; We need &quot;PyTorch/TensorFlow&quot; in Computational Neuroscience! ### Challenges in neural modelling æœ‰ä¸åŒçš„å°ºåº¦ - Mutiple-scale - Large-scale - Multiple purposes ![image-20230823111212460](/BrainPy-course-notes/master_content/Notes.assets/image-20230823111212460.png) &amp;gt; The modeling targets and methods are extremely complex, and we need a general framework. ### Limitations of Existing Brain Simulators çŽ°ä»Šçš„æ¡†æž¶ä¸èƒ½æ»¡è¶³ä»¥ä¸Š ![image-20230823111509523](/BrainPy-course-notes/master_content/Notes.assets/image-20230823111509523.png) ### What are needed for a brain simulator 1. Efficiency High-speed simulation on parallel computing devices, etc. 2. Integration Integrated modeling of simulation, training, and analysis 3. Flexibility New models at all scales can be accommodated 4. Extensibility Extensible to new modeling methods(machine learning) éœ€è¦æ–°çš„èŒƒå¼ ### Our solution: BrainPy 4 levels ![image-20230823111903456](/BrainPy-course-notes/master_content/Notes.assets/image-20230823111903456.png) ## ç¥žç»è®¡ç®—å»ºæ¨¡ä¸¾ä¾‹ ### Image understanding: an ill-posed problem Image Understanding = image segmentation + image object recognition &amp;gt; Chicken vs. Egg dilemma &amp;gt; &amp;gt; - Without segmentation, how to recognize &amp;gt; - Without recognition, how to segment **The solution of brain:** Analysis-by-synthesis çŒœæµ‹ä¸ŽéªŒè¯æ–¹æ³• ### Reverse Hierarchy Theory äººçš„æ„ŸçŸ¥æ˜¯æ•´ä½“åˆ°å±€éƒ¨ ### Two pathways for visual information processing ![image-20230823114517888](/BrainPy-course-notes/master_content/Notes.assets/image-20230823114517888.png) ### Key Computational Issues for Global-to-local Neural Information Processing - What are global and local features - How to rapidly extract global features - How to generate global hypotheses - How to implement from global to local processing - The interplay between global and local features - Others #### How to extract global features **Global first = Topology first**(å¤§èŒƒå›´é¦–å…ˆï¼Œé™ˆéœ–) è§†è§‰ç³»ç»Ÿæ›´æ•æ„ŸäºŽæ‹“æ‰‘æ€§è´¨çš„å·®å¼‚ &amp;gt; DNNs has difficulty to recognize topology **A retina-SC network for topology detection** è§†ç½‘è†œåˆ°ä¸Šä¸˜çš„æ£€æµ‹ï¼ŒGap junction coupling ... ### A Model for Motion Pattern Recognition Reservoir Module Decision-making Module ### How to generate &quot;global&quot; hypotheses in the representation space Attractor neural network ![image-20230823115853980](/BrainPy-course-notes/master_content/Notes.assets/image-20230823115853980.png) Levy Flight in Animal Behaviors ![image-20230823120000911](/BrainPy-course-notes/master_content/Notes.assets/image-20230823120000911.png) ### How to process information from global to local Push-pull Feedback A hierarchical Hopfield Model ### Interplay between global and local features A two-pathway model for object recognition ![image-20230823120750349](/BrainPy-course-notes/master_content/Notes.assets/image-20230823120750349.png) Modeling visual masking å¯ä»¥ç”¨two-pathwayå¾ˆå¥½è§£é‡Š # Programming basics ## Python Basics ### Values - Boolean - String - Integer - Float - ... ### Keywords Not allowed to use keywords, they define structure and rules of a language. ```python help(&quot;keywords&quot;) ``` ### Operators æ•°æ®ä¹‹é—´çš„æ“ä½œ #### For Integers and Floats ```python a=5 b=3 # addition + print(&quot;a+b=&quot;,atb) # subtraction - print(&quot;a-b=&quot;,a-b) # multiplication * print(&quot;axb=&quot;a*b) # division / print(&quot;a/b=&quot;,a/b) # power ** print(&quot;a**b=&quot;,a**b) ``` #### Booleans ```python #Boolean experssions # equals: == print(&quot;5==5&quot;,5==5) # do not equal: != print(&quot;5!-5&quot;,5!=5) # greater than: &amp;gt; print(&quot;5&amp;gt;5&quot;,5&amp;gt;5) # greater than or equal: &amp;gt;= print(&quot;5&amp;gt;=5â€5&amp;gt;=5) ``` ```python # logica operators print(&quot;True and False:&quot;, True and False) print(&quot;True or False:&quot;, True or False) print(&quot;not False:&quot;, not False) ``` ### Modules Not all functionality available comes automatically when starting python. ```python import match import numpy as np print(math.pi) print(np.pi) from numpy import pi print(pi) from numpy import * print(pi) ``` ### Control statements #### If ```python a = 5 # In Python, blocks of code are defined using indentation. if a == 5: print(&quot;ok&quot;) ``` &amp;gt; ok #### For ```python # range(5) means a list with integers, 0, 1, 2, 3, 4 for i in range(5): print(i) ``` &amp;gt; 0 &amp;gt; 1 &amp;gt; 2 &amp;gt; 3 &amp;gt; 4 #### While ```python i = 1 while i 1 &amp;gt; 8 &amp;gt; 1000 ### Functions - Functions are used to abstract components of a program. - Much like a mathematical function, they take some input and then find the result. start a function definition with a keyword def - Then comes the function name, with arguments in braces, and then a colon. ```python def func(args1, args2): pass ``` ### Data types #### List - Group variables together - Specific order - Access item with brankets: [ ] - List can be sliced - List can be multiplied - List can be added - Lists are mutable - Copying a list ```python myList = [0, 1, 2, 0,&quot;name&quot;] print(&quot;myList[0]:&quot;, myList[0]) print(&quot;myList[1]:&quot;, myList[1]) print(&quot;myList[3]:&quot;, myList[3]) print(&quot;myList[-1]:&quot;, myList[-1]) print(&quot;myList[-2]:&quot;, myList[-2]) ``` &amp;gt; myList[0]: 0 &amp;gt; myList[1]: 1 &amp;gt; myList[3]: name &amp;gt; myList[-1]: name &amp;gt; myList[-2]: 2.0 ```python myList = [0, 1.0, &quot;hello&quot;] print(&quot;myList[0:2]:&quot;, mylist[0:2]) print(&quot;myList*2:&quot;, myList*2) myList2 = [2,&quot;yes&quot;] print(&quot;myList+myList2:&quot;, myList+myList2) ``` &amp;gt; myList[0:2]: [0ï¼Œ1.0] &amp;gt; myList*2: [0ï¼Œ1.0ï¼Œ hello&apos;ï¼Œ0ï¼Œ1.0ï¼Œ hello&apos;] &amp;gt; myList+myList2: [0ï¼Œ1.0ï¼Œ&apos;hello&apos;ï¼Œ2ï¼Œyes&apos;] #### tuple Tuples are immutable. #### dictionary A dictionary is a collection of key-value pairs ```python d = {} d[1] = 2 d[&quot;a&quot;] = 3 print(&quot;d: &quot;, d) c = {1:2, &quot;a&quot;:3} print(&quot;c: &quot;, c) print(&quot;c[1]: &quot;, c[1]) ``` &amp;gt; d: {1: 2, &apos;a&apos;: 3} &amp;gt; c: {1: 2, &apos;a&apos;: 3} &amp;gt; c[1]: 2 ### Class In Python, everything is an object. Classes are objects, instances of classes are objects, modules are objects, and functions are objects. 1. a **type** 2. an internal **data representation** (primitive or composite) 3. a set of procedures for **interaction** with the object **a simple example** ```python # define class class Linear(): pass # instantiate object layer1 = Linear() print(layer1) ``` &amp;gt; `` #### Initializing an object ```python # define class class Linear(): # It refers to the object (instance) itself def __init__(self, n_input): self.n_input = n_input layer1 = Linear(100) layer2 = Linear(1000) print(&quot;layer1 : &quot;, layer1.n_input) print(&quot;layer2 : &quot;, layer2.n_input) ``` &amp;gt; layer1 : 100 &amp;gt; layer2 : 1000 #### Class has methods (similar to functions) ```python # define class class Linear(): ### It refers to the the object (instance) itself def __init__(self, n_input, n_output): self.n_input = n_input self.n_output = n_output def compute n params(self): num_params = self.n_input * self.n_output return num_params layerl = Linear(10,100) print(layerl.compute_n_params()) ``` &amp;gt; 1000 ## NumPy Basic ### Numpy Introduction - Fundamental package for scientific computing with Python - N-dimensional array object - Linear algebra, frontier transform, random number capacities - Building block for other packages (e.g. Scipy) ### Array - Arrays are mutable - Arrays attributes - ... ```python A = np.zeros((2, 2)) print(A) ``` &amp;gt; [[0. 0.] &amp;gt; [0. 0.]] ```python a.ndim # 2 dimension a.shape # (2, 5) shape of array a.size # 10 $ of elements a.T # transpose a.dtype # data type ``` #### Array broadcasting When operating on two arrays, numpy compares shapes. Two dimensions are compatible when 1. They are of equal size 2. One of them is 1 ![image-20230823143622229](/BrainPy-course-notes/master_content/Notes.assets/image-20230823143622229.png) ### Vector operations - Inner product - Outer product - Dot product (matrix multiplication) ```python u = [1, 2, 3] v = [1, 1, 1] np.inner(u, v) np.outer(u, v) np.dot(u, v) ``` &amp;gt; 6 &amp;gt; array([[1, 1, 1], &amp;gt; [2, 2, 2], &amp;gt; [3, 3, 3]]) &amp;gt; 6 ### Matrix operations - `np.ones` - `.T` - `np.dot` - `np.eye` - `np.trace` - `np.row_stack` - `np.column_stack` ### Operations along axes ```python a = np.ones((2, 3)) print(a) a.sum() a.sum(axis=0) a.cumsum() a.cumsum(axis=0) ``` ### Slicing arrays ```python a = np.random.random((2, 3)) print(a) a[0,:] # first row, all columns a[0:2] # first and second rows, al columns a[:,1:3]# all rows, second and third columns ``` ### Reshape ```python a = np.ones((10,1)) a.reshape(2,5) ``` ### Linear algebra ```python qr # Computes the QR decomposition cholesky # Computes the Cholesky decomposition inv(A) # Inverse solve(A,b) # Solves Ax = b for A full rank lstsq(A,b) # Solves arg minx //Ax - b//2 eig(A) # Eigenvalue decomposition eigvals(A) # Computes eigenvalues svd(Aï¼Œfull) # Sinqular value decomposition pinv(A) # Computes pseudo-inverse of A ``` ### Fourier transform ```python import numpy.fft fft # 1-dimensional DFT fft2 # 2-dimensional DFT fftn # N-dimensional DFT ifft # 1-dimensional inverse DFT (etc.) rfft # Real DFT (1-dim) ``` ### Random sampling ```python import numpy.random rand(d0, d1, ..., dn) # Random values in a given shape randn(d0, d1, ..., dn) # Random standard normal randint(lo, hi, size) # Random integers [lo hi) choice(a, size, repl, p) # Sample from a shuffle(a) # Permutation (in-place) permutation(a) # Permutation (new array) ``` ### Distributions in random ```python import numpy.random beta binomial chisquare exponential dirichlet gamma laplace lognormal ... ``` ### Scipy - `SciPy` is a library of algorithms and mathematical tools built to work with `NumPy ` arrays. - `scipy.linalg linear algebra` - `scipy.stats statistics` - `scipy.optimize optimization` - `scipy.sparse sparse matrices` - `scipy.signal signal processing` - etc. ## BrainPy introduction ### Modeling demands - Large-scale - Multi-scale - Methods ### BrainPy Architecture - Infrastructure - Functions - Just-in-time compilation - Devices ![image-20230823145349681](/BrainPy-course-notes/master_content/Notes.assets/image-20230823145349681.png) ### Main features #### Dense operators - Compatible with `NumPy`, `TensorFlow`, `PyTorch` and other dense matrix operator syntax. - Users do not need to learn and get started programming directly. #### Dedicated operatorsq - Applies brain dynamics sparse connectivity properties with event-driven computational features. - Reduce the complexity of brain dynamics simulations by several orders of magnitude. #### Numerical Integrators - Ordinary differential equations: brainpy.odeint - Stochastic differential equations: brainpy.sdeint - Fractional differential equations: brainpy.fdeint - Delayed differential equations #### Modular and composable ä»Žå¾®è§‚åˆ°å®è§‚ **brainpy.DynamicalSystem** ![image-20230823151159786](/BrainPy-course-notes/master_content/Notes.assets/image-20230823151159786.png) #### JIT of object-oriented BrainPy provides object-oriented transformations: - `brainpy.math.jit` - `brainpy.math.grad` - `brainpy.math.for_loop` - `brainpy.math.ifelse` ## BrainPy Programming Basics ### Just-in-Time compilation Just In Time Compilation (JIT, or Dynamic Translation), is compilation that is being done during the execution of a program. JIT compilation attempts to use **the benefits of both**. While the interpreted program is being run, the JIT compiler determines the most frequently used code and compiles it to machine code. The advantages of a JIT are due to the fact that since the compilation takes place in run time, a JIT compiler has access to dynamic runtime information enabling it to make better optimizations (such as inlining functions). ```python def gelu(x): sqrt = bm.sqrt(2 / bm.pi) cdf = 0.5 * (1.0 + bm.tanh(sqrt * (x + 0.044715 * (x ** 3)))) y = x *cdf return y &amp;gt;&amp;gt;&amp;gt; gelu_jit = bm.jit(gelu) # ä½¿ç”¨JIT ``` ### Object-oriented JIT compilation - The class object must be inherited from brainpy.BrainPyObject, the base class of BrainPy, whose methods will be automatically JIT compiled. - All time-dependent variables must be defined as brainpy.math.Variable. ```python class LogisticRegression(bp.BrainPyObject): def __init__(self, dimension): super(LogisticRegression, self).__init__() # parameters self.dimension = dimension # variables self.w = bm.Variable(2.0 * bm.ones(dimension) - 1.3) def __call__(self, X, Y): u = bm.dot(((1.0 / (1.0 + bm.exp(-Y * bm.dot(X, self.w))) - 1.0) * Y), X) self.w.value = self.w - u # in-place update ``` **ExampleL Run a neuron model** ```python model = bp.neurons.HH(1000) #ä¸€å…±1000ä¸ªç¥žç»å…ƒ runner = bp.DSRunner(target=model, inputs=(&apos;input&apos;, 10.)) # jité»˜è®¤ä¸ºTrue runner(duration=1000, eval_time=True) #æ¨¡æ‹Ÿ 1000ms ``` ç¦ç”¨JITæ¥debug ### Data operations #### Array ç­‰ä»·äºŽ`numpy`çš„`array` #### BrainPy arrays &amp; JAX arrays ```python t1 = bm.arange(3) print(t1) print(t1.value) ``` &amp;gt; JaxArray([0, 1, 2], dtype=int32) &amp;gt; DeviceArray([0, 1, 2], dtype=int32) #### Variables Arrays that are not marked as dynamic variables will be JIT-compiled as static arrays, and modifications to static arrays will not be valid in the JIT compilation environment. ```python t = bm.arange(4) v = bm.Variable(t) print(v) print(v.value) ``` &amp;gt; Variable([0, 1, 2, 3], dtype=int32) &amp;gt; DeviceArray([0, 1, 2, 3], dtype=int32) ### Variables **In-place updating** å°±åœ°æ›´æ–° #### Indexing and slicing - Indexing: `v[i] = a` or `v[(1, 3)] = c` - Slicing: `v[i:j] = b` - Slicing all values `v[:] = d`, `v[...] = e` #### Augmented assignment - add - subtract - divide - multiply - floor divide - modulo - power - and - or - xor - left shift - right shift #### Value assignment ```python v.value = bm.arange(10) check_no_change(v) ``` #### Update assignment ```python v.update(bm.random.randint(0, 20, size=10)) ``` ### Control flows #### If-else `brainpy.math.where` ```python a = 1. bm.where(a DeviceArray(1., dtype=float32, weak_type=True) `brainpy.math.ifelse` ```python def ifelse(condition, branches, operands): true_fun, false_fun = branches if condition: return true_fun(operands) else: return false_fun(operands) ``` #### For loop ```python import brainpy.math hist_of_out_vars = brainpy.math.for_loop(body_fun, operands) ``` #### While loop ```python i = bm.Variable(bm.zeros(1)) counter = bm.Variable(bm.zeros(1)) def cond_f(): return i[0] $$ (2\pi a\Delta x)c_{\mathrm{M}}\frac{\partial V(x,t)}{\partial t}+(2\pi a\Delta x)i_{\mathrm{ion}}=\frac{\pi a^{2}}{\rho_{\mathrm{L}}}\frac{\partial V(x+\Delta x,t)}{\partial x}-\frac{\pi a^{2}}{\rho_{\mathrm{L}}}\frac{\partial V(x,t)}{\partial x} $$ **Cable Equation** $$ c_\mathrm{M}\frac{\partial V(x,t)}{\partial t}=\frac{a}{2\rho_\mathrm{L}}\frac{\partial^2V(x,t)}{\partial x^2}-i_\mathrm{ion} $$ ç”µæµåœ¨é€šè¿‡é•¿ç›´å¯¼ä½“æ—¶ä¼šæ³„éœ²ç”µæµï¼Œå¦‚ä½•è®°å½•è†œç”µä½ï¼Œå¯ä»¥ä½¿ç”¨æ­¤æ–¹ç¨‹æ¥æè¿° **Passive conduction:** ion currents are caused by leaky channels exclusively $$ i_{\mathrm{ion}}=V(x,t)/r_{\mathrm{M}} $$ -&amp;gt; $$ \begin{aligned}c_\mathrm{M}\frac{\partial V(x,t)}{\partial t}&amp;=\frac{a}{2\rho_\mathrm{L}}\frac{\partial^2V(x,t)}{\partial x^2}-\frac{V(x,t)}{r_\mathrm{M}}\\\\\tau\frac{\partial V(x,t)}{\partial t}&amp;=\lambda^2\frac{\partial^2V(x,t)}{\partial x^2}-V(x,t)\quad\lambda=\sqrt{0.5ar_\mathrm{M}/\rho_\mathrm{L}}\end{aligned} $$ æ²¡æœ‰åŠ¨ä½œç”µä½ï¼Œå•çº¯é€šè¿‡ç”µç¼†ä¼ è¾“ ![image-20230824102932665](/BrainPy-course-notes/master_content/Notes.assets/image-20230824102932665.png) If a constant external current is applied to ð‘¥ = 0 the steady-state membrane potential $ð‘‰_{ss}(ð‘¥)$ is $$ \lambda^2\frac{\mathrm{d}^2V_{\mathrm{ss}}(x)}{\mathrm{d}x^2}-V_{\mathrm{ss}}(x)=0\longrightarrow V_{\mathrm{ss}}(x)=\frac{\lambda\rho_{\mathrm{L}}}{\pi a^2}I_0e^{-x/\lambda} $$ ç”µä¿¡å·æ— è¡°å‡ä¼ æ’­: åŠ¨ä½œç”µä½ ## Action potential &amp; active transport Steps of an action potential: - Depolarization - Repolarization - Hyperpolarization - Resting Characteristics: - All-or-none - Fixed shape - Active electrical property ![image-20230824103322522](/BrainPy-course-notes/master_content/Notes.assets/image-20230824103322522.png) How to simulate an action potential? $$ \begin{aligned} \frac{I(t)}{A}&amp; =c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+i_{\mathrm{ion}} \\ \Rightarrow\quad c_{\mathrm{M}}\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}&amp; =-g_{\mathrm{Cl}}(V_{\mathrm{M}}-E_{\mathrm{Cl}})-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}})-g_{\mathrm{Na}}(V_{\mathrm{M}}-E_{\mathrm{Na}})+\frac{I(t)}{A} \end{aligned} $$ ç¦»å­é€šé“çš„å¼€é—­ä¼šéšç€ç”µåŽ‹è€Œå˜åŒ–ï¼Œç”µå¯¼ä¹Ÿéšç€ç”µåŽ‹è€Œå˜åŒ– Mechanism: voltage-gated ion channels **HHå»ºæ¨¡æ€è·¯ï¼šé€šè¿‡ç”µå¯¼** ### Nodes of Ranvier Saltatory conduction with a much higher speed and less energy consumption ä¸¤ä¸ªéƒŽé£žç»“ä¹‹é—´ä¼šæœ‰ç¦»å­é€šé“ï¼Œæ—¢æœ‰è¢«åŠ¨ä¼ å¯¼ï¼Œä¹Ÿæœ‰ä¸»åŠ¨çš„é˜²æ­¢è¡°å‡ ![image-20230824104220106](/BrainPy-course-notes/master_content/Notes.assets/image-20230824104220106.png) ## The Hodgkin-Huxley Model ### Modeling of each ion channel Modeling of each ion channel: $$ g_m=\bar{g}_mm^x $$ Modeling of each ion gate: $$ \mathcal{C}\underset{}{\operatorname*{\overset{\alpha(\mathrm{V})}{\underset{\beta(\mathrm{V})}{\operatorname*{\longrightarrow}}}}\mathcal{O}} \\ \Rightarrow \begin{aligned} \frac{\mathrm{d}m}{\mathrm{d}t}&amp; =\alpha(V)(1-m)-\beta(V)m \\ &amp;=\frac{m_{\infty}(V)-m}{\tau_{m}(V)} \end{aligned} \\ \\ \begin{aligned}m_\infty(V)&amp;=\frac{\alpha(V)}{\alpha(V)+\beta(V)}.\\\tau_m(V)&amp;=\frac{1}{\alpha(V)+\beta(V)}\end{aligned} $$ $$ \text{If}\ V\text{ is constant:}m(t)=m_\infty(V)+(m_0-m_\infty(V))\mathrm{e}^{-t/\tau_m(V)} $$ ### Voltage clamp $$ \begin{aligned} \frac{I(t)}{A}&amp; =c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+i_{\mathrm{ion}} \\ \Rightarrow\quad c_{\mathrm{M}}\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}&amp; =-g_{\mathrm{Cl}}(V_{\mathrm{M}}-E_{\mathrm{Cl}})-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}})-g_{\mathrm{Na}}(V_{\mathrm{M}}-E_{\mathrm{Na}})+\frac{I(t)}{A} \end{aligned} $$ - The membrane potential is kept constant - The current from capacitors is excluded - Currents must come from leaky/voltage-gated ion channels $$ \begin{aligned}I_{\mathrm{cap}}&amp;=c\frac{dV}{dt}=0\\I_{\mathrm{fb}}&amp;=\quad i_{\mathrm{ion}}=g_{\mathrm{Na}}(V-E_{\mathrm{Na}})+g_{\mathrm{K}}(V-E_{\mathrm{K}})+g_{\mathrm{L}}(V-E_{\mathrm{L}})\end{aligned} $$ åªæµ‹é‡ä¸€ä¸ªç¦»å­é€šé“å°±å¯ä»¥å¾ˆå®¹æ˜“å¾—åˆ°ç”µå¯¼ ![image-20230824111620056](/BrainPy-course-notes/master_content/Notes.assets/image-20230824111620056.png) ### Leaky channel Hyperpolarization â†’ the sodium and potassium channels are closed $$ I_{\mathrm{fb}}=g_{\mathrm{Na}}(V-E_{\mathrm{Na}})+g_{\mathrm{K}}(V-E_{\mathrm{K}})+g_{\mathrm{L}}(V-E_{\mathrm{L}}) $$ $$ \Rightarrow I_{\mathrm{fb}}=g_L(V-E_L) $$ $$ g_\mathrm{L}=0.3\mathrm{mS/cm}^2,E_\mathrm{L}=-54.4\mathrm{mV} $$ #### Potassium and sodium channels Potassium channels: Use choline to eliminate the inward current of Na + Na + current: $I_{fb} - I_{K}$ ![image-20230824112328953](/BrainPy-course-notes/master_content/Notes.assets/image-20230824112328953.png) ![image-20230824112333144](/BrainPy-course-notes/master_content/Notes.assets/image-20230824112333144.png) è½¬åŒ–é€ŸçŽ‡å’Œç”µå¯¼çŽ‡ä¸¤ä¸ªå› ç´  Potassium channels - Resting state (gate closed) - Activated state (gate open) â†’ Activation gate: $g_{\mathrm{K}}=\bar{g}_{K}n^{x}$ Sodium channels - Resting state (gate closed) - Activated state (gate open) - Inactivated state (gate blocked) â†’ Activation gate + inactivation gate: $g_{\mathrm{Na}}=\bar{g}_\text{Na}m^3h$ ![image-20230824113116329](/BrainPy-course-notes/master_content/Notes.assets/image-20230824113116329.png) The gates of sodium channels Modeling of each ion gate: $$ \begin{aligned} &amp;\text{gk}&amp;&amp; =\bar{g}_{K}n^{x} \\ &amp;\text{gNa}&amp;&amp; =\bar{g}_{\mathrm{Na}}m^{3}h \\ &amp;\frac{\mathrm{d}n}{\mathrm{d}t}&amp;&amp; =\alpha_{n}(V)(1-n)-\beta_{n}(V)n \\ &amp;\frac{\mathrm{d}m}{\mathrm{d}t}&amp;&amp; =\alpha_{m}(V)(1-m)-\beta_{m}(V)m \\ &amp;\frac{\mathrm{d}h}{\mathrm{d}t}&amp;&amp; =\alpha_{h}(V)(1-h)-\beta_{h}(V)h \end{aligned} $$ $$ \begin{aligned} \frac{\mathrm{d}m}{\mathrm{d}t}&amp; =\alpha(V)(1-m)-\beta(V)m \\ &amp;=\frac{m_{\infty}(V)-m}{\tau_{m}(V)} \end{aligned} $$ $$ \begin{aligned}m_\infty(V)&amp;=\frac{\alpha(V)}{\alpha(V)+\beta(V)}\\\tau_m(V)&amp;=\frac{1}{\alpha(V)+\beta(V)}\end{aligned}. $$ $$ m(t)=m_\infty(V)+(m_0-m_\infty(V))\mathrm{e}^{-t/\tau_m(V)} $$ ### The Hodgkin-Huxley(HH) Model $$ c_\mathrm{M}\frac{\mathrm{d}V_\mathrm{M}}{\mathrm{d}t}=-g_\mathrm{Cl}(V_\mathrm{M}-E_\mathrm{Cl})-g_\mathrm{K}(V_\mathrm{M}-E_\mathrm{K})-g_\mathrm{Na}(V_\mathrm{M}-E_\mathrm{Na})+\frac{I(t)}{A} $$ æœ¬è´¨æ˜¯4ä¸ªå¾®åˆ†æ–¹ç¨‹è”ç«‹åœ¨ä¸€èµ· $$ \left\{\begin{aligned}&amp;c\frac{\mathrm{d}V}{\mathrm{d}t}=-\bar{g}_\text{Na}m^3h(V-E_\text{Na})-\bar{g}_\text{K}n^4(V-E_\text{K})-\bar{g}_\text{L}(V-E_\text{L})+I_\text{ext},\\&amp;\frac{\mathrm{d}n}{\mathrm{d}t}=\phi\left[\alpha_n(V)(1-n)-\beta_n(V)n\right]\\&amp;\frac{\mathrm{d}m}{\mathrm{d}t}=\phi\left[\alpha_m(V)(1-m)-\beta_m(V)m\right],\\&amp;\frac{\mathrm{d}h}{\mathrm{d}t}=\phi\left[\alpha_h(V)(1-h)-\beta_h(V)h\right],\end{aligned}\right. $$ $$ \begin{aligned}\alpha_n(V)&amp;=\frac{0.01(V+55)}{1-\exp\left(-\frac{V+55}{10}\right)},\quad\beta_n(V)&amp;=0.125\exp\left(-\frac{V+65}{80}\right),\\\alpha_h(V)&amp;=0.07\exp\left(-\frac{V+65}{20}\right),\quad\beta_n(V)&amp;=\frac{1}{\left(\exp\left(-\frac{V+55}{10}\right)+1\right)},\\\alpha_m(V)&amp;=\frac{0.1(V+40)}{1-\exp\left(-(V+40)/10\right)},\quad\beta_m(V)&amp;=4\exp\left(-(V+65)/18\right).\end{aligned} $$ $$ \phi=Q_{10}^{(T-T_{\mathrm{base}})/10} $$ æ¯ä¸€æ­¥ç¬¦åˆç”Ÿç‰©å­¦ ![image-20230824113714178](/BrainPy-course-notes/master_content/Notes.assets/image-20230824113714178.png) #### How to fit each gating variable? **Fitting n:** $g_{\mathbf{K}}=\bar{g}_{K}n^{x}\quad m(t)=m_{\infty}(V)+(m_{0}-\color{red}{\boxed{m_{\infty}(V)}})\mathrm{e}^{-t/\pi_{m}(V)}$ â†’ $g_\mathrm{K}(V,t)=\bar{g}_\mathrm{K}\left[n_\infty(V)-(n_\infty(V)-n_0(V))\mathrm{e}^{-\frac{t}{\tau_n(V)}}\right]^x$ by $g_{\mathrm{K}\infty}=\bar{g}_{\mathrm{K}}n_{\infty}^{x},g_{\mathrm{K}0}=\bar{g}_{\mathrm{K}}n_{0}^{x}$ â†’ $g_{\mathrm{K}}(V,t)=\left[g_{\mathrm{K}\infty}^{1/x}-(g_{\mathrm{K}\infty}^{1/x}-g_{\mathrm{K}0}^{1/x})\mathrm{e}^{-\frac{t}{\tau_{n}(V)}}\right]^{x}$ ![image-20230824114623467](/BrainPy-course-notes/master_content/Notes.assets/image-20230824114623467.png) # Hodgkin-Huxley brain dynamics programming ## Dynamics Programming Basics ### Integrators å¾®åˆ†å™¨ ![image-20230824140806650](/BrainPy-course-notes/master_content/Notes.assets/image-20230824140806650.png) **example** FitzHugh-Nagumo equation $$ \begin{aligned}\tau\dot{w}&amp;=v+a-bw,\\\dot{v}&amp;=v-\frac{ u^3}{3}-w+I_{\mathrm{ext}}.\end{aligned} $$ ```python @bp.odeint(method=&apos;Euler&apos;, dt=0.01) def integral(V, w, t, Iext, a, b, tau): dw = (V + a - b * w) / tau dV = V - V * V * V / 3 - w + Iext return dV, dw ``` **JointEq** In a dynamical system, there may be multiple variables that change dynamically over time. Sometimes these variables are interrelated, and updating one variable requires other variables as inputs. For better integration accuracy, we recommend that you use `brainpy.JointEq` to jointly solve interrelated differential equations. ```python a, b = 0.02, 0.20 dV = lambda V, t, w, Iext: 0.04 * V * V + 5 * V + 140 - w + Iext # ç¬¬ä¸€ä¸ªæ–¹ç¨‹ dw = lambda w, t, V: a * (b * V - w) # ç¬¬äºŒä¸ªæ–¹ç¨‹ joint_eq = bp.JointEq(dV, dw) # è”åˆå¾®åˆ†æ–¹ç¨‹ integral2 = bp.odeint(joint_eq, method=&apos;rk2&apos;) # å®šä¹‰è¯¥è”åˆå¾®åˆ†æ–¹ç¨‹çš„æ•°å€¼ç§¯åˆ†æ–¹æ³• ``` ```python # å£°æ˜Žç§¯åˆ†è¿è¡Œå™¨ runner = bp.integrators.IntegratorRunner( integral, monitors=[&apos;V&apos;] inits=dict(V=0., w=0.) args=dict(a=a, b=b, tau=tau, Iext=Iext), dt=0.01 ) # ä½¿ç”¨ç§¯åˆ†è¿è¡Œå™¨æ¥è¿›è¡Œæ¨¡æ‹Ÿ100msï¼Œç»“åˆæ­¥é•¿dt=0.01 runner.run(100.) plt.plot(runner.mon.ts, runner.mon.V) plt.show() ``` ![image-20230824142019832](/BrainPy-course-notes/master_content/Notes.assets/image-20230824142019832.png) ### `DynamicalSystem` BrainPy provides a generic `SynamicalSystem` class to define various types of dynamical models. BrainPy supports modelings in brain simulation and brain-inspired computing. All these supports are based on one common concept: **Dynamical System** via `brainpy.DynamicalSystem`. #### What is `DynamicalSystem` A `DynamicalSystem` defines the updating rule of the model at single time step. 1. For models with state, `DynamicalSystem` defines the state transition from $t$ to $t + dt$, i.e., $S(t+dt)=F(S(t),x,t,dt)$, where $S$ is the state, $x$ is input, $t$ is the time, and $dt$ is the time step. This is the case for recurrent neural networks (like GRU, LSTM), neuron models (like HH, LIF), or synapse models which are widely used in brain simulation. 2. However, for models in deep learning, like convolution and fully-connected linear layers, `DynamicalSystem` defines the input-to-output mapping, i.e., $y=F(x,t)$. ![img](https://brainpy.readthedocs.io/en/latest/_images/dynamical_system.png) #### How to define `DynamicalSystem` ```python class YourDynamicalSystem(bp.DynamicalSystem): def update(self, x): ... ``` Instead of input x, there are shared arguments across all nodes/layers in the network: - the current time `t`, or - the current running index `i`, or - the current time step `dt`, or - the current phase of training or testing `fit=True/False`. Here, it is necessary to explain the usage of `bp.share`. - `bp.share.save( )`: The function saves shared arguments in the global context. User can save shared arguments in tow ways, for example, if user want to set the current time `t=100`, the current time step `dt=0.1`,the user can use `bp.share.save(&quot;t&quot;,100,&quot;dt&quot;,0.1)` or `bp.share.save(t=100,dt=0.1)`. - `bp.share.load( )`: The function gets the shared data by the `key`, for example, `bp.share.load(&quot;t&quot;)`. - `bp.share.clear_shargs( )`: The function clears the specific shared arguments in the global context, for example, `bp.share.clear_shargs(&quot;t&quot;)`. - `bp.share.clear( )`: The function clears all shared arguments in the global context. #### How to run `DynamicalSystem` As we have stated above that `DynamicalSystem` only defines the updating rule at single time step, to run a `DynamicalSystem` instance over time, we need a for loop mechanism. ![img](https://brainpy.readthedocs.io/en/latest/_images/dynamical_system_and_dsrunner.png) ##### `brainpy.math.for_loop` `for_loop` is a structural control flow API which runs a function with the looping over the inputs. Moreover, this API just-in-time compile the looping process into the machine code. ```python inputs = bp.inputs.section_input([0., 6.0, 0.], [100., 200., 100.]) indices = np.arange(inputs.size) def run(i, x): neu.step_run(i, x) return neu.V.value vs = bm.for_loop(run, (indices, inputs), progress_bar=True) ``` ##### `brainpy.LoopOverTime` Different from `for_loop`, `brainpy.LoopOverTime` is used for constructing a dynamical system that automatically loops the model over time when receiving an input. `for_loop` runs the model over time. While `brainpy.LoopOverTime` creates a model which will run the model over time when calling it. ```python net2.reset_state(batch_size=10) looper = bp.LoopOverTime(net2) out = looper(currents) ``` ##### `brainpy.DSRunner` **Initializing a `DSRunner`** Generally, we can initialize a runner for dynamical systems with the format of: ``` runner = DSRunner(target=instance_of_dynamical_system, inputs=inputs_for_target_DynamicalSystem, monitors=interested_variables_to_monitor, dyn_vars=dynamical_changed_variables, jit=enable_jit_or_not, progress_bar=report_the_running_progress, numpy_mon_after_run=transform_into_numpy_ndarray ) ``` - `target` specifies the model to be simulated. It must an instance of brainpy.DynamicalSystem. - `inputs` is used to define the input operations for specific variables. - It should be the format of `[(target, value, [type, operation])]`, where `target` is the input target, `value` is the input value, `type` is the input type (such as â€œfixâ€, â€œiterâ€, â€œfuncâ€), `operation` is the operation for inputs (such as â€œ+â€, â€œ-â€, â€œ*â€, â€œ/â€, â€œ=â€). Also, if you want to specify multiple inputs, just give multiple `(target, value, [type, operation])`, such as `[(target1, value1), (target2, value2)]`. - It can also be a function, which is used to manually specify the inputs for the target variables. This input function should receive one argument `tdi` which contains the shared arguments like time `t`, time step `dt`, and index `i`. - `monitors` is used to define target variables in the model. During the simulation, the history values of the monitored variables will be recorded. It can also to monitor variables by callable functions and it should be a `dict`. The `key` should be a string for later retrieval by `runner.mon[key]`. The `value` should be a callable function which receives an argument: `tdt`. - `dyn_vars` is used to specify all the dynamically changed [variables](https://brainpy.readthedocs.io/en/latest/tutorial_math/variables.html) used in the `target` model. - `jit` determines whether to use JIT compilation during the simulation. - `progress_bar` determines whether to use progress bar to report the running progress or not. - `numpy_mon_after_run` determines whether to transform the JAX arrays into numpy ndarray or not when the network finishes running. **Running a `DSRunner`** After initialization of the runner, users can call `.run()` function to run the simulation. The format of function `.run()` is showed as follows: ```python runner.run(duration=simulation_time_length, inputs=input_data, reset_state=whether_reset_the_model_states, shared_args=shared_arguments_across_different_layers, progress_bar=report_the_running_progress, eval_time=evaluate_the_running_time ) ``` - `duration` is the simulation time length. - `inputs` is the input data. If `inputs_are_batching=True`, `inputs` must be a PyTree of data with two dimensions: `(num_sample, num_time, ...)`. Otherwise, the `inputs` should be a PyTree of data with one dimension: `(num_time, ...)`. - `reset_state` determines whether to reset the model states. - `shared_args` is shared arguments across different layers. All the layers can access the elements in `shared_args`. - `progress_bar` determines whether to use progress bar to report the running progress or not. - `eval_time` determines whether to evaluate the running time. ### Monitors ```python # initialize monitor through a list of strings runner1 = bp.DSRunner(target=net, monitors=[&apos;E.spike&apos;, &apos;E.V&apos;, &apos;I.spike&apos;, &apos;I.V&apos;], # 4 elements in monitors inputs=[(&apos;E.input&apos;, 20.), (&apos;I.input&apos;, 20.)], jit=True) ``` Once we call the runner with a given time duration, the monitor will automatically record the variable evolutions in the corresponding models. Afterwards, users can access these variable trajectories by using .mon.[variable_name]. The default history times .mon.ts will also be generated after the model finishes its running. Letâ€™s see an example. ```python runner1.run(100.) bp.visualize.raster_plot(runner1.mon.ts, runner1.mon[&apos;E.spike&apos;], show=True) ``` **Initialization with index specification** ```python monitors=[(&apos;E.spike&apos;, [1, 2, 3]), # monitor values of Variable at index of [1, 2, 3] &apos;E.V&apos;], # monitor all values of Variable &apos;V&apos; ``` &amp;gt; The monitor shape of &quot;E.V&quot; is (run length, variable size) = (1000, 3200) &amp;gt; The monitor shape of &quot;E.spike&quot; is (run length, index size) = (1000, 3) **Explicit monitor target** ```python monitors={&apos;spike&apos;: net.E.spike, &apos;V&apos;: net.E.V}, ``` &amp;gt; The monitor shape of &quot;V&quot; is = (1000, 3200) &amp;gt; The monitor shape of &quot;spike&quot; is = (1000, 3200) **Explicit monitor target with index specification** ```python monitors={&apos;E.spike&apos;: (net.E.spike, [1, 2]), # monitor values of Variable at index of [1, 2] &apos;E.V&apos;: net.E.V}, # monitor all values of Variable &apos;V&apos; ``` &amp;gt; The monitor shape of &quot;E.V&quot; is = (1000, 3200) &amp;gt; The monitor shape of &quot;E.spike&quot; is = (1000, 2) ### Inputs In brain dynamics simulation, various inputs are usually given to different units of the dynamical system. In BrainPy, `inputs` can be specified to runners for dynamical systems. The aim of `inputs` is to mimic the input operations in experiments like Transcranial Magnetic Stimulation (TMS) and patch clamp recording. `inputs` should have the format like `(target, value, [type, operation])`, where - `target` is the target variable to inject the input. - `value` is the input value. It can be a scalar, a tensor, or a iterable object/function. - `type` is the type of the input value. It support two types of input: `fix` and `iter`. The first one means that the data is static; the second one denotes the data can be iterable, no matter whether the input value is a tensor or a function. The `iter` type must be explicitly stated. - `operation` is the input operation on the target variable. It should be set as one of `{ + , - , * , / , = }`, and if users do not provide this item explicitly, it will be set to â€˜+â€™ by default, which means that the target variable will be updated as `val = val + input`. #### Static inputs ```python runner6 = bp.DSRunner(target=net, monitors=[&apos;E.spike&apos;], inputs=[(&apos;E.input&apos;, 20.), (&apos;I.input&apos;, 20.)], # static inputs jit=True) runner6.run(100.) bp.visualize.raster_plot(runner6.mon.ts, runner6.mon[&apos;E.spike&apos;]) ``` #### Iterable inputs ```python I, length = bp.inputs.section_input(values=[0, 20., 0], durations=[100, 1000, 100], return_length=True, dt=0.1) runner7 = bp.DSRunner(target=net, monitors=[&apos;E.spike&apos;], inputs=[(&apos;E.input&apos;, I, &apos;iter&apos;), (&apos;I.input&apos;, I, &apos;iter&apos;)], # iterable inputs jit=True) runner7.run(length) bp.visualize.raster_plot(runner7.mon.ts, runner7.mon[&apos;E.spike&apos;]) ``` ## Run a built-in HH model [Using Built-in Models â€” BrainPy documentation](https://brainpy.readthedocs.io/en/latest/tutorial_building/overview_of_dynamic_model.html) ```python import brainpy as bp import brainpy.math as bm current, length = bp.inputs.section_input(values=[0., bm.asarray([1., 2., 4., 8., 10., 15.]), 0.], durations=[10, 2, 25], return_length=True) hh_neurons = bp.neurons.HH(current.shape[1]) runner = bp.DSRunner(hh_neurons, monitors=[&apos;V&apos;, &apos;m&apos;, &apos;h&apos;, &apos;n&apos;], inputs=(&apos;input&apos;, current, &apos;iter&apos;)) runner.run(length) ``` ## Run a HH model from scratch The mathematic expression of the HH model $$ \left\{\begin{aligned}&amp;c\frac{\mathrm{d}V}{\mathrm{d}t}=-\bar{g}_\text{Na}m^3h(V-E_\text{Na})-\bar{g}_\text{K}n^4(V-E_\text{K})-\bar{g}_\text{L}(V-E_\text{L})+I_\text{ext},\\&amp;\frac{\mathrm{d}n}{\mathrm{d}t}=\phi\left[\alpha_n(V)(1-n)-\beta_n(V)n\right]\\&amp;\frac{\mathrm{d}m}{\mathrm{d}t}=\phi\left[\alpha_m(V)(1-m)-\beta_m(V)m\right],\\&amp;\frac{\mathrm{d}h}{\mathrm{d}t}=\phi\left[\alpha_h(V)(1-h)-\beta_h(V)h\right],\end{aligned}\right. $$ $$ \begin{aligned}\alpha_n(V)&amp;=\frac{0.01(V+55)}{1-\exp\left(-\frac{V+55}{10}\right)},\quad\beta_n(V)&amp;=0.125\exp\left(-\frac{V+65}{80}\right),\\\alpha_h(V)&amp;=0.07\exp\left(-\frac{V+65}{20}\right),\quad\beta_n(V)&amp;=\frac{1}{\left(\exp\left(-\frac{V+55}{10}\right)+1\right)},\\\alpha_m(V)&amp;=\frac{0.1(V+40)}{1-\exp\left(-(V+40)/10\right)},\quad\beta_m(V)&amp;=4\exp\left(-(V+65)/18\right).\end{aligned} $$ $$ \phi=Q_{10}^{(T-T_{\mathrm{base}})/10} $$ V: the membrane potential n: activation variable of the Kt channel m: activation variable of the Nat channel h; inactivation variable of the Nat channe ### Define HH model `class` - Inherit `bp.dyn.NeuDyn` ```python import brainpy as bp import brainpy.math as bm class HH(bp.dyn.NeuDyn): def __init__(self, size, ENa=50., gNa=120., Ek=-77., gK=36., EL=-54.387, gL=0.03, V_th=0., C=1.0, T=6.3): super(HH, self).__init__(size=size) ``` ### Initialization ```python import brainpy as bp import brainpy.math as bm class HH(bp.dyn.NeuDyn): def __init__(self, size, ENa=50., gNa=120., Ek=-77., gK=36., EL=-54.387, gL=0.03, V_th=0., C=1.0, T=6.3): super(HH, self).__init__(size=size) # parameters self.ENa = ENa self.EK = EK self.EL = EL self.gNA = gNa self.gK = gK self.gL = gL self.C = C self.V_th = V_th self.T_base = 6.3 self.phi = 3.0 ** ((T - self.T_base) / 10.0) # variable self.V = bm.Variable(-70.68 * bm.ones(self.num)) self.m = bm.Variable(0.0266 * bm.ones(self.num)) self.h = bm.Variable(0.772 * bm.ones(self.num)) self.n = bm.Variable(0.235 * bm.ones(self.num)) self.input = bm.Variable(bm.zeros(self.num)) self.spike = bm.Variable(bm.zeros(self.num, dtype=bool)) self.t_last_spike = bm.Variable(bm.ones(self.num) * -1e7) # å®šä¹‰ç§¯åˆ†å‡½æ•° self.integral = bp.odeint(f=self.derivative, method=&apos;exp_auto&apos;) ``` ### Define the derivative function ```python @property def derivative(self): return bp.JointEq(self.dV, self.dm, self.dh, self.dn) def dV(self, V, t, m, h, n, Iext): I_Na = (self.gNa * m ** 3.0 * h) * (V - self.ENa) I_K = (self.gK * n ** 4.0) * (V - self.EK) I_leak = self.gL * (V - self.EL) dVdt = (- I_Na - I_K - I_leak + Iext) / self.C return dVdt def dm(self, m, t, V): alpha = 0.1 * (V + 40) / (1 - bm.exp(-(V + 40) / 10)) beta = 4.0 * bm.exp(-(V + 65) / 18) dmdt = alpha * (1 - m) - beta * m return self.phi * dmdt def dh(self, h, t, V): alpha = 0.07 * bm.exp(-(V + 65) / 20.) beta = 1 / (1 + bm.exp(-(V + 35) / 10)) dhdt = alpha * (1 - h) - beta * h return self.phi * dhdt def dn(self, n, t, V): alpha = 0.01 * (V + 55) / (1 - bm.exp(-(V + 55) / 10)) beta = 0.125 * bm.exp(-(V + 65) / 80) dndt = alpha * (1 - n) - beta * n return self.phi * dndt ``` ### Complete the `update()` function ```python def update(self, x=None): t = bp.share.load(&apos;t&apos;) dt = bp.share.load(&apos;dt&apos;) # TODO: æ›´æ–°å˜é‡V, m, h, n, æš‚å­˜åœ¨V, m, h, nä¸­ V, m, h, n = self.integral(self.V, self.m, self.h, self.n, t, self.input, dt=dt) #åˆ¤æ–­æ˜¯å¦å‘ç”ŸåŠ¨ä½œç”µä½ self.spike.value = bm.logical_and(self.V = self.V_th) # æ›´æ–°æœ€åŽä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´ self.t_last_spike.value = bm.where(self.spike, t, self.t_last_spike) # TODO: æ›´æ–°å˜é‡V, m, h, nçš„å€¼ self.V.value = V self.m.value = m self.h.value = h self.n.value = n #é‡ç½®è¾“å…¥ self.input[:] = 0 ``` ### Simulation ```python current, length = bp.inputs.section_input(values=[0., bm.asarray([1., 2., 4., 8., 10., 15.]), 0.], durations=[10, 2, 25], return_length=True) hh_neurons = HH(current.shape[1]) runner = bp.DSRunner(hh_neurons, monitors=[&apos;V&apos;, &apos;m&apos;, &apos;h&apos;, &apos;n&apos;], inputs=(&apos;input&apos;, current, &apos;iter&apos;)) runner.run(length) ``` ### Visualization ```python import numpy as np import matplotlib.pyplot as plt bp.visualize.line_plot(runner.mon.ts, runner.mon.V, ylabel=&apos;V (mV)&apos;, plot_ids=np.arange(current.shape[1])) plt.plot(runner.mon.ts, bm.where(current[:, -1]&amp;gt;0, 10, 0) - 90.) plt.figure() plt.plot(runner.mon.ts, runner.mon.m[:, -1]) plt.plot(runner.mon.ts, runner.mon.h[:, -1]) plt.plot(runner.mon.ts, runner.mon.n[:, -1]) plt.legend([&apos;m&apos;, &apos;h&apos;, &apos;n&apos;]) plt.xlabel(&apos;Time (ms)&apos;) ``` ## Customize a conductance-based model ç”µè·¯æ¨¡æ‹Ÿï¼Œå†™æˆç”µå¯¼å½¢å¼ ![image-20230824180831033](/BrainPy-course-notes/master_content/Notes.assets/image-20230824180831033.png) $$ \begin{aligned} \text{gK}&amp; =\bar{g}_\text{K}n^4, \\ \frac{\mathrm{d}n}{\mathrm{d}t}&amp; =\phi[\alpha_n(V)(1-n)-\beta_n(V)n], \end{aligned} $$ åŠ¨åŠ›å­¦å½¢å¼æè¿°ï¼Œå¼•å…¥é—¨æ¡†å˜é‡$n$ $$ \begin{aligned} &amp;\alpha_{n}(V) =\frac{0.01(V+55)}{1-\exp(-\frac{V+55}{10})}, \\ &amp;\beta_{n}(V) =0.125\exp\left(-\frac{V+65}{80}\right). \end{aligned} $$ ç”±æ­¤å¼æ¥å»ºæ¨¡é’¾ç¦»å­é€šé“ ### Programming an ion channel #### Three ion channel ```python import brainpy as bp import brainpy.math as bm class IK(bp.dyn.IonChannel): def __init__(self, size, E=-77., g_max=36., phi=1., method=&apos;exp_auto&apos;): super(IK, self).__init__(size) self.g_max = g_max self.E = E self.phi = phi self.n = bm.Variable(bm.zeros(size)) # variables should be packed with bm.Variable self.integral = bp.odeint(self.dn, method=method) def dn(self, n, t, V): alpha_n = 0.01 * (V + 55) / (1 - bm.exp(-(V + 55) / 10)) beta_n = 0.125 * bm.exp(-(V + 65) / 80) return self.phi * (alpha_n * (1. - n) - beta_n * n) def update(self, V): t = bp.share.load(&apos;t&apos;) dt = bp.share.load(&apos;dt&apos;) self.n.value = self.integral(self.n, t, V, dt=dt) def current(self, V): return self.g_max * self.n ** 4 * (self.E - V) ``` ```python class INa(bp.dyn.IonChannel): def __init__(self, size, E= 50., g_max=120., phi=1., method=&apos;exp_auto&apos;): super(INa, self).__init__(size) self.g_max = g_max self.E = E self.phi = phi self.m = bm.Variable(bm.zeros(size)) # variables should be packed with bm.Variable self.h = bm.Variable(bm.zeros(size)) self.integral_m = bp.odeint(self.dm, method=method) self.integral_h = bp.odeint(self.dh, method=method) def dm(self, m, t, V): # TODO: è®¡ç®—dm/dt alpha_m = 0.11 * (V + 40) / (1 - bm.exp(-(V + 40) / 10)) beta_m = 4 * bm.exp(-(V + 65) / 18) return self.phi * (alpha_m * (1. - m) - beta_m * m) def dh(self, h, t, V): # TODO: è®¡ç®—dh/dt alpha_h = 0.07 * bm.exp(-(V + 65) / 20) beta_h = 1. / (1 + bm.exp(-(V + 35) / 10)) return self.phi * (alpha_h * (1. - h) - beta_h * h) def update(self, V): t = bp.share.load(&apos;t&apos;) dt = bp.share.load(&apos;dt&apos;) # TODO: æ›´æ–°self.m, self.h self.m.value = self.integral_m(self.m, t, V, dt=dt) self.h.value = self.integral_h(self.h, t, V, dt=dt) def current(self, V): return self.g_max * self.m ** 3 * self.h * (self.E - V) ``` ```python class IL(bp.dyn.IonChannel): def __init__(self, size, E=-54.39, g_max=0.03): super(IL, self).__init__(size) self.g_max = g_max self.E = E def current(self, V): return self.g_max * (self.E - V) def update(self, V): pass ``` #### Build a HH model with ion channels **Using customized ion channels** ```python class HH(bp.dyn.CondNeuGroup): def __init__(self, size): super(HH, self).__init__(size, V_initializer=bp.init.Uniform(-80, -60.)) # TODO: åˆå§‹åŒ–ä¸‰ä¸ªç¦»å­é€šé“ self.IK = IK(size, E=-77., g_max=36.) self.INa = INa(size, E=50., g_max=120.) self.IL = IL(size, E=-54.39, g_max=0.03) ``` **Using built-in ion channels** ```python class HH(bp.dyn.CondNeuGroup): def __init__(self, size): super().__init__(size) self.INa = bp.channels.INa_HH1952(size) self.IK = bp.channels.IK_HH1952(size) self.IL = bp.cahnnels.IL(size, E=-54.387, g_max=0.03) ``` #### Simulation ```python neu = HH(1) runner = bp.DSRunner( neu, monitors=[&apos;V&apos;, &apos;IK.n&apos;, &apos;INa.m&apos;, &apos;INa.h&apos;], inputs=(&apos;input&apos;, 1.698) # near the threshold current ) runner.run(200) # the running time is 200 ms import matplotlib.pyplot as plt plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;V&apos;]) plt.xlabel(&apos;t (ms)&apos;) plt.ylabel(&apos;V (mV)&apos;) plt.savefig(&quot;HH.jpg&quot;) plt.show() plt.figure(figsize=(6, 2)) plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;IK.n&apos;], label=&apos;n&apos;) plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;INa.m&apos;], label=&apos;m&apos;) plt.plot(runner.mon[&apos;ts&apos;], runner.mon[&apos;INa.h&apos;], label=&apos;h&apos;) plt.xlabel(&apos;t (ms)&apos;) plt.legend() plt.savefig(&quot;HH_channels.jpg&quot;) plt.show() ``` ![image-20230824184016011](/BrainPy-course-notes/master_content/Notes.assets/image-20230824184016011.png) # Simple Neuron Modeling: Simplified Models ## The Leaky Integrate-and-Fire(LIF) Neuron Model ### The LIF neuron model $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-(V-V_{\mathrm{rest}})+RI(t)\\\\\mathrm{if}V&amp;&amp;gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\text{last}\ {t_{ref}}\end{aligned} $$ åªæœ‰ä¸€ä¸ªå¾®åˆ†æ–¹ç¨‹ï¼Œè¦åŠ ä¸€ä¸ªä¸åº”æœŸ(**t refractory period**)ï¼Œè†œç”µä½ä¸å‘ç”Ÿä»»ä½•æ”¹å˜ï¼Œè®¤ä¸ºç¦»å­é€šé“åªæœ‰æ³„éœ²é€šé“ ![image-20230825101057570](/BrainPy-course-notes/master_content/Notes.assets/image-20230825101057570.png) Given a constant current input: ![image-20230825101410745](/BrainPy-course-notes/master_content/Notes.assets/image-20230825101410745.png) æ²¡æœ‰å»ºæ¨¡å‡†ç¡®å˜åŒ–ï¼Œåªæä¾›ä»€ä¹ˆæ—¶å€™è†œç”µä½çš„å˜åŒ– ### The dynamic features of the LIF model **General solution (constant input):**$V(t)=V_{\text{reset}}+RI_{\text{c}}(1-\mathrm{e}^{-\frac{t-t_0}{\tau}})$ **Firing frequency:** $$ \begin{aligned}T&amp;=-\tau\ln\left(1-\frac{V_{\phi h}-V_{\mathrm{rest}}}{RI_{\varsigma}}\right)\\f&amp;=\frac{1}{T+t_{\mathrm{ref}}}=\frac{1}{t_{\mathrm{ref}}-\tau\ln\left(1-\frac{V_{0}-V_{\mathrm{rest}}}{RI_{\varsigma}}\right)}\end{aligned} $$ **Rheobase current (minimal current):** $$ I_{\theta}=\frac{V_{\mathrm{th}}-V_{\mathrm{reset}}}{R} $$ åŸºå¼ºç”µæµï¼Œå¦‚æžœå°äºŽå®ƒå°†æ— æ³•å‘æ”¾ ### Strengths &amp; weaknesses of the LIF model #### Strengths - Simple, high simulation efficiency - Intuitive - Fits well the subthreshold membrane potential #### Weaknesses - The shape of action potentials is over-simplified - Has no memory of the spiking history - Cannot reproduce diverse firing patterns ### Other Univariate neuron models #### The Quadratic Integrate-and-Fire (QOF) model: $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=a_{0}(V-V_{\mathrm{re}t})(V-V_{\mathrm{c}})+RI(t)\\&amp;\text{if }V&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{re}set}\quad\text{last}\quad t_{\mathrm{ref}}\end{aligned} $$ ![image-20230825103243039](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103243039.png) è†œç”µä½ä»éœ€è¦æ‰‹åŠ¨é‡ç½® #### The Theta neuron model $$ \frac{\mathrm{d}\theta}{\mathrm{d}t}=1-\cos\theta+(1+\cos\theta)(\beta+I(t)) $$ ![image-20230825103331170](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103331170.png) éšå¼è¡¨è¾¾ï¼Œä¸å…·æœ‰ç‰©ç†æ„ä¹‰ï¼Œä½†ä¹Ÿä¼šè¿›è¡Œæ•´åˆå‘æ”¾ #### The Exponential Integrate-and-Fire (ExpIF) model $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-\left(V-V_{\mathrm{res}t}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{3T}}+RI(t)\\\mathrm{if~}V&amp;&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{res}t}\mathrm{last}t_{\mathrm{ref}}\end{aligned} $$ ![image-20230825103501912](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103501912.png) ä»éœ€è¦æ‰‹åŠ¨é‡ç½®è†œç”µä½ ## The Adaptive Exponential Integrate-and-Fire(AdEx) Neuron Model ### The AdEx neuron model Two variables: - ð‘‰: membrane potential - ð‘¤: adaptation variable $$ \begin{aligned} \tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{S_{T}}}-Rw+RI(t) \\ \tau_{w}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{\mathrm{w}}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ \mathrm{if}V&amp; &amp;gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} \end{aligned} $$ ä¸ä¸ºé›¶ï¼Œå°±ä¼šè¡°å‡åˆ°$-w$ ![image-20230825103840880](/BrainPy-course-notes/master_content/Notes.assets/image-20230825103840880.png) - A larger ð‘¤ suppresses ð‘‰ from increasing - ð‘¤ decays exponentially while having a sudden increase when the neuron fires **Firing patterns of the AdEx model** ![image-20230825104254936](/BrainPy-course-notes/master_content/Notes.assets/image-20230825104254936.png) **Categorization of firing patterns** According to the steady-state firing time intervals: - Tonic/regular spiking - Adapting - Bursting - Irregular spiking According to the initial-state features: - Tonic/classic spiking - Initial burst - Delayed spiking ### Other multivariate neuron models #### The Izhikevich model $$ \begin{aligned} &amp;\frac{dV}{dt} =0.04V^{2}+5V+140-u+I \\ &amp;\frac{\mathrm{d}u}{\mathrm{d}t} =a\left(bV-u\right) \\ &amp;\operatorname{if}V &amp;gt;\theta,\quad V\leftarrow c,u\leftarrow u+d\text{ last }t_{\mathrm{ref}} \end{aligned} $$ äºŒæ¬¡æ•´åˆå‘æ”¾å¤šåŠ äº†ä¸€ä¸ª$u$ ![image-20230825104832770](/BrainPy-course-notes/master_content/Notes.assets/image-20230825104832770.png) #### The FitzHughâ€“Nagumo (FHN) model $$ \begin{aligned}\dot{v}&amp;=v-\frac{v^3}3-w+RI_{\mathrm{ext}}\\\tau\dot{w}&amp;=v+a-bw.\end{aligned} $$ æ²¡æœ‰å¯¹è†œç”µä½è¿›è¡Œäººä¸ºçš„é‡ç½®ï¼Œå¯ä»¥æ›´å¥½çš„è¿›è¡ŒåŠ¨åŠ›å­¦åˆ†æžï¼Œæ²¡æœ‰æ‰“ç ´å¾®åˆ†æ–¹ç¨‹çš„è¿žç»­æ€§ ![image-20230825104922636](/BrainPy-course-notes/master_content/Notes.assets/image-20230825104922636.png) #### The Generalized Integrate-and-Fire (GIF) model n+2ä¸ªå˜é‡ $$ \begin{aligned} &amp;\tau{\frac{\mathrm{d}V}{\mathrm{d}t}} =-\left(V-V_{\mathrm{rest}}\right)+R\sum_{j}I_{j}+RI \\ &amp;\frac{\mathrm{d}\Theta}{\mathrm{d}t} =a\left(V-V_{\mathrm{rest}}\right)-b\left(\Theta-\Theta_{\infty}\right) \\ &amp;\frac{\mathrm{d}l_{j}}{\mathrm{d}t} =-k_{j}I_{j},\quad j=1,2,...,n \\ &amp;\operatorname{if}V &amp;gt;\Theta,\quad I_{j}\leftarrow R_{j}I_{j}+A_{j},V\leftarrow V_{\mathrm{reset}},\Theta\leftarrow max(\Theta_{\mathrm{reset}},\Theta) \end{aligned} $$ æ¯ä¸ªå˜é‡éƒ½æ˜¯çº¿æ€§çš„ï¼Œæ³›åŒ–æ€§ä½“çŽ°åœ¨é‡ç½®æ¡ä»¶ä¸Š ![image-20230825105035349](/BrainPy-course-notes/master_content/Notes.assets/image-20230825105035349.png) ## Dynamic analysis: phase-plane analysis ### Phase plane analysis å¯¹åŠ¨åŠ›å­¦ç³»ç»Ÿçš„è¡Œä¸ºæ¥åˆ†æžï¼Œæ™®éå¯¹ä¸¤ä¸ªå˜é‡æ¥è¿›è¡Œåˆ†æž Analyzes the behavior of a dynamical system with (usually two) variables described by ordinary differential equations $$ \begin{aligned} &amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp;&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{S_{T}}}-Rw+RI(t) \\ &amp;\tau_{W}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp;&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ &amp;\mathrm{if}V&amp;&amp; &amp;gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} \end{aligned} $$ **Elements:** - Nullclines: $\mathrm{d}V/\mathrm{d}t=0;\mathrm{d}w/\mathrm{d}t=0$ - Fixed points: $\mathrm{d}V/\mathrm{d}t=0\mathrm{~and~}\mathrm{d}w/\mathrm{d}t=0$ - The vector field - The trajectory of variables å‡è®¾å¤–éƒ¨ç”µæµæ’å®š ![image-20230825110708994](/BrainPy-course-notes/master_content/Notes.assets/image-20230825110708994.png) ### Phase plane analysis for the AdEx neuron model $$ \begin{aligned} &amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp;&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{\Lambda_{T}}}-Rw+RI(t) \\ &amp;\tau_{w}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp;&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ &amp;\text{ifV}&amp;&amp; &amp;gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} \end{aligned} $$ ![image-20230825110811399](/BrainPy-course-notes/master_content/Notes.assets/image-20230825110811399.png) #### Tonic ![image-20230825112857175](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112857175.png) #### Adaptation ![image-20230825112918815](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112918815.png) #### Bursting ![image-20230825112933938](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112933938.png) #### Transient spiking ![image-20230825112950297](/BrainPy-course-notes/master_content/Notes.assets/image-20230825112950297.png) ## Dynamic analysis: bifurcation analysis ### Bifurcation analysis Quantitative analysis of the existence and the properties of fixed points in a dynamical system with a changing parameter æŸä¸ªå¤–ç•Œæ¡ä»¶å˜åŒ–æ—¶ï¼Œå›ºå®šç‚¹çš„å˜åŒ– Elements: - Lines of fixed points - Stability properties of fixed points ![image-20230825114510710](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114510710.png) ### Bifurcation analysis for the AdEx Neuron model bifurcation analysis for 2 variables Variables: ð‘‰ and ð‘¤ Parameters: $I_{ext}$ $$ \begin{aligned} &amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}=-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{{\frac{V-V_{T}}{ST}}}-Rw+RI(t) \\ &amp;\text{-} {\frac{\mathrm{d}w}{\mathrm{d}t}}=a(V-V_{\mathrm{rest}})-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right) \\ &amp;\mathrm{if}V&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{reset}}\ \mathrm{last}\ t_{\mathrm{ref}} \end{aligned} $$ ![image-20230825114801456](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114801456.png) ![image-20230825114742740](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114742740.png) **Subjects: two variables (ð‘‰ and ð‘¤)** ![image-20230825114856403](/BrainPy-course-notes/master_content/Notes.assets/image-20230825114856403.png) ### Extended: The limit cycle The FitzHughâ€“Nagumo (FHN) model $$ \begin{aligned}\dot{v}&amp;=v-\frac{v^3}3-w+RI_\mathrm{ext}\\\tau\dot{w}&amp;=v+a-bw.\end{aligned} $$ This dynamical system, in certain conditions, exhibits a cyclic pattern of variable changes which can be visualized as a closed trajectory in the phase plane. å˜åŒ–é”å®šåˆ°çŽ¯ä¸­ ![image-20230825115348008](/BrainPy-course-notes/master_content/Notes.assets/image-20230825115348008.png) ![image-20230825115354146](/BrainPy-course-notes/master_content/Notes.assets/image-20230825115354146.png) # Reduced Models - brain dynamics programming ## LIF neuron models programming ### Define LIF `class` $$ \begin{aligned}&amp;\tau\frac{\mathrm{d}V}{\mathrm{d}t}=-(V-V_{\mathrm{rest}})+RI(t)\\&amp;\text{if }V&amp;gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\text{last}t_{\mathrm{ref}}\end{aligned} $$ ```python class LIF(bp.dyn.NeuDyn): def __init__(self, size, V_rest=0, V_reset=-5, V_th=20, R=1, tau=10, t_ref=5., **kwargs): # åˆå§‹åŒ–çˆ¶ç±» super(LIF, self).__init__(size=size, **kwargs) ``` ### Initialization ```python class LIF(bp.dyn.NeuDyn): def __init__(self, size, V_rest=0, V_reset=-5, V_th=20, R=1, tau=10, t_ref=5., **kwargs): # åˆå§‹åŒ–çˆ¶ç±» super(LIF, self).__init__(size=size, **kwargs) # åˆå§‹åŒ–å‚æ•° self.V_rest = V_rest self.V_reset = V_reset self.V_th = V_th self.R = R self.tau = tau self.t_ref = t_ref # ä¸åº”æœŸæ—¶é•¿ # åˆå§‹åŒ–å˜é‡ self.V = bm.Variable(bm.random.randn(self.num) + V_reset) self.input = bm.Variable(bm.zeros(self.num)) self.t_last_spike = bm.Variable(bm.ones(self.num) * -1e7) # ä¸Šä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´ self.refractory = bm.Variable(bm.zeros(self.num, dtype=bool)) # æ˜¯å¦å¤„äºŽä¸åº”æœŸ self.spike = bm.Variable(bm.zeros(self.num, dtype=bool)) # è„‰å†²å‘æ”¾çŠ¶æ€ # ä½¿ç”¨æŒ‡æ•°æ¬§æ‹‰æ–¹æ³•è¿›è¡Œç§¯åˆ† self.integral = bp.odeint(f=self.derivative, method=&apos;exponential_euler&apos;) ``` ### Define the derivative function ```python # å®šä¹‰è†œç”µä½å…³äºŽæ—¶é—´å˜åŒ–çš„å¾®åˆ†æ–¹ç¨‹ def derivative(self, V, t, Iext): dVdt = (-V + self.V_rest + self.R * Iext) / self.tau return dVdt ``` ### Complete the `update()` function ```python def update(self): t, dt = bp.share[&apos;t&apos;], bp.share[&apos;dt&apos;] # ä»¥æ•°ç»„çš„æ–¹å¼å¯¹ç¥žç»å…ƒè¿›è¡Œæ›´æ–° refractory = (t - self.t_last_spike) self.V_th # å°†å¤§äºŽé˜ˆå€¼çš„ç¥žç»å…ƒæ ‡è®°ä¸ºå‘æ”¾äº†è„‰å†² self.spike[:] = spike # æ›´æ–°ç¥žç»å…ƒè„‰å†²å‘æ”¾çŠ¶æ€ self.t_last_spike[:] = bm.where(spike, t, self.t_last_spike) # æ›´æ–°æœ€åŽä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´ self.V[:] = bm.where(spike, self.V_reset, V) # å°†å‘æ”¾äº†è„‰å†²çš„ç¥žç»å…ƒè†œç”µä½ç½®ä¸ºV_resetï¼Œå…¶ä½™ä¸å˜ self.refractory[:] = bm.logical_or(refractory, spike) # æ›´æ–°ç¥žç»å…ƒæ˜¯å¦å¤„äºŽä¸åº”æœŸ self.input[:] = 0. # é‡ç½®å¤–ç•Œè¾“å…¥ ``` ### Simulation ```python def run_LIF(): # è¿è¡ŒLIFæ¨¡åž‹ group = LIF(1) runner = bp.DSRunner(group, monitors=[&apos;V&apos;], inputs=(&apos;input&apos;, 22.)) runner(200) # è¿è¡Œæ—¶é•¿ä¸º200ms # ç»“æžœå¯è§†åŒ– fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) plt.plot(runner.mon.ts, runner.mon.V) plt.xlabel(r&apos;$t$ (ms)&apos;) plt.ylabel(r&apos;$V$ (mV)&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) plt.show() ``` ![image-20230825141201825](/BrainPy-course-notes/master_content/Notes.assets/image-20230825141201825.png) ### Input current &amp; firing frequency $$ \begin{gathered} V(t)=V_{\mathrm{reset}}+RI_{\mathrm{c}}(1-\mathrm{e}^{-\frac{t-t_{0}}{\tau}}). \\ T=-\tau\ln\left[1-\frac{V_{\mathrm{th}}-V_{\mathrm{rest}}}{RI_{\mathrm{c}}}\right] \\ f={\frac{1}{T+t_{\mathrm{ref}}}}={\frac{1}{t_{\mathrm{ref}}-\tau\ln\left[1-{\frac{V_{\mathrm{th}}-V_{\mathrm{rest}}}{RI_{c}}}\right]}} \end{gathered} $$ ```python # è¾“å…¥ä¸Žé¢‘çŽ‡çš„å…³ç³» current = bm.arange(0, 600, 2) duration = 1000 LIF_neuron = LIF(current.shape[0]) runner_2 = bp.dyn.DSRunner(LIF_neurons, monitors=[&apos;spike&apos;], inputs={&apos;input&apos;, current}, dt=0.01) runner_2.run(duration) freqs = runner_2.mon.spike.sum(axis=0) / (duration/1000) plt.figure() plt.plot(current, freqs) plt.xlabel(&apos;inputs&apos;) plt.ylabel(&apos;frequencies&apos;) ``` ![image-20230825143405952](/BrainPy-course-notes/master_content/Notes.assets/image-20230825143405952.png) ### Other Univariate neuron models **The Quadratic Integrate-and-Fire (QIF) model** $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=a_{0}(V-V_{\mathrm{res}t})(V-V_{c})+RI(t)\\\mathrm{if~}V&amp;&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{reset~last~}t_{\mathrm{ref}}}\end{aligned} $$ ```python def derivative(self, V, t, I): dVdt = (self.c * (V - self.V_reset) * (V - self.V_c) + self.R * I) / self.tau return dVdt ``` **The Exponential Integrate-and-Fire (ExpIF) model** $$ \begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{\delta_{T}}}+RI(t)\\&amp;\mathrm{if~}V&amp;gt;\theta,\quad V\leftarrow V_{\mathrm{reset}}\mathrm{last}t_{\mathrm{ref}}\end{aligned} $$ ```python def derivative(self, V, t, I): exp_v = self.delta_T * bm.exp((V - self.V_T) / self.delta_T) dvdt = (- (V - self.V_rest) + exp_v + self.R * I) / self.tau return dvdt ``` ## AdEx neuron models programming $$ \begin{gathered} \tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}=-(V-V_{\mathrm{rest}})+\Delta_{T}\mathrm{e}^{{\frac{V-V_{T}}{\Delta T}}}-Rw+RI(t), \\ \tau_{w}\frac{\mathrm{d}w}{\mathrm{d}t}=a(V-V_{\mathrm{rest}})-w+b\tau_{w}\sum_{t^{(f)}}\delta(t-t^{(f)})), \\ \mathrm{if~}V&amp;gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\mathrm{last}t_{\mathrm{ref}}. \end{gathered} $$ ### Define AdEx `class` ```python class AdEx(bp.dyn.NeuDyn): def __init__(self, size, V_rest=-65, V_reset=-68, V_th=-30, V_T=-59.9, delta_T=3.48 a=1., b=1., R=1., tau=10., tau_w=30., tau_ref=0., **kwargs): # åˆå§‹åŒ–çˆ¶ç±» super(AdEx, self).__init__(size=size, **kwargs) ``` ### Initialization ```python class AdEx(bp.dyn.NeuDyn): def __init__(self, size, V_rest=-65, V_reset=-68, V_th=-30, V_T=-59.9, delta_T=3.48 a=1., b=1., R=1., tau=10., tau_w=30., tau_ref=0., **kwargs): # åˆå§‹åŒ–çˆ¶ç±» super(AdEx, self).__init__(size=size, **kwargs) # åˆå§‹åŒ–å‚æ•° self.V_rest = V_rest self.V_reset = V_reset self.V_th = V_th self.V_T = V_T self.delta_T = delta_T self.a = a self.b = b self.R = R self.tau = tau self.tau_w = tau_w self.tau_ref = tau_ref # åˆå§‹åŒ–å˜é‡ self.V = bm.Variable(bm.random.randn(self.num) - 65.) self.w = bm.Variable(bm.zeros(self.num)) self.input = bm.Variable(bm.zeros(self.num)) self.t_last_spike = bm.Variable(bm.ones(self.num) * -1e7) # ä¸Šä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´ self.refractory = bm.Variable(bm.zeros(self.num, dtype=bool)) # æ˜¯å¦å¤„äºŽä¸åº”æœŸ self.spike = bm.Variable(bm.zeros(self.num, dtype=bool)) # è„‰å†²å‘æ”¾çŠ¶æ€ # å®šä¹‰ç§¯åˆ†å™¨ self.integral = bp.odeint(f=self.derivative, method=&apos;exp_auto&apos;) ``` ### Define the derivative function ```python def dV(self, V, t, w, I): exp = self.delta_T * bm.exp((V - self.V_T) / self.delta_T) dVdt = (-V + self.V_rest + exp - self.R * w + self.R * I) / self.tau return dVdt def dw(self, w, t, V): dwdt = (self.a * (V - self.V_rest) - w) / self.tau_w return dwdt @property def derivative(self): return bp.JointEq([self.dV, self.dw]) ``` ### Complete the `update()` function ```python def update(self): t, dt = bp.share[&apos;t&apos;], bp.share[&apos;dt&apos;] V, w = self.integral(self.V.value, self.w.value, t, self.input, dt=dt) # ä»¥æ•°ç»„çš„æ–¹å¼å¯¹ç¥žç»å…ƒè¿›è¡Œæ›´æ–° refractory = (t - self.t_last_spike) self.V_th # å°†å¤§äºŽé˜ˆå€¼çš„ç¥žç»å…ƒæ ‡è®°ä¸ºå‘æ”¾äº†è„‰å†² self.spike[:] = spike # æ›´æ–°ç¥žç»å…ƒè„‰å†²å‘æ”¾çŠ¶æ€ self.t_last_spike[:] = bm.where(spike, t, self.t_last_spike) # æ›´æ–°æœ€åŽä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´ self.V[:] = bm.where(spike, self.V_reset, V) # å°†å‘æ”¾äº†è„‰å†²çš„ç¥žç»å…ƒè†œç”µä½ç½®ä¸ºV_resetï¼Œå…¶ä½™ä¸å˜ self.w[:] = bm.where(spike, w + self.b, w) #æ›´æ–°è‡ªé€‚åº”ç”µæµ self.refractory[:] = bm.logical_or(refractory, spike) # æ›´æ–°ç¥žç»å…ƒæ˜¯å¦å¤„äºŽä¸åº”æœŸ self.input[:] = 0. # é‡ç½®å¤–ç•Œè¾“å…¥ ``` ### Simulation ![image-20230825145518709](/BrainPy-course-notes/master_content/Notes.assets/image-20230825145518709.png) ### Other multivariate neuron models **The Izhikevich model** $$ \begin{aligned} &amp;\frac{dV}{dt} =0.04V^{2}+5V+140-u+I \\ &amp;\frac{\mathrm{d}u}{\mathrm{d}t} =a\left(bV-u\right) \\ &amp;\operatorname{if}V &amp;gt;\theta,\quad V\leftarrow c,u\leftarrow u+d\mathrm{last}t_{\mathrm{ref}} \end{aligned} $$ ```python def dV(self, V, t, u, I): dVdt = 0.04 * V * V + 5 * V + 140 - u + I return dVdt def du(self, u, t, V): dudt = self.a * (self.b * V - u) return dudt ``` **The Generalized Integrate-and-Fire (GIF) model** $$ \begin{aligned} &amp;\tau{\frac{\mathrm{d}V}{\mathrm{d}t}} =-\left(V-V_{\mathrm{rest}}\right)+R\sum_{j}I_{j}+RI \\ &amp;\frac{\mathrm{d}\Theta}{\mathrm{d}t} =a\left(V-V_{\mathrm{est}}\right)-b\left(\Theta-\Theta_{\infty}\right) \\ &amp;\frac{\mathrm{d}I_j}{\mathrm{d}r} =-k_jI_j,\quad j=1,2,\ldots,n \\ &amp;\text{if V} &amp;gt;\Theta,\quad I_{j}\leftarrow R_{j}I_{j}+A_{j},V\leftarrow V_{\mathrm{reset}},\Theta\leftarrow max\left(\Theta_{\mathrm{reset}},\Theta\right) \end{aligned} $$ ```python def dI1(self, I1, t): return - self.k1 * I1 def dI2(self, I2, t): return - self.k2 * I2 def dVth(self, V_th, t, V): return self.a * (V - self.v_rest) - self.b * (V_th - self.V_th_inf) def dV(self, V, t, I1, I2, I): return (- (V - self.V_rest) + self.R * (I + I1 + I2)) / self.tau ``` **Built-in reduced neuron models** ![image-20230825145947800](/BrainPy-course-notes/master_content/Notes.assets/image-20230825145947800.png) ## Dynamic analysis: phase-plane analysis ### Simple case $$ \frac{dx}{dt}=\sin(x)+I, $$ ```python @bp.odeint def int_x(x, t, Iext): return bp.math.sin(x) + Iext ``` ```python pp = bp.analysis.PhasePlane1D( model=int_x, target_vars={&apos;x&apos;: [-10, 10]}, pars_update={&apos;Iext&apos;: 0.}, resolutions={&apos;x&apos;: 0.01} ) pp.plot_vector_field() pp.plot_fixed_point(show=True) ``` ![image-20230825152003373](/BrainPy-course-notes/master_content/Notes.assets/image-20230825152003373.png) - Nullcline: The zero-growth isoclines, such as $f(x,y) = 0$ and $g(x,y) = 0$ - Fixed points: The equilibrium points of the system, which are located at all the nullclines intersect. - Vector field: The vector field of the system. - Limit cycles: The limit cycles. - Trajectories: A simulation trajectory with the given initial values ### Phase plane analysis for AdEx ```python def ppa_AdEx(group): bm.enable_x64() v_range = [-70., -40.] w_range = [-10., 50.] phase_plane_analyzer = bp.analysis.PhasePlane2D( model=group, target_vars={&apos;V&apos;: v_range, &apos;w&apos;: w_range, }, # å¾…åˆ†æžå˜é‡ pars_update={&apos;I&apos;: Iext}, # éœ€è¦æ›´æ–°çš„å˜é‡ resolutions=0.05 ) # ç”»å‡ºV, wçš„é›¶å¢žé•¿æ›²çº¿ phase_plane_analyzer.plot_nullcline() # ç”»å‡ºå¥‡ç‚¹ phase_plane_analyzer.plot_fixed_point() # ç”»å‡ºå‘é‡åœº phase_plane_analyzer.plot_vector_field() # åˆ†æ®µç”»å‡ºV, wçš„å˜åŒ–è½¨è¿¹ group.V[:], group.w[:] = group.V_reset, 0 runner = bp.DSRunner(group, monitors=[&apos;V&apos;, &apos;w&apos;, &apos;spike&apos;], inputs=(&apos;input&apos;, Iext)) runner(500) spike = runner.mon.spike.squeeze() s_idx = np.where(spike)[0] # æ‰¾åˆ°æ‰€æœ‰å‘æ”¾åŠ¨ä½œç”µä½å¯¹åº”çš„index s_idx = np.concatenate(([0], s_idx, [len(spike) - 1])) # åŠ ä¸Šèµ·å§‹ç‚¹å’Œç»ˆæ­¢ç‚¹çš„index for i in range(len(s_idx) - 1): vs = runner.mon.V[s_idx[i]: s_idx[i + 1]] ws = runner.mon.w[s_idx[i]: s_idx[i + 1]] plt.plot(vs, ws, color=&apos;darkslateblue&apos;) # ç”»å‡ºè™šçº¿ x = V_reset plt.plot([group.V_reset, group.V_reset], w_range, &apos;--&apos;, color=&apos;grey&apos;, zorder=-1) plt.show() ``` ![image-20230825152925463](/BrainPy-course-notes/master_content/Notes.assets/image-20230825152925463.png) ## Dynamic analysis: bifurcation analysis ### Simple case $$ \frac{dx}{dt}=\sin(x)+I, $$ ```python bif = bp.analysis.Bifurcation1D( model=int_x, target_vars={&apos;x&apos;: [-10, 10]}, target_pars={&apos;Iext&apos;: [0., 1.5]}, resolutions={&apos;Iext&apos;: 0.005, &apos;x&apos;: 0.05} ) bif.plot_bifurcation(show=True) ``` ![image-20230825154227567](/BrainPy-course-notes/master_content/Notes.assets/image-20230825154227567.png) # Synapse models and their programming ## The biology of synapses ### Neurotransmitter &amp; Synapse When the action potential invades the axon terminals, it causes voltage-gated ð¶ð¶ð‘Žð‘Ž 2+ channels to open (1), which triggers vesicles to bind to the presynaptic membrane (2). Neurotransmitter is released into the synaptic cleft by exocytosis and diffuses across the cleft (3). Binding of the neurotransmitter to receptor molecules in the postsynaptic membrane completes the process of transmission (4). åŽ»æžåŒ–æ—¶é’™ç¦»å­å†…æµï¼Œä¸Žå›Šæ³¡ç›¸ç»“åˆï¼Œ...ï¼Œä¸Žå—ä½“ç»“åˆï¼Œæ‰“å¼€ç¦»å­é€šé“ï¼Œè¶…æžåŒ–ã€åŽ»æžåŒ–çŽ°è±¡ ![image-20230826100321307](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100321307.png) ![image-20230826100418911](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100418911.png) **Neurotransmitter leading to postsynaptic potential.** The binding of neurotransmitter to the postsynaptic membrane receptors changes the membrane potential ($V_m$). These postsynaptic potentials can be either excitatory (depolarizing the membrane), as shown here, or inhibitory (hyperpolarizing the membrane). ![image-20230826100531535](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100531535.png) ### Neurotransmitters å…´å¥‹æ€§ç¥žç»é€’è´¨ï¼š - ä¹™é…°èƒ†ç¢± (ACh) - å„¿èŒ¶é…šèƒº (catecholamines) - è°·æ°¨é…¸ (glutamate) - ç»„èƒº (histamine) - 5-ç¾Ÿè‰²èƒº (serotonin) - æŸäº›ç¥žç»è‚½ç±» (some of neuropeptides) æŠ‘åˆ¶æ€§ç¥žç»é€’è´¨ï¼š - GABA - ç”˜æ°¨é…¸ (glycine) - æŸäº›ç¥žç»è‚½ç±» (some of peptides) ![image-20230826100609904](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100609904.png) ### The postsynaptic response The aim of a synapse model is to describe accurately the postsynaptic response generated by the arrival of an action potential at a presynaptic terminal. 1. The fundamental quantity to be modelled is the time course of the postsynaptic receptor conductance 2. The models: - Simple phenomenological waveforms - More complex kinetic schemes that are analogous to the models of membrane- bound ion channels ![image-20230826100701580](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100701580.png) å»ºæ¨¡è¿™ç§å“åº”æ¨¡å¼ï¼Œæ‰“å¼€å…³é—­çš„æ¦‚çŽ‡... ## Phenomenological synapse models ### Exponential Model ![image-20230826100738460](/BrainPy-course-notes/master_content/Notes.assets/image-20230826100738460.png) **Assumption**: - The release of neurotransmitter, its diffusion across the cleft, the receptor binding, and channel opening all happen very quickly, so that the channels instantaneously jump from the closed to the open state. channelä¼šçž¬é—´å¢žåŠ ç„¶åŽé€æ¸å…³é—­ $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}e^{-(t-t_{0})/\tau} \\ \begin{matrix}\bullet&amp;\tau \ \text{is the time constant}\\\bullet&amp;t_0 \ \text{is the time of the pre-synaptic spike}\\\bullet&amp;\bar{g_{syn}}\ \text{is the maximal conductance}\end{matrix} $$ -&amp;gt; corresponding differential equation $$ \tau\frac{dg_{\mathrm{syn}}(t)}{dt}=-g_{\mathrm{syn}}(t)+\bar{g}_{\mathrm{syn}}\delta\left(t_{0}-t\right) $$ - Can fit with experimental data. - A good approximation for GABA A and AMPA, because the rising phase is much shorter than their decay phase. ### Dual Exponential Model ![image-20230826101203059](/BrainPy-course-notes/master_content/Notes.assets/image-20230826101203059.png) exponential modelä¸Šå‡çš„å¤ªå¿«ï¼Œä¸å¤ªç¬¦åˆæŸäº›synapse Dual exponential synapse provides a general way to describe the synaptic conductance with different rising and decay time constants. $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}\frac{\tau_{1}\tau_{2}}{\tau_{1}-\tau_{2}}\left(\exp\left(-\frac{t-t_{0}}{\tau_{1}}\right)-\exp\left(-\frac{t-t_{0}}{\tau_{2}}\right)\right) \\ \begin{matrix} \bullet &amp;t_1\ \text{is the decay synaptic time constant} \\ \bullet &amp;\tau_2\ \text{is the rise synaptic time constant} \\ \bullet &amp;t_0\ \text{is the time of the pre-synaptic spike} \\ \bullet &amp;\bar{g}_{syn}\ \text{is the maximal conductance} \end{matrix} $$ -&amp;gt;corresponding differential equation $$ \begin{aligned} &amp;g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}g \\ &amp;\frac{dg}{dt}=-\frac{g}{\tau_{\mathrm{decay}}}+h \\ &amp;\frac{dh}{dt}&amp; =-\frac{h}{\tau_{\mathrm{rise}}}+\delta\left(t_{0}-t\right), \end{aligned} $$ The time course of most synaptic conductance can be well described by this sum of two exponentials. ### Synaptic time constants ![image-20230826101544786](/BrainPy-course-notes/master_content/Notes.assets/image-20230826101544786.png) http://compneuro.uwaterloo.ca/research/constants-constraints/neurotransmitter-time-constants-pscs.html #### AMPA synapse - $t_{decay}$ = 0.18 ms in the auditory system of the chick nucleus magnocellularis (Trussell, 1999). - $t_{rise}$ 25 ms and $\tau_{decay}$ =0.77 ms in dentate gyrus basket cells (Geiger et al., 1997). - $t_{rise}$ = 0.2 ms and $\tau_{decay}$ =1.7 ms in in neocortical layer 5 pyramidal neurons (Hausser and Roth, 1997b). - Reversal potential is nearly 0 mV. #### NMDA synapse - The decay time constants (at near-physiological temperature): - 19 ms in dentate gyrus basket cells (Geiger et al., 1997), - 26 ms in neocortical layer 2/3 pyramidal neurons (Feldmeyer et al., 2002), - 89 ms in CA1 pyramidal cells (Diamond, 2001). - The rise time constants are about 2 ms (Feldmeyer et al., 2002). - Reversal potential is nearly 0 mV. #### GABA$_A$ synapse - GABAergic synapses from dentate gyrus basket cells onto other basket cells are faster: $t_{rise}$ = 0.3 ms and $t_{decay}$ = 2.5 ms (Bartos et al., 2001) than synapses from basket cells to granule cells: $t_{rise}$ = 0.26 ms and $t_{decay}$ = 6.5 ms (Kraushaar and Jonas, 2000). - Reversal potential is nearly -80 mV. #### GABA$_B$ synapse - Common models use models with a rise time of about 25-50 ms, a fast decay time in the range of 100-300ms and a slow decay time of 500-1000 ms. - Reversal potential is nearly -90 mV. ### General property of synaptic time constants - The time constants of synaptic conductance vary widely among synapse types. - The synaptic kinetics tends to accelerate during development (T. Takahashi, Neuroscience Research, 2005) . - The synaptic kinetics becomes substantially faster with increasing temperature. ![image-20230826102033433](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102033433.png) ### Current- and Conductance-based Response ![image-20230826102042614](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102042614.png) #### Conductance-based Response Most synaptic ion channels, such as AMPA and GABA, display an approximately linear current-voltage relationship when they open. ![image-20230826102113670](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102113670.png) **For example**: The synapse is located on a thin dendrite, because the local membrane potential V changes considerably when the synapse is activated. #### Current-based Response In some case, we can also approximate the synapses as sources of current and not a conductance. ![image-20230826102150487](/BrainPy-course-notes/master_content/Notes.assets/image-20230826102150487.png) **For example**: The excitatory synapse on a large compartment, because the depolarization of the membrane is small. ## Programming of phenomenological synapse models ### `ProjAlignPostMg2` ![Image Name](https://cdn.kesci.com/upload/rzz4o4uyar.png?imageView2/0/w/960/h/960) ```python brainpy.dyn.ProjAlignPostMg2( pre, delay, comm, syn, out, post ) ``` - ``pre (JointType[DynamicalSystem, AutoDelaySupp])``: The pre-synaptic neuron group. - ``delay (Union[None, int, float])``: The synaptic delay. - ``comm (DynamicalSystem)``: The synaptic communication. - ``syn (ParamDescInit)``: The synaptic dynamics. - ``out (ParamDescInit)``: The synaptic output. - ``post (DynamicalSystem)`` The post-synaptic neuron group. åªéœ€è¦å»ºæ¨¡æ‰€æœ‰postçš„neurons ### CSR matrix ![Image Name](https://cdn.kesci.com/upload/rzz4on32hr.png?imageView2/0/w/960/h/960) ### Exponential Model The single exponential decay synapse model assumes the release of neurotransmitter, its diffusion across the cleft, the receptor binding, and channel opening all happen very quickly, so that the channels instantaneously jump from the closed to the open state. Therefore, its expression is given by $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} e^{-\left(t-t_{0}\right) / \tau} $$ where $\tau$ is the time constant, $t_0$ is the time of the pre-synaptic spike, $\bar{g}_{\mathrm{syn}}$ is the maximal conductance. The corresponding differential equation: $$ \frac{d g}{d t} = -\frac{g}{\tau_{decay}}+\sum_{k} \delta(t-t_{j}^{k}). $$ #### COBA Given the synaptic conductance, the COBA model outputs the post-synaptic current with $$ I_{syn}(t) = g_{\mathrm{syn}}(t) (E - V(t)) $$ ```python class ExponSparseCOBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.COBA.desc(E=E), post=post, ) ``` ```python class SimpleNet(bp.DynSysGroup): def __init__(self, E=0.): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = ExponSparseCOBA(self.pre, self.post, delay=None, prob=1., g_max=1., tau=5., E=E) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` ```python def run_a_net(net): indices = np.arange(1000) # 100 ms conductances, currents, potentials = bm.for_loop(net.step_run, indices, progress_bar=True) ts = indices * bm.get_dt() # --- similar to: # runner = bp.DSRunner(net) # conductances, currents, potentials = runner.run(100.) fig, gs = bp.visualize.get_figure(1, 3, 3.5, 4) fig.add_subplot(gs[0, 0]) plt.plot(ts, conductances) plt.title(&apos;Syn conductance&apos;) fig.add_subplot(gs[0, 1]) plt.plot(ts, currents) plt.title(&apos;Syn current&apos;) fig.add_subplot(gs[0, 2]) plt.plot(ts, potentials) plt.title(&apos;Post V&apos;) plt.show() ``` #### CUBA Given the conductance, this model outputs the post-synaptic current with a identity function: $$ I_{\mathrm{syn}}(t) = g_{\mathrm{syn}}(t) $$ ```python class ExponSparseCUBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.CUBA.desc(), post=post, ) ``` ```python class SimpleNet2(bp.DynSysGroup): def __init__(self, g_max=1.): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = ExponSparseCUBA(self.pre, self.post, delay=None, prob=1., g_max=g_max, tau=5.) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` #### Dense connections Exponential synapse model with the conductance-based (COBA) output current and dense connections. ```python class ExponDenseCOBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.MaskedLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.COBA.desc(E=E), post=post, ) ``` ![Image Name](https://cdn.kesci.com/upload/rzz4p7x6dl.png?imageView2/0/w/960/h/960) Exponential synapse model with the current-based (COBA) output current and dense connections. ```python class ExponDenseCUBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E): super().__init__() self.proj = bp.dyn.ProjAlignPostMg2( pre=pre, delay=delay, comm=bp.dnn.MaskedLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), syn=bp.dyn.Expon.desc(post.num, tau=tau), out=bp.dyn.CUBA.desc(), post=post, ) ``` ### `ProjAlignPreMg2` Synaptic projection which defines the synaptic computation with the dimension of presynaptic neuron group. ![Image Name](https://cdn.kesci.com/upload/rzz4pj1qmk.png?imageView2/0/w/960/h/960) ```python brainpy.dyn.ProjAlignPreMg2( pre, delay, syn, comm, out, post ) ``` - ``pre (JointType[DynamicalSystem, AutoDelaySupp])``: The pre-synaptic neuron group. - ``delay (Union[None, int, float])``: The synaptic delay. - ``syn (ParamDescInit)``: The synaptic dynamics. - ``comm (DynamicalSystem)``: The synaptic communication. - ``out (ParamDescInit)``: The synaptic output. - ``post (DynamicalSystem)`` The post-synaptic neuron group. ### Dual Exponential Model The dual exponential synapse model, also named as **difference of two exponentials model**, is given by: $$ g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} \frac{\tau_{1} \tau_{2}}{\tau_{1}-\tau_{2}}\left(\exp \left(-\frac{t-t_{0}}{\tau_{1}}\right)-\exp \left(-\frac{t-t_{0}}{\tau_{2}}\right)\right) $$ where $\tau_1$ is the time constant of the decay phase, $\tau_2$ is the time constant of the rise phase, $t_0$ is the time of the pre-synaptic spike, $\bar{g}_{\mathrm{syn}}$ is the maximal conductance. The corresponding differential equation: $$ \begin{aligned} &amp;g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} g \\ &amp;\frac{d g}{d t}=-\frac{g}{\tau_{\mathrm{decay}}}+h \\ &amp;\frac{d h}{d t}=-\frac{h}{\tau_{\text {rise }}}+ \delta\left(t_{0}-t\right), \end{aligned} $$ The alpha function is retrieved in the limit when both time constants are equal. ```python class DualExpSparseCOBA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau_decay, tau_rise, E): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.DualExpon.desc(pre.num, tau_decay=tau_decay, tau_rise=tau_rise), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.COBA(E=E), post=post, ) ``` ```python class SimpleNet4(bp.DynSysGroup): def __init__(self, E=0.): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = DualExpSparseCOBA(self.pre, self.post, delay=None, prob=1., g_max=1., tau_decay=5., tau_rise=1., E=E) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` ## Biophysical synapse models ### Limitations of phenomenological models æ‰“å¼€çš„æ•°é‡æ˜¯æœ‰é™çš„ï¼Œè€Œä¸”æœ‰é¥±å’ŒæœŸ 1. Saturation of postsynaptic receptors by previously released transmitter. 2. Certain receptor types also exhibit desensitization that prevents them (re)opening for a period after transmitter-binding, like sodium channels underlying action potential. ![image-20230826111443117](/BrainPy-course-notes/master_content/Notes.assets/image-20230826111443117.png) ### Linetic/Markov models ![image-20230826111733654](/BrainPy-course-notes/master_content/Notes.assets/image-20230826111733654.png) - The simplest kinetic model is a two-state scheme in which receptors can be either closed, ð¶, or open, ð‘‚, and the transition between states depends on transmitter concentration, [ð‘‡], in the synaptic cleft: - ð›¼ and ð›½ are voltage-independent forward and backward rate constants. - ð¶ and ð‘‚ can range from 0 to 1, and describe the fraction of receptors in the closed and open states, respectively. - The synaptic conductance is: $g_{syn}(t)=\bar{g}_{max}g(t)$ ### AMPA/GABA$_A$ synapse model $$ \begin{aligned}\frac{ds}{dt}&amp;=\alpha[T](1-s)-\beta s\\I&amp;=\tilde{g}s(V-E)\end{aligned} $$ - ð›¼[ð‘‡] denotes the transition probability from state (1âˆ’ð‘ ) to state (ð‘ ) - ð›½ represents the transition probability of the other direction - ð¸ is a reverse potential, which can determine whether the direction of ð¼ is inhibition or excitation. - ð¸ = 0 ð‘šð‘šð‘‰ð‘‰ =&amp;gt; Excitatory synapse [AMPA] - ð¸ = âˆ’80 ð‘šð‘šð‘‰ð‘‰ =&amp;gt; Inhibitory synapse [GABA A ] ### Comparison ![image-20230826111950713](/BrainPy-course-notes/master_content/Notes.assets/image-20230826111950713.png) ### NMDA synapse model ![image-20230826112027689](/BrainPy-course-notes/master_content/Notes.assets/image-20230826112027689.png) ![image-20230826112034481](/BrainPy-course-notes/master_content/Notes.assets/image-20230826112034481.png) $$ \begin{aligned} &amp;\frac{ds}{dt} =\alpha[T](1-s)-\beta s \\ &amp;I=\tilde{g}sB(V)(V-E) \\ &amp;B(V )=\frac{1}{1+\exp(-0.062V)[Mg^{2+}]_{o}/3.57} \end{aligned} $$ The magnesium block of the NMDA receptor channel is an extremely fast process compared to the other kinetics of the receptor (Jahr and Stevens 1990a, 1990b). The block can therefore be accurately modeled as an instantaneous function of voltage(Jahr and Stevens 1990b). where $[Mg^{2+}]$ is the external magnesium concentration (1 to 2mM inphysiological conditions) ## Programming of biophysical synapse models ### AMPA synapse model ```python class AMPA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, E=0.): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.AMPA.desc(pre.num, alpha=0.98, beta=0.18, T=0.5, T_dur=0.5), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.COBA(E=E), post=post, ) ``` ```python class SimpleNet(bp.DynSysGroup): def __init__(self, syn_cls): super().__init__() self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.)) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = syn_cls(self.pre, self.post, delay=None, prob=1., g_max=1.) def update(self): self.pre() self.syn() self.post() # monitor the following variables conductance = self.syn.proj.refs[&apos;syn&apos;].g current = self.post.sum_inputs(self.post.V) return conductance, current, self.post.V ``` ```python def run_a_net(net, duration=100): indices = np.arange(int(duration/bm.get_dt())) # duration ms conductances, currents, potentials = bm.for_loop(net.step_run, indices, progress_bar=True) ts = indices * bm.get_dt() # --- similar to: # runner = bp.DSRunner(net) # conductances, currents, potentials = runner.run(100.) fig, gs = bp.visualize.get_figure(1, 3, 3.5, 4) fig.add_subplot(gs[0, 0]) plt.plot(ts, conductances) plt.title(&apos;Syn conductance&apos;) fig.add_subplot(gs[0, 1]) plt.plot(ts, currents) plt.title(&apos;Syn current&apos;) fig.add_subplot(gs[0, 2]) plt.plot(ts, potentials) plt.title(&apos;Post V&apos;) plt.show() ``` ### $\text{GABA}_A$ synapse model ```python class GABAa(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, E=-80.): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.GABAa.desc(pre.num, alpha=0.53, beta=0.18, T=1.0, T_dur=1.0), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.COBA(E=E), post=post, ) ``` ```python run_a_net(SimpleNet(syn_cls=GABAa)) ``` ### NMDA synapse model ```python class NMDA(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, E=0.0): super().__init__() self.proj = bp.dyn.ProjAlignPreMg2( pre=pre, delay=delay, syn=bp.dyn.NMDA.desc(pre.num, a=0.5, tau_decay=100., tau_rise=2.), comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), out=bp.dyn.MgBlock(E=E), post=post, ) ``` ```python run_a_net(SimpleNet(NMDA)) ``` ### Kinetic synapse models are more realistic ```python class SimpleNet5(bp.DynSysGroup): def __init__(self, freqs=10.): super().__init__() self.pre = bp.dyn.PoissonGroup(1, freqs=freqs) self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Constant(-60.)) self.syn = NMDA(self.pre, self.post, delay=None, prob=1., g_max=1., E=0.) def update(self): self.pre() self.syn() self.post() # monitor the following variables return self.syn.proj.refs[&apos;syn&apos;].g, self.post.V ``` ```python def compare_freqs(freqs): fig, _ = bp.visualize.get_figure(1, 1, 4.5, 6.) for freq in freqs: net = SimpleNet5(freqs=freq) indices = np.arange(1000) # 100 ms conductances, potentials = bm.for_loop(net.step_run, indices, progress_bar=True) ts = indices * bm.get_dt() plt.plot(ts, conductances, label=f&apos;{freq} Hz&apos;) plt.legend() plt.ylabel(&apos;g&apos;) plt.show() ``` ```python compare_freqs([10., 100., 1000., 10000.]) ``` ### How to customize a synapse #### Preparations `ProjAlignPostMg2` and `ProjAlignPreMg2` #### Exponential Model ```python class Exponen(bp.dyn.SynDyn, bp.mixin.AlignPost): def __init__(self, size, tau): super().__init__(size) # parameters self.tau = tau # variables self.g = bm.Variable(bm.zeros(self.num)) # integral self.integral = bp.odeint(lambda g, t: -g/tau, method=&apos;exp_auto&apos;) def update(self, pre_spike=None): self.g.value = self.integral(g=self.g.value, t=bp.share[&apos;t&apos;], dt=bp.share[&apos;dt&apos;]) if pre_spike is not None: self.add_current(pre_spike) return self.g.value def add_current(self, x): # specical for bp.mixin.AlignPost self.g += x def return_info(self): return self.g ``` #### AMPA Model ```python class AMPA(bp.dyn.SynDyn): def __init__(self, size, alpha= 0.98, beta=0.18, T=0.5, T_dur=0.5): super().__init__(size=size) # parameters self.alpha = alpha self.beta = beta self.T = T self.T_duration = T_dur # functions self.integral = bp.odeint(method=&apos;exp_auto&apos;, f=self.dg) # variables self.g = bm.Variable(bm.zeros(self.num)) self.spike_arrival_time = bm.Variable(bm.ones(self.num) * -1e7) def dg(self, g, t, TT): return self.alpha * TT * (1 - g) - self.beta * g def update(self, pre_spike): self.spike_arrival_time.value = bm.where(pre_spike, bp.share[&apos;t&apos;], self.spike_arrival_time) TT = ((bp.share[&apos;t&apos;] - self.spike_arrival_time) åšæ—¶é—´å¹³å‡ STP based on firing rate $$ \begin{gathered} \frac{du(t)}{dt}=\frac{-u(t)}{\tau_{f}}+U_{sE}(1-u^{-})\delta\big(t-t_{sp}\big), \\ \frac{dx(t)}{dt}=\frac{1-x(t)}{\tau_{d}}-u^{+}x^{-}\delta\big(t-t_{sp}\big), \\ \frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{s}}+Au^{+}x^{-}\delta\big(t-t_{sp}\big), \\ u^{+}=\lim_{t-t_{sp\rightarrow0^{+}}}u(t), \end{gathered} $$ ![image-20230826155016657](/BrainPy-course-notes/master_content/Notes.assets/image-20230826155016657.png) ä¸¢æŽ‰æ—¶é—´å˜åŒ–çš„å…·ä½“ç»†èŠ‚ï¼ŒæŠ“ä½äº†é‡è¦è¶‹åŠ¿ ### Theoretical analysis of the rate model Suppose the pre-synaptic firing rate keeps as constant, we can calculate the stationary response $$ u_{st}=\frac{U_{SE}R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}},\quad u_{st}^{+}=U_{SE}\frac{1+R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}},\quad x_{st}=\frac{1}{1+u_{st}^{+}\tau_{d}R_{0}}, $$ $$ EPSC_{st}=Au_{st}^{+}x_{st}=A\frac{u_{st}^{+}}{1+u_{st}^{+}\tau_{d}R_{0}},\quad PSV_{st}\propto g_{st}=\tau_{s}Au_{st}^{+}x_{st}R_{0}=A\frac{u_{st}^{+}R_{0}}{1+u_{st}^{+}\tau_{d}R_{0}}, $$ ![image-20230826155234134](/BrainPy-course-notes/master_content/Notes.assets/image-20230826155234134.png) ### Frequency-dependent Gain control of spike information $$ \begin{gathered} u_{st}^{+}=U_{SE}\frac{1+R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}}, \\ x_{st}=\frac{1}{1+u_{st}^{+}\tau_{d}R_{0}}, \\ EPSC_{st}=Au_{st}^{+}x_{st}=A\frac{u_{st}^{+}}{1+u_{st}^{+}\tau_{d}R_{0}}, \end{gathered} $$ Peak frequency: $\theta\sim\frac{1}{\sqrt{U\tau_{f}\tau_{d}}}$ ### Simulation of Frequency-dependent Gain control ![image-20230826155715445](/BrainPy-course-notes/master_content/Notes.assets/image-20230826155715445.png) ## Effects on network dynamics ### STP modeling Working memory ![image-20230826160102332](/BrainPy-course-notes/master_content/Notes.assets/image-20230826160102332.png) ![image-20230826160113456](/BrainPy-course-notes/master_content/Notes.assets/image-20230826160113456.png) # E-I Balanced Neural Network ## Irregular Spiking of Neurons ### Signal process of single neuron External Stimulus -&amp;gt; Single neuron model $$ \begin{aligned}\tau&amp;\frac{\mathrm{d}V}{\mathrm{d}t}=-(V-V_\text{rest })+RI(t)\\\\&amp;\text{if}V&amp;gt;V_\text{th},\quad V\leftarrow V_\text{reset }\text{last}t_\text{ref}\end{aligned} $$ -&amp;gt; ... -&amp;gt; Perception or action çœŸæ­£çš„ç¥žç»å…ƒå¹¶ä¸æ˜¯LIF modelçš„è¾“å‡º ![image-20230827100647851](/BrainPy-course-notes/master_content/Notes.assets/image-20230827100647851.png) Simulation ![image-20230827100706310](/BrainPy-course-notes/master_content/Notes.assets/image-20230827100706310.png) Neuron recorded in vivo ### Irregular Spiking of Neurons ![image-20230827092807270](/BrainPy-course-notes/master_content/Notes.assets/image-20230827092807270.png) #### Statistical Description of Spikes ç”¨ä»¥ä¸‹çš„å˜é‡æ¥è¿›è¡Œç»Ÿè®¡æè¿° - Firing Rate Rate = average over time(single neuron, single run) Spike count $v=\frac{n_{sp}}{T}$ - ISI(Interspike interval distributions) average ISI $\overline{\Delta t}=\frac{1}{n_{sp}-1}\sum_{i=1}^{n_{sp}-1}\Delta t_{i}$ standard deviation ISI: $\sigma_{\Delta t}^{2}=\frac{1}{n_{sp}-1}\sum_{i=1}^{n_{sp}-1}(\Delta t_{i}-\overline{\Delta t})^{2}$ - $C_V$(Coefficient of variation, Fano factor) **çª„è¿˜æ˜¯å®½çš„åˆ†å¸ƒ** ä¿¡æ¯è¡¨å¾æœ‰å¤šå¼ºçš„ä¸ç¨³å®šæ€§ $C_{V}=\sigma_{\Delta t}^{2}/\overline{\Delta t}$ #### Poisson Process In probability theory and statistics, the Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known **constant mean rate** and **independently** of the time since the last eventã€‚ $$ \begin{aligned} &amp;P(X=k\mathrm{~events~in~interval~}t)=e^{-rt}\frac{(rt)^{k}}{k!} \\ &amp;\mathrm{mean:}\quad\overline{X}=rt \\ &amp;\mathrm{variance}:\quad\sigma^{2}=rt\\ &amp;\mathrm{Fano factor:}\quad\frac{\sigma^{2}}{X}=1 \end{aligned} $$ Fano factor -&amp;gt; noise-to-signal ratio #### Irregular Spiking of Neurons LIFåœ¨å•ä¸ªç¥žç»å…ƒçš„æƒ…å†µä¸‹æ˜¯åŸºæœ¬æ²¡æœ‰å¤ªå¤§é—®é¢˜çš„ï¼Œåœ¨æ•´ä¸ªç½‘ç»œä¸­ä¼šå—ç½‘ç»œä¿¡æ¯è°ƒæŽ§ ![image-20230827093614607](/BrainPy-course-notes/master_content/Notes.assets/image-20230827093614607.png) #### Why Irregular? - ä¸å®Œå…¨æ˜¯inputå½±å“çš„ - ä¸èƒ½ç®€å•æ¥è¡¡é‡ On average, a cortical neuron receives inputs from 1000~10000 connected neurons. -&amp;gt; averaged noise ~ 0 ## E-I Balanced Network $$ \begin{gathered} \tau\frac{du_{i}^{E}}{dt}=-u_{i}^{E}+\sum_{j=1}^{K_{E}}J_{EE}r_{j}^{E}+\sum_{j=1}^{K_{I}}J_{EI}r_{j}^{I}+I_{i}^{E} \\ \tau\frac{du_{i}^{I}}{dt}=-u_{i}^{I}+\sum_{j=1}^{K_{I}}J_{II}r_{j}^{I}+\sum_{j=1}^{K_{E}}J_{IE}r_{j}^{E}+I_{i}^{I} \end{gathered} $$ ![image-20230827093708220](/BrainPy-course-notes/master_content/Notes.assets/image-20230827093708220.png) Sparse &amp; random connections:$1\ll K_{\mathrm{E}},K_{1}\ll N_{\mathrm{E}},N_{\mathrm{I}}$ . Neurons fire largely independently to each other. $$ \begin{gathered} \text{Single neuron fires irregularly } r_j^E, r_j^{\prime} \text{with mean rate } \mu \text{and variance } \sigma^2.\\ \text{The mean of recurrent input received by E neuron:} \\ \sim K_{E}J_{EE}\mu-K_{I}J_{EI}\mu \\ \text{The variance of recurrent input received by E neuron:} \\ \sim K_{E}(J_{EE})^{2}\sigma^{2}+K_{I}(J_{EI})^{2}\sigma^{2} \\ \begin{gathered} \\ \text{The balanced condition:} \\ K_{E}J_{EE}-K_{l}J_{El}{\sim}0(1) \\ J_{EE}=\frac{1}{\sqrt{K_{E}}},J_{EI}=\frac{1}{\sqrt{K_{I}}},K_{E}(J_{EE})^{2}\sigma^{2}+K_{I}(J_{EI})^{2}\sigma^{2}\sim O(1) \end{gathered} \end{gathered} $$ $$ \begin{aligned}\frac{I_E}{I_I}&amp;&amp;gt;\frac{J_E}{J_I}&amp;&amp;gt;1\\\\J_E&amp;&amp;gt;1\\\\\text{r not too big}\end{aligned} $$ $$ \overline{I_a}=\overline{F_a}+\overline{R_a}=\sqrt{N}(f_a\mu_0+w_{aE}r_E+w_{aI}r_I),\quad a=E,I,\\ \begin{gathered} w_{ab}~=~p_{ab}j_{ab}q_{b} \\ J_{ij}^{ab}~=~j_{ab}/\sqrt{N}; \\ \frac{f_{E}}{f_{I}}&amp;gt;\frac{w_{EI}}{w_{II}}&amp;gt;\frac{w_{EE}}{w_{IE}}. \end{gathered} $$ ## BrainPy Simulation ### Simulation LIF neuron 4000 (E/I=4/1, P=0.02) ðœ = 20 ms ð‘‰ð‘Ÿð‘’ð‘ ð‘¡ = -60 mV Spiking threshold: -50 mV Refractory period: 5 ms $$ \begin{gathered} \tau\frac{dV}{dt}=(V_{\mathrm{rest}}-V)+I \\ I=g_{exc}(E_{exc}-V)+g_{inh}(E_{inh}-V)+I_{\mathrm{ext}} \end{gathered} \ \ \ \ \ \ \begin{aligned}\tau_{exc}&amp;\frac{dg_{exc}}{dt}=-g_{exc}\\\tau_{inh}&amp;\frac{dg_{inh}}{dt}=-g_{inh}\end{aligned} $$ $$ \begin{array}{l}E_\mathrm{exc}=0\text{mV}\mathrm{and}E_\mathrm{inh}=-80\text{mV},I_\mathrm{ext}=20.\\\tau_\mathrm{exc}=5\text{ ms},\tau_\mathrm{inh}=10\text{ ms},\Delta g_\mathrm{exc}=0.6\text{ and}\Delta g_\mathrm{inh}=6.7.\end{array} $$ ![image-20230827094502860](/BrainPy-course-notes/master_content/Notes.assets/image-20230827094502860.png) ### Synaptic Computation ```python # åŸºäºŽ align post Exponential synaptic computation class Exponential(bp.Projection): def __init__(self, pre, post, delay, prob, g_max, tau, E, label=None): super().__init__() self.pron = bp.dyn.ProjAlignPost2( pre=pre, delay=delay, comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), # éšæœºè¿žæŽ¥ syn=bp.dyn.Expon(size=post.num, tau=tau), # Exponential synapse out=bp.dyn.COBA(E=E), # COBA network post=post, out_label=label ) ``` ### E-I Balanced Network ```python # æž„å»º E-I Balanced Network class EINet(bp.DynamicalSystem): def __init__(self, ne=3200, ni=800): super().__init__() # bp.neurons.LIF() self.E = bp.dyn.LifRef(ne, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Normal(-55., 2.)) self.I = bp.dyn.LifRef(ni, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5., V_initializer=bp.init.Normal(-55., 2.)) #### E2E, E2I, I2E, I2I Exponential synaptic computation # delay=0, prob=0.02, g_max_E=0.6, g_max_I=6.7, tau_E=5, tau_I=10, # reversal potentials E_E=0, E_E=-80, label=EE,EI,IE,II self.E2E = Exponential(self.E, self.E, 0., 0.02, 0.6, 5., 0., &apos;EE&apos;) self.E2I = Exponential(self.E, self.I, 0., 0.02, 0.6, 5., 0., &apos;EI&apos;) self.I2E = Exponential(self.I, self.E, 0., 0.02, 6.7, 5., -80., &apos;IE&apos;) self.I2I = Exponential(self.I, self.I, 0., 0.02, 6.7, 5., -80., &apos;II&apos;) ``` ```python def update(self, inp=0.): # æ›´æ–°çªè§¦ä¼ å…¥ç”µæµ self.E2E() self.E2I() self.I2E() self.I2I() # æ›´æ–°ç¥žç»å…ƒç¾¤ä½“ self.E(inp) self.I(inp) # è®°å½•éœ€è¦ monitorçš„å˜é‡ E_E_inp = self.E.sum_inputs(self.E.V, label=&apos;EE&apos;) #E2Eçš„è¾“å…¥ I_E_inp = self.E.sum_inputs(self.E.V, label=&apos;IE&apos;) # I2Eçš„è¾“å…¥ return self.E.spike, self.I.spike, E_E_inp, I_E_inp ``` ![image-20230827110737553](/BrainPy-course-notes/master_content/Notes.assets/image-20230827110737553.png) ![image-20230827110746410](/BrainPy-course-notes/master_content/Notes.assets/image-20230827110746410.png) ## Properties of E-I Balanced Network - Linear encoding External input strength is â€œlinearlyâ€ encoded by the mean firing rate of the neural population - Fast Response The network responds rapidly to abrupt changes of the input ### Noise speeds up computation å¿«é€Ÿç›¸åº”çš„åŽŸç†ï¼Œå‡åŒ€åˆ†å¸ƒåœ¨é˜ˆå€¼ä¸‹é¢çš„ç©ºé—´ - A neural ensemble jointly encodes stimulus information; - Noise randomizes the distribution of neuronal membrane potentials; - Those neurons (red circle) whose potentials are close to the threshold will fire rapidly; - If the noisy environment is proper, even for a small input, a certain number of neurons will fire instantly to report the presence of a stimulus. ![image-20230827113451626](/BrainPy-course-notes/master_content/Notes.assets/image-20230827113451626.png) # Continuous Attractor Neural Network ## Attractor Models ### The concept of attractor dynamics Different types of attractors: Point attractors, Line attractors, Ring attractors, Plane attractors, Cyclic attractors, Chaotic attractors ![image-20230827140250173](/BrainPy-course-notes/master_content/Notes.assets/image-20230827140250173.png) ç¨³æ€ï¼Œèƒ½é‡æ¢¯åº¦å¸å¼•åˆ°attractor ### Discrete attractor Network Model: Hopfield Model $S_i=\pm1$: the neuronal state $W_{ij}$ : the neuronal connection The network dynamics: $$ S_{i}=\mathrm{sign}\bigg(\sum_{j}w_{ij}S_{j}-\theta\bigg),\quad\mathrm{sign}(x)=1,\mathrm{for}x&amp;gt;0;-1,\mathrm{otherwise} $$ Updating rule: synchronous or asynchronous Consider the network stores $p$ pattern, $\xi_{i}^{\mu},\mathrm{for}\mu=1,\ldots p;i=1,\ldots N$ Setting $w_{ij}=\frac{1}{N}\sum_{\mu=1}^{p}\xi_{i}^{\mu}\xi_{j}^{\mu}$ ![image-20230827140827784](/BrainPy-course-notes/master_content/Notes.assets/image-20230827140827784.png) #### Energy space of Hopfield network $$ \begin{aligned} &amp;\text{Energy function: }E=-\frac{1}{2}\sum_{i,j}w_{ij}S_{i}S_{j}+\theta\sum_{i}S_{i} \\ &amp;\mathrm{Consider}S_{i}\mathrm{~is~updated},S_{i}(t+1)=sign[\sum_{j}w_{ij}S_{j}(t)-\theta] \\ &amp;\Delta E=E(t+1)-E(t)\\ &amp;=-[S_{i}(t+1)-S_{i}(t)]\sum_{j}w_{ij}S_{j}(t)+\theta\left[S_{i}(t+1)-S_{i}(t)\right] \\ &amp;=-[S_{i}(t+1)-S_{i}(t)][\sum_{j}w_{ij}S_{j}(t)-\theta] \\ &amp;\leq0 \end{aligned} $$ åŒæ ·æ¿€æ´»åŒæ ·patternçš„ç¥žç»å…ƒï¼Œ~å¸å¼•å­ #### Auto-associative memory in Hopfield Network A partial/noisy input can retrieve the related memory pattern ![image-20230827141253326](/BrainPy-course-notes/master_content/Notes.assets/image-20230827141253326.png) #### Persistent activity in working memory After the removal of external input, the neurons in the network encoding the stimulus continue to fire persistently. ![image-20230827141421796](/BrainPy-course-notes/master_content/Notes.assets/image-20230827141421796.png) ## Continuous Attractor Neural Network ### Neural coding #### Low-dimensional continuous feature ![image-20230827142520189](/BrainPy-course-notes/master_content/Notes.assets/image-20230827142520189.png) #### Continuous Attractor neural network ![image-20230827142606695](/BrainPy-course-notes/master_content/Notes.assets/image-20230827142606695.png) ### CANN: A rate-based recurrent circuit model $$ \begin{aligned}\tau\frac{\partial U(x,t)}{\partial t}&amp;=-U(x,t)+\rho\int f(x,x&apos;)r(x&apos;,t)dx&apos;+l^{ext}(1)\\r(x,t)&amp;=\frac{U^2(x,t)}{1+k\rho\int U^2(x,t)dx}\quad(2)\\J(x,x&apos;)&amp;=\frac{J_0}{\sqrt{2\pi}a}\exp\left[-\frac{(x-x&apos;)^2}{2a^2}\right](3)\end{aligned} $$ ré¢‘çŽ‡ï¼ŒJå¼ºåº¦ï¼ŒU decay #### A Continuous family of attractor states åšå¹³ç§»çš„æ”¹å˜ï¼Œå˜åŒ–ä¼šè¢«ä¿ç•™ï¼Œline attractorï¼Œå—åˆ°ç¼–ç è¿žç»­åˆºæ¿€ ![image-20230827143707784](/BrainPy-course-notes/master_content/Notes.assets/image-20230827143707784.png) #### Stability analysis derive continuous attractor dynamics åªéœ€è¦çœ‹åœ¨åŽŸå§‹çŠ¶æ€åŠ å…¥ä¸€ä¸ªå°é‡é¡¹ï¼Œå†ä»£å…¥å›ž Consider small fluctuations around a stationary state at z: Projecting $\delta U$ on the $i$th right eigenvector of $F(\delta U)_i(t)=(\delta U)_i(0)e^{-(1-\lambda _i)t/\tau}$ Two cases: - If $\lambda _i ## Computation with CANN ### Persistent activity for working memory When the global inhibition is not too strong, the network spontaneously hold bump activity: $$ k\frac{\tau}{\tau _v}$, Travelling wave ![image-20230827150543244](/BrainPy-course-notes/master_content/Notes.assets/image-20230827150543244.png) #### Levy flights vs. Brownian motion ![image-20230827150851309](/BrainPy-course-notes/master_content/Notes.assets/image-20230827150851309.png) #### LÃ©vy flights in ecology and human cogniDve behaviors ç”Ÿç‰©å­¦å¤§å¤šè¿åŠ¨æœä»Žlevy flights ### Noisy adaptation generates Levy flight in CANN ![image-20230827151343126](/BrainPy-course-notes/master_content/Notes.assets/image-20230827151343126.png) ### Time Delay in Neural Signal Transmission ![image-20230827151622032](/BrainPy-course-notes/master_content/Notes.assets/image-20230827151622032.png) ### Anticipatory Head Direction Signals in Anterior Thalamus æœ‰é¢„æµ‹ç­–ç•¥ï¼Œå®žçŽ°æŠµæ¶ˆä¿¡æ¯ä¼ é€’çš„delay CANNåŠ å…¥è´Ÿåé¦ˆæœºåˆ¶æ˜¯å¯ä»¥å®žçŽ°é¢„æµ‹çš„ ![image-20230827152731895](/BrainPy-course-notes/master_content/Notes.assets/image-20230827152731895.png) ### CANN with STP $$ \begin{gathered} \tau{\frac{\mathrm{d}U(x,t)}{\mathrm{d}t}} {\cal O}=-U(x,t)+\rho\int g^{+}(x)h(x^{\prime},t)J(x,x^{\prime})r(x^{\prime},t)dx^{\prime}+I^{ext}(x,t)(1) \\ \frac{dg(x,t)}{dt}=-\frac{g(x,t)}{\tau_{f}}+G(1-g^{-}(x))r(x^{\prime},t)\quad(2) \\ \frac{dh(x,t)}{dt}=\frac{1-h(x,t)}{\tau_{d}}-g^{+}(x)h(x,t)r(x^{\prime},t)\quad(3) \\ r(x,t)={\frac{U^{2}(x,t)}{1+k\rho\int U^{2}(x,t)dx}}\quad(4) \end{gathered} $$ ## Programming in BrainPy ### Customize a ring CANN in brainpy In simulations, we can not simulate a CANN encoding features ranging $(-\inf, \inf)$. Instead, we simulate a ring attractor network which encodes features ranging $(-\pi, \pi)$. Note that the distance on a ring should be: $$ dist_{ring}(x,x&apos;) = min(|x-x&apos;|,2\pi-|x-x&apos;|) $$ ![Image Name](https://cdn.kesci.com/upload/s01apgi89t.png?imageView2/0/w/320/h/320) ```python class CANN1D(bp.NeuGroupNS): def __init__(self, num, tau=1., k=8.1, a=0.5, A=10., J0=4., z_min=-bm.pi, z_max=bm.pi, **kwargs): super(CANN1D, self).__init__(size=num, **kwargs) # åˆå§‹åŒ–å‚æ•° self.tau = tau self.k = k self.a = a self.A = A self.J0 = J0 # åˆå§‹åŒ–ç‰¹å¾ç©ºé—´ç›¸å…³å‚æ•° self.z_min = z_min self.z_max = z_max self.z_range = z_max - z_min self.x = bm.linspace(z_min, z_max, num) self.rho = num / self.z_range self.dx = self.z_range / num # åˆå§‹åŒ–å˜é‡ self.u = bm.Variable(bm.zeros(num)) self.input = bm.Variable(bm.zeros(num)) self.conn_mat = self.make_conn(self.x) # è¿žæŽ¥çŸ©é˜µ # å®šä¹‰ç§¯åˆ†å‡½æ•° self.integral = bp.odeint(self.derivative) # å¾®åˆ†æ–¹ç¨‹ @property def derivative(self): du = lambda u, t, Irec, Iext: (-u + Irec + Iext) / self.tau return du # å°†è·ç¦»è½¬æ¢åˆ°[-z_range/2, z_range/2)ä¹‹é—´ def dist(self, d): d = bm.remainder(d, self.z_range) d = bm.where(d &amp;gt; 0.5 * self.z_range, d - self.z_range, d) return d # è®¡ç®—è¿žæŽ¥çŸ©é˜µ def make_conn(self, x): assert bm.ndim(x) == 1 d = self.dist(x - x[:, None]) # è·ç¦»çŸ©é˜µ Jxx = self.J0 * bm.exp( -0.5 * bm.square(d / self.a)) / (bm.sqrt(2 * bm.pi) * self.a) return Jxx # èŽ·å–å„ä¸ªç¥žç»å…ƒåˆ°poså¤„ç¥žç»å…ƒçš„è¾“å…¥ def get_stimulus_by_pos(self, pos): return self.A * bm.exp(-0.25 * bm.square(self.dist(self.x - pos) / self.a)) def update(self, x=None): _t = bp.share[&apos;t&apos;] u2 = bm.square(self.u) r = u2 / (1.0 + self.k * bm.sum(u2)) Irec = bm.dot(self.conn_mat, r) self.u[:] = self.integral(self.u, _t,Irec, self.input) self.input[:] = 0. # é‡ç½®å¤–éƒ¨ç”µæµ ``` ### Simulate the persistent activity of CANN after the removal of external input ```python def Persistent_Activity(k=0.1,J0=1.): # ç”ŸæˆCANN cann = CANN1D(num=512, k=k,J0=J0) # ç”Ÿæˆå¤–éƒ¨åˆºæ¿€ï¼Œä»Žç¬¬2åˆ°12msï¼ŒæŒç»­10ms dur1, dur2, dur3 = 2., 10., 10. I1 = cann.get_stimulus_by_pos(0.) Iext, duration = bp.inputs.section_input(values=[0., I1, 0.], durations=[dur1, dur2, dur3], return_length=True) noise_level = 0.1 noise = bm.random.normal(0., noise_level, (int(duration / bm.get_dt()), len(I1))) Iext += noise # è¿è¡Œæ•°å€¼æ¨¡æ‹Ÿ runner = bp.DSRunner(cann, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;]) runner.run(duration) # å¯è§†åŒ– def plot_response(t): fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann.x, I, label=&apos;Iext&apos;) ax.plot(cann.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() # plt.savefig(f&apos;CANN_t={t}.pdf&apos;, transparent=True, dpi=500) plot_response(t=10.) plot_response(t=20.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=1, frame_delay=40, show=True, ) plt.show() Persistent_Activity(k=0.1) ``` ### Simulate the tracking behavior of CANN ```python def smooth_tracking(): cann = CANN1D(num=512, k=8.1) # å®šä¹‰éšæ—¶é—´å˜åŒ–çš„å¤–éƒ¨åˆºæ¿€ v_ext = 1e-3 dur1, dur2, dur3 = 10., 10., 20 num1 = int(dur1 / bm.get_dt()) num2 = int(dur2 / bm.get_dt()) num3 = int(dur3 / bm.get_dt()) position = bm.zeros(num1 + num2 + num3) position[num1: num1 + num2] = bm.linspace(0., 1.5 * bm.pi, num2) position[num1 + num2: ] = 1.5 * bm.pi position = position.reshape((-1, 1)) Iext = cann.get_stimulus_by_pos(position) # è¿è¡Œæ¨¡æ‹Ÿ runner = bp.DSRunner(cann, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;]) runner.run(dur1 + dur2 + dur3) # å¯è§†åŒ– def plot_response(t, extra_fun=None): fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann.x, I, label=&apos;Iext&apos;) ax.plot(cann.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() if extra_fun: extra_fun() # plt.savefig(f&apos;CANN_tracking_t={t}.pdf&apos;, transparent=True, dpi=500) plot_response(t=10.) def f(): plt.annotate(&apos;&apos;, xy=(1.5, 10), xytext=(0.5, 10), arrowprops=dict(arrowstyle=&quot;-&amp;gt;&quot;)) plot_response(t=15., extra_fun=f) def f(): plt.annotate(&apos;&apos;, xy=(-2, 10), xytext=(-3, 10), arrowprops=dict(arrowstyle=&quot;-&amp;gt;&quot;)) plot_response(t=20., extra_fun=f) plot_response(t=30.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=5, frame_delay=50, show=True, ) plt.show() smooth_tracking() ``` ### Customize a CANN with SFASimulate the spontaneous traveling wave ```python class CANN1D_SFA(bp.NeuGroupNS): def __init__(self, num, m = 0.1, tau=1., tau_v=10., k=8.1, a=0.5, A=10., J0=4., z_min=-bm.pi, z_max=bm.pi, **kwargs): super(CANN1D_SFA, self).__init__(size=num, **kwargs) # åˆå§‹åŒ–å‚æ•° self.tau = tau self.tau_v = tau_v #time constant of SFA self.k = k self.a = a self.A = A self.J0 = J0 self.m = m #SFA strength # åˆå§‹åŒ–ç‰¹å¾ç©ºé—´ç›¸å…³å‚æ•° self.z_min = z_min self.z_max = z_max self.z_range = z_max - z_min self.x = bm.linspace(z_min, z_max, num) self.rho = num / self.z_range self.dx = self.z_range / num # åˆå§‹åŒ–å˜é‡ self.u = bm.Variable(bm.zeros(num)) self.v = bm.Variable(bm.zeros(num)) #SFA current self.input = bm.Variable(bm.zeros(num)) self.conn_mat = self.make_conn(self.x) # è¿žæŽ¥çŸ©é˜µ # å®šä¹‰ç§¯åˆ†å‡½æ•° self.integral = bp.odeint(self.derivative) # å¾®åˆ†æ–¹ç¨‹ @property def derivative(self): du = lambda u, t, v, Irec, Iext: (-u + Irec + Iext-v) / self.tau dv = lambda v, t, u: (-v + self.m*u) / self.tau_v return bp.JointEq([du, dv]) # å°†è·ç¦»è½¬æ¢åˆ°[-z_range/2, z_range/2)ä¹‹é—´ def dist(self, d): d = bm.remainder(d, self.z_range) d = bm.where(d &amp;gt; 0.5 * self.z_range, d - self.z_range, d) return d # è®¡ç®—è¿žæŽ¥çŸ©é˜µ def make_conn(self, x): assert bm.ndim(x) == 1 d = self.dist(x - x[:, None]) # è·ç¦»çŸ©é˜µ Jxx = self.J0 * bm.exp( -0.5 * bm.square(d / self.a)) / (bm.sqrt(2 * bm.pi) * self.a) return Jxx # èŽ·å–å„ä¸ªç¥žç»å…ƒåˆ°poså¤„ç¥žç»å…ƒçš„è¾“å…¥ def get_stimulus_by_pos(self, pos): return self.A * bm.exp(-0.25 * bm.square(self.dist(self.x - pos) / self.a)) def update(self, x=None): u2 = bm.square(self.u) r = u2 / (1.0 + self.k * bm.sum(u2)) Irec = bm.dot(self.conn_mat, r) u, v = self.integral(self.u, self.v, bp.share[&apos;t&apos;],Irec, self.input) self.u[:] = bm.where(u&amp;gt;0,u,0) self.v[:] = v self.input[:] = 0. # é‡ç½®å¤–éƒ¨ç”µæµ ``` ### Simulate the spontaneous traveling wave ```python def traveling_wave(num=512,m=0.1,k=0.1): # ç”ŸæˆCANN cann_sfa = CANN1D_SFA(num=num, m=m,k=k) # ç”Ÿæˆå¤–éƒ¨åˆºæ¿€ dur = 1000. noise_level = 0.1 Iext = bm.random.normal(0., noise_level, (int(dur / bm.get_dt()), num)) duration = dur # è¿è¡Œæ•°å€¼æ¨¡æ‹Ÿ runner = bp.DSRunner(cann_sfa, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;]) runner.run(duration) # å¯è§†åŒ– def plot_response(t): fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann_sfa.x, I, label=&apos;Iext&apos;) ax.plot(cann_sfa.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() # plt.savefig(f&apos;CANN_t={t}.pdf&apos;, transparent=True, dpi=500) plot_response(t=100.) plot_response(t=150.) plot_response(t=200.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann_sfa.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann_sfa.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=1, frame_delay=40, show=True, ) plt.show() traveling_wave(num=512,m=0.5,k=0.1) ``` ### Simulate the anticipative tracking ```python def anticipative_tracking(m=10,v_ext=6*1e-3): cann_sfa = CANN1D_SFA(num=512, m=m) # å®šä¹‰éšæ—¶é—´å˜åŒ–çš„å¤–éƒ¨åˆºæ¿€ v_ext = v_ext dur1, dur2, = 10., 1000. num1 = int(dur1 / bm.get_dt()) num2 = int(dur2 / bm.get_dt()) position = np.zeros(num1 + num2) for i in range(num2): pos = position[i+num1-1]+v_ext*bm.dt # the periodical boundary pos = np.where(pos&amp;gt;np.pi, pos-2*np.pi, pos) pos = np.where(pos 0.5 * self.z_range, d - self.z_range, d) return d # è®¡ç®—è¿žæŽ¥çŸ©é˜µ def make_conn(self, x): assert bm.ndim(x) == 1 d = self.dist(x - x[:, None]) # è·ç¦»çŸ©é˜µ Jxx = self.J0 * bm.exp( -0.5 * bm.square(d / self.a)) / (bm.sqrt(2 * bm.pi) * self.a) return Jxx # èŽ·å–å„ä¸ªç¥žç»å…ƒåˆ°poså¤„ç¥žç»å…ƒçš„è¾“å…¥ def get_stimulus_by_pos(self, pos): return self.A * bm.exp(-0.25 * bm.square(self.dist(self.x - pos) / self.a)) def update(self, x=None): u2 = bm.square(self.u) r = u2 / (1.0 + self.k * bm.sum(u2)) Irec = bm.dot(self.conn_mat, (self.g + self.G * (1 - self.g))*self.h*r) u, g, h = self.integral(u=self.u, g=self.g, h=self.h, t=bp.share[&apos;t&apos;], Irec=Irec, Iext=self.input, r=r, dt=bm.dt) self.u[:] = bm.where(u&amp;gt;0,u,0) self.g.value = g self.h.value = h self.input[:] = 0. # é‡ç½®å¤–éƒ¨ç”µæµ ``` ### Simulate traveling wave in CANN with STP ```python def traveling_wave_STP(num=512,k=0.1,J0=12.,tau_d=1000,tau_f=1.,G=0.9): # ç”ŸæˆCANN cann_stp = CANN1D_STP(num=num, k=k,tau_d=tau_d,tau_f=tau_f,G=G, J0=J0) # ç”Ÿæˆå¤–éƒ¨åˆºæ¿€ dur = 1000. noise_level = 0.1 Iext = bm.random.normal(0., noise_level, (int(dur / bm.get_dt()), num)) duration = dur # è¿è¡Œæ•°å€¼æ¨¡æ‹Ÿ runner = bp.DSRunner(cann_stp, inputs=[&apos;input&apos;, Iext, &apos;iter&apos;], monitors=[&apos;u&apos;,&apos;g&apos;,&apos;h&apos;]) runner.run(duration) fig,ax = plt.subplots(figsize=(3,3)) u = bm.as_numpy(runner.mon.u) max_index = np.argmax(u[1000,:]) print(max_index) ax.plot(runner.mon.g[:,max_index],label=&apos;g&apos;) ax.plot(runner.mon.h[:,max_index],label=&apos;h&apos;) ax.legend() # å¯è§†åŒ– def plot_response(t): fig, gs = bp.visualize.get_figure(1, 1, 3, 3) ax = fig.add_subplot(gs[0, 0]) ts = int(t / bm.get_dt()) I, u = Iext[ts], runner.mon.u[ts] ax.plot(cann_stp.x, I, label=&apos;Iext&apos;) ax.plot(cann_stp.x, u, linestyle=&apos;dashed&apos;, label=&apos;U&apos;) ax.set_title(r&apos;$t$&apos; + &apos; = {} ms&apos;.format(t)) ax.set_xlabel(r&apos;$x$&apos;) ax.spines[&apos;top&apos;].set_visible(False) ax.spines[&apos;right&apos;].set_visible(False) ax.legend() plot_response(t=100.) plot_response(t=200.) plot_response(t=300.) bp.visualize.animate_1D( dynamical_vars=[{&apos;ys&apos;: runner.mon.u, &apos;xs&apos;: cann_stp.x, &apos;legend&apos;: &apos;u&apos;}, {&apos;ys&apos;: Iext, &apos;xs&apos;: cann_stp.x, &apos;legend&apos;: &apos;Iext&apos;}], frame_step=1, frame_delay=40, show=True, ) plt.show() traveling_wave_STP(G=0.5,tau_d=50) ``` # Decision-Making Network ## LIP -&amp;gt; Decision-Making ### Coherent motion task åˆ¤æ–­éšæœºç‚¹(å¤§éƒ¨åˆ†ç‚¹)çš„è¿åŠ¨æœå‘ ![image-20230828100425871](/BrainPy-course-notes/master_content/Notes.assets/image-20230828100425871.png) coherenceå½±å“ä»»åŠ¡çš„éš¾åº¦ 0%éš¾ï¼Œ100%ç®€å• ![image-20230828100516123](/BrainPy-course-notes/master_content/Notes.assets/image-20230828100516123.png) ç¼–ç å†³ç­–çš„å“åº”ï¼Œä¸æ˜¯è¿åŠ¨ ### Reaction Time vs. Fixed Duration coherenceè¶Šé«˜ï¼Œååº”æ—¶é—´è¶ŠçŸ­ Fixed Durationå¤šäº†Delay time ![image-20230828100658772](/BrainPy-course-notes/master_content/Notes.assets/image-20230828100658772.png) å®žéªŒè®¾è®¡çº¯ç²¹æŠŠdecision-makingç»™æå–å‡ºæ¥ #### Effect of Difficulty coherenceè¶Šå¤§ï¼Œååº”æ—¶é—´æ˜¯è¶ŠçŸ­ï¼Œsingle neuronå¾ˆéš¾åšåˆ°è¿™ä¹ˆçŸ­çš„decision-makingï¼Œè€ƒè™‘è¦å»ºæ¨¡çš„å› ç´  ![image-20230828101103008](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101103008.png) ![image-20230828101058059](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101058059.png) #### Response of MT Neurons è®°å½•MTçš„ç¥žç»å…ƒï¼Œå¯¹è¿™ç§è¿åŠ¨çš„æœå‘åˆºæ¿€è¿›è¡Œç¼–ç  çº¿æ€§ç¼–ç coherenceè¿åŠ¨å¼ºåº¦çš„æ–¹å‘ åšå†³ç­–åœ¨å®ƒçš„ä¸‹æ¸¸ ![image-20230828101303674](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101303674.png) #### Response of LIP Neurons MTçš„ä¸‹æ¸¸æ‰¾åˆ°LIPçš„ç¥žç»å…ƒ çˆ¬å‡åˆ°ä¸€å®šé«˜åº¦å†åšé€‰æ‹© coherenceä¸Žçˆ¬å‡çš„æ–œçŽ‡ä¹Ÿä¼šæœ‰å½±å“ï¼Œä»»åŠ¡è¶Šéš¾ï¼Œçˆ¬å‡æ–œçŽ‡è¶Šå° ![image-20230828101609881](/BrainPy-course-notes/master_content/Notes.assets/image-20230828101609881.png) ### Ramping-to-threshold(perfect integrator) Model $$ \begin{aligned}\frac{dR}{dt}=I_A-I_B+\text{noise},\quad R(t)&amp;=(I_A-I_B)t+\int_0^tdt\text{noise}.\\\tau_\text{network}&amp;=\infty!\end{aligned} $$ ä¸¤ç§é€‰æ‹©ç§¯åˆ†æ±‚å’Œåšç§¯ç´¯ï¼Œç­‰åˆ°é˜ˆå€¼åšå†³ç­– Accumulates information (evidence) -&amp;gt; Ramping ç›´æŽ¥ä¿å­˜ä¿¡æ¯ï¼Œæ²¡æœ‰ç‰¹åˆ«å¥½çš„ç”Ÿç‰©å¯¹åº” ## A Spiking Network of DM ### A cortical microcircuit model ![image-20230828103055151](Notes.assets/image-20230828103055151.png) A=Upward motion B=Downward motion 2-population excitatory neurons (integrate-and-fire neurons driven by Poisson input) Slow reverberatory excitation mediated by the NMDA receptors at recurrent synapses AMPA receptors ($\tau _{syn}=$1 - 3 ms) NMDA receptors ($\tau _{syn}=$ 50 - 100 ms). ä¸¤ç¾¤ç¥žç»å…ƒåˆ†åˆ«åšä¸åŒçš„é€‰æ‹©ï¼Œä¸Žè‡ªå·±å¯¹æ–¹éƒ½æœ‰è¿žæŽ¥ NMDA ç¼“æ…¢çš„ä¿¡å·ä½¿å¾—æœ‰æ…¢æ…¢å¢žé•¿çš„rampingçš„è¿‡ç¨‹ interneuronsçš„backwardæœ‰æŠ‘åˆ¶ä½œç”¨ #### Coherence-Dependent Input çº¿æ€§ç¼–ç è¿åŠ¨æœå‘çš„ä¿¡æ¯ï¼Œcoherenceå¼ºåº¦å½±å“firing rateï¼Œä¸€ç³»åˆ—æ³Šæ¾è¿‡ç¨‹ï¼ŒåŒæ—¶è¿˜æœ‰noiseã€‚ æœ¬èº«ä¸¤ç§ä¿¡æ¯è¿˜æ˜¯æœ‰å·®å¼‚ ![image-20230828104054275](/BrainPy-course-notes/master_content/Notes.assets/image-20230828104054275.png) #### Duality of this model ä¸åŒcoherenceçš„ç¥žç»å…ƒå“åº” ![image-20230828104432061](/BrainPy-course-notes/master_content/Notes.assets/image-20230828104432061.png) ä¸¤ä¸ªgroupä¼šç«žäº‰ï¼Œå½“æœ‰ä¸€ä¸ªgroupè¾¾åˆ°20%ï¼Œè¿›å…¥è¿™ä¸ªçª—å£ï¼Œå°±ä¼šç›´æŽ¥å‘æ”¾ä¸ŠåŽ» Spontaneous symmetry breaking and stochastic decision making ![image-20230828104600840](/BrainPy-course-notes/master_content/Notes.assets/image-20230828104600840.png) ## Simulation of Spiking DM ### A Cortical Microcircuit Model ç”¨ä¸¤ä¸ªcoherenceç”Ÿæˆå‡ºæ¥çš„åºåˆ— ![image-20230828110300576](/BrainPy-course-notes/master_content/Notes.assets/image-20230828110300576.png) $$ \begin{gathered}C_m\frac{dV(t)}{dt}=-g_L(V(t)-V_L)-I_{syn}(t)\\I_{syn}(t)=I_{\mathrm{ext},\mathrm{AMPA}}\left(t\right)+I_{\mathrm{rec},AMPA}(t)+I_{\mathrm{rec},NMDA}(t)+I_{\mathrm{rec},\mathrm{GABA}}(t)\end{gathered} $$ $$ \begin{gathered} I_{\mathrm{ext},\mathrm{AMPA}}\left(t\right)=g_{\mathrm{ext},\mathrm{AMPA}}\left(V(t)-V_{E}\right)s^{\mathrm{ext},\mathrm{AMPA}}\left(t\right) \\ I_{\mathrm{rec},\mathrm{AMP}\Lambda}\left(t\right)=g_{\mathrm{rec},\mathrm{AMP}\Lambda}\left(V(t)-V_{E}\right)\sum_{j=1}^{Ce}w_{j}s_{j}^{AMPA}(t) \\ I_{\mathrm{rec},\mathrm{NMDA}}\left(t\right)=\frac{g_{\mathrm{NMDA}}(V(t)-V_{E})}{\left(1+\left[\mathrm{Mg}^{2+}\right]\exp(-0.062V(t))/3.57\right)}\sum_{j=1}^{\mathrm{C_E}}w_{j}s_{j}^{\mathrm{NMDA}}\left(t\right) \\ I_\mathrm{rec,GABA}(t)=g_\mathrm{GABA}(V(t)-V_l)\sum_{j=1}^{C_1}s_j^\mathrm{GABA}(t) \end{gathered} $$ $$ w_j=\left\{\begin{matrix}w_+&amp;gt;1,\\w_-E/I conn self.I2B = AMPA(self.I, self.B, &apos;all2all&apos;, 0.5, g_I2E_GABAa, tau=5., E=-70.) self.I2A = AMPA(self.I, self.A, &apos;all2all&apos;, 0.5, g_I2E_GABAa, tau=5., E=-70.) self.I2N = AMPA(self.I, self.N, &apos;all2all&apos;, 0.5, g_I2E_GABAa, tau=5., E=-70.) self.I2I = AMPA(self.I, self.I, &apos;all2all&apos;, 0.5, g_I2I_GABAa, tau=5., E=-70.) # define external projections #### TO DO!!!! self.noise2B = AMPA(self.noise_B, self.B, &apos;one2one&apos;, None, g_ext2E_AMPA, tau=2., E=0.) self.noise2A = AMPA(self.noise_A, self.A, &apos;one2one&apos;, None, g_ext2E_AMPA, tau=2., E=0.) self.noise2N = AMPA(self.noise_N, self.N, &apos;one2one&apos;, None, g_ext2E_AMPA, tau=2., E=0.) self.noise2I = AMPA(self.noise_I, self.I, &apos;one2one&apos;, None, g_ext2I_AMPA, tau=2., E=0.) ``` ```python class Tool: def __init__(self, pre_stimulus_period=100., stimulus_period=1000., delay_period=500.): self.pre_stimulus_period = pre_stimulus_period self.stimulus_period = stimulus_period self.delay_period = delay_period self.freq_variance = 10. self.freq_interval = 50. self.total_period = pre_stimulus_period + stimulus_period + delay_period def generate_freqs(self, mean): # stimulus period n_stim = int(self.stimulus_period / self.freq_interval) n_interval = int(self.freq_interval / bm.get_dt()) freqs_stim = np.random.normal(mean, self.freq_variance, (n_stim, 1)) freqs_stim = np.tile(freqs_stim, (1, n_interval)).flatten() # pre stimulus period freqs_pre = np.zeros(int(self.pre_stimulus_period / bm.get_dt())) # post stimulus period freqs_delay = np.zeros(int(self.delay_period / bm.get_dt())) all_freqs = np.concatenate([freqs_pre, freqs_stim, freqs_delay], axis=0) return bm.asarray(all_freqs) def visualize_results(self, mon, IA_freqs, IB_freqs, t_start=0., title=None): fig, gs = bp.visualize.get_figure(4, 1, 3, 10) axes = [fig.add_subplot(gs[i, 0]) for i in range(4)] ax = axes[0] bp.visualize.raster_plot(mon[&apos;ts&apos;], mon[&apos;A.spike&apos;], markersize=1, ax=ax) if title: ax.set_title(title) ax.set_ylabel(&quot;Group A&quot;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax = axes[1] bp.visualize.raster_plot(mon[&apos;ts&apos;], mon[&apos;B.spike&apos;], markersize=1, ax=ax) ax.set_ylabel(&quot;Group B&quot;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax = axes[2] rateA = bp.measure.firing_rate(mon[&apos;A.spike&apos;], width=10.) rateB = bp.measure.firing_rate(mon[&apos;B.spike&apos;], width=10.) ax.plot(mon[&apos;ts&apos;], rateA, label=&quot;Group A&quot;) ax.plot(mon[&apos;ts&apos;], rateB, label=&quot;Group B&quot;) ax.set_ylabel(&apos;Population activity [Hz]&apos;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax.legend() ax = axes[3] ax.plot(mon[&apos;ts&apos;], IA_freqs, label=&quot;group A&quot;) ax.plot(mon[&apos;ts&apos;], IB_freqs, label=&quot;group B&quot;) ax.set_ylabel(&quot;Input activity [Hz]&quot;) ax.set_xlim(t_start, self.total_period + 1) ax.axvline(self.pre_stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle=&apos;dashed&apos;) ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle=&apos;dashed&apos;) ax.legend() ax.set_xlabel(&quot;Time [ms]&quot;) plt.show() ``` ```python tool = Tool() net = DecisionMakingNet() mu0 = 40. coherence = 25.6 IA_freqs = tool.generate_freqs(mu0 + mu0 / 100. * coherence) IB_freqs = tool.generate_freqs(mu0 - mu0 / 100. * coherence) def give_input(): i = bp.share[&apos;i&apos;] net.IA.freqs[0] = IA_freqs[i] net.IB.freqs[0] = IB_freqs[i] runner = bp.DSRunner(net, inputs=give_input, monitors=[&apos;A.spike&apos;, &apos;B.spike&apos;]) runner.run(tool.total_period) tool.visualize_results(runner.mon, IA_freqs, IB_freqs) ``` ### Results ![image-20230828112245950](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112245950.png) #### Stochastic Decision Making ![image-20230828112253619](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112253619.png) ## A Rate Network of DM ### Reduced Model åŒ–ç®€åˆ°åªæœ‰ä¸¤ç¾¤ç¥žç»å…ƒï¼ŒåªæŽ¥å—å¤–ç•Œè¾“å…¥ä¿¡å·ï¼Œäº’ç›¸å½±å“å¯¹æ–¹ ![image-20230828112326267](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112326267.png) Synaptic variables $$ \begin{gathered} \frac{dS_{1}}{dt} =F(x_1)\gamma(1-S_1)-S_1/\tau_s \\ \frac{dS_2}{dt} =F(x_2)\gamma(1-S_2)-S_2/\tau_s \end{gathered} $$ Input current to each population $$ \begin{gathered} x_{1} =J_{E}S_{1}+J_{I}S_{2}+I_{0}+I_{noise1}+J_{\text{ext }\mu_{1}} \\ x_{2} =J_{E}S_{2}+J_{I}S_{1}+I_{0}+I_{noise2}+J_{\mathrm{ext}}\mu_{2} \end{gathered} $$ Background input $$ I_0+I_{noise}\\ \begin{gathered} dI_{noise1} =-I_{noise1}\frac{dt}{\tau_{0}}+\sigma dW \\ dI_{noise2} =-I_{noise2}\frac{dt}{\tau_{0}}+\sigma dW \end{gathered} $$ Firing rates $$ r_i=F(x_i)=\frac{ax_i-b}{1-\exp(-d(ax_i-b))} $$ Coherence-dependent inputs $$ \begin{array}{l}\mu_1=\mu_0\big(1+c&apos;/100\big)\\\mu_2=\mu_0\big(1-c&apos;/100\big)\end{array} $$ $$ \begin{aligned}&amp;\gamma,a,b,d,J_E,J_I,J_{\mathrm{ext}},I_0,\mu_0,\tau_{\mathrm{AMPA}},\sigma_{\mathrm{noise}}\\&amp;\text{are fixed parameters.}\end{aligned} $$ ```python class DecisionMakingRateModel(bp.dyn.NeuGroup): def __init__(self, size, coherence, JE=0.2609, JI=0.0497, Jext=5.2e-4, I0=0.3255, gamma=6.41e-4, tau=100., tau_n=2., sigma_n=0.02, a=270., b=108., d=0.154, noise_freq=2400., method=&apos;exp_auto&apos;, **kwargs): super(DecisionMakingRateModel, self).__init__(size, **kwargs) # åˆå§‹åŒ–å‚æ•° self.coherence = coherence self.JE = JE self.JI = JI self.Jext = Jext self.I0 = I0 self.gamma = gamma self.tau = tau self.tau_n = tau_n self.sigma_n = sigma_n self.a = a self.b = b self.d = d # åˆå§‹åŒ–å˜é‡ self.s1 = bm.Variable(bm.zeros(self.num) + 0.15) self.s2 = bm.Variable(bm.zeros(self.num) + 0.15) self.r1 = bm.Variable(bm.zeros(self.num)) self.r2 = bm.Variable(bm.zeros(self.num)) self.mu0 = bm.Variable(bm.zeros(self.num)) self.I1_noise = bm.Variable(bm.zeros(self.num)) self.I2_noise = bm.Variable(bm.zeros(self.num)) # å™ªå£°è¾“å…¥çš„ç¥žç»å…ƒ self.noise1 = bp.dyn.PoissonGroup(self.num, freqs=noise_freq) self.noise2 = bp.dyn.PoissonGroup(self.num, freqs=noise_freq) # å®šä¹‰ç§¯åˆ†å‡½æ•° self.integral = bp.odeint(self.derivative, method=method) @property def derivative(self): return bp.JointEq([self.ds1, self.ds2, self.dI1noise, self.dI2noise]) def ds1(self, s1, t, s2, mu0): I1 = self.Jext * mu0 * (1. + self.coherence / 100.) x1 = self.JE * s1 - self.JI * s2 + self.I0 + I1 + self.I1_noise r1 = (self.a * x1 - self.b) / (1. - bm.exp(-self.d * (self.a * x1 - self.b))) return - s1 / self.tau + (1. - s1) * self.gamma * r1 def ds2(self, s2, t, s1, mu0): I2=self.Jext*mu0*(1.- self.coherence / 100.) x2 = self.JE * s2 - self.JI * s1 + self.I0 + I2 + self.I2_noise r2 = (self.a * x2 - self.b) / (1. - bm.exp(-self.d * (self.a * x2 - self.b))) return - s2 / self.tau + (1. - s2) * self.gamma * r2 def dI1noise(self, I1_noise, t, noise1): return (- I1_noise + noise1.spike * bm.sqrt(self.tau_n * self.sigma_n * self.sigma_n)) / self.tau_n def dI2noise(self, I2_noise, t, noise2): return (- I2_noise + noise2.spike * bm.sqrt(self.tau_n * self.sigma_n * self.sigma_n)) / self.tau_n def update(self, tdi): # æ›´æ–°å™ªå£°ç¥žç»å…ƒä»¥äº§ç”Ÿæ–°çš„éšæœºå‘æ”¾ self.noise1.update(tdi) self.noise2.update(tdi) # æ›´æ–°s1ã€s2ã€I1_noiseã€I2_noise integral = self.integral(self.s1, self.s2, self.I1_noise, self.I2_noise, tdi.t, mu0=self.mu0, noise1=self.noise1, noise2=self.noise2, dt=tdi.dt) self.s1.value, self.s2.value, self.I1_noise.value, self.I2_noise.value = integral # ç”¨æ›´æ–°åŽçš„s1ã€s2è®¡ç®—r1ã€r2 I1 = self.Jext * self.mu0 * (1. + self.coherence / 100.) x1 = self.JE * self.s1 + self.JI * self.s2 + self.I0 + I1 + self.I1_noise self.r1.value = (self.a * x1 - self.b) / (1. - bm.exp(-self.d * (self.a * x1 - self.b))) I2 = self.Jext * self.mu0 * (1. - self.coherence / 100.) x2 = self.JE * self.s2 + self.JI * self.s1 + self.I0 + I2 + self.I2_noise self.r2.value = (self.a * x2 - self.b) / (1. - bm.exp(-self.d * (self.a * x2 - self.b))) # é‡ç½®å¤–éƒ¨è¾“å…¥ self.mu0[:] = 0. ``` ```python # å®šä¹‰å„ä¸ªé˜¶æ®µçš„æ—¶é•¿ pre_stimulus_period, stimulus_period, delay_period = 100., 2000., 500. # ç”Ÿæˆæ¨¡åž‹ dmnet = DecisionMakingRateModel(1, coherence=25.6, noise_freq=2400.) # å®šä¹‰ç”µæµéšæ—¶é—´çš„å˜åŒ– inputs, total_period = bp.inputs.constant_input([(0., pre_stimulus_period), (20., stimulus_period), (0., delay_period)]) # è¿è¡Œæ•°å€¼æ¨¡æ‹Ÿ runner = bp.DSRunner(dmnet, monitors=[&apos;s1&apos;, &apos;s2&apos;, &apos;r1&apos;, &apos;r2&apos;], inputs=(&apos;mu0&apos;, inputs, &apos;iter&apos;)) runner.run(total_period) # å¯è§†åŒ– fig, gs = plt.subplots(2, 1, figsize=(6, 6), sharex=&apos;all&apos;) gs[0].plot(runner.mon.ts, runner.mon.s1, label=&apos;s1&apos;) gs[0].plot(runner.mon.ts, runner.mon.s2, label=&apos;s2&apos;) gs[0].axvline(pre_stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[0].axvline(pre_stimulus_period + stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[0].set_ylabel(&apos;gating variable $s$&apos;) gs[0].legend() gs[1].plot(runner.mon.ts, runner.mon.r1, label=&apos;r1&apos;) gs[1].plot(runner.mon.ts, runner.mon.r2, label=&apos;r2&apos;) gs[1].axvline(pre_stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[1].axvline(pre_stimulus_period + stimulus_period, 0., 1., linestyle=&apos;dashed&apos;, color=u&apos;#444444&apos;) gs[1].set_xlabel(&apos;t (ms)&apos;) gs[1].set_ylabel(&apos;firing rate $r$&apos;) gs[1].legend() plt.subplots_adjust(hspace=0.1) plt.show() ``` ### Results ![image-20230828112555018](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112555018.png) ## Phase Plane Analysis å› ä¸ºåªæœ‰ä¸¤ä¸ªvariable ### Model implementation ```python @bp.odeint def int_s1(s1, t, s2, coh=0.5, mu=20.): x1 = JE * s1 + JI * s2 + Ib + JAext * mu * (1. + coh/100) r1 = (a * x1 - b) / (1. - bm.exp(-d * (a * x1 - b))) return - s1 / tau + (1. - s1) * gamma * r1 @bp.odeint def int_s2(s2, t, s1, coh=0.5, mu=20.): x2 = JE * s2 + JI * s1 + Ib + JAext * mu * (1. - coh/100) r2 = (a * x2 - b) / (1. - bm.exp(-d * (a * x2 - b))) return - s2 / tau + (1. - s2) * gamma * r2 ``` ### Without / with input ![image-20230828112709355](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112709355.png) åªå—æ‰°åŠ¨å½±å“ï¼Œæœ‰inputåŽä¸­é—´å˜å¾—ä¸ç¨³å®šï¼Œä½†å¦‚æžœå·²ç»é€‰æ‹©ï¼Œç½‘ç»œä»ç»´æŒä¹‹å‰é€‰æ‹©çš„ç»“æžœ ![image-20230828112811394](/BrainPy-course-notes/master_content/Notes.assets/image-20230828112811394.png) ### Coherence ç¨³å®šç‚¹å¯¹ç½‘ç»œçš„æ‹‰ä¼¸æ›´å¼º ![image-20230828113031946](/BrainPy-course-notes/master_content/Notes.assets/image-20230828113031946.png) ![image-20230828113009219](/BrainPy-course-notes/master_content/Notes.assets/image-20230828113009219.png) # Reservoir Computing å¼•å…¥è®­ç»ƒ å€¾å‘äºŽä½¿ç”¨RNN ![image-20230828140305956](/BrainPy-course-notes/master_content/Notes.assets/image-20230828140305956.png) Connecting different units $$ \begin{aligned} &amp;\textsf{Input to unit i from unit j:} \\ &amp;&amp;&amp;I_{j\rightarrow i}=J_{ij}r_{j}(t) \\ &amp;\textsf{Total input to unit i:} \\ &amp;&amp;&amp;I_{i}^{(tot)}=\sum_{j=1}^{N}J_{ij}r_{j}(t)+I_{i}^{(ext)} \end{aligned} $$ $$ \textsf{Activation of unit i:} \\ \tau\frac{dx_{i}}{dt}=-x_{i}+\sum_{j=1}^{N}J_{ij}\frac{\phi(x_{j})}{1}+I_{i}^{(ext)}(t) $$ è®­ç»ƒèŒƒå¼ ![image-20230828140707998](/BrainPy-course-notes/master_content/Notes.assets/image-20230828140707998.png) ## Echo state machine ### Echo state machine ç±»ä¼¼äººå·¥ç¥žç»ç½‘ç»œRNNï¼Œå¯ä»¥å¤„ç†temporalä¿¡æ¯ ![image-20230828140937455](/BrainPy-course-notes/master_content/Notes.assets/image-20230828140937455.png) $$ \begin{aligned} &amp;\mathbf{x}(n+1) =f(\mathbf{W}^{\mathrm{in}}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n)+\mathbf{W}^{\mathrm{back}}\mathbf{y}(n)) \\ &amp;\mathbf{y}(n+1) =\mathbf{W}^{\mathrm{out}}(\mathbf{u}(n+1),\mathbf{x}(n+1),\mathbf{y}(n)) \end{aligned} $$ For an RNN, the state of its internal neurons reflects the historical information of the external inputs. åæ˜ çš„echoçš„åŽ†å²ä¿¡æ¯ï¼Œå”¯ä¸€ä¾èµ–åŽ†å²ä¿¡æ¯ Assuming that the updates of the network are discrete, the external input at the ð‘›th moment is u(ð‘›) and the neuron state is x(ð‘›), then x(ð‘›) should be determined by u(ð‘›), u(ð‘› - 1), ... uniquely determined. At this point, x(ð‘›) can be regarded as an &quot;echo&quot; of the historical input signals. ä¸éœ€è¦è®­ç»ƒconnection ### Echo state machine with leaky integrator æœ‰ä¸€ä¸ªleakyé¡¹ï¼Œå¼•å…¥decay ### $$ \begin{aligned}\hat{h}(n)=\tanh(W^{in}x(n)+W^{rec}h(n-1)+W^{fb}y(n-1)+b^{rec})\\h(n)=(1-\alpha)x(n-1)+\alpha\hat{h}(n)\end{aligned} $$ where $h(n)$ is a vector of reservoir neuron activations, $W^{in}$ and $W^{rec}$ are the input and recurrent weight matrices respectively, and $\alpha\in(0,1]$ is the leaking rate. The model is also sometimes used without the leaky integration, which is a special case of $\alpha=1$ The linear readout layer is defined as $$ y(n)=W^{out}h(n)+b^{out} $$ where $y(n)$ is network output, $W^{out}$ the output weight matrix, and $b^out$ is the output bias ## Constraints of echo state machine ### Echo state property #### Theorem 1 For the echo state network defined above, the network will be echoey as long as the maximum singular value $\sigma_{max} Provement: &amp;gt; $$ &amp;gt; \begin{aligned} &amp;gt; d(\mathbf{x}(n+1),\mathbf{x}^{\prime}(n+1))&amp; =d(T(\mathbf{x}(n),\mathbf{u}(n+1)),T(\mathbf{x}&apos;(n),\mathbf{u}(n+1))) \\ &amp;gt; &amp;=d(f(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n)),f(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}&apos;(n))) \\ &amp;gt; &amp;\leq d(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n),\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}^{\prime}(n)) \\ &amp;gt; &amp;=d(\mathbf{W}\mathbf{x}(n),\mathbf{W}\mathbf{x}&apos;(n)) \\ &amp;gt; &amp;=||\mathbf{W}(\mathbf{x}(n)-\mathbf{x}^{\prime}(n))|| \\ &amp;gt; &amp;\leq\sigma_{\max}(\mathbf{W})d(\mathbf{x}(n),\mathbf{x}&apos;(n)) &amp;gt; \end{aligned} &amp;gt; $$ #### Theorem 2 For the echo state network defined above, as long as the spectral radius $|\lambda_{max}|$ of the recurrent connection matrix W &amp;gt; 1, then the network must not be echogenic. The spectral radius of the matrix is the absolute value of the largest eigenvalue $\lambda_{max}$. #### How to initialize Using these two theorems, how should we initialize W so that the network has an echo property? If we scale W, i.e., multiply it by a scaling factor $\alpha$, then $\sigma_{max}\alpha_{max}\text{,the network will not have the echo state.}\\\bullet&amp;\text{if}\alpha_{min}\le\alpha\le\alpha_{max}\text{,the network may have the echo state.}\end{array} $$ **$\alpha$è®¾çš„ç•¥å°äºŽ1** ![image-20230828142516052](/BrainPy-course-notes/master_content/Notes.assets/image-20230828142516052.png) ### Global parameters of reservoir è¿™äº›è¶…å‚ä¼šå½±å“reservoir networkçš„æ€§èƒ½ï¼Œéœ€è¦æ‰‹åŠ¨è°ƒå‚ï¼Œå¾ˆéš¾è‡ªåŠ¨åŽ»è°ƒæ•´ - The size $N_x$ - General wisdom: the bigger the reservoir, the better the obtainable performance - Select global parameters with smaller reservoirs, then scale to bigger ones. - Sparsity - Distribution of nonzero elements: - Normal distribution - Uniform distribution - The width of the distributions does not matter - spectral radius of $W$ - scales the width of the distribution of its nonzero elements - determines how fast the influence of an input dies out in a reservoir with time, and how stable the reservoir activations are - The spectral radius should be larger in tasks requiring longer memory of the input - Scaling(-s) to $W^{in}$: - For uniform distributed $W^{in}$, $\alpha$ in the range of the interval $[-a;a]$. - For normal distributed $W^{in}$, one may take the standard deviation as a scaling measure. The leaking rate $\alpha$ ## Training of echo state machine ### Offline learning The advantage of the echo state network is that it does not train recurrent connections within the reservoir, but only the readout layer from the reservoir to the output. çº¿æ€§å±‚çš„ä¼˜åŒ–æ–¹æ³•æ˜¯ç®€å•çš„ **Ridge regression** $$ \begin{aligned}\epsilon_{\mathrm{train}}(n)&amp;=\mathbf{y}(n)-\mathbf{\hat{y}}(n) \\&amp;=\mathbf{y}(n)-\mathbf{W}^{\mathrm{out}}\mathbf{x}(n) \\&amp;L_{\mathrm{ridge}}=\frac{1}{N}\sum_{i=1}^{N}\epsilon_{\mathrm{train}}^{2}(i)+\alpha||\mathbf{W^{out}}||^{2} \\\\W^{out}&amp;=Y^{target}X^T(XX^T+\beta I)^{-1}\end{aligned} $$ ```python trainer = bp.OfflineTrainer(model, fit_method=bp.algorithms.RidgeRegression(1e-7), dt=dt) ``` ### Online learning æ¥ä¸€ä¸ªsampleï¼Œè¿›è¡Œä¸€æ¬¡trainingï¼Œå¯¹è®­ç»ƒèµ„æºå¯ä»¥é¿å…ç“¶é¢ˆ The training data is passed to the trainer in a certain sequence (e.g., time series), and the trainer continuously learns based on the new incoming data. **Recursive Least Squares (RLS) algorithm** $$ E(\mathbf{y},\mathbf{y}^\mathrm{target},n)=\frac{1}{N_\mathrm{y}}\sum_{i=1}^{N_\mathrm{y}}\sum_{j=1}^{n}\lambda^{n-j}\left(y_i(j)-y_i^\mathrm{target}(j)\right)^2, $$ ```python trainer = bp.OnlineTrainer(model, fit_method=bp.algorithms.RLS(), dt=dt) ``` ### Dataset ç»™å®štime sequenceï¼Œå¯ä»¥è®©ç½‘ç»œåŽ»é¢„æµ‹regression ![image-20230828144309742](/BrainPy-course-notes/master_content/Notes.assets/image-20230828144309742.png) ç”¨åˆ°BrainPyé›†æˆçš„`Neuromorphic and Cognitive Datasets` ### Other tasks `MNIST dataset` or `Fashion MNIST` Two aspect: - Running time - Memory Usage ## Echo state machine programming ```python import brainpy as bp import brainpy.math as bm import brainpy_datasets as bd import matplotlib.pyplot as plt # enable x64 computation bm.set_environment(x64=True, mode=bm.batching_mode) bm.set_platform(&apos;cpu&apos;) ``` ### Dataset ```python def plot_mackey_glass_series(ts, x_series, x_tau_series, num_sample): plt.figure(figsize=(13, 5)) plt.subplot(121) plt.title(f&quot;Timeserie - {num_sample} timesteps&quot;) plt.plot(ts[:num_sample], x_series[:num_sample], lw=2, color=&quot;lightgrey&quot;, zorder=0) plt.scatter(ts[:num_sample], x_series[:num_sample], c=ts[:num_sample], cmap=&quot;viridis&quot;, s=6) plt.xlabel(&quot;$t$&quot;) plt.ylabel(&quot;$P(t)$&quot;) ax = plt.subplot(122) ax.margins(0.05) plt.title(f&quot;Phase diagram: $P(t) = f(P(t-\\tau))$&quot;) plt.plot(x_tau_series[: num_sample], x_series[: num_sample], lw=1, color=&quot;lightgrey&quot;, zorder=0) plt.scatter(x_tau_series[:num_sample], x_series[: num_sample], lw=0.5, c=ts[:num_sample], cmap=&quot;viridis&quot;, s=6) plt.xlabel(&quot;$P(t-\\tau)$&quot;) plt.ylabel(&quot;$P(t)$&quot;) cbar = plt.colorbar() cbar.ax.set_ylabel(&apos;$t$&apos;) plt.tight_layout() plt.show() ``` ```python dt = 0.1 mg_data = bd.chaos.MackeyGlassEq(25000, dt=dt, tau=17, beta=0.2, gamma=0.1, n=10) ts = mg_data.ts xs = mg_data.xs ys = mg_data.ys plot_mackey_glass_series(ts, xs, ys, num_sample=int(1000 / dt)) ``` ![image-20230828151451523](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151451523.png) ### Prediction of Mackey-Glass timeseries #### Prepare the data ```python def get_data(t_warm, t_forcast, t_train, sample_rate=1): warmup = int(t_warm / dt) # warmup the reservoir forecast = int(t_forcast / dt) # predict 10 ms ahead train_length = int(t_train / dt) X_warm = xs[:warmup:sample_rate] X_warm = bm.expand_dims(X_warm, 0) X_train = xs[warmup: warmup+train_length: sample_rate] X_train = bm.expand_dims(X_train, 0) Y_train = xs[warmup+forecast: warmup+train_length+forecast: sample_rate] Y_train = bm.expand_dims(Y_train, 0) X_test = xs[warmup + train_length: -forecast: sample_rate] X_test = bm.expand_dims(X_test, 0) Y_test = xs[warmup + train_length + forecast::sample_rate] Y_test = bm.expand_dims(Y_test, 0) return X_warm, X_train, Y_train, X_test, Y_test ``` ```python # First warmup the reservoir using the first 100 ms # Then, train the network in 20000 ms to predict 1 ms chaotic series ahead x_warm, x_train, y_train, x_test, y_test = get_data(100, 1, 20000) ``` ```python sample = 3000 fig = plt.figure(figsize=(15, 5)) plt.plot(x_train[0, :sample], label=&quot;Training data&quot;) plt.plot(y_train[0, :sample], label=&quot;True prediction&quot;) plt.legend() plt.show() ``` ![image-20230828151606545](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151606545.png) #### Prepare the ESN ```python class ESN(bp.DynamicalSystemNS): def __init__(self, num_in, num_hidden, num_out, sr=1., leaky_rate=0.3, Win_initializer=bp.init.Uniform(0, 0.2)): super(ESN, self).__init__() self.r = bp.layers.Reservoir( num_in, num_hidden, Win_initializer=Win_initializer, spectral_radius=sr, leaky_rate=leaky_rate, ) self.o = bp.layers.Dense(num_hidden, num_out, mode=bm.training_mode) def update(self, x): return x &amp;gt;&amp;gt; self.r &amp;gt;&amp;gt; self.o ``` #### Train and test ```python model = ESN(1, 100, 1) model.reset_state(1) trainer = bp.RidgeTrainer(model, alpha=1e-6) ``` ```python # warmup _ = trainer.predict(x_warm) ``` ```python # train _ = trainer.fit([x_train, y_train]) ``` #### Test the training data ```python ys_predict = trainer.predict(x_train) ``` ```python start, end = 1000, 6000 plt.figure(figsize=(15, 7)) plt.subplot(211) plt.plot(bm.as_numpy(ys_predict)[0, start:end, 0], lw=3, label=&quot;ESN prediction&quot;) plt.plot(bm.as_numpy(y_train)[0, start:end, 0], linestyle=&quot;--&quot;, lw=2, label=&quot;True value&quot;) plt.title(f&apos;Mean Square Error: {bp.losses.mean_squared_error(ys_predict, y_train)}&apos;) plt.legend() plt.show() ``` ![image-20230828151747954](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151747954.png) #### Test the testing data ```python ys_predict = trainer.predict(x_test) start, end = 1000, 6000 plt.figure(figsize=(15, 7)) plt.subplot(211) plt.plot(bm.as_numpy(ys_predict)[0, start:end, 0], lw=3, label=&quot;ESN prediction&quot;) plt.plot(bm.as_numpy(y_test)[0,start:end, 0], linestyle=&quot;--&quot;, lw=2, label=&quot;True value&quot;) plt.title(f&apos;Mean Square Error: {bp.losses.mean_squared_error(ys_predict, y_test)}&apos;) plt.legend() plt.show() ``` ![image-20230828151824907](/BrainPy-course-notes/master_content/Notes.assets/image-20230828151824907.png) ### JIT connection operators - Just-in-time randomly generated matrix. - Support for Mat@Vec and Mat@Mat. - Support different random generation methods.(homogenous, uniform, normal) ```python import math, random def jitconn_prob_homo(events, prob, weight, seed, outs): random.seed(seed) max_cdist= math.ceil(2/prob -1) for event in events: if event: post_i = random.randint(1, max_cdist) outs[post_i] += weight ``` ![image-20230828153353131](/BrainPy-course-notes/master_content/Notes.assets/image-20230828153353131.png) ## Applications ### From the perspective of kernel methods ç»´åº¦æ‰©å¼ æ€æƒ³ Non-linear SVMs: Kernel Mapping ![image-20230828153621843](/BrainPy-course-notes/master_content/Notes.assets/image-20230828153621843.png) Kernel methods in neural system? **ä¸Žç»´åº¦æ‰©å¼ çš„æ€æƒ³ç›¸ä¼¼** ![image-20230828153801285](/BrainPy-course-notes/master_content/Notes.assets/image-20230828153801285.png) ### Subcortical pathway for rapid motion processing The first two stages of subcortical visual pathway: Retina -&amp;gt; superior colliculus The first two stages of primary auditory pathway: Inner Ear -&amp;gt; Cochlear Nuclei ç»´åº¦æ‰©å¼ åœ¨subcortical pathwayä¸­ä½“çŽ°ï¼Œreservoir èƒ½å¤Ÿé«˜ç»´å¤„ç†çš„æ›´ç®€å• ### Spatial-temporal tasks ![image-20230828154155803](/BrainPy-course-notes/master_content/Notes.assets/image-20230828154155803.png) æ—¢æœ‰æ—¶é—´ä¿¡æ¯ï¼Œåˆæœ‰ç©ºé—´ä¿¡æ¯çš„datasetï¼Œä½¿ç”¨reservoiræ¥å¤„ç†é«˜ç»´ä¿¡æ¯ï¼Œåä½ Dimension expansion ### Gait recognition inputæ¥äº†å†åšè®¡ç®— ![image-20230828154352087](/BrainPy-course-notes/master_content/Notes.assets/image-20230828154352087.png) ### Spatial-temporal tasks large-scaleï¼Œéšsizeå¢žå¤§ï¼Œaccuracyå¢žå¤§ ![image-20230828154428762](/BrainPy-course-notes/master_content/Notes.assets/image-20230828154428762.png) ### Liquid state machine A liquid state machine (LSM) is a type of reservoir computer that uses a spiking neural network. ä¸ŽESNä¸€æ ·çš„èŒƒå¼ï¼Œéƒ½æ˜¯åŽ»åšdimension expansion å¾ˆéš¾åŽ»åˆ†æžæ€Žä¹ˆworkçš„">
<meta name="twitter:image" content="Notes.assets/image-20230826111831637.png">


<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/BrainPy-course-notes',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'åšä¸»'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="routhleck.github.io/BrainPy-course-notes"/>





  <title>               | BrainPy course notes      </title>
  
















</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/BrainPy-course-notes/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">BrainPy course notes</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        
        
        <li class="menu-item menu-item-home">
          <a href="/BrainPy-course-notes/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            é¦–é¡µ
          </a>
        </li>
      
        
        
        
        <li class="menu-item menu-item-about">
          <a href="/BrainPy-course-notes/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            å…³äºŽ
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
<div id="posts" class="posts-expand">
<header class="post-header">

	<h1 class="post-title" itemprop="name headline">
    
      
    
  </h1>



</header>

  
  
    <p>[TOC]</p>

<h1 id="ç¥žç»è®¡ç®—å»ºæ¨¡ç®€ä»‹">ç¥žç»è®¡ç®—å»ºæ¨¡ç®€ä»‹</h1>

<h2 id="è®¡ç®—ç¥žç»ç§‘å­¦çš„èƒŒæ™¯ä¸Žä½¿å‘½">è®¡ç®—ç¥žç»ç§‘å­¦çš„èƒŒæ™¯ä¸Žä½¿å‘½</h2>

<p>è®¡ç®—ç¥žç»ç§‘å­¦æ˜¯<strong>è„‘ç§‘å­¦</strong>å¯¹<strong>ç±»è„‘æ™ºèƒ½</strong>çš„<strong>æ¡¥æ¢</strong></p>

<h3 id="ä¸¤å¤§ç›®æ ‡">ä¸¤å¤§ç›®æ ‡</h3>

<ul>
  <li>ç”¨è®¡ç®—å»ºæ¨¡çš„æ–¹æ³•æ¥é˜æ˜Žå¤§è„‘åŠŸèƒ½çš„è®¡ç®—åŽŸç†</li>
  <li>å‘å±•ç±»è„‘æ™ºèƒ½çš„æ¨¡åž‹å’Œç®—æ³•</li>
</ul>

<h3 id="prehistory">Prehistory</h3>

<ul>
  <li>1907 LIF model 
ç¥žç»è®¡ç®—çš„æœ¬è´¨</li>
  <li>1950s HH model 
ç”µä½å®šé‡åŒ–æ¨¡åž‹ æœ€fundamentalçš„</li>
  <li>1960s Rollâ€™s cable equation 
æè¿°ä¿¡å·åœ¨è½´çªå’Œæ ‘çªæ€Žä¹ˆä¼ é€’</li>
  <li>1970s Amari, Wilson, Cowan et al.
çŽ°ä»Šå»ºæ¨¡çš„åŸºç¡€</li>
  <li>1982 Hopfield model(Amari-Hopfield model)
å¼•å…¥ç‰©ç†å­¦æŠ€æœ¯ï¼Œå¸å¼•å­æ¨¡åž‹</li>
  <li>1988 Sejnowski et al. â€œComputational Neuroscienceâ€(science)
æå‡ºè®¡ç®—ç¥žç»ç§‘å­¦æ¦‚å¿µ</li>
</ul>

<p><strong>çŽ°åœ¨çš„è®¡ç®—ç¥žç»ç§‘å­¦å¯¹åº”äºŽç‰©ç†å­¦çš„ç¬¬è°·-ä¼½åˆ©ç•¥æ—¶ä»£ï¼Œå¯¹å¤§è„‘å·¥ä½œåŽŸç†è¿˜ç¼ºä¹æ¸…æ™°çš„ç†è®º</strong></p>

<h3 id="three-levels-of-brain-science">Three levels of Brain Science</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823105226568.png" alt="image-20230823105226568" /></p>

<ul>
  <li>å¤§è„‘åšä»€ä¹ˆ
Computational theory 
-&gt; Psychology &amp; Cognitive Science
-&gt; Human-like Cognitive function</li>
  <li>å¤§è„‘æ€Žä¹ˆåš
Representation &amp; Algorithm 
-&gt; Computational Neuroscience
-&gt; Brain-inspired model &amp; algorithm</li>
  <li>å¤§è„‘æ€Žä¹ˆå®žçŽ°
Implementation
-&gt; Neuroscience
-&gt; Neuromorphic computing</li>
</ul>

<h3 id="mission-of-computational-neuroscience">Mission of Computational Neuroscience</h3>

<blockquote>
  <p>What I can not build a computational model, I do not understand</p>
</blockquote>

<h2 id="ç¥žç»è®¡ç®—å»ºæ¨¡çš„ç›®æ ‡ä¸ŽæŒ‘æˆ˜">ç¥žç»è®¡ç®—å»ºæ¨¡çš„ç›®æ ‡ä¸ŽæŒ‘æˆ˜</h2>

<h3 id="limitation-of-deep-learning">Limitation of Deep Learning</h3>

<ul>
  <li>ä¸æ“…é•¿å¯¹æŠ—æ ·æœ¬</li>
  <li>å¯¹å›¾åƒçš„ç†è§£æœ‰é™</li>
</ul>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823105836259.png" alt="image-20230823105836259" /></p>

<h3 id="brain-is-for-processing-dynamical-information">Brain is for Processing Dynamical Information</h3>

<p><strong>We never â€œseeâ€ a static image</strong></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823105918336.png" alt="image-20230823105918336" /></p>

<h3 id="the-missing-link">The missing link</h3>

<p>a computational model of higher cognitive functior</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823110617639.png" alt="image-20230823110617639" /></p>

<p>çŽ°åœ¨åªæ˜¯åšçš„<strong>å±€éƒ¨</strong>çš„ç½‘ç»œï¼Œæ²¡æœ‰ä¸€ä¸ªæˆåŠŸçš„æ¨¡åž‹ï¼Œèƒ½<strong>ä»Žç¥žç»å…ƒå‡ºå‘æž„å»ºç½‘ç»œï¼Œåˆ°ç³»ç»Ÿå±‚é¢ä¸Š</strong></p>

<p><strong>åŽŸå› </strong>: å› ä¸ºç¥žç»ç§‘å­¦åº•å±‚æ•°æ®çš„ç¼ºå¤±ï¼Œå¯ä»¥è€ƒè™‘æ•°æ®é©±åŠ¨ã€å¤§æ•°æ®çš„æ–¹å¼æ¥åŠ å¿«å‘å±•</p>

<h2 id="ç¥žç»è®¡ç®—å»ºæ¨¡çš„å·¥å…·">ç¥žç»è®¡ç®—å»ºæ¨¡çš„å·¥å…·</h2>

<blockquote>
  <p>å·¥æ¬²è¡Œå…¶äº‹ï¼Œå¿…å…ˆåˆ©å…¶å™¨
We need â€œPyTorch/TensorFlowâ€ in Computational Neuroscience!</p>
</blockquote>

<h3 id="challenges-in-neural-modelling">Challenges in neural modelling</h3>

<p>æœ‰ä¸åŒçš„å°ºåº¦</p>

<ul>
  <li>Mutiple-scale</li>
  <li>Large-scale</li>
  <li>Multiple purposes</li>
</ul>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823111212460.png" alt="image-20230823111212460" /></p>

<blockquote>
  <p>The modeling targets and methods are extremely complex, and we need a general framework.</p>
</blockquote>

<h3 id="limitations-of-existing-brain-simulators">Limitations of Existing Brain Simulators</h3>

<p>çŽ°ä»Šçš„æ¡†æž¶ä¸èƒ½æ»¡è¶³ä»¥ä¸Š</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823111509523.png" alt="image-20230823111509523" /></p>

<h3 id="what-are-needed-for-a-brain-simulator">What are needed for a brain simulator</h3>

<ol>
  <li>Efficiency
High-speed simulation on parallel computing devices, etc.</li>
  <li>Integration
Integrated modeling of simulation, training, and analysis</li>
  <li>Flexibility
New models at all scales can be accommodated</li>
  <li>Extensibility
Extensible to new modeling methods(machine learning)</li>
</ol>

<p>éœ€è¦æ–°çš„èŒƒå¼</p>

<h3 id="our-solution-brainpy">Our solution: BrainPy</h3>

<p>4 levels</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823111903456.png" alt="image-20230823111903456" /></p>

<h2 id="ç¥žç»è®¡ç®—å»ºæ¨¡ä¸¾ä¾‹">ç¥žç»è®¡ç®—å»ºæ¨¡ä¸¾ä¾‹</h2>

<h3 id="image-understanding-an-ill-posed-problem">Image understanding: an ill-posed problem</h3>

<p>Image Understanding = image segmentation + image object recognition</p>

<blockquote>
  <p>Chicken vs. Egg dilemma</p>

  <ul>
    <li>Without segmentation, how to recognize</li>
    <li>Without recognition, how to segment</li>
  </ul>
</blockquote>

<p><strong>The solution of brain:</strong> Analysis-by-synthesis çŒœæµ‹ä¸ŽéªŒè¯æ–¹æ³•</p>

<h3 id="reverse-hierarchy-theory">Reverse Hierarchy Theory</h3>

<p>äººçš„æ„ŸçŸ¥æ˜¯æ•´ä½“åˆ°å±€éƒ¨</p>

<h3 id="two-pathways-for-visual-information-processing">Two pathways for visual information processing</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823114517888.png" alt="image-20230823114517888" /></p>

<h3 id="key-computational-issues-for-global-to-local-neural-information-processing">Key Computational Issues for Global-to-local Neural Information Processing</h3>

<ul>
  <li>What are global and local features</li>
  <li>How to rapidly extract global features</li>
  <li>How to generate global hypotheses</li>
  <li>How to implement from global to local processing</li>
  <li>The interplay between global and local features</li>
  <li>Others</li>
</ul>

<h4 id="how-to-extract-global-features">How to extract global features</h4>

<p><strong>Global first = Topology first</strong>(å¤§èŒƒå›´é¦–å…ˆï¼Œé™ˆéœ–)
è§†è§‰ç³»ç»Ÿæ›´æ•æ„ŸäºŽæ‹“æ‰‘æ€§è´¨çš„å·®å¼‚</p>

<blockquote>
  <p>DNNs has difficulty to recognize topology</p>
</blockquote>

<p><strong>A retina-SC network for topology detection</strong></p>

<p>è§†ç½‘è†œåˆ°ä¸Šä¸˜çš„æ£€æµ‹ï¼ŒGap junction coupling â€¦</p>

<h3 id="a-model-for-motion-pattern-recognition">A Model for Motion Pattern Recognition</h3>

<p>Reservoir Module
Decision-making Module</p>

<h3 id="how-to-generate-global-hypotheses-in-the-representation-space">How to generate â€œglobalâ€ hypotheses in the representation space</h3>

<p>Attractor neural network</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823115853980.png" alt="image-20230823115853980" /></p>

<p>Levy Flight in Animal Behaviors</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823120000911.png" alt="image-20230823120000911" /></p>

<h3 id="how-to-process-information-from-global-to-local">How to process information from global to local</h3>

<p>Push-pull Feedback</p>

<p>A hierarchical Hopfield Model</p>

<h3 id="interplay-between-global-and-local-features">Interplay between global and local features</h3>

<p>A two-pathway model for object recognition</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823120750349.png" alt="image-20230823120750349" /></p>

<p>Modeling visual masking å¯ä»¥ç”¨two-pathwayå¾ˆå¥½è§£é‡Š</p>

<h1 id="programming-basics">Programming basics</h1>

<h2 id="python-basics">Python Basics</h2>

<h3 id="values">Values</h3>

<ul>
  <li>Boolean</li>
  <li>String</li>
  <li>Integer</li>
  <li>Float</li>
  <li>â€¦</li>
</ul>

<h3 id="keywords">Keywords</h3>

<p>Not allowed to use keywords, they define structure and rules of a language.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">help</span><span class="p">(</span><span class="s">"keywords"</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="operators">Operators</h3>

<p>æ•°æ®ä¹‹é—´çš„æ“ä½œ</p>

<h4 id="for-integers-and-floats">For Integers and Floats</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="o">=</span><span class="mi">5</span>
<span class="n">b</span><span class="o">=</span><span class="mi">3</span>
<span class="c1"># addition +
</span><span class="k">print</span><span class="p">(</span><span class="s">"a+b="</span><span class="p">,</span><span class="n">atb</span><span class="p">)</span>
<span class="c1"># subtraction -
</span><span class="k">print</span><span class="p">(</span><span class="s">"a-b="</span><span class="p">,</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="p">)</span>
<span class="c1"># multiplication *
</span><span class="k">print</span><span class="p">(</span><span class="s">"axb="</span><span class="n">a</span><span class="o">*</span><span class="n">b</span><span class="p">)</span>
<span class="c1"># division /
</span><span class="k">print</span><span class="p">(</span><span class="s">"a/b="</span><span class="p">,</span><span class="n">a</span><span class="o">/</span><span class="n">b</span><span class="p">)</span>
<span class="c1"># power **
</span><span class="k">print</span><span class="p">(</span><span class="s">"a**b="</span><span class="p">,</span><span class="n">a</span><span class="o">**</span><span class="n">b</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="booleans">Booleans</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Boolean experssions
# equals: ==
</span><span class="k">print</span><span class="p">(</span><span class="s">"5==5"</span><span class="p">,</span><span class="mi">5</span><span class="o">==</span><span class="mi">5</span><span class="p">)</span> 
<span class="c1"># do not equal: !=
</span><span class="k">print</span><span class="p">(</span><span class="s">"5!-5"</span><span class="p">,</span><span class="mi">5</span><span class="o">!=</span><span class="mi">5</span><span class="p">)</span>
<span class="c1"># greater than: &gt;
</span><span class="k">print</span><span class="p">(</span><span class="s">"5&gt;5"</span><span class="p">,</span><span class="mi">5</span><span class="o">&gt;</span><span class="mi">5</span><span class="p">)</span>
<span class="c1"># greater than or equal: &gt;=
</span><span class="k">print</span><span class="p">(</span><span class="s">"5&gt;=5â€5&gt;=5)
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># logica operators
</span><span class="k">print</span><span class="p">(</span><span class="s">"True and False:"</span><span class="p">,</span> <span class="bp">True</span> <span class="ow">and</span> <span class="bp">False</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"True or False:"</span><span class="p">,</span> <span class="bp">True</span> <span class="ow">or</span> <span class="bp">False</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"not False:"</span><span class="p">,</span> <span class="ow">not</span> <span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="modules">Modules</h3>

<p>Not all functionality available comes automatically when starting python.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">match</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="k">print</span><span class="p">(</span><span class="n">math</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">pi</span>
<span class="k">print</span><span class="p">(</span><span class="n">pi</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="o">*</span>
<span class="k">print</span><span class="p">(</span><span class="n">pi</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="control-statements">Control statements</h3>

<h4 id="if">If</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="mi">5</span>
<span class="c1"># In Python, blocks of code are defined using indentation.
</span><span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
	<span class="k">print</span><span class="p">(</span><span class="s">"ok"</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>ok</p>
</blockquote>

<h4 id="for">For</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># range(5) means a list with integers, 0, 1, 2, 3, 4
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>0
1
2
3
4</p>
</blockquote>

<h4 id="while">While</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">i</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">while</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="mi">100</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">+=</span> <span class="n">i</span><span class="o">**</span><span class="mi">3</span> <span class="c1"># a += b is short for a = a+b
</span></code></pre></div></div>

<blockquote>
  <p>1
8
1000</p>
</blockquote>

<h3 id="functions">Functions</h3>

<ul>
  <li>Functions are used to abstract components of a program.</li>
  <li>Much like a mathematical function, they take some input and then find the result. start a function definition with a keyword def</li>
  <li>Then comes the function name, with arguments in braces, and then a colon.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">args1</span><span class="p">,</span> <span class="n">args2</span><span class="p">):</span>
    <span class="k">pass</span>
</code></pre></div></div>

<h3 id="data-types">Data types</h3>

<h4 id="list">List</h4>

<ul>
  <li>Group variables together</li>
  <li>Specific order</li>
  <li>Access item with brankets: [ ]</li>
  <li>List can be sliced</li>
  <li>List can be multiplied</li>
  <li>List can be added</li>
  <li>Lists are mutable</li>
  <li>Copying a list</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">myList</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span><span class="s">"name"</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s">"myList[0]:"</span><span class="p">,</span> <span class="n">myList</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"myList[1]:"</span><span class="p">,</span> <span class="n">myList</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"myList[3]:"</span><span class="p">,</span> <span class="n">myList</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"myList[-1]:"</span><span class="p">,</span> <span class="n">myList</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"myList[-2]:"</span><span class="p">,</span> <span class="n">myList</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
</code></pre></div></div>

<blockquote>
  <p>myList[0]: 0
myList[1]: 1
myList[3]: name
myList[-1]: name
myList[-2]: 2.0</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">myList</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s">"hello"</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s">"myList[0:2]:"</span><span class="p">,</span> <span class="n">mylist</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"myList*2:"</span><span class="p">,</span> <span class="n">myList</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span>
<span class="n">myList2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="s">"yes"</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s">"myList+myList2:"</span><span class="p">,</span> <span class="n">myList</span><span class="o">+</span><span class="n">myList2</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>myList[0:2]: [0ï¼Œ1.0]
myList*2: [0ï¼Œ1.0ï¼Œ helloâ€™ï¼Œ0ï¼Œ1.0ï¼Œ helloâ€™]
myList+myList2: [0ï¼Œ1.0ï¼Œâ€™helloâ€™ï¼Œ2ï¼Œyesâ€™]</p>
</blockquote>

<h4 id="tuple">tuple</h4>

<p>Tuples are immutable.</p>

<h4 id="dictionary">dictionary</h4>

<p>A dictionary is a collection of key-value pairs</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">d</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">d</span><span class="p">[</span><span class="s">"a"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">3</span>

<span class="k">print</span><span class="p">(</span><span class="s">"d: "</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span> <span class="s">"a"</span><span class="p">:</span><span class="mi">3</span><span class="p">}</span>
<span class="k">print</span><span class="p">(</span><span class="s">"c: "</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"c[1]: "</span><span class="p">,</span> <span class="n">c</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<blockquote>
  <p>d: {1: 2, â€˜aâ€™: 3}
c: {1: 2, â€˜aâ€™: 3}
c[1]: 2</p>
</blockquote>

<h3 id="class">Class</h3>

<p>In Python, everything is an object. Classes are objects, instances of
classes are objects, modules are objects, and functions are objects.</p>

 	1.  a <strong>type</strong>
 	2.  an internal <strong>data representation</strong> (primitive or composite)
 	3.  a set of procedures for <strong>interaction</strong> with the object

<p><strong>a simple example</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># define class
</span>
<span class="k">class</span> <span class="nc">Linear</span><span class="p">():</span>
    <span class="k">pass</span>

<span class="c1"># instantiate object
</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">layer1</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p><code class="language-plaintext highlighter-rouge">&lt;__main__.Linear object at 0x7f88ad6c61d0&gt;</code></p>
</blockquote>

<h4 id="initializing-an-object">Initializing an object</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># define class
</span><span class="k">class</span> <span class="nc">Linear</span><span class="p">():</span>
    <span class="c1"># It refers to the object (instance) itself
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_input</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_input</span> <span class="o">=</span> <span class="n">n_input</span>

<span class="n">layer1</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">layer2</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"layer1 : "</span><span class="p">,</span> <span class="n">layer1</span><span class="p">.</span><span class="n">n_input</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"layer2 : "</span><span class="p">,</span> <span class="n">layer2</span><span class="p">.</span><span class="n">n_input</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>layer1 : 100
layer2 : 1000</p>
</blockquote>

<h4 id="class-has-methods-similar-to-functions">Class has methods (similar to functions)</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># define class
</span><span class="k">class</span> <span class="nc">Linear</span><span class="p">():</span>
    <span class="c1">### It refers to the the object (instance) itself
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_input</span><span class="p">,</span> <span class="n">n_output</span><span class="p">):</span>
	    <span class="bp">self</span><span class="p">.</span><span class="n">n_input</span> <span class="o">=</span> <span class="n">n_input</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_output</span> <span class="o">=</span> <span class="n">n_output</span>
	<span class="k">def</span> <span class="nf">compute</span> <span class="n">n</span> <span class="n">params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">num_params</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_input</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_output</span>
        <span class="k">return</span> <span class="n">num_params</span>
<span class="n">layerl</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">layerl</span><span class="p">.</span><span class="n">compute_n_params</span><span class="p">())</span>
</code></pre></div></div>

<blockquote>
  <p>1000</p>
</blockquote>

<h2 id="numpy-basic">NumPy Basic</h2>

<h3 id="numpy-introduction">Numpy Introduction</h3>

<ul>
  <li>Fundamental package for scientific computing with Python</li>
  <li>N-dimensional array object</li>
  <li>Linear algebra, frontier transform, random number capacities</li>
  <li>Building block for other packages (e.g. Scipy)</li>
</ul>

<h3 id="array">Array</h3>

<ul>
  <li>Arrays are mutable</li>
  <li>Arrays attributes</li>
  <li>â€¦</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>[[0. 0.]
	[0. 0.]]</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="p">.</span><span class="n">ndim</span>		<span class="c1"># 2 dimension
</span><span class="n">a</span><span class="p">.</span><span class="n">shape</span>		<span class="c1"># (2, 5) shape of array
</span><span class="n">a</span><span class="p">.</span><span class="n">size</span>		<span class="c1"># 10 $ of elements
</span><span class="n">a</span><span class="p">.</span><span class="n">T</span>			<span class="c1"># transpose
</span><span class="n">a</span><span class="p">.</span><span class="n">dtype</span>		<span class="c1"># data type
</span></code></pre></div></div>

<h4 id="array-broadcasting">Array broadcasting</h4>

<p>When operating on two arrays, numpy compares shapes. Two dimensions are compatible when</p>
<ol>
  <li>They are of equal size</li>
  <li>One of them is 1</li>
</ol>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823143622229.png" alt="image-20230823143622229" /></p>

<h3 id="vector-operations">Vector operations</h3>

<ul>
  <li>Inner product</li>
  <li>Outer product</li>
  <li>Dot product (matrix multiplication)</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">u</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">v</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">np</span><span class="p">.</span><span class="n">inner</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="n">np</span><span class="p">.</span><span class="n">outer</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>6
array([[1, 1, 1],
			[2, 2, 2],
			[3, 3, 3]])
6</p>
</blockquote>

<h3 id="matrix-operations">Matrix operations</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">np.ones</code></li>
  <li><code class="language-plaintext highlighter-rouge">.T</code></li>
  <li><code class="language-plaintext highlighter-rouge">np.dot</code></li>
  <li><code class="language-plaintext highlighter-rouge">np.eye</code></li>
  <li><code class="language-plaintext highlighter-rouge">np.trace</code></li>
  <li><code class="language-plaintext highlighter-rouge">np.row_stack</code></li>
  <li><code class="language-plaintext highlighter-rouge">np.column_stack</code></li>
</ul>

<h3 id="operations-along-axes">Operations along axes</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="n">a</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>

<span class="n">a</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">a</span><span class="p">.</span><span class="n">cumsum</span><span class="p">()</span>

<span class="n">a</span><span class="p">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="slicing-arrays">Slicing arrays</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> 	<span class="c1"># first row, all columns
</span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> 	<span class="c1"># first and second rows, al columns
</span><span class="n">a</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span><span class="c1"># all rows, second and third columns
</span></code></pre></div></div>

<h3 id="reshape">Reshape</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">a</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="linear-algebra">Linear algebra</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">qr</span>				<span class="c1"># Computes the QR decomposition
</span><span class="n">cholesky</span>		<span class="c1"># Computes the Cholesky decomposition
</span><span class="n">inv</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>			<span class="c1"># Inverse
</span><span class="n">solve</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>		<span class="c1"># Solves Ax = b for A full rank
</span><span class="n">lstsq</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>		<span class="c1"># Solves arg minx //Ax - b//2
</span><span class="n">eig</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>			<span class="c1"># Eigenvalue decomposition
</span><span class="n">eigvals</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>		<span class="c1"># Computes eigenvalues
</span><span class="n">svd</span><span class="p">(</span><span class="n">A</span><span class="err">ï¼Œ</span><span class="n">full</span><span class="p">)</span>		<span class="c1"># Sinqular value decomposition
</span><span class="n">pinv</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>			<span class="c1"># Computes pseudo-inverse of A
</span></code></pre></div></div>

<h3 id="fourier-transform">Fourier transform</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy.fft</span>
<span class="n">fft</span>		<span class="c1"># 1-dimensional DFT
</span><span class="n">fft2</span>	<span class="c1"># 2-dimensional DFT
</span><span class="n">fftn</span>	<span class="c1"># N-dimensional DFT
</span><span class="n">ifft</span>	<span class="c1"># 1-dimensional inverse DFT (etc.)
</span><span class="n">rfft</span>	<span class="c1"># Real DFT (1-dim)
</span></code></pre></div></div>

<h3 id="random-sampling">Random sampling</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy.random</span>
<span class="n">rand</span><span class="p">(</span><span class="n">d0</span><span class="p">,</span> <span class="n">d1</span><span class="p">,</span> <span class="p">...,</span> <span class="n">dn</span><span class="p">)</span>		<span class="c1"># Random values in a given shape
</span><span class="n">randn</span><span class="p">(</span><span class="n">d0</span><span class="p">,</span> <span class="n">d1</span><span class="p">,</span> <span class="p">...,</span> <span class="n">dn</span><span class="p">)</span>		<span class="c1"># Random standard normal
</span><span class="n">randint</span><span class="p">(</span><span class="n">lo</span><span class="p">,</span> <span class="n">hi</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>		<span class="c1"># Random integers [lo hi)
</span><span class="n">choice</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">repl</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>	<span class="c1"># Sample from a
</span><span class="n">shuffle</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>					<span class="c1"># Permutation (in-place)
</span><span class="n">permutation</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>				<span class="c1"># Permutation (new array)
</span></code></pre></div></div>

<h3 id="distributions-in-random">Distributions in random</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy.random</span>
<span class="n">beta</span>
<span class="n">binomial</span>
<span class="n">chisquare</span>
<span class="n">exponential</span>
<span class="n">dirichlet</span>
<span class="n">gamma</span>
<span class="n">laplace</span>
<span class="n">lognormal</span>
<span class="p">...</span>
</code></pre></div></div>

<h3 id="scipy">Scipy</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">SciPy</code> is a library of algorithms and mathematical tools built to work with <code class="language-plaintext highlighter-rouge">NumPy </code> arrays.</li>
  <li><code class="language-plaintext highlighter-rouge">scipy.linalg linear algebra</code></li>
  <li><code class="language-plaintext highlighter-rouge">scipy.stats statistics</code></li>
  <li><code class="language-plaintext highlighter-rouge">scipy.optimize optimization</code></li>
  <li><code class="language-plaintext highlighter-rouge">scipy.sparse sparse matrices</code></li>
  <li><code class="language-plaintext highlighter-rouge">scipy.signal signal processing</code></li>
  <li>etc.</li>
</ul>

<h2 id="brainpy-introduction">BrainPy introduction</h2>

<h3 id="modeling-demands">Modeling demands</h3>

<ul>
  <li>Large-scale</li>
  <li>Multi-scale</li>
  <li>Methods</li>
</ul>

<h3 id="brainpy-architecture">BrainPy Architecture</h3>

<ul>
  <li>Infrastructure</li>
  <li>Functions</li>
  <li>Just-in-time compilation</li>
  <li>Devices</li>
</ul>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823145349681.png" alt="image-20230823145349681" /></p>

<h3 id="main-features">Main features</h3>

<h4 id="dense-operators">Dense operators</h4>

<ul>
  <li>Compatible with <code class="language-plaintext highlighter-rouge">NumPy</code>, <code class="language-plaintext highlighter-rouge">TensorFlow</code>, <code class="language-plaintext highlighter-rouge">PyTorch</code> and other dense matrix operator syntax.</li>
  <li>Users do not need to learn and get started programming directly.</li>
</ul>

<h4 id="dedicated-operatorsq">Dedicated operatorsq</h4>

<ul>
  <li>Applies brain dynamics sparse connectivity properties with event-driven computational features.</li>
  <li>Reduce the complexity of brain dynamics simulations by several orders of magnitude.</li>
</ul>

<h4 id="numerical-integrators">Numerical Integrators</h4>

<ul>
  <li>Ordinary differential equations: brainpy.odeint</li>
  <li>Stochastic differential equations: brainpy.sdeint</li>
  <li>Fractional differential equations: brainpy.fdeint</li>
  <li>Delayed differential equations</li>
</ul>

<h4 id="modular-and-composable">Modular and composable</h4>

<p>ä»Žå¾®è§‚åˆ°å®è§‚</p>

<p><strong>brainpy.DynamicalSystem</strong></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230823151159786.png" alt="image-20230823151159786" /></p>

<h4 id="jit-of-object-oriented">JIT of object-oriented</h4>

<p>BrainPy provides object-oriented transformations:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">brainpy.math.jit</code></li>
  <li><code class="language-plaintext highlighter-rouge">brainpy.math.grad</code></li>
  <li><code class="language-plaintext highlighter-rouge">brainpy.math.for_loop</code></li>
  <li><code class="language-plaintext highlighter-rouge">brainpy.math.ifelse</code></li>
</ul>

<h2 id="brainpy-programming-basics">BrainPy Programming Basics</h2>

<h3 id="just-in-time-compilation">Just-in-Time compilation</h3>

<p>Just In Time Compilation (JIT, or Dynamic Translation), is compilation that is being done during the execution of a program.</p>

<p>JIT compilation attempts to use <strong>the benefits of both</strong>. While the interpreted program is being run, the JIT compiler determines the most frequently used code and compiles it to machine code.</p>

<p>The advantages of a JIT are due to the fact that since the compilation takes place in run time, a JIT compiler has access to dynamic runtime information enabling it to make better optimizations (such as inlining functions).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">sqrt</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span>
    <span class="n">cdf</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">bm</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">sqrt</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.044715</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">3</span><span class="p">))))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span><span class="n">cdf</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">gelu_jit</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">jit</span><span class="p">(</span><span class="n">gelu</span><span class="p">)</span> <span class="c1"># ä½¿ç”¨JIT
</span></code></pre></div></div>

<h3 id="object-oriented-jit-compilation">Object-oriented JIT compilation</h3>

<ul>
  <li>The class object must be inherited from brainpy.BrainPyObject, the base class of BrainPy, whose methods will be automatically JIT compiled.</li>
  <li>All time-dependent variables must be defined as brainpy.math.Variable.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">BrainPyObject</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dimension</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        
        <span class="c1"># parameters
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dimension</span> <span class="o">=</span> <span class="n">dimension</span>
        
        <span class="c1"># variables
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">dimension</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.3</span><span class="p">)</span>
        
	<span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">dot</span><span class="p">(((</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">Y</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">w</span><span class="p">)))</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">Y</span><span class="p">),</span> <span class="n">X</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">w</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">w</span> <span class="o">-</span> <span class="n">u</span> <span class="c1"># in-place update
</span></code></pre></div></div>

<p><strong>ExampleL Run a neuron model</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">neurons</span><span class="p">.</span><span class="n">HH</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span> <span class="c1">#ä¸€å…±1000ä¸ªç¥žç»å…ƒ
</span><span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="s">'input'</span><span class="p">,</span> <span class="mf">10.</span><span class="p">))</span> <span class="c1"># jité»˜è®¤ä¸ºTrue
</span><span class="n">runner</span><span class="p">(</span><span class="n">duration</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">eval_time</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1">#æ¨¡æ‹Ÿ 1000ms
</span></code></pre></div></div>

<p>ç¦ç”¨JITæ¥debug</p>

<h3 id="data-operations">Data operations</h3>

<h4 id="array-1">Array</h4>

<p>ç­‰ä»·äºŽ<code class="language-plaintext highlighter-rouge">numpy</code>çš„<code class="language-plaintext highlighter-rouge">array</code></p>

<h4 id="brainpy-arrays--jax-arrays">BrainPy arrays &amp; JAX arrays</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t1</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">t1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">t1</span><span class="p">.</span><span class="n">value</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>JaxArray([0, 1, 2], dtype=int32)
DeviceArray([0, 1, 2], dtype=int32)</p>
</blockquote>

<h4 id="variables">Variables</h4>

<p>Arrays that are not marked as dynamic variables will be JIT-compiled as static arrays, and modifications to static arrays will not be valid in the JIT compilation environment.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">v</span><span class="p">.</span><span class="n">value</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>Variable([0, 1, 2, 3], dtype=int32)
DeviceArray([0, 1, 2, 3], dtype=int32)</p>
</blockquote>

<h3 id="variables-1">Variables</h3>

<p><strong>In-place updating</strong> å°±åœ°æ›´æ–°</p>

<h4 id="indexing-and-slicing">Indexing and slicing</h4>

<ul>
  <li>Indexing: <code class="language-plaintext highlighter-rouge">v[i] = a</code> or <code class="language-plaintext highlighter-rouge">v[(1, 3)] = c</code></li>
  <li>Slicing: <code class="language-plaintext highlighter-rouge">v[i:j] = b</code></li>
  <li>Slicing all values <code class="language-plaintext highlighter-rouge">v[:] = d</code>, <code class="language-plaintext highlighter-rouge">v[...] = e</code></li>
</ul>

<h4 id="augmented-assignment">Augmented assignment</h4>

<ul>
  <li>add</li>
  <li>subtract</li>
  <li>divide</li>
  <li>multiply</li>
  <li>floor divide</li>
  <li>modulo</li>
  <li>power</li>
  <li>and</li>
  <li>or</li>
  <li>xor</li>
  <li>left shift</li>
  <li>right shift</li>
</ul>

<h4 id="value-assignment">Value assignment</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">v</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">check_no_change</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="update-assignment">Update assignment</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">v</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="control-flows">Control flows</h3>

<h4 id="if-else">If-else</h4>

<p><code class="language-plaintext highlighter-rouge">brainpy.math.where</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">a</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>
</code></pre></div></div>

<blockquote>
  <p>DeviceArray(1., dtype=float32, weak_type=True)</p>
</blockquote>

<p><code class="language-plaintext highlighter-rouge">brainpy.math.ifelse</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ifelse</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">branches</span><span class="p">,</span> <span class="n">operands</span><span class="p">):</span>
	<span class="n">true_fun</span><span class="p">,</span> <span class="n">false_fun</span> <span class="o">=</span> <span class="n">branches</span>
    <span class="k">if</span> <span class="n">condition</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">true_fun</span><span class="p">(</span><span class="n">operands</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">false_fun</span><span class="p">(</span><span class="n">operands</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="for-loop">For loop</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">brainpy.math</span>
<span class="n">hist_of_out_vars</span> <span class="o">=</span> <span class="n">brainpy</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">for_loop</span><span class="p">(</span><span class="n">body_fun</span><span class="p">,</span> <span class="n">operands</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="while-loop">While loop</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">i</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">counter</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">cond_f</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">10</span>

<span class="k">def</span> <span class="nf">body_f</span><span class="p">():</span>
    <span class="n">i</span><span class="p">.</span><span class="n">value</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">counter</span><span class="p">.</span><span class="n">value</span> <span class="o">+=</span> <span class="n">i</span>

<span class="n">bm</span><span class="p">.</span><span class="n">while_loop</span><span class="p">(</span><span class="n">body_f</span><span class="p">,</span> <span class="n">cond_f</span><span class="p">,</span> <span class="n">operands</span><span class="o">=</span><span class="p">())</span>

<span class="k">print</span><span class="p">(</span><span class="n">counter</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="single-neuron-modeling-conductance-based-models">Single Neuron Modeling: Conductance-Based Models</h1>

<h2 id="neuronal-structureing-potential-and-equivalent-circuits">Neuronal structure,ing potential, and equivalent circuits</h2>

<h3 id="neuronal-structure">Neuronal structure</h3>

<ul>
  <li>Cell body/soma</li>
  <li>Axon</li>
  <li>Dendrites</li>
  <li>Synapses</li>
</ul>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824100656397.png" alt="image-20230824100656397" /></p>

<h3 id="resting-potential">Resting potential</h3>

<p>Transport proteins for ions in neuron cell membranes:</p>

<ul>
  <li>Ion channels: Na + channels, K + channels, â€¦ (gated/non-gated)</li>
  <li>Ion pumps: the Na + -K + pump</li>
</ul>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824100812962.png" alt="image-20230824100812962" /></p>

<p>ç¦»å­æµ“åº¦åœ¨èƒžå†…å¤–çš„å·®å¼‚äº§ç”Ÿçš„ç”µåŠ¿å·®</p>

<ul>
  <li>
    <p>Ion concentration difference â†’ chemical gradient â†’ electrical gradient</p>
  </li>
  <li>
    <p>Nernst Equation:</p>
  </li>
</ul>

\[E=\dfrac{RT}{zF}\ln\dfrac{[\mathrm{ion}]_{\mathrm{out}}}{[\mathrm{ion}]_{\mathrm{in}}}\]

<ul>
  <li>Goldman-Hodgkin-Katz (GHK) Equation:</li>
</ul>

\[V_m=\frac{RT}{F}\ln\left(\frac{P_{\mathrm{Na}}[\mathrm{Na}^+]_{\mathrm{out}}+P_{\mathrm{K}}[\mathrm{K}^+]_{\mathrm{out}}+P_{\mathrm{Cl}}[\mathrm{Cl}^-]_{\mathrm{in}}}{P_{\mathrm{Na}}[\mathrm{Na}^+]_{\mathrm{in}}+P_{\mathrm{K}}[\mathrm{K}^+]_{\mathrm{in}}+P_{\mathrm{Cl}}[\mathrm{Cl}^-]_{\mathrm{out}}}\right)\]

<h3 id="equivalent-circuits">Equivalent circuits</h3>

<p>Components of an equivalent circuit:</p>

<ul>
  <li>Battery</li>
  <li>Capacitor</li>
  <li>Resistor</li>
</ul>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824101350048.png" alt="image-20230824101350048" /></p>

<p>Considering the potassium channel <strong>ONLY</strong>:</p>

\[\begin{gathered}
0=I_{\mathrm{cap}}+I_{K}=c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+{\frac{V_{\mathrm{M}}-E_{\mathrm{K}}}{R_{\mathrm{K}}}}, \\
c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}=-{\frac{V_{\mathrm{M}}-E_{\mathrm{K}}}{R_{\mathrm{K}}}}=-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}}). 
\end{gathered}\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824101433908.png" alt="image-20230824101433908" /></p>

<p><strong>Considering the Na + , K + , and Cl - channels and the external current I(t):</strong></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824101648270.png" alt="image-20230824101648270" /></p>

\[\begin{aligned}
\frac{I(t)}{A}&amp; =c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+i_{\mathrm{ion}}  \\
\Rightarrow\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}}&amp; =-g_{\mathrm{Cl}}(V_{\mathrm{M}}-E_{\mathrm{Cl}})-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}})-g_{\mathrm{Na}}(V_{\mathrm{M}}-E_{\mathrm{Na}})+\frac{I(t)}{A} 
\end{aligned}\]

<p>Steady-state membrane potential given a constant current input I:</p>

\[\begin{array}{rcl}\Rightarrow&amp;c_{M}\frac{\mathrm{d}V_{M}}{\mathrm{d}t}=-(g_{C1}+g_{K}+g_{Na})V_{M}+g_{C1}E_{C1}+g_{K}E_{K}+g_{Na}E_{Na}+\frac{I(t)}{A}\\\\V_{sS}=\frac{g_{CM}E_{C1}+g_{K}E_{K}+g_{Na}E_{Na}+I/A}{g_{C1}+g_{K}+g_{Na}}&amp;
\xrightarrow{I=0}
&amp;V_{sN,I=0}=E_{R}=\frac{g_{CC}E_{C1}+g_{K}E_{K}+g_{Na}E_{Na}}{g_{C1}+g_{K}+g_{Na}}\end{array}\]

<h2 id="cable-theory--passive-conduction">Cable Theory &amp; passive conduction</h2>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824102017607.png" alt="image-20230824102017607" /></p>

<p>Considering the axon as a long cylindrical cable:</p>

\[I_{\mathrm{cross}}(x,t)={I_{\mathrm{cross}}(x+\Delta x,t)}+I_{\mathrm{ion}}(x,t)+I_{\mathrm{cap}}(x,t)\]

\[V(x+\Delta x,t)-V(x,t)=-I_{\mathrm{cross}}(x,t)R_{\mathrm{L}}=-I_{\mathrm{cross}}(x,t)\frac{\Delta x}{\pi a^{2}}\rho_{\mathrm{L}} \\
{I_{\mathrm{cross}}(x,t)} =-\frac{\pi a^{2}}{\rho_{\mathrm{L}}}\frac{\partial V(x,t)}{\partial x}  \\
{I_{\mathrm{ion}}} =(2\pi a\Delta x)i_{\mathrm{ion}}  \\
I_{\mathrm{cap}}(x,t) =(2\pi a\Delta x)c_{\mathrm{M}}\frac{\partial V(x,t)}{\partial t}\]

<p>-&gt; 
\((2\pi a\Delta x)c_{\mathrm{M}}\frac{\partial V(x,t)}{\partial t}+(2\pi a\Delta x)i_{\mathrm{ion}}=\frac{\pi a^{2}}{\rho_{\mathrm{L}}}\frac{\partial V(x+\Delta x,t)}{\partial x}-\frac{\pi a^{2}}{\rho_{\mathrm{L}}}\frac{\partial V(x,t)}{\partial x}\)</p>

<p><strong>Cable Equation</strong></p>

\[c_\mathrm{M}\frac{\partial V(x,t)}{\partial t}=\frac{a}{2\rho_\mathrm{L}}\frac{\partial^2V(x,t)}{\partial x^2}-i_\mathrm{ion}\]

<p>ç”µæµåœ¨é€šè¿‡é•¿ç›´å¯¼ä½“æ—¶ä¼šæ³„éœ²ç”µæµï¼Œå¦‚ä½•è®°å½•è†œç”µä½ï¼Œå¯ä»¥ä½¿ç”¨æ­¤æ–¹ç¨‹æ¥æè¿°</p>

<p><strong>Passive conduction:</strong> ion currents are caused by leaky channels exclusively</p>

<p>\(i_{\mathrm{ion}}=V(x,t)/r_{\mathrm{M}}\)
-&gt;</p>

<p>\(\begin{aligned}c_\mathrm{M}\frac{\partial V(x,t)}{\partial t}&amp;=\frac{a}{2\rho_\mathrm{L}}\frac{\partial^2V(x,t)}{\partial x^2}-\frac{V(x,t)}{r_\mathrm{M}}\\\\\tau\frac{\partial V(x,t)}{\partial t}&amp;=\lambda^2\frac{\partial^2V(x,t)}{\partial x^2}-V(x,t)\quad\lambda=\sqrt{0.5ar_\mathrm{M}/\rho_\mathrm{L}}\end{aligned}\)
æ²¡æœ‰åŠ¨ä½œç”µä½ï¼Œå•çº¯é€šè¿‡ç”µç¼†ä¼ è¾“</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824102932665.png" alt="image-20230824102932665" /></p>

<p>If a constant external current is applied to ð‘¥ = 0  the steady-state membrane potential $ð‘‰_{ss}(ð‘¥)$ is</p>

\[\lambda^2\frac{\mathrm{d}^2V_{\mathrm{ss}}(x)}{\mathrm{d}x^2}-V_{\mathrm{ss}}(x)=0\longrightarrow V_{\mathrm{ss}}(x)=\frac{\lambda\rho_{\mathrm{L}}}{\pi a^2}I_0e^{-x/\lambda}\]

<p>ç”µä¿¡å·æ— è¡°å‡ä¼ æ’­: åŠ¨ä½œç”µä½</p>

<h2 id="action-potential--active-transport">Action potential &amp; active transport</h2>

<p>Steps of an action potential:</p>

<ul>
  <li>Depolarization</li>
  <li>Repolarization</li>
  <li>Hyperpolarization</li>
  <li>Resting</li>
</ul>

<p>Characteristics:</p>

<ul>
  <li>All-or-none</li>
  <li>Fixed shape</li>
  <li>Active electrical property</li>
</ul>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824103322522.png" alt="image-20230824103322522" /></p>

<p>How to simulate an action potential?</p>

\[\begin{aligned}
\frac{I(t)}{A}&amp; =c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+i_{\mathrm{ion}}  \\
\Rightarrow\quad c_{\mathrm{M}}\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}&amp; =-g_{\mathrm{Cl}}(V_{\mathrm{M}}-E_{\mathrm{Cl}})-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}})-g_{\mathrm{Na}}(V_{\mathrm{M}}-E_{\mathrm{Na}})+\frac{I(t)}{A} 
\end{aligned}\]

<p>ç¦»å­é€šé“çš„å¼€é—­ä¼šéšç€ç”µåŽ‹è€Œå˜åŒ–ï¼Œç”µå¯¼ä¹Ÿéšç€ç”µåŽ‹è€Œå˜åŒ–</p>

<p>Mechanism: voltage-gated ion channels</p>

<p><strong>HHå»ºæ¨¡æ€è·¯ï¼šé€šè¿‡ç”µå¯¼</strong></p>

<h3 id="nodes-of-ranvier">Nodes of Ranvier</h3>

<p>Saltatory conduction with a much higher speed and less energy consumption</p>

<p>ä¸¤ä¸ªéƒŽé£žç»“ä¹‹é—´ä¼šæœ‰ç¦»å­é€šé“ï¼Œæ—¢æœ‰è¢«åŠ¨ä¼ å¯¼ï¼Œä¹Ÿæœ‰ä¸»åŠ¨çš„é˜²æ­¢è¡°å‡</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824104220106.png" alt="image-20230824104220106" /></p>

<h2 id="the-hodgkin-huxley-model">The Hodgkin-Huxley Model</h2>

<h3 id="modeling-of-each-ion-channel">Modeling of each ion channel</h3>

<p>Modeling of each ion channel:</p>

\[g_m=\bar{g}_mm^x\]

<p>Modeling of each ion gate:</p>

\[\mathcal{C}\underset{}{\operatorname*{\overset{\alpha(\mathrm{V})}{\underset{\beta(\mathrm{V})}{\operatorname*{\longrightarrow}}}}\mathcal{O}}

\\
\Rightarrow
\begin{aligned}
\frac{\mathrm{d}m}{\mathrm{d}t}&amp; =\alpha(V)(1-m)-\beta(V)m  \\
&amp;=\frac{m_{\infty}(V)-m}{\tau_{m}(V)}
\end{aligned}

\\
\\

\begin{aligned}m_\infty(V)&amp;=\frac{\alpha(V)}{\alpha(V)+\beta(V)}.\\\tau_m(V)&amp;=\frac{1}{\alpha(V)+\beta(V)}\end{aligned}\]

\[\text{If}\ V\text{ is constant:}m(t)=m_\infty(V)+(m_0-m_\infty(V))\mathrm{e}^{-t/\tau_m(V)}\]

<h3 id="voltage-clamp">Voltage clamp</h3>

\[\begin{aligned}
\frac{I(t)}{A}&amp; =c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+i_{\mathrm{ion}}  \\
\Rightarrow\quad c_{\mathrm{M}}\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}&amp; =-g_{\mathrm{Cl}}(V_{\mathrm{M}}-E_{\mathrm{Cl}})-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}})-g_{\mathrm{Na}}(V_{\mathrm{M}}-E_{\mathrm{Na}})+\frac{I(t)}{A} 
\end{aligned}\]

<ul>
  <li>The membrane potential is kept constant</li>
  <li>The current from capacitors is excluded</li>
  <li>Currents must come from leaky/voltage-gated ion channels</li>
</ul>

\[\begin{aligned}I_{\mathrm{cap}}&amp;=c\frac{dV}{dt}=0\\I_{\mathrm{fb}}&amp;=\quad i_{\mathrm{ion}}=g_{\mathrm{Na}}(V-E_{\mathrm{Na}})+g_{\mathrm{K}}(V-E_{\mathrm{K}})+g_{\mathrm{L}}(V-E_{\mathrm{L}})\end{aligned}\]

<p>åªæµ‹é‡ä¸€ä¸ªç¦»å­é€šé“å°±å¯ä»¥å¾ˆå®¹æ˜“å¾—åˆ°ç”µå¯¼</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824111620056.png" alt="image-20230824111620056" /></p>

<h3 id="leaky-channel">Leaky channel</h3>

<p>Hyperpolarization â†’ the sodium and potassium channels are closed</p>

\[I_{\mathrm{fb}}=g_{\mathrm{Na}}(V-E_{\mathrm{Na}})+g_{\mathrm{K}}(V-E_{\mathrm{K}})+g_{\mathrm{L}}(V-E_{\mathrm{L}})\]

\[\Rightarrow I_{\mathrm{fb}}=g_L(V-E_L)\]

\[g_\mathrm{L}=0.3\mathrm{mS/cm}^2,E_\mathrm{L}=-54.4\mathrm{mV}\]

<h4 id="potassium-and-sodium-channels">Potassium and sodium channels</h4>

<p>Potassium channels: Use choline to eliminate the inward current of Na +
Na + current: $I_{fb} - I_{K}$</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824112328953.png" alt="image-20230824112328953" /></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824112333144.png" alt="image-20230824112333144" /></p>

<p>è½¬åŒ–é€ŸçŽ‡å’Œç”µå¯¼çŽ‡ä¸¤ä¸ªå› ç´ </p>

<p>Potassium channels</p>

<ul>
  <li>Resting state (gate closed)</li>
  <li>Activated state (gate open)</li>
</ul>

<p>â†’ Activation gate: $g_{\mathrm{K}}=\bar{g}_{K}n^{x}$</p>

<p>Sodium channels</p>

<ul>
  <li>Resting state (gate closed)</li>
  <li>Activated state (gate open)</li>
  <li>Inactivated state (gate blocked)</li>
</ul>

<p>â†’ Activation gate + inactivation gate: $g_{\mathrm{Na}}=\bar{g}_\text{Na}m^3h$</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824113116329.png" alt="image-20230824113116329" /></p>

<p>The gates of sodium channels</p>

<p>Modeling of each ion gate:</p>

\[\begin{aligned}
&amp;\text{gk}&amp;&amp; =\bar{g}_{K}n^{x}  \\
&amp;\text{gNa}&amp;&amp; =\bar{g}_{\mathrm{Na}}m^{3}h  \\
&amp;\frac{\mathrm{d}n}{\mathrm{d}t}&amp;&amp; =\alpha_{n}(V)(1-n)-\beta_{n}(V)n  \\
&amp;\frac{\mathrm{d}m}{\mathrm{d}t}&amp;&amp; =\alpha_{m}(V)(1-m)-\beta_{m}(V)m  \\
&amp;\frac{\mathrm{d}h}{\mathrm{d}t}&amp;&amp; =\alpha_{h}(V)(1-h)-\beta_{h}(V)h 
\end{aligned}\]

\[\begin{aligned}
\frac{\mathrm{d}m}{\mathrm{d}t}&amp; =\alpha(V)(1-m)-\beta(V)m  \\
&amp;=\frac{m_{\infty}(V)-m}{\tau_{m}(V)}
\end{aligned}\]

\[\begin{aligned}m_\infty(V)&amp;=\frac{\alpha(V)}{\alpha(V)+\beta(V)}\\\tau_m(V)&amp;=\frac{1}{\alpha(V)+\beta(V)}\end{aligned}.\]

\[m(t)=m_\infty(V)+(m_0-m_\infty(V))\mathrm{e}^{-t/\tau_m(V)}\]

<h3 id="the-hodgkin-huxleyhh-model">The Hodgkin-Huxley(HH) Model</h3>

\[c_\mathrm{M}\frac{\mathrm{d}V_\mathrm{M}}{\mathrm{d}t}=-g_\mathrm{Cl}(V_\mathrm{M}-E_\mathrm{Cl})-g_\mathrm{K}(V_\mathrm{M}-E_\mathrm{K})-g_\mathrm{Na}(V_\mathrm{M}-E_\mathrm{Na})+\frac{I(t)}{A}\]

<p>æœ¬è´¨æ˜¯4ä¸ªå¾®åˆ†æ–¹ç¨‹è”ç«‹åœ¨ä¸€èµ·</p>

\[\left\{\begin{aligned}&amp;c\frac{\mathrm{d}V}{\mathrm{d}t}=-\bar{g}_\text{Na}m^3h(V-E_\text{Na})-\bar{g}_\text{K}n^4(V-E_\text{K})-\bar{g}_\text{L}(V-E_\text{L})+I_\text{ext},\\&amp;\frac{\mathrm{d}n}{\mathrm{d}t}=\phi\left[\alpha_n(V)(1-n)-\beta_n(V)n\right]\\&amp;\frac{\mathrm{d}m}{\mathrm{d}t}=\phi\left[\alpha_m(V)(1-m)-\beta_m(V)m\right],\\&amp;\frac{\mathrm{d}h}{\mathrm{d}t}=\phi\left[\alpha_h(V)(1-h)-\beta_h(V)h\right],\end{aligned}\right.\]

\[\begin{aligned}\alpha_n(V)&amp;=\frac{0.01(V+55)}{1-\exp\left(-\frac{V+55}{10}\right)},\quad\beta_n(V)&amp;=0.125\exp\left(-\frac{V+65}{80}\right),\\\alpha_h(V)&amp;=0.07\exp\left(-\frac{V+65}{20}\right),\quad\beta_n(V)&amp;=\frac{1}{\left(\exp\left(-\frac{V+55}{10}\right)+1\right)},\\\alpha_m(V)&amp;=\frac{0.1(V+40)}{1-\exp\left(-(V+40)/10\right)},\quad\beta_m(V)&amp;=4\exp\left(-(V+65)/18\right).\end{aligned}\]

\[\phi=Q_{10}^{(T-T_{\mathrm{base}})/10}\]

<p>æ¯ä¸€æ­¥ç¬¦åˆç”Ÿç‰©å­¦</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824113714178.png" alt="image-20230824113714178" /></p>

<h4 id="how-to-fit-each-gating-variable">How to fit each gating variable?</h4>

<p><strong>Fitting n:</strong> $g_{\mathbf{K}}=\bar{g}<em>{K}n^{x}\quad m(t)=m</em>{\infty}(V)+(m_{0}-\color{red}{\boxed{m_{\infty}(V)}})\mathrm{e}^{-t/\pi_{m}(V)}$</p>

<p>â†’ $g_\mathrm{K}(V,t)=\bar{g}<em>\mathrm{K}\left[n</em>\infty(V)-(n_\infty(V)-n_0(V))\mathrm{e}^{-\frac{t}{\tau_n(V)}}\right]^x$</p>

<p>by $g_{\mathrm{K}\infty}=\bar{g}<em>{\mathrm{K}}n</em>{\infty}^{x},g_{\mathrm{K}0}=\bar{g}<em>{\mathrm{K}}n</em>{0}^{x}$</p>

<p>â†’ $g_{\mathrm{K}}(V,t)=\left[g_{\mathrm{K}\infty}^{1/x}-(g_{\mathrm{K}\infty}^{1/x}-g_{\mathrm{K}0}^{1/x})\mathrm{e}^{-\frac{t}{\tau_{n}(V)}}\right]^{x}$</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824114623467.png" alt="image-20230824114623467" /></p>

<h1 id="hodgkin-huxley-brain-dynamics-programming">Hodgkin-Huxley brain dynamics programming</h1>

<h2 id="dynamics-programming-basics">Dynamics Programming Basics</h2>

<h3 id="integrators">Integrators</h3>

<p>å¾®åˆ†å™¨</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824140806650.png" alt="image-20230824140806650" /></p>

<p><strong>example</strong></p>

<p>FitzHugh-Nagumo equation</p>

\[\begin{aligned}\tau\dot{w}&amp;=v+a-bw,\\\dot{v}&amp;=v-\frac{\nu^3}{3}-w+I_{\mathrm{ext}}.\end{aligned}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s">'Euler'</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">integral</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">Iext</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>
    <span class="n">dw</span> <span class="o">=</span> <span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="n">a</span> <span class="o">-</span> <span class="n">b</span> <span class="o">*</span> <span class="n">w</span><span class="p">)</span> <span class="o">/</span> <span class="n">tau</span>
    <span class="n">dV</span> <span class="o">=</span> <span class="n">V</span> <span class="o">-</span> <span class="n">V</span> <span class="o">*</span> <span class="n">V</span> <span class="o">*</span> <span class="n">V</span> <span class="o">/</span> <span class="mi">3</span> <span class="o">-</span> <span class="n">w</span> <span class="o">+</span> <span class="n">Iext</span>
    <span class="k">return</span> <span class="n">dV</span><span class="p">,</span> <span class="n">dw</span>
</code></pre></div></div>

<p><strong>JointEq</strong></p>

<p>In a dynamical system, there may be multiple variables that change dynamically over time. Sometimes these variables are interrelated, and updating one variable requires other variables as inputs. For better integration accuracy, we recommend that you use <code class="language-plaintext highlighter-rouge">brainpy.JointEq</code> to jointly solve interrelated differential equations.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.20</span>
<span class="n">dV</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">V</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">Iext</span><span class="p">:</span> <span class="mf">0.04</span> <span class="o">*</span> <span class="n">V</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="mi">140</span> <span class="o">-</span> <span class="n">w</span> <span class="o">+</span> <span class="n">Iext</span>	<span class="c1"># ç¬¬ä¸€ä¸ªæ–¹ç¨‹
</span><span class="n">dw</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">:</span> <span class="n">a</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">V</span> <span class="o">-</span> <span class="n">w</span><span class="p">)</span>								<span class="c1"># ç¬¬äºŒä¸ªæ–¹ç¨‹
</span><span class="n">joint_eq</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">JointEq</span><span class="p">(</span><span class="n">dV</span><span class="p">,</span> <span class="n">dw</span><span class="p">)</span>										<span class="c1"># è”åˆå¾®åˆ†æ–¹ç¨‹
</span><span class="n">integral2</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="n">joint_eq</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'rk2'</span><span class="p">)</span>						<span class="c1"># å®šä¹‰è¯¥è”åˆå¾®åˆ†æ–¹ç¨‹çš„æ•°å€¼ç§¯åˆ†æ–¹æ³•
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># å£°æ˜Žç§¯åˆ†è¿è¡Œå™¨
</span><span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">integrators</span><span class="p">.</span><span class="n">IntegratorRunner</span><span class="p">(</span>
	<span class="n">integral</span><span class="p">,</span>
    <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'V'</span><span class="p">]</span>
    <span class="n">inits</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">V</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="n">args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">Iext</span><span class="o">=</span><span class="n">Iext</span><span class="p">),</span>
    <span class="n">dt</span><span class="o">=</span><span class="mf">0.01</span>
<span class="p">)</span>

<span class="c1"># ä½¿ç”¨ç§¯åˆ†è¿è¡Œå™¨æ¥è¿›è¡Œæ¨¡æ‹Ÿ100msï¼Œç»“åˆæ­¥é•¿dt=0.01
</span><span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="mf">100.</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">V</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824142019832.png" alt="image-20230824142019832" /></p>

<h3 id="dynamicalsystem"><code class="language-plaintext highlighter-rouge">DynamicalSystem</code></h3>

<p>BrainPy provides a generic <code class="language-plaintext highlighter-rouge">SynamicalSystem</code> class to define various types of dynamical models.</p>

<p>BrainPy supports modelings in brain simulation and brain-inspired computing.</p>

<p>All these supports are based on one common concept: <strong>Dynamical System</strong> via <code class="language-plaintext highlighter-rouge">brainpy.DynamicalSystem</code>.</p>

<h4 id="what-is-dynamicalsystem">What is <code class="language-plaintext highlighter-rouge">DynamicalSystem</code></h4>

<p>A <code class="language-plaintext highlighter-rouge">DynamicalSystem</code> defines the updating rule of the model at single time step.</p>

<ol>
  <li>For models with state, <code class="language-plaintext highlighter-rouge">DynamicalSystem</code> defines the state transition from $t$ to $t + dt$, i.e., $S(t+dt)=F(S(t),x,t,dt)$, where $S$ is the state, $x$ is input, $t$ is the time, and $dt$ is the time step. This is the case for recurrent neural networks (like GRU, LSTM), neuron models (like HH, LIF), or synapse models which are widely used in brain simulation.</li>
  <li>However, for models in deep learning, like convolution and fully-connected linear layers, <code class="language-plaintext highlighter-rouge">DynamicalSystem</code> defines the input-to-output mapping, i.e., $y=F(x,t)$.</li>
</ol>

<p><img src="https://brainpy.readthedocs.io/en/latest/_images/dynamical_system.png" alt="img" /></p>

<h4 id="how-to-define-dynamicalsystem">How to define <code class="language-plaintext highlighter-rouge">DynamicalSystem</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">YourDynamicalSystem</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">DynamicalSystem</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="p">...</span>
</code></pre></div></div>

<p>Instead of input x, there are shared arguments across all nodes/layers in the network:</p>

<ul>
  <li>the current time <code class="language-plaintext highlighter-rouge">t</code>, or</li>
  <li>the current running index <code class="language-plaintext highlighter-rouge">i</code>, or</li>
  <li>the current time step <code class="language-plaintext highlighter-rouge">dt</code>, or</li>
  <li>the current phase of training or testing <code class="language-plaintext highlighter-rouge">fit=True/False</code>.</li>
</ul>

<p>Here, it is necessary to explain the usage of <code class="language-plaintext highlighter-rouge">bp.share</code>.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">bp.share.save( )</code>: The function saves shared arguments in the global context. User can save shared arguments in tow ways, for example, if user want to set the current time <code class="language-plaintext highlighter-rouge">t=100</code>, the current time step <code class="language-plaintext highlighter-rouge">dt=0.1</code>,the user can use <code class="language-plaintext highlighter-rouge">bp.share.save("t",100,"dt",0.1)</code> or <code class="language-plaintext highlighter-rouge">bp.share.save(t=100,dt=0.1)</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">bp.share.load( )</code>: The function gets the shared data by the <code class="language-plaintext highlighter-rouge">key</code>, for example, <code class="language-plaintext highlighter-rouge">bp.share.load("t")</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">bp.share.clear_shargs( )</code>: The function clears the specific shared arguments in the global context, for example, <code class="language-plaintext highlighter-rouge">bp.share.clear_shargs("t")</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">bp.share.clear( )</code>: The function clears all shared arguments in the global context.</li>
</ul>

<h4 id="how-to-run-dynamicalsystem">How to run <code class="language-plaintext highlighter-rouge">DynamicalSystem</code></h4>

<p>As we have stated above that <code class="language-plaintext highlighter-rouge">DynamicalSystem</code> only defines the updating rule at single time step, to run a <code class="language-plaintext highlighter-rouge">DynamicalSystem</code> instance over time, we need a for loop mechanism.</p>

<p><img src="https://brainpy.readthedocs.io/en/latest/_images/dynamical_system_and_dsrunner.png" alt="img" /></p>

<h5 id="brainpymathfor_loop"><code class="language-plaintext highlighter-rouge">brainpy.math.for_loop</code></h5>

<p><code class="language-plaintext highlighter-rouge">for_loop</code> is a structural control flow API which runs a function with the looping over the inputs. Moreover, this API just-in-time compile the looping process into the machine code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">section_input</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">100.</span><span class="p">,</span> <span class="mf">200.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">])</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">neu</span><span class="p">.</span><span class="n">step_run</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">neu</span><span class="p">.</span><span class="n">V</span><span class="p">.</span><span class="n">value</span>

<span class="n">vs</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">for_loop</span><span class="p">(</span><span class="n">run</span><span class="p">,</span> <span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">inputs</span><span class="p">),</span> <span class="n">progress_bar</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<h5 id="brainpyloopovertime"><code class="language-plaintext highlighter-rouge">brainpy.LoopOverTime</code></h5>

<p>Different from <code class="language-plaintext highlighter-rouge">for_loop</code>, <code class="language-plaintext highlighter-rouge">brainpy.LoopOverTime</code> is used for constructing a dynamical system that automatically loops the model over time when receiving an input.</p>

<p><code class="language-plaintext highlighter-rouge">for_loop</code> runs the model over time. While <code class="language-plaintext highlighter-rouge">brainpy.LoopOverTime</code> creates a model which will run the model over time when calling it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net2</span><span class="p">.</span><span class="n">reset_state</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">looper</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">LoopOverTime</span><span class="p">(</span><span class="n">net2</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">looper</span><span class="p">(</span><span class="n">currents</span><span class="p">)</span>
</code></pre></div></div>

<h5 id="brainpydsrunner"><code class="language-plaintext highlighter-rouge">brainpy.DSRunner</code></h5>

<p><strong>Initializing a <code class="language-plaintext highlighter-rouge">DSRunner</code></strong></p>

<p>Generally, we can initialize a runner for dynamical systems with the format of:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>runner = DSRunner(target=instance_of_dynamical_system,
                  inputs=inputs_for_target_DynamicalSystem,
                  monitors=interested_variables_to_monitor,
                  dyn_vars=dynamical_changed_variables,
                  jit=enable_jit_or_not,
                  progress_bar=report_the_running_progress,
                  numpy_mon_after_run=transform_into_numpy_ndarray
                  )
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">target</code> specifies the model to be simulated. It must an instance of brainpy.DynamicalSystem.</li>
  <li><code class="language-plaintext highlighter-rouge">inputs</code> is used to define the input operations for specific variables.
    <ul>
      <li>It should be the format of <code class="language-plaintext highlighter-rouge">[(target, value, [type, operation])]</code>, where <code class="language-plaintext highlighter-rouge">target</code> is the input target, <code class="language-plaintext highlighter-rouge">value</code> is the input value, <code class="language-plaintext highlighter-rouge">type</code> is the input type (such as â€œfixâ€, â€œiterâ€, â€œfuncâ€), <code class="language-plaintext highlighter-rouge">operation</code> is the operation for inputs (such as â€œ+â€, â€œ-â€, â€œ*â€, â€œ/â€, â€œ=â€). Also, if you want to specify multiple inputs, just give multiple <code class="language-plaintext highlighter-rouge">(target, value, [type, operation])</code>, such as <code class="language-plaintext highlighter-rouge">[(target1, value1), (target2, value2)]</code>.</li>
      <li>It can also be a function, which is used to manually specify the inputs for the target variables. This input function should receive one argument <code class="language-plaintext highlighter-rouge">tdi</code> which contains the shared arguments like time <code class="language-plaintext highlighter-rouge">t</code>, time step <code class="language-plaintext highlighter-rouge">dt</code>, and index <code class="language-plaintext highlighter-rouge">i</code>.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">monitors</code> is used to define target variables in the model. During the simulation, the history values of the monitored variables will be recorded. It can also to monitor variables by callable functions and it should be a <code class="language-plaintext highlighter-rouge">dict</code>. The <code class="language-plaintext highlighter-rouge">key</code> should be a string for later retrieval by <code class="language-plaintext highlighter-rouge">runner.mon[key]</code>. The <code class="language-plaintext highlighter-rouge">value</code> should be a callable function which receives an argument: <code class="language-plaintext highlighter-rouge">tdt</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">dyn_vars</code> is used to specify all the dynamically changed <a href="https://brainpy.readthedocs.io/en/latest/tutorial_math/variables.html">variables</a> used in the <code class="language-plaintext highlighter-rouge">target</code> model.</li>
  <li><code class="language-plaintext highlighter-rouge">jit</code> determines whether to use JIT compilation during the simulation.</li>
  <li><code class="language-plaintext highlighter-rouge">progress_bar</code> determines whether to use progress bar to report the running progress or not.</li>
  <li><code class="language-plaintext highlighter-rouge">numpy_mon_after_run</code> determines whether to transform the JAX arrays into numpy ndarray or not when the network finishes running.</li>
</ul>

<p><strong>Running a <code class="language-plaintext highlighter-rouge">DSRunner</code></strong></p>

<p>After initialization of the runner, users can call <code class="language-plaintext highlighter-rouge">.run()</code> function to run the simulation. The format of function <code class="language-plaintext highlighter-rouge">.run()</code> is showed as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">duration</span><span class="o">=</span><span class="n">simulation_time_length</span><span class="p">,</span>
           <span class="n">inputs</span><span class="o">=</span><span class="n">input_data</span><span class="p">,</span>
           <span class="n">reset_state</span><span class="o">=</span><span class="n">whether_reset_the_model_states</span><span class="p">,</span>
           <span class="n">shared_args</span><span class="o">=</span><span class="n">shared_arguments_across_different_layers</span><span class="p">,</span>
           <span class="n">progress_bar</span><span class="o">=</span><span class="n">report_the_running_progress</span><span class="p">,</span>
           <span class="n">eval_time</span><span class="o">=</span><span class="n">evaluate_the_running_time</span>
           <span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">duration</code> is the simulation time length.</li>
  <li><code class="language-plaintext highlighter-rouge">inputs</code> is the input data. If <code class="language-plaintext highlighter-rouge">inputs_are_batching=True</code>, <code class="language-plaintext highlighter-rouge">inputs</code> must be a PyTree of data with two dimensions: <code class="language-plaintext highlighter-rouge">(num_sample, num_time, ...)</code>. Otherwise, the <code class="language-plaintext highlighter-rouge">inputs</code> should be a PyTree of data with one dimension: <code class="language-plaintext highlighter-rouge">(num_time, ...)</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">reset_state</code> determines whether to reset the model states.</li>
  <li><code class="language-plaintext highlighter-rouge">shared_args</code> is shared arguments across different layers. All the layers can access the elements in <code class="language-plaintext highlighter-rouge">shared_args</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">progress_bar</code> determines whether to use progress bar to report the running progress or not.</li>
  <li><code class="language-plaintext highlighter-rouge">eval_time</code> determines whether to evaluate the running time.</li>
</ul>

<h3 id="monitors">Monitors</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># initialize monitor through a list of strings
</span><span class="n">runner1</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">net</span><span class="p">,</span>
                      <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'E.spike'</span><span class="p">,</span> <span class="s">'E.V'</span><span class="p">,</span> <span class="s">'I.spike'</span><span class="p">,</span> <span class="s">'I.V'</span><span class="p">],</span>  <span class="c1"># 4 elements in monitors
</span>                      <span class="n">inputs</span><span class="o">=</span><span class="p">[(</span><span class="s">'E.input'</span><span class="p">,</span> <span class="mf">20.</span><span class="p">),</span> <span class="p">(</span><span class="s">'I.input'</span><span class="p">,</span> <span class="mf">20.</span><span class="p">)],</span>
                      <span class="n">jit</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>Once we call the runner with a given time duration, the monitor will automatically record the variable evolutions in the corresponding models. Afterwards, users can access these variable trajectories by using .mon.[variable_name]. The default history times .mon.ts will also be generated after the model finishes its running. Letâ€™s see an example.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">runner1</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="mf">100.</span><span class="p">)</span>
<span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">raster_plot</span><span class="p">(</span><span class="n">runner1</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner1</span><span class="p">.</span><span class="n">mon</span><span class="p">[</span><span class="s">'E.spike'</span><span class="p">],</span> <span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Initialization with index specification</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">monitors</span><span class="o">=</span><span class="p">[(</span><span class="s">'E.spike'</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span>  <span class="c1"># monitor values of Variable at index of [1, 2, 3]
</span>                                <span class="s">'E.V'</span><span class="p">],</span>  <span class="c1"># monitor all values of Variable 'V'
</span>
</code></pre></div></div>

<blockquote>
  <p>The monitor shape of â€œE.Vâ€ is (run length, variable size) = (1000, 3200)
The monitor shape of â€œE.spikeâ€ is (run length, index size) = (1000, 3)</p>
</blockquote>

<p><strong>Explicit monitor target</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">monitors</span><span class="o">=</span><span class="p">{</span><span class="s">'spike'</span><span class="p">:</span> <span class="n">net</span><span class="p">.</span><span class="n">E</span><span class="p">.</span><span class="n">spike</span><span class="p">,</span> <span class="s">'V'</span><span class="p">:</span> <span class="n">net</span><span class="p">.</span><span class="n">E</span><span class="p">.</span><span class="n">V</span><span class="p">},</span>
</code></pre></div></div>

<blockquote>
  <p>The monitor shape of â€œVâ€ is = (1000, 3200)
The monitor shape of â€œspikeâ€ is = (1000, 3200)</p>
</blockquote>

<p><strong>Explicit monitor target with index specification</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">monitors</span><span class="o">=</span><span class="p">{</span><span class="s">'E.spike'</span><span class="p">:</span> <span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">E</span><span class="p">.</span><span class="n">spike</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span>  <span class="c1"># monitor values of Variable at index of [1, 2]
</span>                                <span class="s">'E.V'</span><span class="p">:</span> <span class="n">net</span><span class="p">.</span><span class="n">E</span><span class="p">.</span><span class="n">V</span><span class="p">},</span>  <span class="c1"># monitor all values of Variable 'V'
</span></code></pre></div></div>

<blockquote>
  <p>The monitor shape of â€œE.Vâ€ is = (1000, 3200)
The monitor shape of â€œE.spikeâ€ is = (1000, 2)</p>
</blockquote>

<h3 id="inputs">Inputs</h3>

<p>In brain dynamics simulation, various inputs are usually given to different units of the dynamical system. In BrainPy, <code class="language-plaintext highlighter-rouge">inputs</code> can be specified to runners for dynamical systems. The aim of <code class="language-plaintext highlighter-rouge">inputs</code> is to mimic the input operations in experiments like Transcranial Magnetic Stimulation (TMS) and patch clamp recording.</p>

<p><code class="language-plaintext highlighter-rouge">inputs</code> should have the format like <code class="language-plaintext highlighter-rouge">(target, value, [type, operation])</code>, where</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">target</code> is the target variable to inject the input.</li>
  <li><code class="language-plaintext highlighter-rouge">value</code> is the input value. It can be a scalar, a tensor, or a iterable object/function.</li>
  <li><code class="language-plaintext highlighter-rouge">type</code> is the type of the input value. It support two types of input: <code class="language-plaintext highlighter-rouge">fix</code> and <code class="language-plaintext highlighter-rouge">iter</code>. The first one means that the data is static; the second one denotes the data can be iterable, no matter whether the input value is a tensor or a function. The <code class="language-plaintext highlighter-rouge">iter</code> type must be explicitly stated.</li>
  <li><code class="language-plaintext highlighter-rouge">operation</code> is the input operation on the target variable. It should be set as one of <code class="language-plaintext highlighter-rouge">{ + , - , * , / , = }</code>, and if users do not provide this item explicitly, it will be set to â€˜+â€™ by default, which means that the target variable will be updated as <code class="language-plaintext highlighter-rouge">val = val + input</code>.</li>
</ul>

<h4 id="static-inputs">Static inputs</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">runner6</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">net</span><span class="p">,</span>
                      <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'E.spike'</span><span class="p">],</span>
                      <span class="n">inputs</span><span class="o">=</span><span class="p">[(</span><span class="s">'E.input'</span><span class="p">,</span> <span class="mf">20.</span><span class="p">),</span> <span class="p">(</span><span class="s">'I.input'</span><span class="p">,</span> <span class="mf">20.</span><span class="p">)],</span>  <span class="c1"># static inputs
</span>                      <span class="n">jit</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">runner6</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="mf">100.</span><span class="p">)</span>
<span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">raster_plot</span><span class="p">(</span><span class="n">runner6</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner6</span><span class="p">.</span><span class="n">mon</span><span class="p">[</span><span class="s">'E.spike'</span><span class="p">])</span>
</code></pre></div></div>

<h4 id="iterable-inputs">Iterable inputs</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">I</span><span class="p">,</span> <span class="n">length</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">section_input</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">20.</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                                    <span class="n">durations</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
                                    <span class="n">return_length</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                    <span class="n">dt</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">runner7</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">net</span><span class="p">,</span>
                      <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'E.spike'</span><span class="p">],</span>
                      <span class="n">inputs</span><span class="o">=</span><span class="p">[(</span><span class="s">'E.input'</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="s">'iter'</span><span class="p">),</span> <span class="p">(</span><span class="s">'I.input'</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="s">'iter'</span><span class="p">)],</span>  <span class="c1"># iterable inputs
</span>                      <span class="n">jit</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">runner7</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
<span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">raster_plot</span><span class="p">(</span><span class="n">runner7</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner7</span><span class="p">.</span><span class="n">mon</span><span class="p">[</span><span class="s">'E.spike'</span><span class="p">])</span>
</code></pre></div></div>

<h2 id="run-a-built-in-hh-model">Run a built-in HH model</h2>

<p><a href="https://brainpy.readthedocs.io/en/latest/tutorial_building/overview_of_dynamic_model.html">Using Built-in Models â€” BrainPy documentation</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">brainpy</span> <span class="k">as</span> <span class="n">bp</span>
<span class="kn">import</span> <span class="nn">brainpy.math</span> <span class="k">as</span> <span class="n">bm</span>

<span class="n">current</span><span class="p">,</span> <span class="n">length</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">section_input</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="n">bm</span><span class="p">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">15.</span><span class="p">]),</span> <span class="mf">0.</span><span class="p">],</span>
                                         <span class="n">durations</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">25</span><span class="p">],</span>
                                         <span class="n">return_length</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">hh_neurons</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">neurons</span><span class="p">.</span><span class="n">HH</span><span class="p">(</span><span class="n">current</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">hh_neurons</span><span class="p">,</span> <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'V'</span><span class="p">,</span> <span class="s">'m'</span><span class="p">,</span> <span class="s">'h'</span><span class="p">,</span> <span class="s">'n'</span><span class="p">],</span> <span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="s">'input'</span><span class="p">,</span> <span class="n">current</span><span class="p">,</span> <span class="s">'iter'</span><span class="p">))</span>

<span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="run-a-hh-model-from-scratch">Run a HH model from scratch</h2>

<p>The mathematic expression of the HH model</p>

\[\left\{\begin{aligned}&amp;c\frac{\mathrm{d}V}{\mathrm{d}t}=-\bar{g}_\text{Na}m^3h(V-E_\text{Na})-\bar{g}_\text{K}n^4(V-E_\text{K})-\bar{g}_\text{L}(V-E_\text{L})+I_\text{ext},\\&amp;\frac{\mathrm{d}n}{\mathrm{d}t}=\phi\left[\alpha_n(V)(1-n)-\beta_n(V)n\right]\\&amp;\frac{\mathrm{d}m}{\mathrm{d}t}=\phi\left[\alpha_m(V)(1-m)-\beta_m(V)m\right],\\&amp;\frac{\mathrm{d}h}{\mathrm{d}t}=\phi\left[\alpha_h(V)(1-h)-\beta_h(V)h\right],\end{aligned}\right.\]

\[\begin{aligned}\alpha_n(V)&amp;=\frac{0.01(V+55)}{1-\exp\left(-\frac{V+55}{10}\right)},\quad\beta_n(V)&amp;=0.125\exp\left(-\frac{V+65}{80}\right),\\\alpha_h(V)&amp;=0.07\exp\left(-\frac{V+65}{20}\right),\quad\beta_n(V)&amp;=\frac{1}{\left(\exp\left(-\frac{V+55}{10}\right)+1\right)},\\\alpha_m(V)&amp;=\frac{0.1(V+40)}{1-\exp\left(-(V+40)/10\right)},\quad\beta_m(V)&amp;=4\exp\left(-(V+65)/18\right).\end{aligned}\]

\[\phi=Q_{10}^{(T-T_{\mathrm{base}})/10}\]

<p>V: the membrane potential</p>

<p>n: activation variable of the Kt channel</p>

<p>m: activation variable of the Nat channel</p>

<p>h; inactivation variable of the Nat channe</p>

<h3 id="define-hh-model-class">Define HH model <code class="language-plaintext highlighter-rouge">class</code></h3>

<ul>
  <li>Inherit <code class="language-plaintext highlighter-rouge">bp.dyn.NeuDyn</code></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">brainpy</span> <span class="k">as</span> <span class="n">bp</span>
<span class="kn">import</span> <span class="nn">brainpy.math</span> <span class="k">as</span> <span class="n">bm</span>

<span class="k">class</span> <span class="nc">HH</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">NeuDyn</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span>
                <span class="n">ENa</span><span class="o">=</span><span class="mf">50.</span><span class="p">,</span> <span class="n">gNa</span><span class="o">=</span><span class="mf">120.</span><span class="p">,</span>
                <span class="n">Ek</span><span class="o">=-</span><span class="mf">77.</span><span class="p">,</span> <span class="n">gK</span><span class="o">=</span><span class="mf">36.</span><span class="p">,</span>
                <span class="n">EL</span><span class="o">=-</span><span class="mf">54.387</span><span class="p">,</span> <span class="n">gL</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span>
                <span class="n">V_th</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mf">6.3</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HH</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="initialization">Initialization</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">brainpy</span> <span class="k">as</span> <span class="n">bp</span>
<span class="kn">import</span> <span class="nn">brainpy.math</span> <span class="k">as</span> <span class="n">bm</span>

<span class="k">class</span> <span class="nc">HH</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">NeuDyn</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span>
                <span class="n">ENa</span><span class="o">=</span><span class="mf">50.</span><span class="p">,</span> <span class="n">gNa</span><span class="o">=</span><span class="mf">120.</span><span class="p">,</span>
                <span class="n">Ek</span><span class="o">=-</span><span class="mf">77.</span><span class="p">,</span> <span class="n">gK</span><span class="o">=</span><span class="mf">36.</span><span class="p">,</span>
                <span class="n">EL</span><span class="o">=-</span><span class="mf">54.387</span><span class="p">,</span> <span class="n">gL</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span>
                <span class="n">V_th</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mf">6.3</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HH</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
        
        <span class="c1"># parameters
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">ENa</span> <span class="o">=</span> <span class="n">ENa</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">EK</span> <span class="o">=</span> <span class="n">EK</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">EL</span> <span class="o">=</span> <span class="n">EL</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gNA</span> <span class="o">=</span> <span class="n">gNa</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gK</span> <span class="o">=</span> <span class="n">gK</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gL</span> <span class="o">=</span> <span class="n">gL</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">C</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">V_th</span> <span class="o">=</span> <span class="n">V_th</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">T_base</span> <span class="o">=</span> <span class="mf">6.3</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">phi</span> <span class="o">=</span> <span class="mf">3.0</span> <span class="o">**</span> <span class="p">((</span><span class="n">T</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">T_base</span><span class="p">)</span> <span class="o">/</span> <span class="mf">10.0</span><span class="p">)</span>
        
        <span class="c1"># variable
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="o">-</span><span class="mf">70.68</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">0.0266</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">0.772</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">0.235</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="nb">input</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">spike</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">t_last_spike</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e7</span><span class="p">)</span>
        
        <span class="c1"># å®šä¹‰ç§¯åˆ†å‡½æ•°
</span>    	<span class="bp">self</span><span class="p">.</span><span class="n">integral</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">derivative</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'exp_auto'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="define-the-derivative-function">Define the derivative function</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="nb">property</span>
<span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">bp</span><span class="p">.</span><span class="n">JointEq</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dV</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dm</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dh</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dn</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">dV</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">Iext</span><span class="p">):</span>
    <span class="n">I_Na</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">gNa</span> <span class="o">*</span> <span class="n">m</span> <span class="o">**</span> <span class="mf">3.0</span> <span class="o">*</span> <span class="n">h</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">ENa</span><span class="p">)</span>
    <span class="n">I_K</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">gK</span> <span class="o">*</span> <span class="n">n</span> <span class="o">**</span> <span class="mf">4.0</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">EK</span><span class="p">)</span>
    <span class="n">I_leak</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">gL</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">EL</span><span class="p">)</span>
    <span class="n">dVdt</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span> <span class="n">I_Na</span> <span class="o">-</span> <span class="n">I_K</span> <span class="o">-</span> <span class="n">I_leak</span> <span class="o">+</span> <span class="n">Iext</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">C</span>
    <span class="k">return</span> <span class="n">dVdt</span>

<span class="k">def</span> <span class="nf">dm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">40</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">40</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="mf">4.0</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">65</span><span class="p">)</span> <span class="o">/</span> <span class="mi">18</span><span class="p">)</span>
    <span class="n">dmdt</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">m</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">phi</span> <span class="o">*</span> <span class="n">dmdt</span>

<span class="k">def</span> <span class="nf">dh</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.07</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">65</span><span class="p">)</span> <span class="o">/</span> <span class="mf">20.</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">35</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">dhdt</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">)</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">h</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">phi</span> <span class="o">*</span> <span class="n">dhdt</span>

<span class="k">def</span> <span class="nf">dn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">55</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">55</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="mf">0.125</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">65</span><span class="p">)</span> <span class="o">/</span> <span class="mi">80</span><span class="p">)</span>
    <span class="n">dndt</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">n</span><span class="p">)</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">n</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">phi</span> <span class="o">*</span> <span class="n">dndt</span>
</code></pre></div></div>

<h3 id="complete-the-update-function">Complete the <code class="language-plaintext highlighter-rouge">update()</code> function</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'t'</span><span class="p">)</span>
    <span class="n">dt</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'dt'</span><span class="p">)</span>
    <span class="c1"># TODO: æ›´æ–°å˜é‡V, m, h, n, æš‚å­˜åœ¨V, m, h, nä¸­
</span>    <span class="n">V</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">V</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="n">dt</span><span class="p">)</span>

    <span class="c1">#åˆ¤æ–­æ˜¯å¦å‘ç”ŸåŠ¨ä½œç”µä½
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">spike</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">logical_and</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">V</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_th</span><span class="p">,</span> <span class="n">V</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_th</span><span class="p">)</span>
    <span class="c1"># æ›´æ–°æœ€åŽä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">t_last_spike</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">spike</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">t_last_spike</span><span class="p">)</span>

    <span class="c1"># TODO: æ›´æ–°å˜é‡V, m, h, nçš„å€¼
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">V</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">V</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">m</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">m</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">h</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">n</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">n</span>

    <span class="c1">#é‡ç½®è¾“å…¥
</span>    <span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">[:]</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div></div>

<h3 id="simulation">Simulation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">current</span><span class="p">,</span> <span class="n">length</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">section_input</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="n">bm</span><span class="p">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">15.</span><span class="p">]),</span> <span class="mf">0.</span><span class="p">],</span>
                                          <span class="n">durations</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">25</span><span class="p">],</span>
                                          <span class="n">return_length</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">hh_neurons</span> <span class="o">=</span> <span class="n">HH</span><span class="p">(</span><span class="n">current</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">hh_neurons</span><span class="p">,</span> <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'V'</span><span class="p">,</span> <span class="s">'m'</span><span class="p">,</span> <span class="s">'h'</span><span class="p">,</span> <span class="s">'n'</span><span class="p">],</span> <span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="s">'input'</span><span class="p">,</span> <span class="n">current</span><span class="p">,</span> <span class="s">'iter'</span><span class="p">))</span>

<span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="visualization">Visualization</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">line_plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">V</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s">'V (mV)'</span><span class="p">,</span> <span class="n">plot_ids</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">current</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">current</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="mf">90.</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">m</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">h</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">n</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">([</span><span class="s">'m'</span><span class="p">,</span> <span class="s">'h'</span><span class="p">,</span> <span class="s">'n'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Time (ms)'</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="customize-a-conductance-based-model">Customize a conductance-based model</h2>

<p>ç”µè·¯æ¨¡æ‹Ÿï¼Œå†™æˆç”µå¯¼å½¢å¼</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824180831033.png" alt="image-20230824180831033" /></p>

\[\begin{aligned}
\text{gK}&amp; =\bar{g}_\text{K}n^4,  \\
\frac{\mathrm{d}n}{\mathrm{d}t}&amp; =\phi[\alpha_n(V)(1-n)-\beta_n(V)n], 
\end{aligned}\]

<p>åŠ¨åŠ›å­¦å½¢å¼æè¿°ï¼Œå¼•å…¥é—¨æ¡†å˜é‡$n$</p>

\[\begin{aligned}
&amp;\alpha_{n}(V) =\frac{0.01(V+55)}{1-\exp(-\frac{V+55}{10})},  \\
&amp;\beta_{n}(V) =0.125\exp\left(-\frac{V+65}{80}\right). 
\end{aligned}\]

<p>ç”±æ­¤å¼æ¥å»ºæ¨¡é’¾ç¦»å­é€šé“</p>

<h3 id="programming-an-ion-channel">Programming an ion channel</h3>

<h4 id="three-ion-channel">Three ion channel</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">brainpy</span> <span class="k">as</span> <span class="n">bp</span>
<span class="kn">import</span> <span class="nn">brainpy.math</span> <span class="k">as</span> <span class="n">bm</span>

<span class="k">class</span> <span class="nc">IK</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">IonChannel</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">E</span><span class="o">=-</span><span class="mf">77.</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">36.</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'exp_auto'</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">IK</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">g_max</span> <span class="o">=</span> <span class="n">g_max</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">=</span> <span class="n">E</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">phi</span> <span class="o">=</span> <span class="n">phi</span>

    <span class="bp">self</span><span class="p">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>  <span class="c1"># variables should be packed with bm.Variable
</span>    
    <span class="bp">self</span><span class="p">.</span><span class="n">integral</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dn</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">dn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="n">alpha_n</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">55</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">55</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">beta_n</span> <span class="o">=</span> <span class="mf">0.125</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">65</span><span class="p">)</span> <span class="o">/</span> <span class="mi">80</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">phi</span> <span class="o">*</span> <span class="p">(</span><span class="n">alpha_n</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">n</span><span class="p">)</span> <span class="o">-</span> <span class="n">beta_n</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'t'</span><span class="p">)</span>
    <span class="n">dt</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'dt'</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">n</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="n">dt</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">current</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">g_max</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">n</span> <span class="o">**</span> <span class="mi">4</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">-</span> <span class="n">V</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">INa</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">IonChannel</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span> <span class="mf">50.</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">120.</span><span class="p">,</span> <span class="n">phi</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'exp_auto'</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">INa</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">g_max</span> <span class="o">=</span> <span class="n">g_max</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">=</span> <span class="n">E</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">phi</span> <span class="o">=</span> <span class="n">phi</span>

    <span class="bp">self</span><span class="p">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>  <span class="c1"># variables should be packed with bm.Variable
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>
    
    <span class="bp">self</span><span class="p">.</span><span class="n">integral_m</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dm</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">integral_h</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dh</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">dm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="c1"># TODO: è®¡ç®—dm/dt
</span>    <span class="n">alpha_m</span> <span class="o">=</span> <span class="mf">0.11</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">40</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">40</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">beta_m</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">65</span><span class="p">)</span> <span class="o">/</span> <span class="mi">18</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">phi</span> <span class="o">*</span> <span class="p">(</span><span class="n">alpha_m</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span> <span class="o">-</span> <span class="n">beta_m</span> <span class="o">*</span> <span class="n">m</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">dh</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="c1"># TODO: è®¡ç®—dh/dt
</span>    <span class="n">alpha_h</span> <span class="o">=</span> <span class="mf">0.07</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">65</span><span class="p">)</span> <span class="o">/</span> <span class="mi">20</span><span class="p">)</span>
    <span class="n">beta_h</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">V</span> <span class="o">+</span> <span class="mi">35</span><span class="p">)</span> <span class="o">/</span> <span class="mi">10</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">phi</span> <span class="o">*</span> <span class="p">(</span><span class="n">alpha_h</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">h</span><span class="p">)</span> <span class="o">-</span> <span class="n">beta_h</span> <span class="o">*</span> <span class="n">h</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'t'</span><span class="p">)</span>
    <span class="n">dt</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'dt'</span><span class="p">)</span>
    <span class="c1"># TODO: æ›´æ–°self.m, self.h
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">m</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral_m</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">m</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="n">dt</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral_h</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="n">dt</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">current</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">g_max</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">m</span> <span class="o">**</span> <span class="mi">3</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">h</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">-</span> <span class="n">V</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">IL</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">IonChannel</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">E</span><span class="o">=-</span><span class="mf">54.39</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">0.03</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">IL</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">g_max</span> <span class="o">=</span> <span class="n">g_max</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">=</span> <span class="n">E</span>

  <span class="k">def</span> <span class="nf">current</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">g_max</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">-</span> <span class="n">V</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="k">pass</span>
</code></pre></div></div>

<h4 id="build-a-hh-model-with-ion-channels">Build a HH model with ion channels</h4>

<p><strong>Using customized ion channels</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">HH</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">CondNeuGroup</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">HH</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">V_initializer</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">Uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">80</span><span class="p">,</span> <span class="o">-</span><span class="mf">60.</span><span class="p">))</span>
    <span class="c1"># TODO: åˆå§‹åŒ–ä¸‰ä¸ªç¦»å­é€šé“
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">IK</span> <span class="o">=</span> <span class="n">IK</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">E</span><span class="o">=-</span><span class="mf">77.</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">36.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">INa</span> <span class="o">=</span> <span class="n">INa</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">50.</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">120.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">IL</span> <span class="o">=</span> <span class="n">IL</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">E</span><span class="o">=-</span><span class="mf">54.39</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">0.03</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Using built-in ion channels</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">HH</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">CondNeuGroup</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">INa</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">channels</span><span class="p">.</span><span class="n">INa_HH1952</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">IK</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">channels</span><span class="p">.</span><span class="n">IK_HH1952</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">IL</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">cahnnels</span><span class="p">.</span><span class="n">IL</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">E</span><span class="o">=-</span><span class="mf">54.387</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">0.03</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="simulation-1">Simulation</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">neu</span> <span class="o">=</span> <span class="n">HH</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span>
    <span class="n">neu</span><span class="p">,</span> 
    <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'V'</span><span class="p">,</span> <span class="s">'IK.n'</span><span class="p">,</span> <span class="s">'INa.m'</span><span class="p">,</span> <span class="s">'INa.h'</span><span class="p">],</span> 
    <span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="s">'input'</span><span class="p">,</span> <span class="mf">1.698</span><span class="p">)</span>  <span class="c1"># near the threshold current
</span><span class="p">)</span>

<span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>  <span class="c1"># the running time is 200 ms
</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">[</span><span class="s">'ts'</span><span class="p">],</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">[</span><span class="s">'V'</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'t (ms)'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'V (mV)'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">"HH.jpg"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">[</span><span class="s">'ts'</span><span class="p">],</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">[</span><span class="s">'IK.n'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'n'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">[</span><span class="s">'ts'</span><span class="p">],</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">[</span><span class="s">'INa.m'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'m'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">[</span><span class="s">'ts'</span><span class="p">],</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">[</span><span class="s">'INa.h'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'h'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'t (ms)'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">"HH_channels.jpg"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230824184016011.png" alt="image-20230824184016011" /></p>

<h1 id="simple-neuron-modeling-simplified-models">Simple Neuron Modeling: Simplified Models</h1>

<h2 id="the-leaky-integrate-and-firelif-neuron-model">The Leaky Integrate-and-Fire(LIF) Neuron Model</h2>

<h3 id="the-lif-neuron-model">The LIF neuron model</h3>

\[\begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-(V-V_{\mathrm{rest}})+RI(t)\\\\\mathrm{if}V&amp;&gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\text{last}\ {t_{ref}}\end{aligned}\]

<p>åªæœ‰ä¸€ä¸ªå¾®åˆ†æ–¹ç¨‹ï¼Œè¦åŠ ä¸€ä¸ªä¸åº”æœŸ(<strong>t refractory period</strong>)ï¼Œè†œç”µä½ä¸å‘ç”Ÿä»»ä½•æ”¹å˜ï¼Œè®¤ä¸ºç¦»å­é€šé“åªæœ‰æ³„éœ²é€šé“</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825101057570.png" alt="image-20230825101057570" /></p>

<p>Given a constant current input:</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825101410745.png" alt="image-20230825101410745" /></p>

<p>æ²¡æœ‰å»ºæ¨¡å‡†ç¡®å˜åŒ–ï¼Œåªæä¾›ä»€ä¹ˆæ—¶å€™è†œç”µä½çš„å˜åŒ–</p>

<h3 id="the-dynamic-features-of-the-lif-model">The dynamic features of the LIF model</h3>

<p><strong>General solution (constant input):</strong>$V(t)=V_{\text{reset}}+RI_{\text{c}}(1-\mathrm{e}^{-\frac{t-t_0}{\tau}})$</p>

<p><strong>Firing frequency:</strong></p>

\[\begin{aligned}T&amp;=-\tau\ln\left(1-\frac{V_{\phi h}-V_{\mathrm{rest}}}{RI_{\varsigma}}\right)\\f&amp;=\frac{1}{T+t_{\mathrm{ref}}}=\frac{1}{t_{\mathrm{ref}}-\tau\ln\left(1-\frac{V_{0}-V_{\mathrm{rest}}}{RI_{\varsigma}}\right)}\end{aligned}\]

<p><strong>Rheobase current (minimal current):</strong></p>

\[I_{\theta}=\frac{V_{\mathrm{th}}-V_{\mathrm{reset}}}{R}\]

<p>åŸºå¼ºç”µæµï¼Œå¦‚æžœå°äºŽå®ƒå°†æ— æ³•å‘æ”¾</p>

<h3 id="strengths--weaknesses-of-the-lif-model">Strengths &amp; weaknesses of the LIF model</h3>

<h4 id="strengths">Strengths</h4>

<ul>
  <li>Simple, high simulation efficiency</li>
  <li>Intuitive</li>
  <li>Fits well the subthreshold membrane potential</li>
</ul>

<h4 id="weaknesses">Weaknesses</h4>

<ul>
  <li>The shape of action potentials is over-simplified</li>
  <li>Has no memory of the spiking history</li>
  <li>Cannot reproduce diverse firing patterns</li>
</ul>

<h3 id="other-univariate-neuron-models">Other Univariate neuron models</h3>

<h4 id="the-quadratic-integrate-and-fire-qof-model">The Quadratic Integrate-and-Fire (QOF) model:</h4>

\[\begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=a_{0}(V-V_{\mathrm{re}t})(V-V_{\mathrm{c}})+RI(t)\\&amp;\text{if }V&gt;\theta,\quad V\leftarrow V_{\mathrm{re}set}\quad\text{last}\quad t_{\mathrm{ref}}\end{aligned}\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825103243039.png" alt="image-20230825103243039" /></p>

<p>è†œç”µä½ä»éœ€è¦æ‰‹åŠ¨é‡ç½®</p>

<h4 id="the-theta-neuron-model">The Theta neuron model</h4>

\[\frac{\mathrm{d}\theta}{\mathrm{d}t}=1-\cos\theta+(1+\cos\theta)(\beta+I(t))\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825103331170.png" alt="image-20230825103331170" /></p>

<p>éšå¼è¡¨è¾¾ï¼Œä¸å…·æœ‰ç‰©ç†æ„ä¹‰ï¼Œä½†ä¹Ÿä¼šè¿›è¡Œæ•´åˆå‘æ”¾</p>

<h4 id="the-exponential-integrate-and-fire-expif-model">The Exponential Integrate-and-Fire (ExpIF) model</h4>

\[\begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-\left(V-V_{\mathrm{res}t}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{3T}}+RI(t)\\\mathrm{if~}V&amp;&gt;\theta,\quad V\leftarrow V_{\mathrm{res}t}\mathrm{last}t_{\mathrm{ref}}\end{aligned}\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825103501912.png" alt="image-20230825103501912" /></p>

<p>ä»éœ€è¦æ‰‹åŠ¨é‡ç½®è†œç”µä½</p>

<h2 id="the-adaptive-exponential-integrate-and-fireadex-neuron-model">The Adaptive Exponential Integrate-and-Fire(AdEx) Neuron Model</h2>

<h3 id="the-adex-neuron-model">The AdEx neuron model</h3>

<p>Two variables:</p>

<ul>
  <li>ð‘‰: membrane potential</li>
  <li>ð‘¤: adaptation variable</li>
</ul>

\[\begin{aligned}
\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{S_{T}}}-Rw+RI(t)  \\
\tau_{w}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{\mathrm{w}}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right)  \\
\mathrm{if}V&amp; &gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} 
\end{aligned}\]

<p>ä¸ä¸ºé›¶ï¼Œå°±ä¼šè¡°å‡åˆ°$-w$</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825103840880.png" alt="image-20230825103840880" /></p>

<ul>
  <li>A larger ð‘¤ suppresses ð‘‰ from increasing</li>
  <li>ð‘¤ decays exponentially while having a sudden increase when the neuron fires</li>
</ul>

<p><strong>Firing patterns of the AdEx model</strong></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825104254936.png" alt="image-20230825104254936" /></p>

<p><strong>Categorization of firing patterns</strong></p>

<p>According to the steady-state firing time intervals:</p>

<ul>
  <li>Tonic/regular spiking</li>
  <li>Adapting</li>
  <li>Bursting</li>
  <li>Irregular spiking</li>
</ul>

<p>According to the initial-state features:</p>

<ul>
  <li>Tonic/classic spiking</li>
  <li>Initial burst</li>
  <li>Delayed spiking</li>
</ul>

<h3 id="other-multivariate-neuron-models">Other multivariate neuron models</h3>

<h4 id="the-izhikevich-model">The Izhikevich model</h4>

\[\begin{aligned}
&amp;\frac{dV}{dt} =0.04V^{2}+5V+140-u+I  \\
&amp;\frac{\mathrm{d}u}{\mathrm{d}t} =a\left(bV-u\right)  \\
&amp;\operatorname{if}V &gt;\theta,\quad V\leftarrow c,u\leftarrow u+d\text{ last }t_{\mathrm{ref}} 
\end{aligned}\]

<p>äºŒæ¬¡æ•´åˆå‘æ”¾å¤šåŠ äº†ä¸€ä¸ª$u$</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825104832770.png" alt="image-20230825104832770" /></p>

<h4 id="the-fitzhughnagumo-fhn-model">The FitzHughâ€“Nagumo (FHN) model</h4>

\[\begin{aligned}\dot{v}&amp;=v-\frac{v^3}3-w+RI_{\mathrm{ext}}\\\tau\dot{w}&amp;=v+a-bw.\end{aligned}\]

<p>æ²¡æœ‰å¯¹è†œç”µä½è¿›è¡Œäººä¸ºçš„é‡ç½®ï¼Œå¯ä»¥æ›´å¥½çš„è¿›è¡ŒåŠ¨åŠ›å­¦åˆ†æžï¼Œæ²¡æœ‰æ‰“ç ´å¾®åˆ†æ–¹ç¨‹çš„è¿žç»­æ€§</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825104922636.png" alt="image-20230825104922636" /></p>

<h4 id="the-generalized-integrate-and-fire-gif-model">The Generalized Integrate-and-Fire (GIF) model</h4>

<p>n+2ä¸ªå˜é‡</p>

\[\begin{aligned}
&amp;\tau{\frac{\mathrm{d}V}{\mathrm{d}t}} =-\left(V-V_{\mathrm{rest}}\right)+R\sum_{j}I_{j}+RI  \\
&amp;\frac{\mathrm{d}\Theta}{\mathrm{d}t} =a\left(V-V_{\mathrm{rest}}\right)-b\left(\Theta-\Theta_{\infty}\right)  \\
&amp;\frac{\mathrm{d}l_{j}}{\mathrm{d}t} =-k_{j}I_{j},\quad j=1,2,...,n  \\
&amp;\operatorname{if}V &gt;\Theta,\quad I_{j}\leftarrow R_{j}I_{j}+A_{j},V\leftarrow V_{\mathrm{reset}},\Theta\leftarrow max(\Theta_{\mathrm{reset}},\Theta) 
\end{aligned}\]

<p>æ¯ä¸ªå˜é‡éƒ½æ˜¯çº¿æ€§çš„ï¼Œæ³›åŒ–æ€§ä½“çŽ°åœ¨é‡ç½®æ¡ä»¶ä¸Š</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825105035349.png" alt="image-20230825105035349" /></p>

<h2 id="dynamic-analysis-phase-plane-analysis">Dynamic analysis: phase-plane analysis</h2>

<h3 id="phase-plane-analysis">Phase plane analysis</h3>

<p>å¯¹åŠ¨åŠ›å­¦ç³»ç»Ÿçš„è¡Œä¸ºæ¥åˆ†æžï¼Œæ™®éå¯¹ä¸¤ä¸ªå˜é‡æ¥è¿›è¡Œåˆ†æž</p>

<p>Analyzes the behavior of a dynamical system with (usually two) variables described by ordinary differential equations</p>

\[\begin{aligned}
&amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp;&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{S_{T}}}-Rw+RI(t)  \\
&amp;\tau_{W}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp;&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right)  \\
&amp;\mathrm{if}V&amp;&amp; &gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} 
\end{aligned}\]

<p><strong>Elements:</strong></p>

<ul>
  <li>Nullclines: $\mathrm{d}V/\mathrm{d}t=0;\mathrm{d}w/\mathrm{d}t=0$</li>
  <li>Fixed points: $\mathrm{d}V/\mathrm{d}t=0\mathrm{~and~}\mathrm{d}w/\mathrm{d}t=0$</li>
  <li>The vector field</li>
  <li>The trajectory of variables</li>
</ul>

<p>å‡è®¾å¤–éƒ¨ç”µæµæ’å®š</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825110708994.png" alt="image-20230825110708994" /></p>

<h3 id="phase-plane-analysis-for-the-adex-neuron-model">Phase plane analysis for the AdEx neuron model</h3>

\[\begin{aligned}
&amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp;&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{\Lambda_{T}}}-Rw+RI(t)  \\
&amp;\tau_{w}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp;&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right)  \\
&amp;\text{ifV}&amp;&amp; &gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} 
\end{aligned}\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825110811399.png" alt="image-20230825110811399" /></p>

<h4 id="tonic">Tonic</h4>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825112857175.png" alt="image-20230825112857175" /></p>

<h4 id="adaptation">Adaptation</h4>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825112918815.png" alt="image-20230825112918815" /></p>

<h4 id="bursting">Bursting</h4>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825112933938.png" alt="image-20230825112933938" /></p>

<h4 id="transient-spiking">Transient spiking</h4>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825112950297.png" alt="image-20230825112950297" /></p>

<h2 id="dynamic-analysis-bifurcation-analysis">Dynamic analysis: bifurcation analysis</h2>

<h3 id="bifurcation-analysis">Bifurcation analysis</h3>

<p>Quantitative analysis of the existence and the properties of fixed points in a dynamical system with a changing parameter</p>

<p>æŸä¸ªå¤–ç•Œæ¡ä»¶å˜åŒ–æ—¶ï¼Œå›ºå®šç‚¹çš„å˜åŒ–</p>

<p>Elements:</p>

<ul>
  <li>Lines of fixed points</li>
  <li>Stability properties of fixed points</li>
</ul>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825114510710.png" alt="image-20230825114510710" /></p>

<h3 id="bifurcation-analysis-for-the-adex-neuron-model">Bifurcation analysis for the AdEx Neuron model</h3>

<p>bifurcation analysis for 2 variables
Variables: ð‘‰ and ð‘¤
Parameters: $I_{ext}$</p>

\[\begin{aligned}
&amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}=-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{ST}}}-Rw+RI(t) \\
&amp;\text{-} {\frac{\mathrm{d}w}{\mathrm{d}t}}=a(V-V_{\mathrm{rest}})-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right)  \\
&amp;\mathrm{if}V&gt;\theta,\quad V\leftarrow V_{\mathrm{reset}}\ \mathrm{last}\ t_{\mathrm{ref}}
\end{aligned}\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825114801456.png" alt="image-20230825114801456" /></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825114742740.png" alt="image-20230825114742740" /></p>

<p><strong>Subjects: two variables (ð‘‰ and ð‘¤)</strong></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825114856403.png" alt="image-20230825114856403" /></p>

<h3 id="extended-the-limit-cycle">Extended: The limit cycle</h3>

<p>The FitzHughâ€“Nagumo (FHN) model</p>

\[\begin{aligned}\dot{v}&amp;=v-\frac{v^3}3-w+RI_\mathrm{ext}\\\tau\dot{w}&amp;=v+a-bw.\end{aligned}\]

<p>This dynamical system, in certain conditions, exhibits a cyclic pattern of variable changes which can be visualized as a closed trajectory in the phase plane.</p>

<p>å˜åŒ–é”å®šåˆ°çŽ¯ä¸­</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825115348008.png" alt="image-20230825115348008" /></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825115354146.png" alt="image-20230825115354146" /></p>

<h1 id="reduced-models---brain-dynamics-programming">Reduced Models - brain dynamics programming</h1>

<h2 id="lif-neuron-models-programming">LIF neuron models programming</h2>

<h3 id="define-lif-class">Define LIF <code class="language-plaintext highlighter-rouge">class</code></h3>

\[\begin{aligned}&amp;\tau\frac{\mathrm{d}V}{\mathrm{d}t}=-(V-V_{\mathrm{rest}})+RI(t)\\&amp;\text{if }V&gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\text{last}t_{\mathrm{ref}}\end{aligned}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LIF</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">NeuDyn</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">V_rest</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">R</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">t_ref</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># åˆå§‹åŒ–çˆ¶ç±»
</span>        <span class="nb">super</span><span class="p">(</span><span class="n">LIF</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="initialization-1">Initialization</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LIF</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">NeuDyn</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">V_rest</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mi">5</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">R</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">t_ref</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># åˆå§‹åŒ–çˆ¶ç±»
</span>        <span class="nb">super</span><span class="p">(</span><span class="n">LIF</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="c1"># åˆå§‹åŒ–å‚æ•°
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">V_rest</span> <span class="o">=</span> <span class="n">V_rest</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">V_reset</span> <span class="o">=</span> <span class="n">V_reset</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">V_th</span> <span class="o">=</span> <span class="n">V_th</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">R</span> <span class="o">=</span> <span class="n">R</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">t_ref</span> <span class="o">=</span> <span class="n">t_ref</span>  <span class="c1"># ä¸åº”æœŸæ—¶é•¿
</span>        
        <span class="c1"># åˆå§‹åŒ–å˜é‡
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">)</span> <span class="o">+</span> <span class="n">V_reset</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="nb">input</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">t_last_spike</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e7</span><span class="p">)</span>  <span class="c1"># ä¸Šä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">refractory</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">))</span>  <span class="c1"># æ˜¯å¦å¤„äºŽä¸åº”æœŸ
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">spike</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">))</span>  <span class="c1"># è„‰å†²å‘æ”¾çŠ¶æ€
</span>        
        <span class="c1"># ä½¿ç”¨æŒ‡æ•°æ¬§æ‹‰æ–¹æ³•è¿›è¡Œç§¯åˆ†
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">integral</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">derivative</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'exponential_euler'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="define-the-derivative-function-1">Define the derivative function</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># å®šä¹‰è†œç”µä½å…³äºŽæ—¶é—´å˜åŒ–çš„å¾®åˆ†æ–¹ç¨‹
</span><span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">Iext</span><span class="p">):</span>
    <span class="n">dVdt</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">V</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_rest</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">R</span> <span class="o">*</span> <span class="n">Iext</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau</span>
    <span class="k">return</span> <span class="n">dVdt</span>
</code></pre></div></div>

<h3 id="complete-the-update-function-1">Complete the <code class="language-plaintext highlighter-rouge">update()</code> function</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">t</span><span class="p">,</span> <span class="n">dt</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'t'</span><span class="p">],</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'dt'</span><span class="p">]</span>
    <span class="c1"># ä»¥æ•°ç»„çš„æ–¹å¼å¯¹ç¥žç»å…ƒè¿›è¡Œæ›´æ–°
</span>    <span class="n">refractory</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">t_last_spike</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="p">.</span><span class="n">t_ref</span>  <span class="c1"># åˆ¤æ–­ç¥žç»å…ƒæ˜¯å¦å¤„äºŽä¸åº”æœŸ
</span>    <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">V</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="n">dt</span><span class="p">)</span>  <span class="c1"># æ ¹æ®æ—¶é—´æ­¥é•¿æ›´æ–°è†œç”µä½
</span>    <span class="n">V</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">refractory</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">V</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>  <span class="c1"># è‹¥å¤„äºŽä¸åº”æœŸï¼Œåˆ™è¿”å›žåŽŸå§‹è†œç”µä½self.Vï¼Œå¦åˆ™è¿”å›žæ›´æ–°åŽçš„è†œç”µä½V
</span>    <span class="n">spike</span> <span class="o">=</span> <span class="n">V</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_th</span>  <span class="c1"># å°†å¤§äºŽé˜ˆå€¼çš„ç¥žç»å…ƒæ ‡è®°ä¸ºå‘æ”¾äº†è„‰å†²
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">spike</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">spike</span>  <span class="c1"># æ›´æ–°ç¥žç»å…ƒè„‰å†²å‘æ”¾çŠ¶æ€
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">t_last_spike</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">spike</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">t_last_spike</span><span class="p">)</span>  <span class="c1"># æ›´æ–°æœ€åŽä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">V</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">spike</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_reset</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>  <span class="c1"># å°†å‘æ”¾äº†è„‰å†²çš„ç¥žç»å…ƒè†œç”µä½ç½®ä¸ºV_resetï¼Œå…¶ä½™ä¸å˜
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">refractory</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">refractory</span><span class="p">,</span> <span class="n">spike</span><span class="p">)</span>  <span class="c1"># æ›´æ–°ç¥žç»å…ƒæ˜¯å¦å¤„äºŽä¸åº”æœŸ
</span>    <span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">[:]</span> <span class="o">=</span> <span class="mf">0.</span>  <span class="c1"># é‡ç½®å¤–ç•Œè¾“å…¥
</span></code></pre></div></div>

<h3 id="simulation-2">Simulation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">run_LIF</span><span class="p">():</span>
  <span class="c1"># è¿è¡ŒLIFæ¨¡åž‹
</span>
  <span class="n">group</span> <span class="o">=</span> <span class="n">LIF</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'V'</span><span class="p">],</span> <span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="s">'input'</span><span class="p">,</span> <span class="mf">22.</span><span class="p">))</span>
  <span class="n">runner</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>  <span class="c1"># è¿è¡Œæ—¶é•¿ä¸º200ms
</span>
  <span class="c1"># ç»“æžœå¯è§†åŒ–
</span>  <span class="n">fig</span><span class="p">,</span> <span class="n">gs</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">get_figure</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
  <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">V</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$t$ (ms)'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$V$ (mV)'</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'top'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'right'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825141201825.png" alt="image-20230825141201825" /></p>

<h3 id="input-current--firing-frequency">Input current &amp; firing frequency</h3>

\[\begin{gathered}
V(t)=V_{\mathrm{reset}}+RI_{\mathrm{c}}(1-\mathrm{e}^{-\frac{t-t_{0}}{\tau}}). \\
T=-\tau\ln\left[1-\frac{V_{\mathrm{th}}-V_{\mathrm{rest}}}{RI_{\mathrm{c}}}\right] \\
f={\frac{1}{T+t_{\mathrm{ref}}}}={\frac{1}{t_{\mathrm{ref}}-\tau\ln\left[1-{\frac{V_{\mathrm{th}}-V_{\mathrm{rest}}}{RI_{c}}}\right]}} 
\end{gathered}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># è¾“å…¥ä¸Žé¢‘çŽ‡çš„å…³ç³»
</span>
<span class="n">current</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">600</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">duration</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">LIF_neuron</span> <span class="o">=</span> <span class="n">LIF</span><span class="p">(</span><span class="n">current</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">runner_2</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">LIF_neurons</span><span class="p">,</span> <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'spike'</span><span class="p">],</span> <span class="n">inputs</span><span class="o">=</span><span class="p">{</span><span class="s">'input'</span><span class="p">,</span> <span class="n">current</span><span class="p">},</span> <span class="n">dt</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="n">runner_2</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">duration</span><span class="p">)</span>

<span class="n">freqs</span> <span class="o">=</span> <span class="n">runner_2</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">spike</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">duration</span><span class="o">/</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">freqs</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'inputs'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'frequencies'</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825143405952.png" alt="image-20230825143405952" /></p>

<h3 id="other-univariate-neuron-models-1">Other Univariate neuron models</h3>

<p><strong>The Quadratic Integrate-and-Fire (QIF) model</strong></p>

\[\begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=a_{0}(V-V_{\mathrm{res}t})(V-V_{c})+RI(t)\\\mathrm{if~}V&amp;&gt;\theta,\quad V\leftarrow V_{\mathrm{reset~last~}t_{\mathrm{ref}}}\end{aligned}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">I</span><span class="p">):</span>
    <span class="n">dVdt</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">c</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_reset</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_c</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">R</span> <span class="o">*</span> <span class="n">I</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau</span>
    <span class="k">return</span> <span class="n">dVdt</span>
</code></pre></div></div>

<p><strong>The Exponential Integrate-and-Fire (ExpIF) model</strong></p>

\[\begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{\delta_{T}}}+RI(t)\\&amp;\mathrm{if~}V&gt;\theta,\quad V\leftarrow V_{\mathrm{reset}}\mathrm{last}t_{\mathrm{ref}}\end{aligned}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">I</span><span class="p">):</span>
    <span class="n">exp_v</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">delta_T</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">((</span><span class="n">V</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_T</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">delta_T</span><span class="p">)</span>
    <span class="n">dvdt</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span> <span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_rest</span><span class="p">)</span> <span class="o">+</span> <span class="n">exp_v</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">R</span> <span class="o">*</span> <span class="n">I</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau</span>
    <span class="k">return</span> <span class="n">dvdt</span>
</code></pre></div></div>

<h2 id="adex-neuron-models-programming">AdEx neuron models programming</h2>

\[\begin{gathered}
\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}=-(V-V_{\mathrm{rest}})+\Delta_{T}\mathrm{e}^{\Delta T}}}-Rw+RI(t), \\
\tau_{w}\frac{\mathrm{d}w}{\mathrm{d}t}=a(V-V_{\mathrm{rest}})-w+b\tau_{w}\sum_{t^{(f)}}\delta(t-t^{(f)})), \\
\mathrm{if~}V&gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\mathrm{last}t_{\mathrm{ref}}. 
\end{gathered}\]

<h3 id="define-adex-class">Define AdEx <code class="language-plaintext highlighter-rouge">class</code></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AdEx</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">NeuDyn</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span>
                <span class="n">V_rest</span><span class="o">=-</span><span class="mi">65</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mi">68</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=-</span><span class="mi">30</span><span class="p">,</span> <span class="n">V_T</span><span class="o">=-</span><span class="mf">59.9</span><span class="p">,</span> <span class="n">delta_T</span><span class="o">=</span><span class="mf">3.48</span>
                <span class="n">a</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">R</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">tau_w</span><span class="o">=</span><span class="mf">30.</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># åˆå§‹åŒ–çˆ¶ç±»
</span>        <span class="nb">super</span><span class="p">(</span><span class="n">AdEx</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="initialization-2">Initialization</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AdEx</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">NeuDyn</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span>
                <span class="n">V_rest</span><span class="o">=-</span><span class="mi">65</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mi">68</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=-</span><span class="mi">30</span><span class="p">,</span> <span class="n">V_T</span><span class="o">=-</span><span class="mf">59.9</span><span class="p">,</span> <span class="n">delta_T</span><span class="o">=</span><span class="mf">3.48</span>
                <span class="n">a</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">R</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">tau_w</span><span class="o">=</span><span class="mf">30.</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># åˆå§‹åŒ–çˆ¶ç±»
</span>        <span class="nb">super</span><span class="p">(</span><span class="n">AdEx</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="c1"># åˆå§‹åŒ–å‚æ•°
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">V_rest</span> <span class="o">=</span> <span class="n">V_rest</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">V_reset</span> <span class="o">=</span> <span class="n">V_reset</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">V_th</span> <span class="o">=</span> <span class="n">V_th</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">V_T</span> <span class="o">=</span> <span class="n">V_T</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">delta_T</span> <span class="o">=</span> <span class="n">delta_T</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">a</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">R</span> <span class="o">=</span> <span class="n">R</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tau_w</span> <span class="o">=</span> <span class="n">tau_w</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">tau_ref</span> <span class="o">=</span> <span class="n">tau_ref</span>
        
        <span class="c1"># åˆå§‹åŒ–å˜é‡
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">)</span> <span class="o">-</span> <span class="mf">65.</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="nb">input</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">t_last_spike</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e7</span><span class="p">)</span>  <span class="c1"># ä¸Šä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">refractory</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">))</span>  <span class="c1"># æ˜¯å¦å¤„äºŽä¸åº”æœŸ
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">spike</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">))</span>  <span class="c1"># è„‰å†²å‘æ”¾çŠ¶æ€
</span>        
        <span class="c1"># å®šä¹‰ç§¯åˆ†å™¨
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">integral</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">derivative</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'exp_auto'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="define-the-derivative-function-2">Define the derivative function</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">dV</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">I</span><span class="p">):</span>
	<span class="n">exp</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">delta_T</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">((</span><span class="n">V</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_T</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">delta_T</span><span class="p">)</span>
    <span class="n">dVdt</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">V</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_rest</span> <span class="o">+</span> <span class="n">exp</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">R</span> <span class="o">*</span> <span class="n">w</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">R</span> <span class="o">*</span> <span class="n">I</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau</span>
    <span class="k">return</span> <span class="n">dVdt</span>

<span class="k">def</span> <span class="nf">dw</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="n">dwdt</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_rest</span><span class="p">)</span> <span class="o">-</span> <span class="n">w</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau_w</span>
    <span class="k">return</span> <span class="n">dwdt</span>

<span class="o">@</span><span class="nb">property</span>
<span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">bp</span><span class="p">.</span><span class="n">JointEq</span><span class="p">([</span><span class="bp">self</span><span class="p">.</span><span class="n">dV</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dw</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="complete-the-update-function-2">Complete the <code class="language-plaintext highlighter-rouge">update()</code> function</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">t</span><span class="p">,</span> <span class="n">dt</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'t'</span><span class="p">],</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'dt'</span><span class="p">]</span>
    <span class="n">V</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">V</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">w</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="n">dt</span><span class="p">)</span>
    <span class="c1"># ä»¥æ•°ç»„çš„æ–¹å¼å¯¹ç¥žç»å…ƒè¿›è¡Œæ›´æ–°
</span>    <span class="n">refractory</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">t_last_spike</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="p">.</span><span class="n">t_ref</span>  <span class="c1"># åˆ¤æ–­ç¥žç»å…ƒæ˜¯å¦å¤„äºŽä¸åº”æœŸ
</span>    <span class="n">V</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">refractory</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">V</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>  <span class="c1"># è‹¥å¤„äºŽä¸åº”æœŸï¼Œåˆ™è¿”å›žåŽŸå§‹è†œç”µä½self.Vï¼Œå¦åˆ™è¿”å›žæ›´æ–°åŽçš„è†œç”µä½V
</span>    <span class="n">spike</span> <span class="o">=</span> <span class="n">V</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_th</span>  <span class="c1"># å°†å¤§äºŽé˜ˆå€¼çš„ç¥žç»å…ƒæ ‡è®°ä¸ºå‘æ”¾äº†è„‰å†²
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">spike</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">spike</span>  <span class="c1"># æ›´æ–°ç¥žç»å…ƒè„‰å†²å‘æ”¾çŠ¶æ€
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">t_last_spike</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">spike</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">t_last_spike</span><span class="p">)</span>  <span class="c1"># æ›´æ–°æœ€åŽä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">V</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">spike</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_reset</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>  <span class="c1"># å°†å‘æ”¾äº†è„‰å†²çš„ç¥žç»å…ƒè†œç”µä½ç½®ä¸ºV_resetï¼Œå…¶ä½™ä¸å˜
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">w</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">spike</span><span class="p">,</span> <span class="n">w</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>  <span class="c1">#æ›´æ–°è‡ªé€‚åº”ç”µæµ
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">refractory</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">refractory</span><span class="p">,</span> <span class="n">spike</span><span class="p">)</span>  <span class="c1"># æ›´æ–°ç¥žç»å…ƒæ˜¯å¦å¤„äºŽä¸åº”æœŸ
</span>    <span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">[:]</span> <span class="o">=</span> <span class="mf">0.</span>  <span class="c1"># é‡ç½®å¤–ç•Œè¾“å…¥
</span></code></pre></div></div>

<h3 id="simulation-3">Simulation</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825145518709.png" alt="image-20230825145518709" /></p>

<h3 id="other-multivariate-neuron-models-1">Other multivariate neuron models</h3>

<p><strong>The Izhikevich model</strong></p>

\[\begin{aligned}
&amp;\frac{dV}{dt} =0.04V^{2}+5V+140-u+I  \\
&amp;\frac{\mathrm{d}u}{\mathrm{d}t} =a\left(bV-u\right)  \\
&amp;\operatorname{if}V &gt;\theta,\quad V\leftarrow c,u\leftarrow u+d\mathrm{last}t_{\mathrm{ref}} 
\end{aligned}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">dV</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">I</span><span class="p">):</span>
    <span class="n">dVdt</span> <span class="o">=</span> <span class="mf">0.04</span> <span class="o">*</span> <span class="n">V</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="mi">140</span> <span class="o">-</span> <span class="n">u</span> <span class="o">+</span> <span class="n">I</span>
    <span class="k">return</span> <span class="n">dVdt</span>

<span class="k">def</span> <span class="nf">du</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="n">dudt</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">b</span> <span class="o">*</span> <span class="n">V</span> <span class="o">-</span> <span class="n">u</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dudt</span>
</code></pre></div></div>

<p><strong>The Generalized Integrate-and-Fire (GIF) model</strong></p>

\[\begin{aligned}
&amp;\tau{\frac{\mathrm{d}V}{\mathrm{d}t}} =-\left(V-V_{\mathrm{rest}}\right)+R\sum_{j}I_{j}+RI  \\
&amp;\frac{\mathrm{d}\Theta}{\mathrm{d}t} =a\left(V-V_{\mathrm{est}}\right)-b\left(\Theta-\Theta_{\infty}\right)  \\
&amp;\frac{\mathrm{d}I_j}{\mathrm{d}r} =-k_jI_j,\quad j=1,2,\ldots,n  \\
&amp;\text{if V} &gt;\Theta,\quad I_{j}\leftarrow R_{j}I_{j}+A_{j},V\leftarrow V_{\mathrm{reset}},\Theta\leftarrow max\left(\Theta_{\mathrm{reset}},\Theta\right) 
\end{aligned}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">dI1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">I1</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">k1</span> <span class="o">*</span> <span class="n">I1</span>

<span class="k">def</span> <span class="nf">dI2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">I2</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">k2</span> <span class="o">*</span> <span class="n">I2</span>

<span class="k">def</span> <span class="nf">dVth</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V_th</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">v_rest</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span> <span class="o">*</span> <span class="p">(</span><span class="n">V_th</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_th_inf</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">dV</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">I1</span><span class="p">,</span> <span class="n">I2</span><span class="p">,</span> <span class="n">I</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="o">-</span> <span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">V_rest</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">R</span> <span class="o">*</span> <span class="p">(</span><span class="n">I</span> <span class="o">+</span> <span class="n">I1</span> <span class="o">+</span> <span class="n">I2</span><span class="p">))</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau</span>
</code></pre></div></div>

<p><strong>Built-in reduced neuron models</strong></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825145947800.png" alt="image-20230825145947800" /></p>

<h2 id="dynamic-analysis-phase-plane-analysis-1">Dynamic analysis: phase-plane analysis</h2>

<h3 id="simple-case">Simple case</h3>

\[\frac{dx}{dt}=\sin(x)+I,\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">bp</span><span class="p">.</span><span class="n">odeint</span>
<span class="k">def</span> <span class="nf">int_x</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">Iext</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">bp</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">Iext</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pp</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">analysis</span><span class="p">.</span><span class="n">PhasePlane1D</span><span class="p">(</span>
	<span class="n">model</span><span class="o">=</span><span class="n">int_x</span><span class="p">,</span>
	<span class="n">target_vars</span><span class="o">=</span><span class="p">{</span><span class="s">'x'</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]},</span>
	<span class="n">pars_update</span><span class="o">=</span><span class="p">{</span><span class="s">'Iext'</span><span class="p">:</span> <span class="mf">0.</span><span class="p">},</span>
    <span class="n">resolutions</span><span class="o">=</span><span class="p">{</span><span class="s">'x'</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">}</span>
<span class="p">)</span>
<span class="n">pp</span><span class="p">.</span><span class="n">plot_vector_field</span><span class="p">()</span>
<span class="n">pp</span><span class="p">.</span><span class="n">plot_fixed_point</span><span class="p">(</span><span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825152003373.png" alt="image-20230825152003373" /></p>

<ul>
  <li>Nullcline: The zero-growth isoclines, such as $f(x,y) = 0$ and $g(x,y) = 0$</li>
  <li>Fixed points: The equilibrium points of the system, which are located at all the nullclines intersect.</li>
  <li>Vector field: The vector field of the system.</li>
  <li>Limit cycles: The limit cycles.</li>
  <li>Trajectories: A simulation trajectory with the given initial values</li>
</ul>

<h3 id="phase-plane-analysis-for-adex">Phase plane analysis for AdEx</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ppa_AdEx</span><span class="p">(</span><span class="n">group</span><span class="p">):</span>
    <span class="n">bm</span><span class="p">.</span><span class="n">enable_x64</span><span class="p">()</span>
    
    <span class="n">v_range</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">70.</span><span class="p">,</span> <span class="o">-</span><span class="mf">40.</span><span class="p">]</span>
    <span class="n">w_range</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">10.</span><span class="p">,</span> <span class="mf">50.</span><span class="p">]</span>
    
    <span class="n">phase_plane_analyzer</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">analysis</span><span class="p">.</span><span class="n">PhasePlane2D</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">group</span><span class="p">,</span>
        <span class="n">target_vars</span><span class="o">=</span><span class="p">{</span><span class="s">'V'</span><span class="p">:</span> <span class="n">v_range</span><span class="p">,</span> <span class="s">'w'</span><span class="p">:</span> <span class="n">w_range</span><span class="p">,</span> <span class="p">},</span>  <span class="c1"># å¾…åˆ†æžå˜é‡
</span>        <span class="n">pars_update</span><span class="o">=</span><span class="p">{</span><span class="s">'I'</span><span class="p">:</span> <span class="n">Iext</span><span class="p">},</span>  <span class="c1"># éœ€è¦æ›´æ–°çš„å˜é‡
</span>        <span class="n">resolutions</span><span class="o">=</span><span class="mf">0.05</span>
    <span class="p">)</span>

    <span class="c1"># ç”»å‡ºV, wçš„é›¶å¢žé•¿æ›²çº¿
</span>    <span class="n">phase_plane_analyzer</span><span class="p">.</span><span class="n">plot_nullcline</span><span class="p">()</span>
    <span class="c1"># ç”»å‡ºå¥‡ç‚¹
</span>    <span class="n">phase_plane_analyzer</span><span class="p">.</span><span class="n">plot_fixed_point</span><span class="p">()</span>
    <span class="c1"># ç”»å‡ºå‘é‡åœº
</span>    <span class="n">phase_plane_analyzer</span><span class="p">.</span><span class="n">plot_vector_field</span><span class="p">()</span>
    
    <span class="c1"># åˆ†æ®µç”»å‡ºV, wçš„å˜åŒ–è½¨è¿¹
</span>    <span class="n">group</span><span class="p">.</span><span class="n">V</span><span class="p">[:],</span> <span class="n">group</span><span class="p">.</span><span class="n">w</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">group</span><span class="p">.</span><span class="n">V_reset</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'V'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">,</span> <span class="s">'spike'</span><span class="p">],</span> <span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="s">'input'</span><span class="p">,</span> <span class="n">Iext</span><span class="p">))</span>
    <span class="n">runner</span><span class="p">(</span><span class="mi">500</span><span class="p">)</span>
    <span class="n">spike</span> <span class="o">=</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">spike</span><span class="p">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="n">s_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">spike</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># æ‰¾åˆ°æ‰€æœ‰å‘æ”¾åŠ¨ä½œç”µä½å¯¹åº”çš„index
</span>    <span class="n">s_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">(([</span><span class="mi">0</span><span class="p">],</span> <span class="n">s_idx</span><span class="p">,</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">spike</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]))</span>  <span class="c1"># åŠ ä¸Šèµ·å§‹ç‚¹å’Œç»ˆæ­¢ç‚¹çš„index
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s_idx</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">vs</span> <span class="o">=</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">V</span><span class="p">[</span><span class="n">s_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span> <span class="n">s_idx</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]]</span>
        <span class="n">ws</span> <span class="o">=</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">w</span><span class="p">[</span><span class="n">s_idx</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span> <span class="n">s_idx</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]]</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">vs</span><span class="p">,</span> <span class="n">ws</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'darkslateblue'</span><span class="p">)</span>
        
    <span class="c1"># ç”»å‡ºè™šçº¿ x = V_reset
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">([</span><span class="n">group</span><span class="p">.</span><span class="n">V_reset</span><span class="p">,</span> <span class="n">group</span><span class="p">.</span><span class="n">V_reset</span><span class="p">],</span> <span class="n">w_range</span><span class="p">,</span> <span class="s">'--'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'grey'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825152925463.png" alt="image-20230825152925463" /></p>

<h2 id="dynamic-analysis-bifurcation-analysis-1">Dynamic analysis: bifurcation analysis</h2>

<h3 id="simple-case-1">Simple case</h3>

\[\frac{dx}{dt}=\sin(x)+I,\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bif</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">analysis</span><span class="p">.</span><span class="n">Bifurcation1D</span><span class="p">(</span>
	<span class="n">model</span><span class="o">=</span><span class="n">int_x</span><span class="p">,</span>
	<span class="n">target_vars</span><span class="o">=</span><span class="p">{</span><span class="s">'x'</span><span class="p">:</span> <span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]},</span>
	<span class="n">target_pars</span><span class="o">=</span><span class="p">{</span><span class="s">'Iext'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]},</span>
	<span class="n">resolutions</span><span class="o">=</span><span class="p">{</span><span class="s">'Iext'</span><span class="p">:</span> <span class="mf">0.005</span><span class="p">,</span> <span class="s">'x'</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">}</span>
<span class="p">)</span>
<span class="n">bif</span><span class="p">.</span><span class="n">plot_bifurcation</span><span class="p">(</span><span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230825154227567.png" alt="image-20230825154227567" /></p>

<h1 id="synapse-models-and-their-programming">Synapse models and their programming</h1>

<h2 id="the-biology-of-synapses">The biology of synapses</h2>

<h3 id="neurotransmitter--synapse">Neurotransmitter &amp; Synapse</h3>

<p>When the action potential invades the axon terminals, it causes voltage-gated ð¶ð¶ð‘Žð‘Ž 2+ channels to open (1), which triggers vesicles to bind to the presynaptic membrane (2). Neurotransmitter is released into the synaptic cleft by exocytosis and diffuses across the cleft (3). Binding of the neurotransmitter to receptor molecules in the postsynaptic membrane completes the process of transmission (4).</p>

<p>åŽ»æžåŒ–æ—¶é’™ç¦»å­å†…æµï¼Œä¸Žå›Šæ³¡ç›¸ç»“åˆï¼Œâ€¦ï¼Œä¸Žå—ä½“ç»“åˆï¼Œæ‰“å¼€ç¦»å­é€šé“ï¼Œè¶…æžåŒ–ã€åŽ»æžåŒ–çŽ°è±¡</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826100321307.png" alt="image-20230826100321307" /></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826100418911.png" alt="image-20230826100418911" /></p>

<p><strong>Neurotransmitter leading to postsynaptic potential.</strong></p>

<p>The binding of neurotransmitter to the postsynaptic membrane receptors changes the membrane potential ($V_m$). These postsynaptic potentials can be either excitatory (depolarizing the membrane), as shown here, or inhibitory (hyperpolarizing the membrane).</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826100531535.png" alt="image-20230826100531535" /></p>

<h3 id="neurotransmitters">Neurotransmitters</h3>

<p>å…´å¥‹æ€§ç¥žç»é€’è´¨ï¼š</p>

<ul>
  <li>ä¹™é…°èƒ†ç¢± (ACh)</li>
  <li>å„¿èŒ¶é…šèƒº (catecholamines)</li>
  <li>è°·æ°¨é…¸ (glutamate)</li>
  <li>ç»„èƒº (histamine)</li>
  <li>5-ç¾Ÿè‰²èƒº (serotonin)</li>
  <li>æŸäº›ç¥žç»è‚½ç±» (some of neuropeptides)</li>
</ul>

<p>æŠ‘åˆ¶æ€§ç¥žç»é€’è´¨ï¼š</p>

<ul>
  <li>GABA</li>
  <li>ç”˜æ°¨é…¸ (glycine)</li>
  <li>æŸäº›ç¥žç»è‚½ç±» (some of peptides)</li>
</ul>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826100609904.png" alt="image-20230826100609904" /></p>

<h3 id="the-postsynaptic-response">The postsynaptic response</h3>

<p>The aim of a synapse model is to describe accurately the postsynaptic response generated by the arrival of an action potential at a presynaptic terminal.</p>

<ol>
  <li>The fundamental quantity to be modelled is the time course of the postsynaptic receptor conductance</li>
  <li>The models:
    <ul>
      <li>Simple phenomenological waveforms</li>
      <li>More complex kinetic schemes that are analogous to the models of membrane- bound ion channels</li>
    </ul>
  </li>
</ol>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826100701580.png" alt="image-20230826100701580" /></p>

<p>å»ºæ¨¡è¿™ç§å“åº”æ¨¡å¼ï¼Œæ‰“å¼€å…³é—­çš„æ¦‚çŽ‡â€¦</p>

<h2 id="phenomenological-synapse-models">Phenomenological synapse models</h2>

<h3 id="exponential-model">Exponential Model</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826100738460.png" alt="image-20230826100738460" /></p>

<p><strong>Assumption</strong>:</p>

<ul>
  <li>The release of neurotransmitter, its diffusion across the cleft, the receptor binding, and channel opening all happen very quickly, so that the channels instantaneously jump from the closed to the open state. channelä¼šçž¬é—´å¢žåŠ ç„¶åŽé€æ¸å…³é—­</li>
</ul>

\[g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}e^{-(t-t_{0})/\tau}
\\
\begin{matrix}\bullet&amp;\tau \ \text{is the time constant}\\\bullet&amp;t_0 \ \text{is the time of the pre-synaptic spike}\\\bullet&amp;\bar{g_{syn}}\ \text{is the maximal conductance}\end{matrix}\]

<p>-&gt; corresponding differential equation</p>

\[\tau\frac{dg_{\mathrm{syn}}(t)}{dt}=-g_{\mathrm{syn}}(t)+\bar{g}_{\mathrm{syn}}\delta\left(t_{0}-t\right)\]

<ul>
  <li>Can fit with experimental data.</li>
  <li>A good approximation for GABA A and AMPA, because the rising phase is much shorter than their decay phase.</li>
</ul>

<h3 id="dual-exponential-model">Dual Exponential Model</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826101203059.png" alt="image-20230826101203059" /></p>

<p>exponential modelä¸Šå‡çš„å¤ªå¿«ï¼Œä¸å¤ªç¬¦åˆæŸäº›synapse</p>

<p>Dual exponential synapse provides a general way to describe the synaptic conductance with different rising and decay time constants.</p>

\[g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}\frac{\tau_{1}\tau_{2}}{\tau_{1}-\tau_{2}}\left(\exp\left(-\frac{t-t_{0}}{\tau_{1}}\right)-\exp\left(-\frac{t-t_{0}}{\tau_{2}}\right)\right)
\\
\begin{matrix}
\bullet &amp;t_1\ \text{is the decay synaptic time constant} \\
\bullet &amp;\tau_2\ \text{is the rise synaptic time constant} \\
\bullet &amp;t_0\ \text{is the time of the pre-synaptic spike} \\
\bullet &amp;\bar{g}_{syn}\ \text{is the maximal conductance}
\end{matrix}\]

<p>-&gt;corresponding differential equation</p>

\[\begin{aligned}
&amp;g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}g \\
&amp;\frac{dg}{dt}=-\frac{g}{\tau_{\mathrm{decay}}}+h \\
&amp;\frac{dh}{dt}&amp; =-\frac{h}{\tau_{\mathrm{rise}}}+\delta\left(t_{0}-t\right), 
\end{aligned}\]

<p>The time course of most synaptic conductance can be well described by this sum of two exponentials.</p>

<h3 id="synaptic-time-constants">Synaptic time constants</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826101544786.png" alt="image-20230826101544786" /></p>

<p>http://compneuro.uwaterloo.ca/research/constants-constraints/neurotransmitter-time-constants-pscs.html</p>

<h4 id="ampa-synapse">AMPA synapse</h4>

<ul>
  <li>$t_{decay}$ = 0.18 ms in the auditory system of the chick nucleus magnocellularis (Trussell, 1999).</li>
  <li>$t_{rise}$ 25 ms and $\tau_{decay}$ =0.77 ms in dentate gyrus basket cells (Geiger et al., 1997).</li>
  <li>$t_{rise}$ = 0.2 ms and $\tau_{decay}$ =1.7 ms in in neocortical layer 5 pyramidal neurons (Hausser and Roth, 1997b).</li>
  <li>Reversal potential is nearly 0 mV.</li>
</ul>

<h4 id="nmda-synapse">NMDA synapse</h4>

<ul>
  <li>The decay time constants (at near-physiological temperature):
    <ul>
      <li>19 ms in dentate gyrus basket cells (Geiger et al., 1997),</li>
      <li>26 ms in neocortical layer 2/3 pyramidal neurons (Feldmeyer et al., 2002),</li>
      <li>89 ms in CA1 pyramidal cells (Diamond, 2001).</li>
    </ul>
  </li>
  <li>The rise time constants are about 2 ms (Feldmeyer et al., 2002).</li>
  <li>Reversal potential is nearly 0 mV.</li>
</ul>

<h4 id="gaba_a-synapse">GABA$_A$ synapse</h4>

<ul>
  <li>GABAergic synapses from dentate gyrus basket cells onto other basket cells are faster: $t_{rise}$ = 0.3 ms and $t_{decay}$ = 2.5 ms (Bartos et al., 2001) than synapses from basket cells to granule cells: $t_{rise}$ = 0.26 ms and $t_{decay}$ = 6.5 ms (Kraushaar and Jonas, 2000).</li>
  <li>Reversal potential is nearly -80 mV.</li>
</ul>

<h4 id="gaba_b-synapse">GABA$_B$ synapse</h4>

<ul>
  <li>Common models use models with a rise time of about 25-50 ms, a fast decay time in the range of 100-300ms and a slow decay time of 500-1000 ms.</li>
  <li>Reversal potential is nearly -90 mV.</li>
</ul>

<h3 id="general-property-of-synaptic-time-constants">General property of synaptic time constants</h3>

<ul>
  <li>The time constants of synaptic conductance vary widely among synapse types.</li>
  <li>The synaptic kinetics tends to accelerate during development (T. Takahashi, Neuroscience Research, 2005) .</li>
  <li>The synaptic kinetics becomes substantially faster with increasing temperature.</li>
</ul>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826102033433.png" alt="image-20230826102033433" /></p>

<h3 id="current--and-conductance-based-response">Current- and Conductance-based Response</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826102042614.png" alt="image-20230826102042614" /></p>

<h4 id="conductance-based-response">Conductance-based Response</h4>

<p>Most synaptic ion channels, such as AMPA and GABA, display an approximately linear current-voltage relationship when they open.</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826102113670.png" alt="image-20230826102113670" /></p>

<p><strong>For example</strong>:
The synapse is located on a thin dendrite, because the local membrane potential V changes considerably when the synapse is activated.</p>

<h4 id="current-based-response">Current-based Response</h4>

<p>In some case, we can also approximate the synapses as sources of current and not a conductance.</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826102150487.png" alt="image-20230826102150487" /></p>

<p><strong>For example</strong>:</p>

<p>The excitatory synapse on a large compartment, because the depolarization of the membrane is small.</p>

<h2 id="programming-of-phenomenological-synapse-models">Programming of phenomenological synapse models</h2>

<h3 id="projalignpostmg2"><code class="language-plaintext highlighter-rouge">ProjAlignPostMg2</code></h3>

<p><img src="https://cdn.kesci.com/upload/rzz4o4uyar.png?imageView2/0/w/960/h/960" alt="Image Name" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">brainpy</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPostMg2</span><span class="p">(</span>  
   <span class="n">pre</span><span class="p">,</span>  
   <span class="n">delay</span><span class="p">,</span>  
   <span class="n">comm</span><span class="p">,</span>  
   <span class="n">syn</span><span class="p">,</span>  
   <span class="n">out</span><span class="p">,</span>  
   <span class="n">post</span>  
<span class="p">)</span>  
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">pre (JointType[DynamicalSystem, AutoDelaySupp])</code>: The pre-synaptic neuron group.</li>
  <li><code class="language-plaintext highlighter-rouge">delay (Union[None, int, float])</code>: The synaptic delay.</li>
  <li><code class="language-plaintext highlighter-rouge">comm (DynamicalSystem)</code>: The synaptic communication.</li>
  <li><code class="language-plaintext highlighter-rouge">syn (ParamDescInit)</code>: The synaptic dynamics.</li>
  <li><code class="language-plaintext highlighter-rouge">out (ParamDescInit)</code>: The synaptic output.</li>
  <li><code class="language-plaintext highlighter-rouge">post (DynamicalSystem)</code> The post-synaptic neuron group.</li>
</ul>

<p>åªéœ€è¦å»ºæ¨¡æ‰€æœ‰postçš„neurons</p>

<h3 id="csr-matrix">CSR matrix</h3>

<p><img src="https://cdn.kesci.com/upload/rzz4on32hr.png?imageView2/0/w/960/h/960" alt="Image Name" /></p>

<h3 id="exponential-model-1">Exponential Model</h3>

<p>The single exponential decay synapse model assumes the release of neurotransmitter, its diffusion across the cleft, the receptor binding, and channel opening all happen very quickly, so that the channels instantaneously jump from the closed to the open state. Therefore, its expression is given by</p>

\[g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} e^{-\left(t-t_{0}\right) / \tau}\]

<p>where $\tau$ is the time constant, $t_0$ is the time of the pre-synaptic spike, $\bar{g}_{\mathrm{syn}}$ is the maximal conductance.</p>

<p>The corresponding differential equation:</p>

\[\frac{d g}{d t} = -\frac{g}{\tau_{decay}}+\sum_{k} \delta(t-t_{j}^{k}).\]

<h4 id="coba">COBA</h4>

<p>Given the synaptic conductance, the COBA model outputs the post-synaptic current with</p>

\[I_{syn}(t) = g_{\mathrm{syn}}(t) (E - V(t))\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ExponSparseCOBA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">Projection</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">g_max</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">E</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    
    <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPostMg2</span><span class="p">(</span>
      <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">,</span> 
      <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">,</span> 
      <span class="n">comm</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">EventCSRLinear</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="n">FixedProb</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">),</span> <span class="n">g_max</span><span class="p">),</span>
      <span class="n">syn</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">Expon</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">),</span>
      <span class="n">out</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">COBA</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">E</span><span class="o">=</span><span class="n">E</span><span class="p">),</span>
      <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">,</span> 
    <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SimpleNet</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">DynSysGroup</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">pre</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">SpikeTimeGroup</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">times</span><span class="o">=</span><span class="p">(</span><span class="mf">10.</span><span class="p">,</span> <span class="mf">30.</span><span class="p">,</span> <span class="mf">50.</span><span class="p">,</span> <span class="mf">70.</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">post</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">LifRef</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">V_rest</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=-</span><span class="mf">50.</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span>
                              <span class="n">V_initializer</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">Constant</span><span class="p">(</span><span class="o">-</span><span class="mf">60.</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">syn</span> <span class="o">=</span> <span class="n">ExponSparseCOBA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">prob</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="n">E</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">pre</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">syn</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">()</span>
    
    <span class="c1"># monitor the following variables
</span>    <span class="n">conductance</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">syn</span><span class="p">.</span><span class="n">proj</span><span class="p">.</span><span class="n">refs</span><span class="p">[</span><span class="s">'syn'</span><span class="p">].</span><span class="n">g</span>
    <span class="n">current</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">sum_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">V</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">conductance</span><span class="p">,</span> <span class="n">current</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">V</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">run_a_net</span><span class="p">(</span><span class="n">net</span><span class="p">):</span>
  <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>  <span class="c1"># 100 ms
</span>  <span class="n">conductances</span><span class="p">,</span> <span class="n">currents</span><span class="p">,</span> <span class="n">potentials</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">for_loop</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">step_run</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">progress_bar</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  <span class="n">ts</span> <span class="o">=</span> <span class="n">indices</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">()</span>
  
  <span class="c1"># --- similar to: 
</span>  <span class="c1"># runner = bp.DSRunner(net)
</span>  <span class="c1"># conductances, currents, potentials = runner.run(100.)
</span>  
  <span class="n">fig</span><span class="p">,</span> <span class="n">gs</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">get_figure</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
  <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">conductances</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Syn conductance'</span><span class="p">)</span>
  <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">currents</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Syn current'</span><span class="p">)</span>
  <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">potentials</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Post V'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="cuba">CUBA</h4>

<p>Given the conductance, this model outputs the post-synaptic current with a identity function:</p>

\[I_{\mathrm{syn}}(t) = g_{\mathrm{syn}}(t)\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ExponSparseCUBA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">Projection</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">g_max</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    
    <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPostMg2</span><span class="p">(</span>
      <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">,</span> 
      <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">,</span> 
      <span class="n">comm</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">EventCSRLinear</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="n">FixedProb</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">),</span> <span class="n">g_max</span><span class="p">),</span>
      <span class="n">syn</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">Expon</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">),</span>
      <span class="n">out</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">CUBA</span><span class="p">.</span><span class="n">desc</span><span class="p">(),</span>
      <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">,</span> 
    <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SimpleNet2</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">DynSysGroup</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    
    <span class="bp">self</span><span class="p">.</span><span class="n">pre</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">SpikeTimeGroup</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">times</span><span class="o">=</span><span class="p">(</span><span class="mf">10.</span><span class="p">,</span> <span class="mf">30.</span><span class="p">,</span> <span class="mf">50.</span><span class="p">,</span> <span class="mf">70.</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">post</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">LifRef</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">V_rest</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=-</span><span class="mf">50.</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span>
                              <span class="n">V_initializer</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">Constant</span><span class="p">(</span><span class="o">-</span><span class="mf">60.</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">syn</span> <span class="o">=</span> <span class="n">ExponSparseCUBA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">prob</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="n">g_max</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">5.</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">pre</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">syn</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">()</span>
    
    <span class="c1"># monitor the following variables
</span>    <span class="n">conductance</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">syn</span><span class="p">.</span><span class="n">proj</span><span class="p">.</span><span class="n">refs</span><span class="p">[</span><span class="s">'syn'</span><span class="p">].</span><span class="n">g</span>
    <span class="n">current</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">sum_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">V</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">conductance</span><span class="p">,</span> <span class="n">current</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">V</span>
</code></pre></div></div>

<h4 id="dense-connections">Dense connections</h4>

<p>Exponential synapse model with the conductance-based (COBA) output current and dense connections.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ExponDenseCOBA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">Projection</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">g_max</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">E</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    
    <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPostMg2</span><span class="p">(</span>
      <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">,</span> 
      <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">,</span> 
      <span class="n">comm</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">MaskedLinear</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="n">FixedProb</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">),</span> <span class="n">g_max</span><span class="p">),</span>
      <span class="n">syn</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">Expon</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">),</span>
      <span class="n">out</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">COBA</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">E</span><span class="o">=</span><span class="n">E</span><span class="p">),</span>
      <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">,</span> 
    <span class="p">)</span>
</code></pre></div></div>

<p><img src="https://cdn.kesci.com/upload/rzz4p7x6dl.png?imageView2/0/w/960/h/960" alt="Image Name" /></p>

<p>Exponential synapse model with the current-based (COBA) output current and dense connections.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ExponDenseCUBA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">Projection</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">g_max</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">E</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    
    <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPostMg2</span><span class="p">(</span>
      <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">,</span> 
      <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">,</span> 
      <span class="n">comm</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">MaskedLinear</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="n">FixedProb</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">),</span> <span class="n">g_max</span><span class="p">),</span>
      <span class="n">syn</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">Expon</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">),</span>
      <span class="n">out</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">CUBA</span><span class="p">.</span><span class="n">desc</span><span class="p">(),</span>
      <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">,</span> 
    <span class="p">)</span>
</code></pre></div></div>

<h3 id="projalignpremg2"><code class="language-plaintext highlighter-rouge">ProjAlignPreMg2</code></h3>

<p>Synaptic projection which defines the synaptic computation with the dimension of presynaptic neuron group.</p>

<p><img src="https://cdn.kesci.com/upload/rzz4pj1qmk.png?imageView2/0/w/960/h/960" alt="Image Name" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">brainpy</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPreMg2</span><span class="p">(</span>  
   <span class="n">pre</span><span class="p">,</span>  
   <span class="n">delay</span><span class="p">,</span>  
   <span class="n">syn</span><span class="p">,</span>  
   <span class="n">comm</span><span class="p">,</span>  
   <span class="n">out</span><span class="p">,</span>  
   <span class="n">post</span>  
<span class="p">)</span>  
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">pre (JointType[DynamicalSystem, AutoDelaySupp])</code>: The pre-synaptic neuron group.</li>
  <li><code class="language-plaintext highlighter-rouge">delay (Union[None, int, float])</code>: The synaptic delay.</li>
  <li><code class="language-plaintext highlighter-rouge">syn (ParamDescInit)</code>: The synaptic dynamics.</li>
  <li><code class="language-plaintext highlighter-rouge">comm (DynamicalSystem)</code>: The synaptic communication.</li>
  <li><code class="language-plaintext highlighter-rouge">out (ParamDescInit)</code>: The synaptic output.</li>
  <li><code class="language-plaintext highlighter-rouge">post (DynamicalSystem)</code> The post-synaptic neuron group.</li>
</ul>

<h3 id="dual-exponential-model-1">Dual Exponential Model</h3>

<p>The dual exponential synapse model, also named as <strong>difference of two exponentials model</strong>, is given by:</p>

\[g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} \frac{\tau_{1} \tau_{2}}{\tau_{1}-\tau_{2}}\left(\exp \left(-\frac{t-t_{0}}{\tau_{1}}\right)-\exp \left(-\frac{t-t_{0}}{\tau_{2}}\right)\right)\]

<p>where $\tau_1$ is the time constant of the decay phase, $\tau_2$ is the time constant of the rise phase, $t_0$ is the time of the pre-synaptic spike, $\bar{g}_{\mathrm{syn}}$ is the maximal conductance.</p>

<p>The corresponding differential equation:</p>

\[\begin{aligned}  
&amp;g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} g \\  
&amp;\frac{d g}{d t}=-\frac{g}{\tau_{\mathrm{decay}}}+h \\  
&amp;\frac{d h}{d t}=-\frac{h}{\tau_{\text {rise }}}+ \delta\left(t_{0}-t\right),  
\end{aligned}\]

<p>The alpha function is retrieved in the limit when both time constants are equal.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DualExpSparseCOBA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">Projection</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">g_max</span><span class="p">,</span> <span class="n">tau_decay</span><span class="p">,</span> <span class="n">tau_rise</span><span class="p">,</span> <span class="n">E</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    
    <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPreMg2</span><span class="p">(</span>
      <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">,</span> 
      <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">,</span> 
      <span class="n">syn</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">DualExpon</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">tau_decay</span><span class="o">=</span><span class="n">tau_decay</span><span class="p">,</span> <span class="n">tau_rise</span><span class="o">=</span><span class="n">tau_rise</span><span class="p">),</span>
      <span class="n">comm</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">CSRLinear</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="n">FixedProb</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">),</span> <span class="n">g_max</span><span class="p">),</span>
      <span class="n">out</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">COBA</span><span class="p">(</span><span class="n">E</span><span class="o">=</span><span class="n">E</span><span class="p">),</span>
      <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">,</span> 
    <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SimpleNet4</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">DynSysGroup</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    
    <span class="bp">self</span><span class="p">.</span><span class="n">pre</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">SpikeTimeGroup</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">times</span><span class="o">=</span><span class="p">(</span><span class="mf">10.</span><span class="p">,</span> <span class="mf">30.</span><span class="p">,</span> <span class="mf">50.</span><span class="p">,</span> <span class="mf">70.</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">post</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">LifRef</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">V_rest</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=-</span><span class="mf">50.</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span>
                              <span class="n">V_initializer</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">Constant</span><span class="p">(</span><span class="o">-</span><span class="mf">60.</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">syn</span> <span class="o">=</span> <span class="n">DualExpSparseCOBA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">prob</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> 
                                 <span class="n">tau_decay</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">tau_rise</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="n">E</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">pre</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">syn</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">()</span>
    
    <span class="c1"># monitor the following variables
</span>    <span class="n">conductance</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">syn</span><span class="p">.</span><span class="n">proj</span><span class="p">.</span><span class="n">refs</span><span class="p">[</span><span class="s">'syn'</span><span class="p">].</span><span class="n">g</span>
    <span class="n">current</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">sum_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">V</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">conductance</span><span class="p">,</span> <span class="n">current</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">V</span>
</code></pre></div></div>

<h2 id="biophysical-synapse-models">Biophysical synapse models</h2>

<h3 id="limitations-of-phenomenological-models">Limitations of phenomenological models</h3>

<p>æ‰“å¼€çš„æ•°é‡æ˜¯æœ‰é™çš„ï¼Œè€Œä¸”æœ‰é¥±å’ŒæœŸ</p>

<ol>
  <li>Saturation of postsynaptic receptors by previously released transmitter.</li>
  <li>Certain receptor types also exhibit desensitization that prevents them (re)opening for a period after transmitter-binding, like sodium channels underlying action potential.</li>
</ol>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826111443117.png" alt="image-20230826111443117" /></p>

<h3 id="lineticmarkov-models">Linetic/Markov models</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826111733654.png" alt="image-20230826111733654" /></p>

<ul>
  <li>The simplest kinetic model is a two-state scheme in which receptors can be either closed, ð¶, or open, ð‘‚, and the transition between states depends on transmitter concentration, [ð‘‡], in the synaptic cleft:</li>
  <li>ð›¼ and ð›½ are voltage-independent forward and backward rate constants.</li>
  <li>ð¶ and ð‘‚ can range from 0 to 1, and describe the fraction of receptors in the closed and open states, respectively.</li>
  <li>The synaptic conductance is: $g_{syn}(t)=\bar{g}_{max}g(t)$</li>
</ul>

<h3 id="ampagaba_a-synapse-model">AMPA/GABA$_A$ synapse model</h3>

<p><img src="Notes.assets/image-20230826111831637.png" alt="image-20230826111831637" style="zoom:50%;" /></p>

<p><img src="Notes.assets/image-20230826111841073.png" alt="image-20230826111841073" style="zoom:50%;" /></p>

\[\begin{aligned}\frac{ds}{dt}&amp;=\alpha[T](1-s)-\beta s\\I&amp;=\tilde{g}s(V-E)\end{aligned}\]

<ul>
  <li>ð›¼[ð‘‡] denotes the transition probability from state (1âˆ’ð‘ ) to state (ð‘ )</li>
  <li>ð›½ represents the transition probability of the other direction</li>
  <li>ð¸ is a reverse potential, which can determine whether the direction of ð¼ is inhibition or excitation.</li>
  <li>ð¸ = 0 ð‘šð‘šð‘‰ð‘‰ =&gt; Excitatory synapse [AMPA]</li>
  <li>ð¸ = âˆ’80 ð‘šð‘šð‘‰ð‘‰ =&gt; Inhibitory synapse [GABA A ]</li>
</ul>

<h3 id="comparison">Comparison</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826111950713.png" alt="image-20230826111950713" /></p>

<h3 id="nmda-synapse-model">NMDA synapse model</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826112027689.png" alt="image-20230826112027689" /></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826112034481.png" alt="image-20230826112034481" /></p>

\[\begin{aligned}
&amp;\frac{ds}{dt} =\alpha[T](1-s)-\beta s  \\
&amp;I=\tilde{g}sB(V)(V-E) \\
&amp;B(V )=\frac{1}{1+\exp(-0.062V)[Mg^{2+}]_{o}/3.57} 
\end{aligned}\]

<p>The magnesium block of the NMDA receptor channel is an extremely fast process compared to the other kinetics of the receptor (Jahr and Stevens 1990a, 1990b). The block can therefore be accurately modeled as an instantaneous function of voltage(Jahr and Stevens 1990b).</p>

<p>where $[Mg^{2+}]$ is the external magnesium concentration (1 to 2mM inphysiological conditions)</p>

<h2 id="programming-of-biophysical-synapse-models">Programming of biophysical synapse models</h2>

<h3 id="ampa-synapse-model">AMPA synapse model</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AMPA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">Projection</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">g_max</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPreMg2</span><span class="p">(</span>
          <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">,</span> 
          <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">,</span> 
          <span class="n">syn</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">AMPA</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.18</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">T_dur</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
          <span class="n">comm</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">CSRLinear</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="n">FixedProb</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">),</span> <span class="n">g_max</span><span class="p">),</span>
          <span class="n">out</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">COBA</span><span class="p">(</span><span class="n">E</span><span class="o">=</span><span class="n">E</span><span class="p">),</span>
          <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">,</span> 
        <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SimpleNet</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">DynSysGroup</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">syn_cls</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">pre</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">SpikeTimeGroup</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">times</span><span class="o">=</span><span class="p">(</span><span class="mf">10.</span><span class="p">,</span> <span class="mf">30.</span><span class="p">,</span> <span class="mf">50.</span><span class="p">,</span> <span class="mf">70.</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">post</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">LifRef</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">V_rest</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=-</span><span class="mf">50.</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span>
                              <span class="n">V_initializer</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">Constant</span><span class="p">(</span><span class="o">-</span><span class="mf">60.</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">syn</span> <span class="o">=</span> <span class="n">syn_cls</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">prob</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">pre</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">syn</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">()</span>
    
    <span class="c1"># monitor the following variables
</span>    <span class="n">conductance</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">syn</span><span class="p">.</span><span class="n">proj</span><span class="p">.</span><span class="n">refs</span><span class="p">[</span><span class="s">'syn'</span><span class="p">].</span><span class="n">g</span>
    <span class="n">current</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">sum_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">V</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">conductance</span><span class="p">,</span> <span class="n">current</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">V</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">run_a_net</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">duration</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
  <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">duration</span><span class="o">/</span><span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">()))</span>  <span class="c1"># duration ms
</span>  <span class="n">conductances</span><span class="p">,</span> <span class="n">currents</span><span class="p">,</span> <span class="n">potentials</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">for_loop</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">step_run</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">progress_bar</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  <span class="n">ts</span> <span class="o">=</span> <span class="n">indices</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">()</span>
  
  <span class="c1"># --- similar to: 
</span>  <span class="c1"># runner = bp.DSRunner(net)
</span>  <span class="c1"># conductances, currents, potentials = runner.run(100.)
</span>  
  <span class="n">fig</span><span class="p">,</span> <span class="n">gs</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">get_figure</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
  <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">conductances</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Syn conductance'</span><span class="p">)</span>
  <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">currents</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Syn current'</span><span class="p">)</span>
  <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">potentials</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Post V'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="textgaba_a-synapse-model">$\text{GABA}_A$ synapse model</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GABAa</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">Projection</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">g_max</span><span class="p">,</span> <span class="n">E</span><span class="o">=-</span><span class="mf">80.</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPreMg2</span><span class="p">(</span>
          <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">,</span> 
          <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">,</span> 
          <span class="n">syn</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">GABAa</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.53</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.18</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">T_dur</span><span class="o">=</span><span class="mf">1.0</span><span class="p">),</span>
          <span class="n">comm</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">CSRLinear</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="n">FixedProb</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">),</span> <span class="n">g_max</span><span class="p">),</span>
          <span class="n">out</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">COBA</span><span class="p">(</span><span class="n">E</span><span class="o">=</span><span class="n">E</span><span class="p">),</span>
          <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">,</span> 
        <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">run_a_net</span><span class="p">(</span><span class="n">SimpleNet</span><span class="p">(</span><span class="n">syn_cls</span><span class="o">=</span><span class="n">GABAa</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="nmda-synapse-model-1">NMDA synapse model</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">NMDA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">Projection</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">g_max</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPreMg2</span><span class="p">(</span>
          <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">,</span> 
          <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">,</span> 
          <span class="n">syn</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">NMDA</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">tau_decay</span><span class="o">=</span><span class="mf">100.</span><span class="p">,</span> <span class="n">tau_rise</span><span class="o">=</span><span class="mf">2.</span><span class="p">),</span> 
          <span class="n">comm</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">CSRLinear</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="n">FixedProb</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">),</span> <span class="n">g_max</span><span class="p">),</span> 
          <span class="n">out</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">MgBlock</span><span class="p">(</span><span class="n">E</span><span class="o">=</span><span class="n">E</span><span class="p">),</span> 
          <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">,</span> 
        <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">run_a_net</span><span class="p">(</span><span class="n">SimpleNet</span><span class="p">(</span><span class="n">NMDA</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="kinetic-synapse-models-are-more-realistic">Kinetic synapse models are more realistic</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SimpleNet5</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">DynSysGroup</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">freqs</span><span class="o">=</span><span class="mf">10.</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    
    <span class="bp">self</span><span class="p">.</span><span class="n">pre</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">PoissonGroup</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">freqs</span><span class="o">=</span><span class="n">freqs</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">post</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">LifRef</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span>  <span class="n">V_rest</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=-</span><span class="mf">50.</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span>
                              <span class="n">V_initializer</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">Constant</span><span class="p">(</span><span class="o">-</span><span class="mf">60.</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">syn</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">prob</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">g_max</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">pre</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">syn</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">()</span>
    
    <span class="c1"># monitor the following variables
</span>    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">syn</span><span class="p">.</span><span class="n">proj</span><span class="p">.</span><span class="n">refs</span><span class="p">[</span><span class="s">'syn'</span><span class="p">].</span><span class="n">g</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">post</span><span class="p">.</span><span class="n">V</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compare_freqs</span><span class="p">(</span><span class="n">freqs</span><span class="p">):</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">get_figure</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mf">6.</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">freqs</span><span class="p">:</span>
      <span class="n">net</span> <span class="o">=</span> <span class="n">SimpleNet5</span><span class="p">(</span><span class="n">freqs</span><span class="o">=</span><span class="n">freq</span><span class="p">)</span>
      <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>  <span class="c1"># 100 ms
</span>      <span class="n">conductances</span><span class="p">,</span> <span class="n">potentials</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">for_loop</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">step_run</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">progress_bar</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
      <span class="n">ts</span> <span class="o">=</span> <span class="n">indices</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">()</span>
      <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">conductances</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">freq</span><span class="si">}</span><span class="s"> Hz'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'g'</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">compare_freqs</span><span class="p">([</span><span class="mf">10.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">1000.</span><span class="p">,</span> <span class="mf">10000.</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="how-to-customize-a-synapse">How to customize a synapse</h3>

<h4 id="preparations">Preparations</h4>

<p><code class="language-plaintext highlighter-rouge">ProjAlignPostMg2</code> and <code class="language-plaintext highlighter-rouge">ProjAlignPreMg2</code></p>

<h4 id="exponential-model-2">Exponential Model</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Exponen</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">SynDyn</span><span class="p">,</span> <span class="n">bp</span><span class="p">.</span><span class="n">mixin</span><span class="p">.</span><span class="n">AlignPost</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    
    <span class="c1"># parameters
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>
    
    <span class="c1"># variables
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">g</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
    
    <span class="c1"># integral
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">integral</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="k">lambda</span> <span class="n">g</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="o">-</span><span class="n">g</span><span class="o">/</span><span class="n">tau</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'exp_auto'</span><span class="p">)</span>  
  
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre_spike</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral</span><span class="p">(</span><span class="n">g</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'t'</span><span class="p">],</span> <span class="n">dt</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'dt'</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">pre_spike</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="p">.</span><span class="n">add_current</span><span class="p">(</span><span class="n">pre_spike</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">.</span><span class="n">value</span>
      
  <span class="k">def</span> <span class="nf">add_current</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>  <span class="c1"># specical for bp.mixin.AlignPost
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">g</span> <span class="o">+=</span> <span class="n">x</span>
    
  <span class="k">def</span> <span class="nf">return_info</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">g</span>
</code></pre></div></div>

<h4 id="ampa-model">AMPA Model</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AMPA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">SynDyn</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span> <span class="mf">0.98</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.18</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">T_dur</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>

    <span class="c1"># parameters
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">T</span> <span class="o">=</span> <span class="n">T</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">T_duration</span> <span class="o">=</span> <span class="n">T_dur</span>

    <span class="c1"># functions
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">integral</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s">'exp_auto'</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dg</span><span class="p">)</span>
    
    <span class="c1"># variables
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">g</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">spike_arrival_time</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e7</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">dg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">TT</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">TT</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">g</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">*</span> <span class="n">g</span>
  
  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre_spike</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">spike_arrival_time</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">pre_spike</span><span class="p">,</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'t'</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">spike_arrival_time</span><span class="p">)</span>
    <span class="n">TT</span> <span class="o">=</span> <span class="p">((</span><span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">spike_arrival_time</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">T_duration</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">T</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">,</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'t'</span><span class="p">],</span> <span class="n">TT</span><span class="p">,</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'dt'</span><span class="p">])</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">.</span><span class="n">value</span>

  <span class="k">def</span> <span class="nf">return_info</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">g</span>
</code></pre></div></div>

<h4 id="synapse-outputs">Synapse outputs</h4>

<p><strong>COBA</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">COBA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">SynOut</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">E</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">=</span> <span class="n">E</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">conductance</span><span class="p">,</span> <span class="n">potential</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">conductance</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">-</span> <span class="n">potential</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>CUBA</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CUBA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">SynOut</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">E</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">=</span> <span class="n">E</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">conductance</span><span class="p">,</span> <span class="n">potential</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">conductance</span>
</code></pre></div></div>

<h4 id="mg-blocking">Mg Blocking</h4>

<p>The voltage dependence is due to the blocking of the pore of the NMDA receptor from the outside by a positively charged magnesium ion. The channel is nearly completely blocked at resting potential, but the magnesium block is relieved if the cell is depolarized. The fraction of channels $B(V)$ that are not blocked by magnesium can be fitted to</p>

\[B(V) = {1 \over 1 + \exp(-0.062V) [Mg^{2+}]_o/3.57}\]

<p>Here, $[{Mg}^{2+}]_{o}$ is the extracellular magnesium concentration, usually 1 mM.</p>

<p>If we make the approximation that the magnesium block changes instantaneously with voltage and is independent of the gating of the channel, the net NMDA receptor-mediated synaptic current is given by</p>

\[I=\bar{g}sB(V)(V-E)\]

<p>where $V(t)$ is the post-synaptic neuron potential, $E$ is the reversal potential.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MgBlock</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">SynOut</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">cc_Mg</span><span class="o">=</span> <span class="mf">1.2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span> <span class="mf">0.062</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span> <span class="mf">3.57</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">=</span> <span class="n">E</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">cc_Mg</span> <span class="o">=</span> <span class="n">cc_Mg</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">conductance</span><span class="p">,</span> <span class="n">potential</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">conductance</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">-</span> <span class="n">potential</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">cc_Mg</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">potential</span><span class="p">))</span>
</code></pre></div></div>

<h4 id="masked-matrix">Masked matrix</h4>

<p><img src="https://cdn.kesci.com/upload/rzz4kw4ybf.png?imageView2/0/w/960/h/960" alt="Image Name" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MaskedLinear</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">conn</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    
    <span class="c1"># connection and weight
</span>    <span class="n">weight</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">parameter</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="p">(</span><span class="n">conn</span><span class="p">.</span><span class="n">pre_num</span><span class="p">,</span> <span class="n">conn</span><span class="p">.</span><span class="n">post_num</span><span class="p">))</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">mode</span><span class="p">,</span> <span class="n">bm</span><span class="p">.</span><span class="n">TrainingMode</span><span class="p">):</span>
      <span class="n">weight</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">TrainVar</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span>

    <span class="c1"># connection
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">conn</span> <span class="o">=</span> <span class="n">conn</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">sharding</span><span class="p">.</span><span class="n">partition</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="n">require</span><span class="p">(</span><span class="s">'conn_mat'</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">mask</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="short-term-synaptic-plasticity">Short-term Synaptic Plasticity</h1>

<h2 id="synaptic-transmission-and-plasticity">Synaptic transmission and plasticity</h2>

<h3 id="process-of-chemical-synaptic-transmission">Process of Chemical synaptic transmission</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826140307862.png" alt="image-20230826140307862" /></p>

<h3 id="epsp-and-epsc">EPSP and EPSC</h3>

<p>EPSP: Excitatory Post Synaptic Potential
EPSC: Excitatory Post Synaptic Current</p>

<p>Post synaptic current: $I(t)=g(t)\bigl[V_{\mathrm{post}}(\mathrm{t})-E_{reversal}\bigr]$</p>

<p>Dynamics of post-synaptic conductance (exponential model): $\frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{S}}+A\delta(t-t_{sp})$</p>

<p>The synaptic strength is characterized as EPSC, which refers to the post synaptic current increment at each spike, $EPSC_{n}=A(V_{\mathrm{rest}}-E_{reversal})$</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826140558289.png" alt="image-20230826140558289" /></p>

<h3 id="synaptic-plasticity">Synaptic plasticity</h3>

<p>å®žé™…ä¸Šçªè§¦å¼ºåº¦ä¼šä¸€ç›´å˜æ¢</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826140745735.png" alt="image-20230826140745735" /></p>

<h2 id="phenomenological-model-of-stp">Phenomenological model of STP</h2>

<h3 id="short-term-depression-observed-between-pyramidal-cells">Short-term depression observed between pyramidal cells</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826140825584.png" alt="image-20230826140825584" /></p>

<p>è†œç‰‡å‰æŠ€æœ¯ï¼Œè®°å½•è†œç”µä½å˜åŒ–</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826140947752.png" alt="image-20230826140947752" /></p>

<h3 id="modeling-neuro-transmitter-consumption">Modeling neuro-transmitter consumption</h3>

<p>Dynamics of three-factor STD:</p>

\[\begin{gathered}
\frac{dx(t)}{dt}=\frac{z(t)}{\tau_{rec}}-U_{SE}x(t)\delta\big(t-t_{sp}\big), \\
\frac{dy(t)}{dt}=-\frac{y(t)}{\tau_{in}}+U_{SE}x(t)\delta\big(t-t_{sp}\big), \\
x(t)+y(t)+z(t)=1, \\
\frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{s}}+g_{max}y(t), 
\end{gathered}\]

<p>$x$: Fraction of available neuro-transmitter
$y$: Fraction of active neuro-transmitter
$z$: Fraction of inactive neuro-transmitter
$U_{se}$: Release probability of active neuro-transmitter
$t_{sp}$: Pre-synaptic spike time
$g(t)$:  Post-synaptic conductance
$A$: total amount of neuro-transmitter
$\tau_{in}$ &amp; $\tau_{rec}$ &amp; $\tau_s$: Time constants</p>

<h3 id="simulate-the-three-factor-std">Simulate the three-factor STD</h3>

\[\begin{gathered}
\frac{dx(t)}{dt}=\frac{z(t)}{\tau_{rec}}-U_{SE}x(t)\delta\big(t-t_{sp}\big), \\
\frac{dy(t)}{dt}=-\frac{y(t)}{\tau_{in}}+U_{SE}x(t)\delta\big(t-t_{sp}\big), \\
x(t)+y(t)+z(t)=1, \\
\frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{s}}+Ay(t), 
\end{gathered}\]

\[\tau_{rec}=500ms,\quad\tau_{in}=3ms,\quad fr=20hz\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826142313326.png" alt="image-20230826142313326" /></p>

<h3 id="simplify-the-dynamics-of-neuro-transmitter-consumption">Simplify the dynamics of neuro-transmitter consumption</h3>

<p>å˜é‡å¤ªå¤šäº†ï¼Œæ¨¡åž‹å¤æ‚</p>

<p>In general, the inactivation time constants is much shorter (3ms) than the spike time interval, i.e., ðœ 2- â‰ª Î”ð‘¡, so the formulation can be approximately simplified,</p>

\[\begin{aligned}\frac{dy(t)}{dt}&amp;=-\frac{y(t)}{\tau_{in}}+U_{SE}x(t)\delta\big(t-t_{sp}\big)\\&amp;\Longrightarrow\color{red}{\left\{\begin{array}{c}y(t)=U_{SE}x^-\delta_1(t-t_{sp}),\\x^-=\lim_{t-t_{sp}\to0^-}x(t)\end{array}\right.}\end{aligned}\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826142632613.png" alt="image-20230826142632613" /></p>

<p>Simplified model:</p>

\[\begin{gathered}
\frac{dx(t)}{dt} =\frac{1-x(t)}{\tau_{rec}}-U_{SE}x^{-}\delta\big(t-t_{sp}\big), \\
\frac{dg(t)}{dt} =-\frac{g(t)}{\tau_{s}}+AU_{SE}x^{-}\delta\big(t-t_{sp}\big), \\
EPSC=AU_{SE}x^{-}, 
\end{gathered}\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826143004941.png" alt="image-20230826143004941" /></p>

<h3 id="infer-model-parameters-from-experimental-data">Infer model parameters from experimental data</h3>

<p>æŽ¨æ–­è¶…å‚ï¼ŒEPSCçš„ç†è®ºè§£</p>

<p>Short term depression model:</p>

\[\begin{aligned}\frac{dx(t)}{dt}&amp;=\frac{1-x(t)}{\tau_{rec}}-U_{SE}x^{-}\delta(t-t_{sp}),\\EPSC&amp;=AU_{SE}x^{-},\end{aligned}\]

<p>Iterative expression for EPSCs:</p>

\[x_{1}^{-}=1, EPSC_{1}=AU_{SE},  \\
x_{n+1}^{-}=1-x_{n}^{-}(1-U_{SE})\mathrm{e}^{-\frac{\Delta t}{\tau_{rec}}} \\
EPSC_{n+1}=AU_{SE}-EPSC_{n}(1-U_{SE})e^{-\frac{\Delta t}{\tau_{rec}}}\]

<h4 id="short-term-facilitation-observed-between-pyramidal-cells-and-interneurons">Short-term facilitation observed between pyramidal cells and interneurons</h4>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826143735036.png" alt="image-20230826143735036" /></p>

<p>çŸ­æ—¶ç¨‹å¢žå¼º</p>

<h3 id="modeling-neuro-transmitter-release-probability">Modeling neuro-transmitter release probability</h3>

<p>å…ˆå‰æ¼æŽ‰é‡Šæ”¾æ¦‚çŽ‡çš„å»ºæ¨¡</p>

<p>The release probability can also be modelled as a dynamical variable ð‘¢(ð‘¡),</p>

\[\begin{gathered}
\frac{du(t)}{dt}=\frac{-u(t)}{\tau_{f}}+U_{SE}(1-u^{-})\delta\big(t-t_{sp}\big), \\
\frac{dx(t)}{dt}=\frac{1-x(t)}{\tau_{d}}-u(t)x^{-}\delta\big(t-t_{sp}+\delta t\big), \\
\frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{S}}+Au(t)x^{-}\delta\big(t-t_{sp}+\delta t\big), \\
EPSC=Au(t)x^{-}, 
\end{gathered}\]

<p>$U_{SE}$ might reflect the concentration of $Ca^{2+}$</p>

<p>The release probability can also be modelled as a dynamical variable ð‘¢(ð‘¡),</p>

\[\begin{gathered}
\frac{du(t)}{dt}=\frac{-u(t)}{\tau_{f}}+U_{SE}(1-u^{-})\delta\big(t-t_{sp}\big), \\
\frac{dx(t)}{dt}=\frac{1-x(t)}{\tau_{d}}-u^{+}x^{-}\delta\big(t-t_{sp}\big), \\
\frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{s}}+Au^{+}x^{-}\delta\big(t-t_{sp}\big), \\
{EPSC=Au^{+}x^{-},\quad u^{+}=\lim_{t-t_{sp}\to0^{+}}u(t),} 
\end{gathered}\]

<h3 id="std-and-stf-under-different-parameter-regime">STD and STF under different parameter regime</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826145216601.png" alt="image-20230826145216601" /></p>

<h3 id="derivation-of-iterative-expressions-for-epscs">Derivation of iterative expressions for EPSCs</h3>

\[\begin{gathered}
\mathrm{Iterative~expression~for~}x_{n},u_{n},EPSC_{n}; \\
u_{1}^{+}=U_{SE},\quad x_{1}^{-}=1, \\
x_{n+1}^{-}=1-x_{n}^{-}(1-u_{n}^{+})\mathrm{e}^{-\frac{\Delta t}{\tau_{rec}}}, \\
u_{n+1}^{+}=u_{n}^{+}e^{-\frac{\Delta t}{\tau_{f}}}+U_{SE}\left(1-u_{n}^{+}e^{-\frac{\Delta t}{\tau_{f}}}\right), \\
EPSC_{n+1}=Au_{n}^{+}x_{n}^{-}, 
\end{gathered}\]

\[\begin{gathered}
\mathrm{Steady~state~of~}x_{n},u_{n},EPSC_{n}; \\
u_{st}^{+}=\frac{U_{SE}}{1-(1-U_{SE})e^{-\frac{\Delta t}{\tau_{f}}}}\geq U_{SE}=u_{1}^{+}, \\
x_{st}^{-}=\frac{1}{1+(1-u_{st}^{+})\mathrm{e}^{-\frac{\Delta t}{\tau_{rec}}}}\leq1=x_{1}^{-}, \\
EPSC_{st}=Au_{st}^{+}x_{st}^{-} 
\end{gathered}\]

<h3 id="prediction-for-complex-post-synaptic-patterns">Prediction for complex post-synaptic patterns</h3>

<p>Infer model parameters by fitting experiments:</p>

\[EPSC_{n+1} = Au_nx_n\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826145401318.png" alt="image-20230826145401318" /></p>

<p>Simulate with complex pre-synaptic spike trains and compare with vitro experimental results (patch-clamp)</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826145419287.png" alt="image-20230826145419287" /><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826145424279.png" alt="image-20230826145424279" /></p>

<h2 id="effects-on-information-transmission">Effects on information transmission</h2>

<h3 id="mean-field-analysis-of-stp-model">Mean-field Analysis of STP model</h3>

<p>STP based on spiking time</p>

\[\begin{gathered}
\frac{du(t)}{dt}=\frac{-u(t)}{\tau_{f}}+U_{sE}(1-u^{-})\delta\big(t-t_{sp}\big), \\
\frac{dx(t)}{dt}=\frac{1-x(t)}{\tau_{d}}-u^{+}x^{-}\delta\big(t-t_{sp}\big), \\
\frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{s}}+Au^{+}x^{-}\delta\big(t-t_{sp}\big), \\
u^{+}=\lim_{t-t_{sp\rightarrow0^{+}}}u(t), 
\end{gathered}\]

<p>-&gt;åšæ—¶é—´å¹³å‡</p>

<p>STP based on firing rate</p>

\[\begin{gathered}
\frac{du(t)}{dt}=\frac{-u(t)}{\tau_{f}}+U_{sE}(1-u^{-})\delta\big(t-t_{sp}\big), \\
\frac{dx(t)}{dt}=\frac{1-x(t)}{\tau_{d}}-u^{+}x^{-}\delta\big(t-t_{sp}\big), \\
\frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{s}}+Au^{+}x^{-}\delta\big(t-t_{sp}\big), \\
u^{+}=\lim_{t-t_{sp\rightarrow0^{+}}}u(t), 
\end{gathered}\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826155016657.png" alt="image-20230826155016657" /></p>

<p>ä¸¢æŽ‰æ—¶é—´å˜åŒ–çš„å…·ä½“ç»†èŠ‚ï¼ŒæŠ“ä½äº†é‡è¦è¶‹åŠ¿</p>

<h3 id="theoretical-analysis-of-the-rate-model">Theoretical analysis of the rate model</h3>

<p>Suppose the pre-synaptic firing rate keeps as constant, we can calculate the stationary response</p>

\[u_{st}=\frac{U_{SE}R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}},\quad u_{st}^{+}=U_{SE}\frac{1+R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}},\quad x_{st}=\frac{1}{1+u_{st}^{+}\tau_{d}R_{0}},\]

\[EPSC_{st}=Au_{st}^{+}x_{st}=A\frac{u_{st}^{+}}{1+u_{st}^{+}\tau_{d}R_{0}},\quad PSV_{st}\propto g_{st}=\tau_{s}Au_{st}^{+}x_{st}R_{0}=A\frac{u_{st}^{+}R_{0}}{1+u_{st}^{+}\tau_{d}R_{0}},\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826155234134.png" alt="image-20230826155234134" /></p>

<h3 id="frequency-dependent-gain-control-of-spike-information">Frequency-dependent Gain control of spike information</h3>

\[\begin{gathered}
u_{st}^{+}=U_{SE}\frac{1+R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}}, \\
x_{st}=\frac{1}{1+u_{st}^{+}\tau_{d}R_{0}}, \\
EPSC_{st}=Au_{st}^{+}x_{st}=A\frac{u_{st}^{+}}{1+u_{st}^{+}\tau_{d}R_{0}}, 
\end{gathered}\]

<p>Peak frequency: $\theta\sim\frac{1}{\sqrt{U\tau_{f}\tau_{d}}}$</p>

<h3 id="simulation-of-frequency-dependent-gain-control">Simulation of Frequency-dependent Gain control</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826155715445.png" alt="image-20230826155715445" /></p>

<h2 id="effects-on-network-dynamics">Effects on network dynamics</h2>

<h3 id="stp-modeling-working-memory">STP modeling Working memory</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826160102332.png" alt="image-20230826160102332" /></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230826160113456.png" alt="image-20230826160113456" /></p>

<h1 id="e-i-balanced-neural-network">E-I Balanced Neural Network</h1>

<h2 id="irregular-spiking-of-neurons">Irregular Spiking of Neurons</h2>

<h3 id="signal-process-of-single-neuron">Signal process of single neuron</h3>

<p>External Stimulus -&gt;</p>

<p>Single neuron model</p>

\[\begin{aligned}\tau&amp;\frac{\mathrm{d}V}{\mathrm{d}t}=-(V-V_\text{rest })+RI(t)\\\\&amp;\text{if}V&gt;V_\text{th},\quad V\leftarrow V_\text{reset }\text{last}t_\text{ref}\end{aligned}\]

<p>-&gt; â€¦ -&gt; Perception or action</p>

<p>çœŸæ­£çš„ç¥žç»å…ƒå¹¶ä¸æ˜¯LIF modelçš„è¾“å‡º</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827100647851.png" alt="image-20230827100647851" /></p>

<p>Simulation</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827100706310.png" alt="image-20230827100706310" /></p>

<p>Neuron recorded in vivo</p>

<h3 id="irregular-spiking-of-neurons-1">Irregular Spiking of Neurons</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827092807270.png" alt="image-20230827092807270" /></p>

<h4 id="statistical-description-of-spikes">Statistical Description of Spikes</h4>

<p>ç”¨ä»¥ä¸‹çš„å˜é‡æ¥è¿›è¡Œç»Ÿè®¡æè¿°</p>

<ul>
  <li>Firing Rate
Rate = average over time(single neuron, single run)
Spike count $v=\frac{n_{sp}}{T}$</li>
  <li>ISI(Interspike interval distributions)
average ISI $\overline{\Delta t}=\frac{1}{n_{sp}-1}\sum_{i=1}^{n_{sp}-1}\Delta t_{i}$
standard deviation ISI: $\sigma_{\Delta t}^{2}=\frac{1}{n_{sp}-1}\sum_{i=1}^{n_{sp}-1}(\Delta t_{i}-\overline{\Delta t})^{2}$</li>
  <li>$C_V$(Coefficient of variation, Fano factor) <strong>çª„è¿˜æ˜¯å®½çš„åˆ†å¸ƒ</strong> ä¿¡æ¯è¡¨å¾æœ‰å¤šå¼ºçš„ä¸ç¨³å®šæ€§
$C_{V}=\sigma_{\Delta t}^{2}/\overline{\Delta t}$</li>
</ul>

<h4 id="poisson-process">Poisson Process</h4>

<p>In probability theory and statistics, the Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known <strong>constant mean rate</strong> and <strong>independently</strong> of the time since the last eventã€‚</p>

\[\begin{aligned}
&amp;P(X=k\mathrm{~events~in~interval~}t)=e^{-rt}\frac{(rt)^{k}}{k!} \\
&amp;\mathrm{mean:}\quad\overline{X}=rt \\
&amp;\mathrm{variance}:\quad\sigma^{2}=rt\\
&amp;\mathrm{Fano factor:}\quad\frac{\sigma^{2}}{X}=1
\end{aligned}\]

<p>Fano factor -&gt; noise-to-signal ratio</p>

<h4 id="irregular-spiking-of-neurons-2">Irregular Spiking of Neurons</h4>

<p>LIFåœ¨å•ä¸ªç¥žç»å…ƒçš„æƒ…å†µä¸‹æ˜¯åŸºæœ¬æ²¡æœ‰å¤ªå¤§é—®é¢˜çš„ï¼Œåœ¨æ•´ä¸ªç½‘ç»œä¸­ä¼šå—ç½‘ç»œä¿¡æ¯è°ƒæŽ§</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827093614607.png" alt="image-20230827093614607" /></p>

<h4 id="why-irregular">Why Irregular?</h4>

<ul>
  <li>ä¸å®Œå…¨æ˜¯inputå½±å“çš„</li>
  <li>ä¸èƒ½ç®€å•æ¥è¡¡é‡</li>
</ul>

<p>On average, a cortical neuron receives inputs from 1000~10000 connected neurons. -&gt; averaged noise ~ 0</p>

<h2 id="e-i-balanced-network">E-I Balanced Network</h2>

\[\begin{gathered}
\tau\frac{du_{i}^{E}}{dt}=-u_{i}^{E}+\sum_{j=1}^{K_{E}}J_{EE}r_{j}^{E}+\sum_{j=1}^{K_{I}}J_{EI}r_{j}^{I}+I_{i}^{E} \\
\tau\frac{du_{i}^{I}}{dt}=-u_{i}^{I}+\sum_{j=1}^{K_{I}}J_{II}r_{j}^{I}+\sum_{j=1}^{K_{E}}J_{IE}r_{j}^{E}+I_{i}^{I} 
\end{gathered}\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827093708220.png" alt="image-20230827093708220" /></p>

<p>Sparse &amp; random connections:$1\ll K_{\mathrm{E}},K_{1}\ll N_{\mathrm{E}},N_{\mathrm{I}}$ . Neurons fire largely independently to each other.</p>

\[\begin{gathered}
\text{Single neuron fires irregularly } r_j^E, r_j^{\prime} \text{with mean rate } \mu \text{and variance } \sigma^2.\\
\text{The mean of recurrent input received by E neuron:} \\
\sim K_{E}J_{EE}\mu-K_{I}J_{EI}\mu  \\
\text{The variance of recurrent input received by E neuron:} \\
\sim K_{E}(J_{EE})^{2}\sigma^{2}+K_{I}(J_{EI})^{2}\sigma^{2} \\
\begin{gathered} \\
\text{The balanced condition:} \\
K_{E}J_{EE}-K_{l}J_{El}{\sim}0(1) \\
J_{EE}=\frac{1}{\sqrt{K_{E}}},J_{EI}=\frac{1}{\sqrt{K_{I}}},K_{E}(J_{EE})^{2}\sigma^{2}+K_{I}(J_{EI})^{2}\sigma^{2}\sim O(1) 
\end{gathered}
\end{gathered}\]

\[\begin{aligned}\frac{I_E}{I_I}&amp;&gt;\frac{J_E}{J_I}&amp;&gt;1\\\\J_E&amp;&gt;1\\\\\text{r not too big}\end{aligned}\]

\[\overline{I_a}=\overline{F_a}+\overline{R_a}=\sqrt{N}(f_a\mu_0+w_{aE}r_E+w_{aI}r_I),\quad a=E,I,\\
\begin{gathered}
w_{ab}~=~p_{ab}j_{ab}q_{b} \\
J_{ij}^{ab}~=~j_{ab}/\sqrt{N}; \\
\frac{f_{E}}{f_{I}}&gt;\frac{w_{EI}}{w_{II}}&gt;\frac{w_{EE}}{w_{IE}}. 
\end{gathered}\]

<h2 id="brainpy-simulation">BrainPy Simulation</h2>

<h3 id="simulation-4">Simulation</h3>

<p>LIF neuron 4000 (E/I=4/1, P=0.02)
ðœ = 20 ms
ð‘‰ð‘Ÿð‘’ð‘ ð‘¡ = -60 mV
Spiking threshold: -50 mV
Refractory period: 5 ms</p>

\[\begin{gathered}
\tau\frac{dV}{dt}=(V_{\mathrm{rest}}-V)+I \\
I=g_{exc}(E_{exc}-V)+g_{inh}(E_{inh}-V)+I_{\mathrm{ext}} 
\end{gathered} \ \ \ \ \ \
\begin{aligned}\tau_{exc}&amp;\frac{dg_{exc}}{dt}=-g_{exc}\\\tau_{inh}&amp;\frac{dg_{inh}}{dt}=-g_{inh}\end{aligned}\]

\[\begin{array}{l}E_\mathrm{exc}=0\text{mV}\mathrm{and}E_\mathrm{inh}=-80\text{mV},I_\mathrm{ext}=20.\\\tau_\mathrm{exc}=5\text{ ms},\tau_\mathrm{inh}=10\text{ ms},\Delta g_\mathrm{exc}=0.6\text{ and}\Delta g_\mathrm{inh}=6.7.\end{array}\]

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827094502860.png" alt="image-20230827094502860" /></p>

<h3 id="synaptic-computation">Synaptic Computation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># åŸºäºŽ align post Exponential synaptic computation
</span><span class="k">class</span> <span class="nc">Exponential</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">Projection</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">delay</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">g_max</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">E</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pron</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPost2</span><span class="p">(</span>
        	<span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">,</span>
            <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">,</span>
            <span class="n">comm</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">EventCSRLinear</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">conn</span><span class="p">.</span><span class="n">FixedProb</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">),</span> <span class="n">g_max</span><span class="p">),</span> <span class="c1"># éšæœºè¿žæŽ¥
</span>            <span class="n">syn</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">Expon</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">),</span> <span class="c1"># Exponential synapse
</span>            <span class="n">out</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">COBA</span><span class="p">(</span><span class="n">E</span><span class="o">=</span><span class="n">E</span><span class="p">),</span> <span class="c1"># COBA network
</span>            <span class="n">post</span><span class="o">=</span><span class="n">post</span><span class="p">,</span>
            <span class="n">out_label</span><span class="o">=</span><span class="n">label</span>
        <span class="p">)</span>
</code></pre></div></div>

<h3 id="e-i-balanced-network-1">E-I Balanced Network</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># æž„å»º E-I Balanced Network
</span>
<span class="k">class</span> <span class="nc">EINet</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">DynamicalSystem</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ne</span><span class="o">=</span><span class="mi">3200</span><span class="p">,</span> <span class="n">ni</span><span class="o">=</span><span class="mi">800</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        
        <span class="c1"># bp.neurons.LIF()
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">E</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">LifRef</span><span class="p">(</span><span class="n">ne</span><span class="p">,</span> <span class="n">V_rest</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=-</span><span class="mf">50.</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span>
                              <span class="n">V_initializer</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="o">-</span><span class="mf">55.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">I</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">LifRef</span><span class="p">(</span><span class="n">ni</span><span class="p">,</span> <span class="n">V_rest</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=-</span><span class="mf">50.</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mf">60.</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span>
                              <span class="n">V_initializer</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">Normal</span><span class="p">(</span><span class="o">-</span><span class="mf">55.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">))</span>
    	<span class="c1">#### E2E, E2I, I2E, I2I Exponential synaptic computation
</span>        <span class="c1"># delay=0, prob=0.02, g_max_E=0.6, g_max_I=6.7, tau_E=5, tau_I=10,
</span>        <span class="c1"># reversal potentials E_E=0, E_E=-80, label=EE,EI,IE,II
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">E2E</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="s">'EE'</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">E2I</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="s">'EI'</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">I2E</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">6.7</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="o">-</span><span class="mf">80.</span><span class="p">,</span> <span class="s">'IE'</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">I2I</span> <span class="o">=</span> <span class="n">Exponential</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">6.7</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="o">-</span><span class="mf">80.</span><span class="p">,</span> <span class="s">'II'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
    <span class="c1"># æ›´æ–°çªè§¦ä¼ å…¥ç”µæµ
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">E2E</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">E2I</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">I2E</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">I2I</span><span class="p">()</span>
    
    <span class="c1"># æ›´æ–°ç¥žç»å…ƒç¾¤ä½“
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
    
    <span class="c1"># è®°å½•éœ€è¦ monitorçš„å˜é‡
</span>    <span class="n">E_E_inp</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">.</span><span class="n">sum_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">.</span><span class="n">V</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'EE'</span><span class="p">)</span> <span class="c1">#E2Eçš„è¾“å…¥
</span>    <span class="n">I_E_inp</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">.</span><span class="n">sum_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">.</span><span class="n">V</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'IE'</span><span class="p">)</span> <span class="c1"># I2Eçš„è¾“å…¥
</span>    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">E</span><span class="p">.</span><span class="n">spike</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">.</span><span class="n">spike</span><span class="p">,</span> <span class="n">E_E_inp</span><span class="p">,</span> <span class="n">I_E_inp</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827110737553.png" alt="image-20230827110737553" /></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827110746410.png" alt="image-20230827110746410" /></p>

<h2 id="properties-of-e-i-balanced-network">Properties of E-I Balanced Network</h2>

<ul>
  <li>Linear encoding
External input strength is â€œlinearlyâ€ encoded by the mean firing rate of the neural population</li>
  <li>Fast Response
The network responds rapidly to abrupt changes of the input</li>
</ul>

<h3 id="noise-speeds-up-computation">Noise speeds up computation</h3>

<p>å¿«é€Ÿç›¸åº”çš„åŽŸç†ï¼Œå‡åŒ€åˆ†å¸ƒåœ¨é˜ˆå€¼ä¸‹é¢çš„ç©ºé—´</p>

<ul>
  <li>A neural ensemble jointly encodes stimulus information;</li>
  <li>Noise randomizes the distribution of neuronal membrane potentials;</li>
  <li>Those neurons (red circle) whose potentials are close to the threshold will fire rapidly;</li>
  <li>If the noisy environment is proper, even for a small input, a certain number of neurons will fire instantly to report the presence of a stimulus.</li>
</ul>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827113451626.png" alt="image-20230827113451626" /></p>

<h1 id="continuous-attractor-neural-network">Continuous Attractor Neural Network</h1>

<h2 id="attractor-models">Attractor Models</h2>

<h3 id="the-concept-of-attractor-dynamics">The concept of attractor dynamics</h3>

<p>Different types of attractors:
Point attractors, Line attractors, Ring attractors, Plane attractors, Cyclic attractors, Chaotic attractors</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827140250173.png" alt="image-20230827140250173" /></p>

<p>ç¨³æ€ï¼Œèƒ½é‡æ¢¯åº¦å¸å¼•åˆ°attractor</p>

<h3 id="discrete-attractor-network-model-hopfield-model">Discrete attractor Network Model: Hopfield Model</h3>

<p>$S_i=\pm1$: the neuronal state
$W_{ij}$ : the neuronal connection</p>

<p>The network dynamics:</p>

\[S_{i}=\mathrm{sign}\bigg(\sum_{j}w_{ij}S_{j}-\theta\bigg),\quad\mathrm{sign}(x)=1,\mathrm{for}x&gt;0;-1,\mathrm{otherwise}\]

<p>Updating rule: synchronous or asynchronous
Consider the network stores $p$ pattern, $\xi_{i}^{\mu},\mathrm{for}\mu=1,\ldots p;i=1,\ldots N$
Setting $w_{ij}=\frac{1}{N}\sum_{\mu=1}^{p}\xi_{i}^{\mu}\xi_{j}^{\mu}$</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827140827784.png" alt="image-20230827140827784" /></p>

<h4 id="energy-space-of-hopfield-network">Energy space of Hopfield network</h4>

\[\begin{aligned}
&amp;\text{Energy function: }E=-\frac{1}{2}\sum_{i,j}w_{ij}S_{i}S_{j}+\theta\sum_{i}S_{i} \\
&amp;\mathrm{Consider}S_{i}\mathrm{~is~updated},S_{i}(t+1)=sign[\sum_{j}w_{ij}S_{j}(t)-\theta] \\
&amp;\Delta E=E(t+1)-E(t)\\
&amp;=-[S_{i}(t+1)-S_{i}(t)]\sum_{j}w_{ij}S_{j}(t)+\theta\left[S_{i}(t+1)-S_{i}(t)\right] \\
&amp;=-[S_{i}(t+1)-S_{i}(t)][\sum_{j}w_{ij}S_{j}(t)-\theta] \\
&amp;\leq0
\end{aligned}\]

<p>åŒæ ·æ¿€æ´»åŒæ ·patternçš„ç¥žç»å…ƒï¼Œ~å¸å¼•å­</p>

<h4 id="auto-associative-memory-in-hopfield-network">Auto-associative memory in Hopfield Network</h4>

<p>A partial/noisy input can retrieve the related memory pattern</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827141253326.png" alt="image-20230827141253326" /></p>

<h4 id="persistent-activity-in-working-memory">Persistent activity in working memory</h4>

<p>After the removal of external input, the neurons in the network encoding the stimulus continue to fire persistently.</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827141421796.png" alt="image-20230827141421796" /></p>

<h2 id="continuous-attractor-neural-network-1">Continuous Attractor Neural Network</h2>

<h3 id="neural-coding">Neural coding</h3>

<h4 id="low-dimensional-continuous-feature">Low-dimensional continuous feature</h4>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827142520189.png" alt="image-20230827142520189" /></p>

<h4 id="continuous-attractor-neural-network-2">Continuous Attractor neural network</h4>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827142606695.png" alt="image-20230827142606695" /></p>

<h3 id="cann-a-rate-based-recurrent-circuit-model">CANN: A rate-based recurrent circuit model</h3>

<p><img src="Notes.assets/image-20230827142918529.png" alt="image-20230827142918529" style="zoom:50%;" /></p>

\[\begin{aligned}\tau\frac{\partial U(x,t)}{\partial t}&amp;=-U(x,t)+\rho\int f(x,x')r(x',t)dx'+l^{ext}(1)\\r(x,t)&amp;=\frac{U^2(x,t)}{1+k\rho\int U^2(x,t)dx}\quad(2)\\J(x,x')&amp;=\frac{J_0}{\sqrt{2\pi}a}\exp\left[-\frac{(x-x')^2}{2a^2}\right](3)\end{aligned}\]

<p>ré¢‘çŽ‡ï¼ŒJå¼ºåº¦ï¼ŒU decay</p>

<p><img src="Notes.assets/image-20230827143435002.png" alt="image-20230827143435002" style="zoom:50%;" /></p>

<h4 id="a-continuous-family-of-attractor-states">A Continuous family of attractor states</h4>

<p>åšå¹³ç§»çš„æ”¹å˜ï¼Œå˜åŒ–ä¼šè¢«ä¿ç•™ï¼Œline attractorï¼Œå—åˆ°ç¼–ç è¿žç»­åˆºæ¿€</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827143707784.png" alt="image-20230827143707784" /></p>

<h4 id="stability-analysis-derive-continuous-attractor-dynamics">Stability analysis derive continuous attractor dynamics</h4>

<p>åªéœ€è¦çœ‹åœ¨åŽŸå§‹çŠ¶æ€åŠ å…¥ä¸€ä¸ªå°é‡é¡¹ï¼Œå†ä»£å…¥å›ž</p>

<p>Consider small fluctuations around a stationary state at z:</p>

<p>Projecting $\delta U$ on the $i$th right eigenvector of $F(\delta U)_i(t)=(\delta U)_i(0)e^{-(1-\lambda _i)t/\tau}$</p>

<p>Two cases:</p>

<ul>
  <li>If $\lambda _i &lt; 1$, the projection decays exponentially</li>
  <li>If $\lambda _i$ = 1, the projection is sustained</li>
</ul>

<h4 id="spectra-of-the-kernel-f">Spectra of the kernel F</h4>

\[\begin{aligned}\bullet&amp;\lambda_0=1-2k\rho A\sqrt{2\pi}a&lt;1,\quad&amp;\mathbf{u}_0(x\mid z)=\overline{\mathbf{U}}(x\mid z);\\\bullet&amp;\lambda_1=1,\quad&amp;\mathbf{u}_1(x\mid z)=\frac{d\overline{\mathbf{U}}(x\mid z)}{dz},\text{the tangent of the valley}\\\bullet&amp;\lambda_n=\frac1{2^{n-2}},\quad&amp;\mathbf{u}_n(z)=\text{Combination of }\mathbf{v}_n(z)\end{aligned}\]

<p>$\mathbf{v}_{n}(z)\sim e^{-(c-z)^{2}/4a^{2}}(\frac{d}{dc})^{n}e^{-(c-z)^{2}/2a^{2}},$ the wave functions of quantumn harmonic osscilator</p>

<p>Note the decay time constant is: $\frac{\tau}{1-\lambda _n}$</p>

<h4 id="only-bump-position-shift-survives">Only bump position shift survives</h4>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827144540676.png" alt="image-20230827144540676" /></p>

<p>Ring attractor network for head-direction cell in fruit fly</p>

<p><img src="Notes.assets/image-20230827144557359.png" alt="image-20230827144557359" style="zoom:50%;" /></p>

<h2 id="computation-with-cann">Computation with CANN</h2>

<h3 id="persistent-activity-for-working-memory">Persistent activity for working memory</h3>

<p>When the global inhibition is not too strong, the network spontaneously hold bump activity:</p>

\[k&lt;\frac{\rho J_{0}^{2}}{8\sqrt{2\pi}a}\]

\[\begin{aligned}
&amp;\tilde{U}(x|z) =\quad U_{0}\exp\left[-\frac{(x-z)^{2}}{4a^{2}}\right],  \\
&amp;\tilde{r}(x|z) =\quad r_{0}\exp\left[-\frac{(x-z)^{2}}{2a^{2}}\right],  \\
&amp;U_{0}=[1+(1-k/k_{c})^{1/2}]A/(4\sqrt{\pi}ak) \\
&amp;r_0=[1+(1-k/k_{c})^{1/2}]/(2\sqrt{2\pi}ak\rho).
\end{aligned}\]

<h3 id="smooth-tracking-by-cann">Smooth tracking by CANN</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827145334859.png" alt="image-20230827145334859" /></p>

<p>Project the network dynamics on $v_1(t)$
$\tau{\frac{\partial\mathbf{U}<em>\mathbf{v}_{1}}{\partial t}}=-\mathbf{U}</em>\mathbf{v}<em>{1}+(\mathbf{J}*\mathbf{r})*\mathbf{v}</em>{1}+\mathbf{I}^{ext}*\mathbf{v}_{1}$</p>

<p>Consider</p>

<p>\(\begin{aligned}&amp;I^{ext}(t)=\alpha\overline{U}(x\mid z_0)+\sigma\xi_c(t)\\&amp;\mathbf{U}*\mathbf{v}_1\equiv\int dxU(x\mid z)\nu_1(x\mid z)\\\\&amp;\tau\frac{dz}{dt}=-\alpha(z-z_0)e^{-(z-z_0)^2/8a^2}+\beta\xi(t)\end{aligned}\)
1st term: the force of the signal that pulls the bump back to the stimulus position
2nd term: random shift</p>

<h3 id="population-decoding-via-template-matching">Population decoding via template matching</h3>

\[\hat{x}=\max_{z}\sum_{i}r_{i}f_{i}(z)\]

<ul>
  <li>The noisy bump is the population activity when the stimulus $x=0$</li>
  <li>Among three positions, the red one($z=0$) has the maximum overlap with the observed data.</li>
</ul>

<h2 id="computation-and-dynamics-of-adaptive-cann">Computation and Dynamics of Adaptive CANN</h2>

<h3 id="adaptive-continuous-attractor-neural-network">Adaptive Continuous Attractor neural network</h3>

\[\begin{aligned}
&amp;\tau{\frac{dU(x,t)}{dt}} =-U(x,t)+\rho\int dx'J(x-x^{\prime})r(x',t)-V(x,t)+I^{ext}(x,t)  \\
&amp;\tau_{_{\nu}}\frac{dV(x,t)}{dt} =-V(x,t)+mU(x,t) 
\end{aligned}\]

<p>$V(x,t)$ represents the SFA effect,</p>

<p>$V(x,t)=\frac{m}{\tau_{\nu}}\int_{-\infty}^{â€˜}e^{-\frac{t-tâ€™}{\tau_{\nu}}}U(x,tâ€™)dtâ€™$</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827150142710.png" alt="image-20230827150142710" /></p>

<p>SFA(Spike frequency Adaptation):</p>

<ul>
  <li>Neuronal response attenuates after experiencing prolonged firing.</li>
  <li>Slow negative feedback modulation to neuronal response.</li>
</ul>

<h3 id="intrinsic-mobility-of-a-cann">Intrinsic mobility of A-CANN</h3>

<p>Traveling Wave: a moving bump in the network without relying on external drive</p>

<p>The mechanism: SFA suppresses localized neural activity and triggers</p>

<p>$m&gt;\frac{\tau}{\tau _v}$, Travelling wave</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827150543244.png" alt="image-20230827150543244" /></p>

<h4 id="levy-flights-vs-brownian-motion">Levy flights vs. Brownian motion</h4>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827150851309.png" alt="image-20230827150851309" /></p>

<h4 id="lÃ©vy-flights-in-ecology-and-human-cognidve-behaviors">LÃ©vy flights in ecology and human cogniDve behaviors</h4>

<p>ç”Ÿç‰©å­¦å¤§å¤šè¿åŠ¨æœä»Žlevy flights</p>

<h3 id="noisy-adaptation-generates-levy-flight-in-cann">Noisy adaptation generates Levy flight in CANN</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827151343126.png" alt="image-20230827151343126" /></p>

<h3 id="time-delay-in-neural-signal-transmission">Time Delay in Neural Signal Transmission</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827151622032.png" alt="image-20230827151622032" /></p>

<h3 id="anticipatory-head-direction-signals-in-anterior-thalamus">Anticipatory Head Direction Signals in Anterior Thalamus</h3>

<p>æœ‰é¢„æµ‹ç­–ç•¥ï¼Œå®žçŽ°æŠµæ¶ˆä¿¡æ¯ä¼ é€’çš„delay</p>

<p>CANNåŠ å…¥è´Ÿåé¦ˆæœºåˆ¶æ˜¯å¯ä»¥å®žçŽ°é¢„æµ‹çš„</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230827152731895.png" alt="image-20230827152731895" /></p>

<h3 id="cann-with-stp">CANN with STP</h3>

\[\begin{gathered}
\tau{\frac{\mathrm{d}U(x,t)}{\mathrm{d}t}} {\cal O}=-U(x,t)+\rho\int g^{+}(x)h(x^{\prime},t)J(x,x^{\prime})r(x^{\prime},t)dx^{\prime}+I^{ext}(x,t)(1) \\
\frac{dg(x,t)}{dt}=-\frac{g(x,t)}{\tau_{f}}+G(1-g^{-}(x))r(x^{\prime},t)\quad(2) \\
\frac{dh(x,t)}{dt}=\frac{1-h(x,t)}{\tau_{d}}-g^{+}(x)h(x,t)r(x^{\prime},t)\quad(3) \\
r(x,t)={\frac{U^{2}(x,t)}{1+k\rho\int U^{2}(x,t)dx}}\quad(4) 
\end{gathered}\]

<h2 id="programming-in-brainpy">Programming in BrainPy</h2>

<h3 id="customize-a-ring-cann-in-brainpy">Customize a ring CANN in brainpy</h3>

<p>In simulations, we can not simulate a CANN encoding features ranging $(-\inf, \inf)$. Instead, we simulate a ring attractor network which encodes features ranging $(-\pi, \pi)$. Note that the distance on a ring should be:</p>

\[dist_{ring}(x,x') = min(|x-x'|,2\pi-|x-x'|)\]

<p><img src="https://cdn.kesci.com/upload/s01apgi89t.png?imageView2/0/w/320/h/320" alt="Image Name" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CANN1D</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">NeuGroupNS</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">8.1</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">A</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">J0</span><span class="o">=</span><span class="mf">4.</span><span class="p">,</span>
               <span class="n">z_min</span><span class="o">=-</span><span class="n">bm</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">z_max</span><span class="o">=</span><span class="n">bm</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">CANN1D</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">num</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># åˆå§‹åŒ–å‚æ•°
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">a</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">A</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">J0</span> <span class="o">=</span> <span class="n">J0</span>

    <span class="c1"># åˆå§‹åŒ–ç‰¹å¾ç©ºé—´ç›¸å…³å‚æ•°
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">z_min</span> <span class="o">=</span> <span class="n">z_min</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">z_max</span> <span class="o">=</span> <span class="n">z_max</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span> <span class="o">=</span> <span class="n">z_max</span> <span class="o">-</span> <span class="n">z_min</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">z_min</span><span class="p">,</span> <span class="n">z_max</span><span class="p">,</span> <span class="n">num</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">rho</span> <span class="o">=</span> <span class="n">num</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">dx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span> <span class="o">/</span> <span class="n">num</span>

    <span class="c1"># åˆå§‹åŒ–å˜é‡
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">u</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="nb">input</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">conn_mat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">make_conn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># è¿žæŽ¥çŸ©é˜µ
</span>
    <span class="c1"># å®šä¹‰ç§¯åˆ†å‡½æ•°
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">integral</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">derivative</span><span class="p">)</span>

  <span class="c1"># å¾®åˆ†æ–¹ç¨‹
</span>  <span class="o">@</span><span class="nb">property</span>
  <span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">du</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">u</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">Irec</span><span class="p">,</span> <span class="n">Iext</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="n">u</span> <span class="o">+</span> <span class="n">Irec</span> <span class="o">+</span> <span class="n">Iext</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau</span>
    <span class="k">return</span> <span class="n">du</span>

  <span class="c1"># å°†è·ç¦»è½¬æ¢åˆ°[-z_range/2, z_range/2)ä¹‹é—´
</span>  <span class="k">def</span> <span class="nf">dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">d</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span><span class="p">,</span> <span class="n">d</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">d</span>

  <span class="c1"># è®¡ç®—è¿žæŽ¥çŸ©é˜µ
</span>  <span class="k">def</span> <span class="nf">make_conn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">bm</span><span class="p">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">d</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dist</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">])</span>  <span class="c1"># è·ç¦»çŸ©é˜µ
</span>    <span class="n">Jxx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">J0</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span>
      <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">d</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">Jxx</span>

  <span class="c1"># èŽ·å–å„ä¸ªç¥žç»å…ƒåˆ°poså¤„ç¥žç»å…ƒçš„è¾“å…¥
</span>  <span class="k">def</span> <span class="nf">get_stimulus_by_pos</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pos</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.25</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dist</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">pos</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">_t</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'t'</span><span class="p">]</span>
    <span class="n">u2</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">u</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">u2</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">k</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">u2</span><span class="p">))</span>
    <span class="n">Irec</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conn_mat</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">u</span><span class="p">[:]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">u</span><span class="p">,</span> <span class="n">_t</span><span class="p">,</span><span class="n">Irec</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">[:]</span> <span class="o">=</span> <span class="mf">0.</span>  <span class="c1"># é‡ç½®å¤–éƒ¨ç”µæµ
</span></code></pre></div></div>

<h3 id="simulate-the-persistent-activity-of-cann-after-the-removal-of-external-input">Simulate the persistent activity of CANN after the removal of external input</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">Persistent_Activity</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">J0</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
    <span class="c1"># ç”ŸæˆCANN
</span>    <span class="n">cann</span> <span class="o">=</span> <span class="n">CANN1D</span><span class="p">(</span><span class="n">num</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span><span class="n">J0</span><span class="o">=</span><span class="n">J0</span><span class="p">)</span>

    <span class="c1"># ç”Ÿæˆå¤–éƒ¨åˆºæ¿€ï¼Œä»Žç¬¬2åˆ°12msï¼ŒæŒç»­10ms
</span>    <span class="n">dur1</span><span class="p">,</span> <span class="n">dur2</span><span class="p">,</span> <span class="n">dur3</span> <span class="o">=</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">10.</span>
    <span class="n">I1</span> <span class="o">=</span> <span class="n">cann</span><span class="p">.</span><span class="n">get_stimulus_by_pos</span><span class="p">(</span><span class="mf">0.</span><span class="p">)</span>
    <span class="n">Iext</span><span class="p">,</span> <span class="n">duration</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">section_input</span><span class="p">(</span><span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="n">I1</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
                                             <span class="n">durations</span><span class="o">=</span><span class="p">[</span><span class="n">dur1</span><span class="p">,</span> <span class="n">dur2</span><span class="p">,</span> <span class="n">dur3</span><span class="p">],</span>
                                             <span class="n">return_length</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">noise_level</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">noise_level</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">duration</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">()),</span> <span class="nb">len</span><span class="p">(</span><span class="n">I1</span><span class="p">)))</span>
    <span class="n">Iext</span> <span class="o">+=</span> <span class="n">noise</span>
    <span class="c1"># è¿è¡Œæ•°å€¼æ¨¡æ‹Ÿ
</span>    <span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">cann</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s">'input'</span><span class="p">,</span> <span class="n">Iext</span><span class="p">,</span> <span class="s">'iter'</span><span class="p">],</span> <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'u'</span><span class="p">])</span>
    <span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">duration</span><span class="p">)</span>

    <span class="c1"># å¯è§†åŒ–
</span>    <span class="k">def</span> <span class="nf">plot_response</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">gs</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">get_figure</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">ts</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">t</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">())</span>
        <span class="n">I</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="n">Iext</span><span class="p">[</span><span class="n">ts</span><span class="p">],</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">u</span><span class="p">[</span><span class="n">ts</span><span class="p">]</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cann</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Iext'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cann</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'U'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">'$t$'</span> <span class="o">+</span> <span class="s">' = {} ms'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$x$'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'top'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'right'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="c1"># plt.savefig(f'CANN_t={t}.pdf', transparent=True, dpi=500)
</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">20.</span><span class="p">)</span>

    <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">animate_1D</span><span class="p">(</span>
        <span class="n">dynamical_vars</span><span class="o">=</span><span class="p">[{</span><span class="s">'ys'</span><span class="p">:</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">u</span><span class="p">,</span> <span class="s">'xs'</span><span class="p">:</span> <span class="n">cann</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="s">'legend'</span><span class="p">:</span> <span class="s">'u'</span><span class="p">},</span>
                        <span class="p">{</span><span class="s">'ys'</span><span class="p">:</span> <span class="n">Iext</span><span class="p">,</span> <span class="s">'xs'</span><span class="p">:</span> <span class="n">cann</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="s">'legend'</span><span class="p">:</span> <span class="s">'Iext'</span><span class="p">}],</span>
        <span class="n">frame_step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">frame_delay</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
        <span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">Persistent_Activity</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="simulate-the-tracking-behavior-of-cann">Simulate the tracking behavior of CANN</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">smooth_tracking</span><span class="p">():</span>
    <span class="n">cann</span> <span class="o">=</span> <span class="n">CANN1D</span><span class="p">(</span><span class="n">num</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">8.1</span><span class="p">)</span>

    <span class="c1"># å®šä¹‰éšæ—¶é—´å˜åŒ–çš„å¤–éƒ¨åˆºæ¿€
</span>    <span class="n">v_ext</span> <span class="o">=</span> <span class="mf">1e-3</span>
    <span class="n">dur1</span><span class="p">,</span> <span class="n">dur2</span><span class="p">,</span> <span class="n">dur3</span> <span class="o">=</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mi">20</span>
    <span class="n">num1</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">dur1</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">())</span>
    <span class="n">num2</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">dur2</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">())</span>
    <span class="n">num3</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">dur3</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">())</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num1</span> <span class="o">+</span> <span class="n">num2</span> <span class="o">+</span> <span class="n">num3</span><span class="p">)</span>
    <span class="n">position</span><span class="p">[</span><span class="n">num1</span><span class="p">:</span> <span class="n">num1</span> <span class="o">+</span> <span class="n">num2</span><span class="p">]</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">num2</span><span class="p">)</span>
    <span class="n">position</span><span class="p">[</span><span class="n">num1</span> <span class="o">+</span> <span class="n">num2</span><span class="p">:</span> <span class="p">]</span> <span class="o">=</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">pi</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">position</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">Iext</span> <span class="o">=</span> <span class="n">cann</span><span class="p">.</span><span class="n">get_stimulus_by_pos</span><span class="p">(</span><span class="n">position</span><span class="p">)</span>

    <span class="c1"># è¿è¡Œæ¨¡æ‹Ÿ
</span>    <span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">cann</span><span class="p">,</span>
                         <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s">'input'</span><span class="p">,</span> <span class="n">Iext</span><span class="p">,</span> <span class="s">'iter'</span><span class="p">],</span>
                         <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'u'</span><span class="p">])</span>
    <span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">dur1</span> <span class="o">+</span> <span class="n">dur2</span> <span class="o">+</span> <span class="n">dur3</span><span class="p">)</span>

    <span class="c1"># å¯è§†åŒ–
</span>    <span class="k">def</span> <span class="nf">plot_response</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">extra_fun</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">gs</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">get_figure</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">ts</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">t</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">())</span>
        <span class="n">I</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="n">Iext</span><span class="p">[</span><span class="n">ts</span><span class="p">],</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">u</span><span class="p">[</span><span class="n">ts</span><span class="p">]</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cann</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Iext'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cann</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'U'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">'$t$'</span> <span class="o">+</span> <span class="s">' = {} ms'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$x$'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'top'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'right'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">extra_fun</span><span class="p">:</span> <span class="n">extra_fun</span><span class="p">()</span>
        <span class="c1"># plt.savefig(f'CANN_tracking_t={t}.pdf', transparent=True, dpi=500)
</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">f</span><span class="p">():</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">''</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s">"-&gt;"</span><span class="p">))</span>

    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">15.</span><span class="p">,</span> <span class="n">extra_fun</span><span class="o">=</span><span class="n">f</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">f</span><span class="p">():</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">''</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s">"-&gt;"</span><span class="p">))</span>

    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">extra_fun</span><span class="o">=</span><span class="n">f</span><span class="p">)</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">30.</span><span class="p">)</span>

    <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">animate_1D</span><span class="p">(</span>
        <span class="n">dynamical_vars</span><span class="o">=</span><span class="p">[{</span><span class="s">'ys'</span><span class="p">:</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">u</span><span class="p">,</span> <span class="s">'xs'</span><span class="p">:</span> <span class="n">cann</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="s">'legend'</span><span class="p">:</span> <span class="s">'u'</span><span class="p">},</span>
                        <span class="p">{</span><span class="s">'ys'</span><span class="p">:</span> <span class="n">Iext</span><span class="p">,</span> <span class="s">'xs'</span><span class="p">:</span> <span class="n">cann</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="s">'legend'</span><span class="p">:</span> <span class="s">'Iext'</span><span class="p">}],</span>
        <span class="n">frame_step</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">frame_delay</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">smooth_tracking</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="customize-a-cann-with-sfasimulate-the-spontaneous-traveling-wave">Customize a CANN with SFASimulate the spontaneous traveling wave</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CANN1D_SFA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">NeuGroupNS</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">tau_v</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">8.1</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">A</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">J0</span><span class="o">=</span><span class="mf">4.</span><span class="p">,</span>
               <span class="n">z_min</span><span class="o">=-</span><span class="n">bm</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">z_max</span><span class="o">=</span><span class="n">bm</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">CANN1D_SFA</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">num</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># åˆå§‹åŒ–å‚æ•°
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">tau_v</span> <span class="o">=</span> <span class="n">tau_v</span> <span class="c1">#time constant of SFA
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">a</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">A</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">J0</span> <span class="o">=</span> <span class="n">J0</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">m</span> <span class="c1">#SFA strength
</span>      
    <span class="c1"># åˆå§‹åŒ–ç‰¹å¾ç©ºé—´ç›¸å…³å‚æ•°
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">z_min</span> <span class="o">=</span> <span class="n">z_min</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">z_max</span> <span class="o">=</span> <span class="n">z_max</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span> <span class="o">=</span> <span class="n">z_max</span> <span class="o">-</span> <span class="n">z_min</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">z_min</span><span class="p">,</span> <span class="n">z_max</span><span class="p">,</span> <span class="n">num</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">rho</span> <span class="o">=</span> <span class="n">num</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">dx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span> <span class="o">/</span> <span class="n">num</span>

    <span class="c1"># åˆå§‹åŒ–å˜é‡
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">u</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num</span><span class="p">))</span> <span class="c1">#SFA current
</span>    <span class="bp">self</span><span class="p">.</span><span class="nb">input</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">conn_mat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">make_conn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># è¿žæŽ¥çŸ©é˜µ
</span>
    <span class="c1"># å®šä¹‰ç§¯åˆ†å‡½æ•°
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">integral</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">derivative</span><span class="p">)</span>

  <span class="c1"># å¾®åˆ†æ–¹ç¨‹
</span>  <span class="o">@</span><span class="nb">property</span>
  <span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">du</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">u</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">Irec</span><span class="p">,</span> <span class="n">Iext</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="n">u</span> <span class="o">+</span> <span class="n">Irec</span> <span class="o">+</span> <span class="n">Iext</span><span class="o">-</span><span class="n">v</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau</span>
    <span class="n">dv</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">v</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">u</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="n">v</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">m</span><span class="o">*</span><span class="n">u</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau_v</span>
    <span class="k">return</span> <span class="n">bp</span><span class="p">.</span><span class="n">JointEq</span><span class="p">([</span><span class="n">du</span><span class="p">,</span> <span class="n">dv</span><span class="p">])</span>

  <span class="c1"># å°†è·ç¦»è½¬æ¢åˆ°[-z_range/2, z_range/2)ä¹‹é—´
</span>  <span class="k">def</span> <span class="nf">dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">d</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span><span class="p">,</span> <span class="n">d</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">d</span>

  <span class="c1"># è®¡ç®—è¿žæŽ¥çŸ©é˜µ
</span>  <span class="k">def</span> <span class="nf">make_conn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">bm</span><span class="p">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">d</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dist</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">])</span>  <span class="c1"># è·ç¦»çŸ©é˜µ
</span>    <span class="n">Jxx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">J0</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span>
      <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">d</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">Jxx</span>

  <span class="c1"># èŽ·å–å„ä¸ªç¥žç»å…ƒåˆ°poså¤„ç¥žç»å…ƒçš„è¾“å…¥
</span>  <span class="k">def</span> <span class="nf">get_stimulus_by_pos</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pos</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.25</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dist</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">pos</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">u2</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">u</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">u2</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">k</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">u2</span><span class="p">))</span>
    <span class="n">Irec</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conn_mat</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>
    <span class="n">u</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">u</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">,</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'t'</span><span class="p">],</span><span class="n">Irec</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">u</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">u</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">,</span><span class="n">u</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">v</span>
    <span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">[:]</span> <span class="o">=</span> <span class="mf">0.</span>  <span class="c1"># é‡ç½®å¤–éƒ¨ç”µæµ
</span></code></pre></div></div>

<h3 id="simulate-the-spontaneous-traveling-wave">Simulate the spontaneous traveling wave</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">traveling_wave</span><span class="p">(</span><span class="n">num</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span><span class="n">m</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="c1"># ç”ŸæˆCANN
</span>    <span class="n">cann_sfa</span> <span class="o">=</span> <span class="n">CANN1D_SFA</span><span class="p">(</span><span class="n">num</span><span class="o">=</span><span class="n">num</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="n">m</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>

    <span class="c1"># ç”Ÿæˆå¤–éƒ¨åˆºæ¿€
</span>    <span class="n">dur</span> <span class="o">=</span> <span class="mf">1000.</span>
    <span class="n">noise_level</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">Iext</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">noise_level</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">dur</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">()),</span> <span class="n">num</span><span class="p">))</span>
    <span class="n">duration</span> <span class="o">=</span> <span class="n">dur</span>
    <span class="c1"># è¿è¡Œæ•°å€¼æ¨¡æ‹Ÿ
</span>    <span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">cann_sfa</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s">'input'</span><span class="p">,</span> <span class="n">Iext</span><span class="p">,</span> <span class="s">'iter'</span><span class="p">],</span> <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'u'</span><span class="p">])</span>
    <span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">duration</span><span class="p">)</span>

    <span class="c1"># å¯è§†åŒ–
</span>    <span class="k">def</span> <span class="nf">plot_response</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">gs</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">get_figure</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">ts</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">t</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">())</span>
        <span class="n">I</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="n">Iext</span><span class="p">[</span><span class="n">ts</span><span class="p">],</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">u</span><span class="p">[</span><span class="n">ts</span><span class="p">]</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cann_sfa</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Iext'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cann_sfa</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'U'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">'$t$'</span> <span class="o">+</span> <span class="s">' = {} ms'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$x$'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'top'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'right'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="c1"># plt.savefig(f'CANN_t={t}.pdf', transparent=True, dpi=500)
</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">100.</span><span class="p">)</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">150.</span><span class="p">)</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">200.</span><span class="p">)</span>

    <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">animate_1D</span><span class="p">(</span>
        <span class="n">dynamical_vars</span><span class="o">=</span><span class="p">[{</span><span class="s">'ys'</span><span class="p">:</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">u</span><span class="p">,</span> <span class="s">'xs'</span><span class="p">:</span> <span class="n">cann_sfa</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="s">'legend'</span><span class="p">:</span> <span class="s">'u'</span><span class="p">},</span>
                        <span class="p">{</span><span class="s">'ys'</span><span class="p">:</span> <span class="n">Iext</span><span class="p">,</span> <span class="s">'xs'</span><span class="p">:</span> <span class="n">cann_sfa</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="s">'legend'</span><span class="p">:</span> <span class="s">'Iext'</span><span class="p">}],</span>
        <span class="n">frame_step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">frame_delay</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
        <span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
<span class="n">traveling_wave</span><span class="p">(</span><span class="n">num</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span><span class="n">m</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="simulate-the-anticipative-tracking">Simulate the anticipative tracking</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">anticipative_tracking</span><span class="p">(</span><span class="n">m</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">v_ext</span><span class="o">=</span><span class="mi">6</span><span class="o">*</span><span class="mf">1e-3</span><span class="p">):</span>
    <span class="n">cann_sfa</span> <span class="o">=</span> <span class="n">CANN1D_SFA</span><span class="p">(</span><span class="n">num</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="n">m</span><span class="p">)</span>
    
    <span class="c1"># å®šä¹‰éšæ—¶é—´å˜åŒ–çš„å¤–éƒ¨åˆºæ¿€
</span>    <span class="n">v_ext</span> <span class="o">=</span> <span class="n">v_ext</span>
    <span class="n">dur1</span><span class="p">,</span> <span class="n">dur2</span><span class="p">,</span> <span class="o">=</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">1000.</span>
    <span class="n">num1</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">dur1</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">())</span>
    <span class="n">num2</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">dur2</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">())</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num1</span> <span class="o">+</span> <span class="n">num2</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num2</span><span class="p">):</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">position</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">num1</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">v_ext</span><span class="o">*</span><span class="n">bm</span><span class="p">.</span><span class="n">dt</span>
        <span class="c1"># the periodical boundary
</span>        <span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">pos</span><span class="o">&gt;</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">pos</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">pos</span><span class="p">)</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">pos</span><span class="o">&lt;-</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">pos</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">pos</span><span class="p">)</span>
        <span class="c1"># update
</span>        <span class="n">position</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">num1</span><span class="p">]</span> <span class="o">=</span> <span class="n">pos</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">position</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">Iext</span> <span class="o">=</span> <span class="n">cann_sfa</span><span class="p">.</span><span class="n">get_stimulus_by_pos</span><span class="p">(</span><span class="n">position</span><span class="p">)</span>

    <span class="c1"># è¿è¡Œæ¨¡æ‹Ÿ
</span>    <span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">cann_sfa</span><span class="p">,</span>
                         <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s">'input'</span><span class="p">,</span> <span class="n">Iext</span><span class="p">,</span> <span class="s">'iter'</span><span class="p">],</span>
                         <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'u'</span><span class="p">],</span>
                         <span class="n">dyn_vars</span><span class="o">=</span><span class="n">cann_sfa</span><span class="p">.</span><span class="nb">vars</span><span class="p">())</span>
    <span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">dur1</span> <span class="o">+</span> <span class="n">dur2</span><span class="p">)</span>

    <span class="c1"># å¯è§†åŒ–
</span>    <span class="k">def</span> <span class="nf">plot_response</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">extra_fun</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">gs</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">get_figure</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">ts</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">t</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">())</span>
        <span class="n">I</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="n">Iext</span><span class="p">[</span><span class="n">ts</span><span class="p">],</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">u</span><span class="p">[</span><span class="n">ts</span><span class="p">]</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cann_sfa</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Iext'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cann_sfa</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="mi">10</span><span class="o">*</span><span class="n">u</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'U'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">'$t$'</span> <span class="o">+</span> <span class="s">' = {} ms'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$x$'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'top'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'right'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">200.</span><span class="p">)</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">400.</span><span class="p">)</span>
    <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">animate_1D</span><span class="p">(</span>
        <span class="n">dynamical_vars</span><span class="o">=</span><span class="p">[{</span><span class="s">'ys'</span><span class="p">:</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">u</span><span class="p">,</span> <span class="s">'xs'</span><span class="p">:</span> <span class="n">cann_sfa</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="s">'legend'</span><span class="p">:</span> <span class="s">'u'</span><span class="p">},</span>
                        <span class="p">{</span><span class="s">'ys'</span><span class="p">:</span> <span class="n">Iext</span><span class="p">,</span> <span class="s">'xs'</span><span class="p">:</span> <span class="n">cann_sfa</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="s">'legend'</span><span class="p">:</span> <span class="s">'Iext'</span><span class="p">}],</span>
        <span class="n">frame_step</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">frame_delay</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">anticipative_tracking</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="customize-a-cann-with-stp">Customize a CANN with STP</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CANN1D_STP</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">NeuGroupNS</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">tau_f</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">tau_d</span><span class="o">=</span><span class="mf">30.</span><span class="p">,</span> <span class="n">G</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">8.1</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">A</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">J0</span><span class="o">=</span><span class="mf">12.</span><span class="p">,</span>
               <span class="n">z_min</span><span class="o">=-</span><span class="n">bm</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">z_max</span><span class="o">=</span><span class="n">bm</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">CANN1D_STP</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">num</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># åˆå§‹åŒ–å‚æ•°
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">tau_f</span> <span class="o">=</span> <span class="n">tau_f</span> <span class="c1">#time constant of u
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">tau_d</span> <span class="o">=</span> <span class="n">tau_d</span> <span class="c1">#time constant of h
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">G</span> <span class="o">=</span> <span class="n">G</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">a</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">A</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">J0</span> <span class="o">=</span> <span class="n">J0</span>
      
    <span class="c1"># åˆå§‹åŒ–ç‰¹å¾ç©ºé—´ç›¸å…³å‚æ•°
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">z_min</span> <span class="o">=</span> <span class="n">z_min</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">z_max</span> <span class="o">=</span> <span class="n">z_max</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span> <span class="o">=</span> <span class="n">z_max</span> <span class="o">-</span> <span class="n">z_min</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">z_min</span><span class="p">,</span> <span class="n">z_max</span><span class="p">,</span> <span class="n">num</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">rho</span> <span class="o">=</span> <span class="n">num</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">dx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span> <span class="o">/</span> <span class="n">num</span>

    <span class="c1"># åˆå§‹åŒ–å˜é‡
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">u</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">g</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num</span><span class="p">))</span> <span class="c1">#neuro-transmitter release probability
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num</span><span class="p">))</span> <span class="c1">#neuro-transmitter available fraction
</span>    <span class="bp">self</span><span class="p">.</span><span class="nb">input</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num</span><span class="p">))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">conn_mat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">make_conn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># è¿žæŽ¥çŸ©é˜µ
</span>
    <span class="c1"># å®šä¹‰ç§¯åˆ†å‡½æ•°
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">integral</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">derivative</span><span class="p">)</span>

  <span class="c1"># å¾®åˆ†æ–¹ç¨‹
</span>  <span class="o">@</span><span class="nb">property</span>
  <span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">du</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">u</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">Irec</span><span class="p">,</span> <span class="n">Iext</span><span class="p">:</span> <span class="p">(</span><span class="o">-</span><span class="n">u</span> <span class="o">+</span> <span class="n">Irec</span> <span class="o">+</span> <span class="n">Iext</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau</span>
    <span class="n">dg</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">g</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">r</span><span class="p">:</span> <span class="o">-</span><span class="n">g</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau_f</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">G</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">r</span> 
    <span class="n">dh</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">r</span><span class="p">:</span>  <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau_d</span> <span class="o">-</span> <span class="p">(</span><span class="n">g</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">G</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">g</span><span class="p">))</span> <span class="o">*</span> <span class="n">h</span> <span class="o">*</span><span class="n">r</span>
    <span class="k">return</span> <span class="n">bp</span><span class="p">.</span><span class="n">JointEq</span><span class="p">([</span><span class="n">du</span><span class="p">,</span> <span class="n">dg</span><span class="p">,</span> <span class="n">dh</span><span class="p">])</span>

  <span class="c1"># å°†è·ç¦»è½¬æ¢åˆ°[-z_range/2, z_range/2)ä¹‹é—´
</span>  <span class="k">def</span> <span class="nf">dist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">remainder</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">d</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span><span class="p">,</span> <span class="n">d</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">z_range</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">d</span>

  <span class="c1"># è®¡ç®—è¿žæŽ¥çŸ©é˜µ
</span>  <span class="k">def</span> <span class="nf">make_conn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">bm</span><span class="p">.</span><span class="n">ndim</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">d</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dist</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">])</span>  <span class="c1"># è·ç¦»çŸ©é˜µ
</span>    <span class="n">Jxx</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">J0</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span>
      <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">d</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">Jxx</span>

  <span class="c1"># èŽ·å–å„ä¸ªç¥žç»å…ƒåˆ°poså¤„ç¥žç»å…ƒçš„è¾“å…¥
</span>  <span class="k">def</span> <span class="nf">get_stimulus_by_pos</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pos</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.25</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dist</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">pos</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">a</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">u2</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">u</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">u2</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">k</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">u2</span><span class="p">))</span> 
    <span class="n">Irec</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conn_mat</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">g</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">G</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">))</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="o">*</span><span class="n">r</span><span class="p">)</span>
    <span class="n">u</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral</span><span class="p">(</span><span class="n">u</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">u</span><span class="p">,</span> <span class="n">g</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'t'</span><span class="p">],</span> <span class="n">Irec</span><span class="o">=</span><span class="n">Irec</span><span class="p">,</span> <span class="n">Iext</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="n">r</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="n">bm</span><span class="p">.</span><span class="n">dt</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">u</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">u</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">,</span><span class="n">u</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">g</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">h</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">h</span>
    <span class="bp">self</span><span class="p">.</span><span class="nb">input</span><span class="p">[:]</span> <span class="o">=</span> <span class="mf">0.</span>  <span class="c1"># é‡ç½®å¤–éƒ¨ç”µæµ
</span></code></pre></div></div>

<h3 id="simulate-traveling-wave-in-cann-with-stp">Simulate traveling wave in CANN with STP</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">traveling_wave_STP</span><span class="p">(</span><span class="n">num</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">J0</span><span class="o">=</span><span class="mf">12.</span><span class="p">,</span><span class="n">tau_d</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span><span class="n">tau_f</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span><span class="n">G</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
    <span class="c1"># ç”ŸæˆCANN
</span>    <span class="n">cann_stp</span> <span class="o">=</span> <span class="n">CANN1D_STP</span><span class="p">(</span><span class="n">num</span><span class="o">=</span><span class="n">num</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span><span class="n">tau_d</span><span class="o">=</span><span class="n">tau_d</span><span class="p">,</span><span class="n">tau_f</span><span class="o">=</span><span class="n">tau_f</span><span class="p">,</span><span class="n">G</span><span class="o">=</span><span class="n">G</span><span class="p">,</span> <span class="n">J0</span><span class="o">=</span><span class="n">J0</span><span class="p">)</span>

    <span class="c1"># ç”Ÿæˆå¤–éƒ¨åˆºæ¿€
</span>    <span class="n">dur</span> <span class="o">=</span> <span class="mf">1000.</span>
    <span class="n">noise_level</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">Iext</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">noise_level</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">dur</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">()),</span> <span class="n">num</span><span class="p">))</span>
    <span class="n">duration</span> <span class="o">=</span> <span class="n">dur</span>
    <span class="c1"># è¿è¡Œæ•°å€¼æ¨¡æ‹Ÿ
</span>    <span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">cann_stp</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s">'input'</span><span class="p">,</span> <span class="n">Iext</span><span class="p">,</span> <span class="s">'iter'</span><span class="p">],</span> <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'u'</span><span class="p">,</span><span class="s">'g'</span><span class="p">,</span><span class="s">'h'</span><span class="p">])</span>
    <span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">duration</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">as_numpy</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">u</span><span class="p">)</span>
    <span class="n">max_index</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="mi">1000</span><span class="p">,:])</span>
    <span class="k">print</span><span class="p">(</span><span class="n">max_index</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">g</span><span class="p">[:,</span><span class="n">max_index</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s">'g'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">h</span><span class="p">[:,</span><span class="n">max_index</span><span class="p">],</span><span class="n">label</span><span class="o">=</span><span class="s">'h'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="c1"># å¯è§†åŒ–
</span>    <span class="k">def</span> <span class="nf">plot_response</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">gs</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">get_figure</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">ts</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">t</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">())</span>
        <span class="n">I</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="n">Iext</span><span class="p">[</span><span class="n">ts</span><span class="p">],</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">u</span><span class="p">[</span><span class="n">ts</span><span class="p">]</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cann_stp</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">I</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Iext'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cann_stp</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'U'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s">'$t$'</span> <span class="o">+</span> <span class="s">' = {} ms'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s">'$x$'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'top'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">spines</span><span class="p">[</span><span class="s">'right'</span><span class="p">].</span><span class="n">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">100.</span><span class="p">)</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">200.</span><span class="p">)</span>
    <span class="n">plot_response</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="mf">300.</span><span class="p">)</span>

    <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">animate_1D</span><span class="p">(</span>
        <span class="n">dynamical_vars</span><span class="o">=</span><span class="p">[{</span><span class="s">'ys'</span><span class="p">:</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">u</span><span class="p">,</span> <span class="s">'xs'</span><span class="p">:</span> <span class="n">cann_stp</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="s">'legend'</span><span class="p">:</span> <span class="s">'u'</span><span class="p">},</span>
                        <span class="p">{</span><span class="s">'ys'</span><span class="p">:</span> <span class="n">Iext</span><span class="p">,</span> <span class="s">'xs'</span><span class="p">:</span> <span class="n">cann_stp</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> <span class="s">'legend'</span><span class="p">:</span> <span class="s">'Iext'</span><span class="p">}],</span>
        <span class="n">frame_step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">frame_delay</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
        <span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
<span class="n">traveling_wave_STP</span><span class="p">(</span><span class="n">G</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="n">tau_d</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="decision-making-network">Decision-Making Network</h1>

<h2 id="lip---decision-making">LIP -&gt; Decision-Making</h2>

<h3 id="coherent-motion-task">Coherent motion task</h3>

<p>åˆ¤æ–­éšæœºç‚¹(å¤§éƒ¨åˆ†ç‚¹)çš„è¿åŠ¨æœå‘</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828100425871.png" alt="image-20230828100425871" /></p>

<p>coherenceå½±å“ä»»åŠ¡çš„éš¾åº¦
0%éš¾ï¼Œ100%ç®€å•</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828100516123.png" alt="image-20230828100516123" /></p>

<p>ç¼–ç å†³ç­–çš„å“åº”ï¼Œä¸æ˜¯è¿åŠ¨</p>

<h3 id="reaction-time-vs-fixed-duration">Reaction Time vs. Fixed Duration</h3>

<p>coherenceè¶Šé«˜ï¼Œååº”æ—¶é—´è¶ŠçŸ­</p>

<p>Fixed Durationå¤šäº†Delay time</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828100658772.png" alt="image-20230828100658772" /></p>

<p>å®žéªŒè®¾è®¡çº¯ç²¹æŠŠdecision-makingç»™æå–å‡ºæ¥</p>

<h4 id="effect-of-difficulty">Effect of Difficulty</h4>

<p>coherenceè¶Šå¤§ï¼Œååº”æ—¶é—´æ˜¯è¶ŠçŸ­ï¼Œsingle neuronå¾ˆéš¾åšåˆ°è¿™ä¹ˆçŸ­çš„decision-makingï¼Œè€ƒè™‘è¦å»ºæ¨¡çš„å› ç´ </p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828101103008.png" alt="image-20230828101103008" /></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828101058059.png" alt="image-20230828101058059" /></p>

<h4 id="response-of-mt-neurons">Response of MT Neurons</h4>

<p>è®°å½•MTçš„ç¥žç»å…ƒï¼Œå¯¹è¿™ç§è¿åŠ¨çš„æœå‘åˆºæ¿€è¿›è¡Œç¼–ç </p>

<p>çº¿æ€§ç¼–ç coherenceè¿åŠ¨å¼ºåº¦çš„æ–¹å‘</p>

<p>åšå†³ç­–åœ¨å®ƒçš„ä¸‹æ¸¸</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828101303674.png" alt="image-20230828101303674" /></p>

<h4 id="response-of-lip-neurons">Response of LIP Neurons</h4>

<p>MTçš„ä¸‹æ¸¸æ‰¾åˆ°LIPçš„ç¥žç»å…ƒ</p>

<p>çˆ¬å‡åˆ°ä¸€å®šé«˜åº¦å†åšé€‰æ‹©</p>

<p>coherenceä¸Žçˆ¬å‡çš„æ–œçŽ‡ä¹Ÿä¼šæœ‰å½±å“ï¼Œä»»åŠ¡è¶Šéš¾ï¼Œçˆ¬å‡æ–œçŽ‡è¶Šå°</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828101609881.png" alt="image-20230828101609881" /></p>

<h3 id="ramping-to-thresholdperfect-integrator-model">Ramping-to-threshold(perfect integrator) Model</h3>

\[\begin{aligned}\frac{dR}{dt}=I_A-I_B+\text{noise},\quad R(t)&amp;=(I_A-I_B)t+\int_0^tdt\text{noise}.\\\tau_\text{network}&amp;=\infty!\end{aligned}\]

<p>ä¸¤ç§é€‰æ‹©ç§¯åˆ†æ±‚å’Œåšç§¯ç´¯ï¼Œç­‰åˆ°é˜ˆå€¼åšå†³ç­–</p>

<p>Accumulates information (evidence) -&gt; Ramping</p>

<p>ç›´æŽ¥ä¿å­˜ä¿¡æ¯ï¼Œæ²¡æœ‰ç‰¹åˆ«å¥½çš„ç”Ÿç‰©å¯¹åº”</p>

<h2 id="a-spiking-network-of-dm">A Spiking Network of DM</h2>

<h3 id="a-cortical-microcircuit-model">A cortical microcircuit model</h3>

<p><img src="Notes.assets/image-20230828103055151.png" alt="image-20230828103055151" /></p>

<p>A=Upward motion B=Downward motion</p>

<p>2-population excitatory neurons (integrate-and-fire neurons driven by Poisson input)
Slow reverberatory excitation mediated by the NMDA receptors at recurrent synapses
AMPA receptors ($\tau _{syn}=$1 - 3 ms)
NMDA receptors ($\tau _{syn}=$ 50 - 100 ms).</p>

<p>ä¸¤ç¾¤ç¥žç»å…ƒåˆ†åˆ«åšä¸åŒçš„é€‰æ‹©ï¼Œä¸Žè‡ªå·±å¯¹æ–¹éƒ½æœ‰è¿žæŽ¥
NMDA ç¼“æ…¢çš„ä¿¡å·ä½¿å¾—æœ‰æ…¢æ…¢å¢žé•¿çš„rampingçš„è¿‡ç¨‹
interneuronsçš„backwardæœ‰æŠ‘åˆ¶ä½œç”¨</p>

<h4 id="coherence-dependent-input">Coherence-Dependent Input</h4>

<p>çº¿æ€§ç¼–ç è¿åŠ¨æœå‘çš„ä¿¡æ¯ï¼Œcoherenceå¼ºåº¦å½±å“firing rateï¼Œä¸€ç³»åˆ—æ³Šæ¾è¿‡ç¨‹ï¼ŒåŒæ—¶è¿˜æœ‰noiseã€‚</p>

<p>æœ¬èº«ä¸¤ç§ä¿¡æ¯è¿˜æ˜¯æœ‰å·®å¼‚</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828104054275.png" alt="image-20230828104054275" /></p>

<h4 id="duality-of-this-model">Duality of this model</h4>

<p>ä¸åŒcoherenceçš„ç¥žç»å…ƒå“åº”</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828104432061.png" alt="image-20230828104432061" /></p>

<p>ä¸¤ä¸ªgroupä¼šç«žäº‰ï¼Œå½“æœ‰ä¸€ä¸ªgroupè¾¾åˆ°20%ï¼Œè¿›å…¥è¿™ä¸ªçª—å£ï¼Œå°±ä¼šç›´æŽ¥å‘æ”¾ä¸ŠåŽ»</p>

<p>Spontaneous symmetry breaking and stochastic decision making</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828104600840.png" alt="image-20230828104600840" /></p>

<h2 id="simulation-of-spiking-dm">Simulation of Spiking DM</h2>

<h3 id="a-cortical-microcircuit-model-1">A Cortical Microcircuit Model</h3>

<p>ç”¨ä¸¤ä¸ªcoherenceç”Ÿæˆå‡ºæ¥çš„åºåˆ—</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828110300576.png" alt="image-20230828110300576" /></p>

\[\begin{gathered}C_m\frac{dV(t)}{dt}=-g_L(V(t)-V_L)-I_{syn}(t)\\I_{syn}(t)=I_{\mathrm{ext},\mathrm{AMPA}}\left(t\right)+I_{\mathrm{rec},AMPA}(t)+I_{\mathrm{rec},NMDA}(t)+I_{\mathrm{rec},\mathrm{GABA}}(t)\end{gathered}\]

\[\begin{gathered}
I_{\mathrm{ext},\mathrm{AMPA}}\left(t\right)=g_{\mathrm{ext},\mathrm{AMPA}}\left(V(t)-V_{E}\right)s^{\mathrm{ext},\mathrm{AMPA}}\left(t\right) \\
I_{\mathrm{rec},\mathrm{AMP}\Lambda}\left(t\right)=g_{\mathrm{rec},\mathrm{AMP}\Lambda}\left(V(t)-V_{E}\right)\sum_{j=1}^{Ce}w_{j}s_{j}^{AMPA}(t) \\
I_{\mathrm{rec},\mathrm{NMDA}}\left(t\right)=\frac{g_{\mathrm{NMDA}}(V(t)-V_{E})}{\left(1+\left[\mathrm{Mg}^{2+}\right]\exp(-0.062V(t))/3.57\right)}\sum_{j=1}^{\mathrm{C_E}}w_{j}s_{j}^{\mathrm{NMDA}}\left(t\right) \\
I_\mathrm{rec,GABA}(t)=g_\mathrm{GABA}(V(t)-V_l)\sum_{j=1}^{C_1}s_j^\mathrm{GABA}(t) 
\end{gathered}\]

\[w_j=\left\{\begin{matrix}w_+&gt;1,\\w_-&lt;1,\\others=1.\end{matrix}\right.\]

<p>å››ç±»ç¥žç»å…ƒï¼Œä¸‰ç±»ä¿¡å·</p>

<p>å¤–ç•Œè¾“å…¥çš„ä¿¡å·ï¼Œrecurrentä¿¡å·ï¼Œå…¶å®ƒç¥žç»å…ƒçš„ä¿¡å·ï¼ŒæŠ‘åˆ¶ç¥žç»å…ƒçš„ä¿¡å·
éƒ½æœ‰AMPAå’ŒNMDAè¿™ä¸¤ä¸ªsynapseï¼Œè¿˜æœ‰æŠ‘åˆ¶çš„GABA</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AMPA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">Projection</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">conn</span><span class="p">,</span> <span class="n">delay</span><span class="p">,</span> <span class="n">g_max</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">E</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">conn</span> <span class="o">==</span> <span class="s">'all2all'</span><span class="p">:</span>
      <span class="n">comm</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">AllToAll</span><span class="p">(</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">g_max</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">conn</span> <span class="o">==</span> <span class="s">'one2one'</span><span class="p">:</span>
      <span class="n">comm</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">OneToOne</span><span class="p">(</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">g_max</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="nb">ValueError</span>
    <span class="n">syn</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">Expon</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">COBA</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">E</span><span class="o">=</span><span class="n">E</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPostMg2</span><span class="p">(</span>
      <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">,</span> <span class="n">comm</span><span class="o">=</span><span class="n">comm</span><span class="p">,</span>
      <span class="n">syn</span><span class="o">=</span><span class="n">syn</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">,</span> <span class="n">post</span><span class="o">=</span><span class="n">post</span>
    <span class="p">)</span>


<span class="k">class</span> <span class="nc">NMDA</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">Projection</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre</span><span class="p">,</span> <span class="n">post</span><span class="p">,</span> <span class="n">conn</span><span class="p">,</span> <span class="n">delay</span><span class="p">,</span> <span class="n">g_max</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">conn</span> <span class="o">==</span> <span class="s">'all2all'</span><span class="p">:</span>
      <span class="n">comm</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">AllToAll</span><span class="p">(</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">post</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">g_max</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">conn</span> <span class="o">==</span> <span class="s">'one2one'</span><span class="p">:</span>
      <span class="n">comm</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dnn</span><span class="p">.</span><span class="n">OneToOne</span><span class="p">(</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">g_max</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="nb">ValueError</span>
    <span class="n">syn</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">NMDA</span><span class="p">.</span><span class="n">desc</span><span class="p">(</span><span class="n">pre</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">tau_decay</span><span class="o">=</span><span class="mf">100.</span><span class="p">,</span> <span class="n">tau_rise</span><span class="o">=</span><span class="mf">2.</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">MgBlock</span><span class="p">(</span><span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">cc_Mg</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">ProjAlignPreMg2</span><span class="p">(</span>
      <span class="n">pre</span><span class="o">=</span><span class="n">pre</span><span class="p">,</span> <span class="n">delay</span><span class="o">=</span><span class="n">delay</span><span class="p">,</span> <span class="n">syn</span><span class="o">=</span><span class="n">syn</span><span class="p">,</span>
      <span class="n">comm</span><span class="o">=</span><span class="n">comm</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">,</span> <span class="n">post</span><span class="o">=</span><span class="n">post</span>
    <span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DecisionMakingNet</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">DynSysGroup</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="mf">0.15</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    <span class="c1"># ç½‘ç»œä¸­å„ç»„ç¥žç»å…ƒçš„æ•°ç›®
</span>    <span class="n">num_exc</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1600</span> <span class="o">*</span> <span class="n">scale</span><span class="p">)</span>
    <span class="n">num_I</span><span class="p">,</span> <span class="n">num_A</span><span class="p">,</span> <span class="n">num_B</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">400</span> <span class="o">*</span> <span class="n">scale</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">f</span> <span class="o">*</span> <span class="n">num_exc</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">f</span> <span class="o">*</span> <span class="n">num_exc</span><span class="p">)</span>
    <span class="n">num_N</span> <span class="o">=</span> <span class="n">num_exc</span> <span class="o">-</span> <span class="n">num_A</span> <span class="o">-</span> <span class="n">num_B</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">num_A</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_B</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_N</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_I</span> <span class="o">=</span> <span class="n">num_A</span><span class="p">,</span> <span class="n">num_B</span><span class="p">,</span> <span class="n">num_N</span><span class="p">,</span> <span class="n">num_I</span>

    <span class="n">poisson_freq</span> <span class="o">=</span> <span class="mf">2400.</span>  <span class="c1"># Hz
</span>    <span class="n">w_pos</span> <span class="o">=</span> <span class="mf">1.7</span>
    <span class="n">w_neg</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">f</span> <span class="o">*</span> <span class="p">(</span><span class="n">w_pos</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">f</span><span class="p">)</span>
    <span class="n">g_ext2E_AMPA</span> <span class="o">=</span> <span class="mf">2.1</span>  <span class="c1"># nS
</span>    <span class="n">g_ext2I_AMPA</span> <span class="o">=</span> <span class="mf">1.62</span>  <span class="c1"># nS
</span>    <span class="n">g_E2E_AMPA</span> <span class="o">=</span> <span class="mf">0.05</span> <span class="o">/</span> <span class="n">scale</span>  <span class="c1"># nS
</span>    <span class="n">g_E2I_AMPA</span> <span class="o">=</span> <span class="mf">0.04</span> <span class="o">/</span> <span class="n">scale</span>  <span class="c1"># nS
</span>    <span class="n">g_E2E_NMDA</span> <span class="o">=</span> <span class="mf">0.165</span> <span class="o">/</span> <span class="n">scale</span>  <span class="c1"># nS
</span>    <span class="n">g_E2I_NMDA</span> <span class="o">=</span> <span class="mf">0.13</span> <span class="o">/</span> <span class="n">scale</span>  <span class="c1"># nS
</span>    <span class="n">g_I2E_GABAa</span> <span class="o">=</span> <span class="mf">1.3</span> <span class="o">/</span> <span class="n">scale</span>  <span class="c1"># nS
</span>    <span class="n">g_I2I_GABAa</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">scale</span>  <span class="c1"># nS
</span>
    <span class="n">neu_par</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">V_rest</span><span class="o">=-</span><span class="mf">70.</span><span class="p">,</span> <span class="n">V_reset</span><span class="o">=-</span><span class="mf">55.</span><span class="p">,</span> <span class="n">V_th</span><span class="o">=-</span><span class="mf">50.</span><span class="p">,</span> <span class="n">V_initializer</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">OneInit</span><span class="p">(</span><span class="o">-</span><span class="mf">70.</span><span class="p">))</span>

    <span class="c1"># E neurons/pyramid neurons
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">LifRef</span><span class="p">(</span><span class="n">num_A</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">R</span><span class="o">=</span><span class="mf">0.04</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="o">**</span><span class="n">neu_par</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">LifRef</span><span class="p">(</span><span class="n">num_B</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">R</span><span class="o">=</span><span class="mf">0.04</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="o">**</span><span class="n">neu_par</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">LifRef</span><span class="p">(</span><span class="n">num_N</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">20.</span><span class="p">,</span> <span class="n">R</span><span class="o">=</span><span class="mf">0.04</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="o">**</span><span class="n">neu_par</span><span class="p">)</span>

    <span class="c1"># I neurons/interneurons
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">I</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">LifRef</span><span class="p">(</span><span class="n">num_I</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">R</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">tau_ref</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="o">**</span><span class="n">neu_par</span><span class="p">)</span>

    <span class="c1"># poisson stimulus  # 'freqs' as bm.Variable
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">IA</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">PoissonGroup</span><span class="p">(</span><span class="n">num_A</span><span class="p">,</span> <span class="n">freqs</span><span class="o">=</span><span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">IB</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">PoissonGroup</span><span class="p">(</span><span class="n">num_B</span><span class="p">,</span> <span class="n">freqs</span><span class="o">=</span><span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>

    <span class="c1"># noise neurons
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">noise_B</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">PoissonGroup</span><span class="p">(</span><span class="n">num_B</span><span class="p">,</span> <span class="n">freqs</span><span class="o">=</span><span class="n">poisson_freq</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">noise_A</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">PoissonGroup</span><span class="p">(</span><span class="n">num_A</span><span class="p">,</span> <span class="n">freqs</span><span class="o">=</span><span class="n">poisson_freq</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">noise_N</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">PoissonGroup</span><span class="p">(</span><span class="n">num_N</span><span class="p">,</span> <span class="n">freqs</span><span class="o">=</span><span class="n">poisson_freq</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">noise_I</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">PoissonGroup</span><span class="p">(</span><span class="n">num_I</span><span class="p">,</span> <span class="n">freqs</span><span class="o">=</span><span class="n">poisson_freq</span><span class="p">)</span>

    <span class="c1"># define external inputs
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">IA2A</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">IA</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="s">'one2one'</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">g_ext2E_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">IB2B</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">IB</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="s">'one2one'</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">g_ext2E_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>

    <span class="c1"># define AMPA projections from N
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">N2B_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_AMPA</span> <span class="o">*</span> <span class="n">w_neg</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">N2A_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_AMPA</span> <span class="o">*</span> <span class="n">w_neg</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">N2N_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">N2I_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2I_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>

    <span class="c1"># define NMDA projections from N
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">N2B_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_NMDA</span> <span class="o">*</span> <span class="n">w_neg</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">N2A_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_NMDA</span> <span class="o">*</span> <span class="n">w_neg</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">N2N_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_NMDA</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">N2I_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2I_NMDA</span><span class="p">)</span>

    <span class="c1"># define AMPA projections from B
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">B2B_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_AMPA</span> <span class="o">*</span> <span class="n">w_pos</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">B2A_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_AMPA</span> <span class="o">*</span> <span class="n">w_neg</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">B2N_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">B2I_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2I_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>

    <span class="c1"># define NMDA projections from B
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">B2B_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_NMDA</span> <span class="o">*</span> <span class="n">w_pos</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">B2A_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_NMDA</span> <span class="o">*</span> <span class="n">w_neg</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">B2N_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_NMDA</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">B2I_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2I_NMDA</span><span class="p">)</span>

    <span class="c1"># define AMPA projections from A
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">A2B_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_AMPA</span> <span class="o">*</span> <span class="n">w_neg</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">A2A_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_AMPA</span> <span class="o">*</span> <span class="n">w_pos</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">A2N_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">A2I_AMPA</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2I_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>

    <span class="c1"># define NMDA projections from A
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">A2B_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_NMDA</span> <span class="o">*</span> <span class="n">w_neg</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">A2A_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_NMDA</span> <span class="o">*</span> <span class="n">w_pos</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">A2N_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2E_NMDA</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">A2I_NMDA</span> <span class="o">=</span> <span class="n">NMDA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_E2I_NMDA</span><span class="p">)</span>

    <span class="c1"># define I-&gt;E/I conn
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">I2B</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_I2E_GABAa</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">E</span><span class="o">=-</span><span class="mf">70.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">I2A</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_I2E_GABAa</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">E</span><span class="o">=-</span><span class="mf">70.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">I2N</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_I2E_GABAa</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">E</span><span class="o">=-</span><span class="mf">70.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">I2I</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="s">'all2all'</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">g_I2I_GABAa</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">E</span><span class="o">=-</span><span class="mf">70.</span><span class="p">)</span>

    <span class="c1"># define external projections
</span>    <span class="c1">#### TO DO!!!!
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">noise2B</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">noise_B</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">B</span><span class="p">,</span> <span class="s">'one2one'</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">g_ext2E_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">noise2A</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">noise_A</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">A</span><span class="p">,</span> <span class="s">'one2one'</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">g_ext2E_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">noise2N</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">noise_N</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">N</span><span class="p">,</span> <span class="s">'one2one'</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">g_ext2E_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">noise2I</span> <span class="o">=</span> <span class="n">AMPA</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">noise_I</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I</span><span class="p">,</span> <span class="s">'one2one'</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">g_ext2I_AMPA</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">E</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Tool</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre_stimulus_period</span><span class="o">=</span><span class="mf">100.</span><span class="p">,</span> <span class="n">stimulus_period</span><span class="o">=</span><span class="mf">1000.</span><span class="p">,</span> <span class="n">delay_period</span><span class="o">=</span><span class="mf">500.</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span> <span class="o">=</span> <span class="n">pre_stimulus_period</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">stimulus_period</span> <span class="o">=</span> <span class="n">stimulus_period</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">delay_period</span> <span class="o">=</span> <span class="n">delay_period</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">freq_variance</span> <span class="o">=</span> <span class="mf">10.</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">freq_interval</span> <span class="o">=</span> <span class="mf">50.</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">total_period</span> <span class="o">=</span> <span class="n">pre_stimulus_period</span> <span class="o">+</span> <span class="n">stimulus_period</span> <span class="o">+</span> <span class="n">delay_period</span>

  <span class="k">def</span> <span class="nf">generate_freqs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mean</span><span class="p">):</span>
    <span class="c1"># stimulus period
</span>    <span class="n">n_stim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">stimulus_period</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">freq_interval</span><span class="p">)</span>
    <span class="n">n_interval</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">freq_interval</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">())</span>
    <span class="n">freqs_stim</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">freq_variance</span><span class="p">,</span> <span class="p">(</span><span class="n">n_stim</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">freqs_stim</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">freqs_stim</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_interval</span><span class="p">)).</span><span class="n">flatten</span><span class="p">()</span>
    <span class="c1"># pre stimulus period
</span>    <span class="n">freqs_pre</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">()))</span>
    <span class="c1"># post stimulus period
</span>    <span class="n">freqs_delay</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">delay_period</span> <span class="o">/</span> <span class="n">bm</span><span class="p">.</span><span class="n">get_dt</span><span class="p">()))</span>
    <span class="n">all_freqs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">freqs_pre</span><span class="p">,</span> <span class="n">freqs_stim</span><span class="p">,</span> <span class="n">freqs_delay</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">bm</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">all_freqs</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">visualize_results</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mon</span><span class="p">,</span> <span class="n">IA_freqs</span><span class="p">,</span> <span class="n">IB_freqs</span><span class="p">,</span> <span class="n">t_start</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">gs</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">get_figure</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="p">[</span><span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">raster_plot</span><span class="p">(</span><span class="n">mon</span><span class="p">[</span><span class="s">'ts'</span><span class="p">],</span> <span class="n">mon</span><span class="p">[</span><span class="s">'A.spike'</span><span class="p">],</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">title</span><span class="p">:</span> <span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Group A"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">t_start</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">total_period</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">stimulus_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">delay_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">bp</span><span class="p">.</span><span class="n">visualize</span><span class="p">.</span><span class="n">raster_plot</span><span class="p">(</span><span class="n">mon</span><span class="p">[</span><span class="s">'ts'</span><span class="p">],</span> <span class="n">mon</span><span class="p">[</span><span class="s">'B.spike'</span><span class="p">],</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Group B"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">t_start</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">total_period</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">stimulus_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">delay_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">rateA</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">measure</span><span class="p">.</span><span class="n">firing_rate</span><span class="p">(</span><span class="n">mon</span><span class="p">[</span><span class="s">'A.spike'</span><span class="p">],</span> <span class="n">width</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>
    <span class="n">rateB</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">measure</span><span class="p">.</span><span class="n">firing_rate</span><span class="p">(</span><span class="n">mon</span><span class="p">[</span><span class="s">'B.spike'</span><span class="p">],</span> <span class="n">width</span><span class="o">=</span><span class="mf">10.</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mon</span><span class="p">[</span><span class="s">'ts'</span><span class="p">],</span> <span class="n">rateA</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Group A"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mon</span><span class="p">[</span><span class="s">'ts'</span><span class="p">],</span> <span class="n">rateB</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Group B"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Population activity [Hz]'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">t_start</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">total_period</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">stimulus_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">delay_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mon</span><span class="p">[</span><span class="s">'ts'</span><span class="p">],</span> <span class="n">IA_freqs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"group A"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mon</span><span class="p">[</span><span class="s">'ts'</span><span class="p">],</span> <span class="n">IB_freqs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"group B"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Input activity [Hz]"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">t_start</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">total_period</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">stimulus_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">pre_stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">stimulus_period</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">delay_period</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"Time [ms]"</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tool</span> <span class="o">=</span> <span class="n">Tool</span><span class="p">()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">DecisionMakingNet</span><span class="p">()</span>

<span class="n">mu0</span> <span class="o">=</span> <span class="mf">40.</span>
<span class="n">coherence</span> <span class="o">=</span> <span class="mf">25.6</span>
<span class="n">IA_freqs</span> <span class="o">=</span> <span class="n">tool</span><span class="p">.</span><span class="n">generate_freqs</span><span class="p">(</span><span class="n">mu0</span> <span class="o">+</span> <span class="n">mu0</span> <span class="o">/</span> <span class="mf">100.</span> <span class="o">*</span> <span class="n">coherence</span><span class="p">)</span>
<span class="n">IB_freqs</span> <span class="o">=</span> <span class="n">tool</span><span class="p">.</span><span class="n">generate_freqs</span><span class="p">(</span><span class="n">mu0</span> <span class="o">-</span> <span class="n">mu0</span> <span class="o">/</span> <span class="mf">100.</span> <span class="o">*</span> <span class="n">coherence</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">give_input</span><span class="p">():</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">share</span><span class="p">[</span><span class="s">'i'</span><span class="p">]</span>
    <span class="n">net</span><span class="p">.</span><span class="n">IA</span><span class="p">.</span><span class="n">freqs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">IA_freqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">net</span><span class="p">.</span><span class="n">IB</span><span class="p">.</span><span class="n">freqs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">IB_freqs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">give_input</span><span class="p">,</span> <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'A.spike'</span><span class="p">,</span> <span class="s">'B.spike'</span><span class="p">])</span>
<span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">tool</span><span class="p">.</span><span class="n">total_period</span><span class="p">)</span>
<span class="n">tool</span><span class="p">.</span><span class="n">visualize_results</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">,</span> <span class="n">IA_freqs</span><span class="p">,</span> <span class="n">IB_freqs</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="results">Results</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828112245950.png" alt="image-20230828112245950" /></p>

<h4 id="stochastic-decision-making">Stochastic Decision Making</h4>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828112253619.png" alt="image-20230828112253619" /></p>

<h2 id="a-rate-network-of-dm">A Rate Network of DM</h2>

<h3 id="reduced-model">Reduced Model</h3>

<p>åŒ–ç®€åˆ°åªæœ‰ä¸¤ç¾¤ç¥žç»å…ƒï¼ŒåªæŽ¥å—å¤–ç•Œè¾“å…¥ä¿¡å·ï¼Œäº’ç›¸å½±å“å¯¹æ–¹</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828112326267.png" alt="image-20230828112326267" /></p>

<p>Synaptic variables</p>

\[\begin{gathered}
\frac{dS_{1}}{dt} =F(x_1)\gamma(1-S_1)-S_1/\tau_s \\
\frac{dS_2}{dt} =F(x_2)\gamma(1-S_2)-S_2/\tau_s 
\end{gathered}\]

<p>Input current to each population</p>

<p>\(\begin{gathered}
x_{1} =J_{E}S_{1}+J_{I}S_{2}+I_{0}+I_{noise1}+J_{\text{ext }\mu_{1}} \\
x_{2} =J_{E}S_{2}+J_{I}S_{1}+I_{0}+I_{noise2}+J_{\mathrm{ext}}\mu_{2} 
\end{gathered}\)
Background input</p>

<p>\(I_0+I_{noise}\\
\begin{gathered}
dI_{noise1} =-I_{noise1}\frac{dt}{\tau_{0}}+\sigma dW \\
dI_{noise2} =-I_{noise2}\frac{dt}{\tau_{0}}+\sigma dW 
\end{gathered}\)
Firing rates</p>

<p>\(r_i=F(x_i)=\frac{ax_i-b}{1-\exp(-d(ax_i-b))}\)
Coherence-dependent inputs</p>

\[\begin{array}{l}\mu_1=\mu_0\big(1+c'/100\big)\\\mu_2=\mu_0\big(1-c'/100\big)\end{array}\]

\[\begin{aligned}&amp;\gamma,a,b,d,J_E,J_I,J_{\mathrm{ext}},I_0,\mu_0,\tau_{\mathrm{AMPA}},\sigma_{\mathrm{noise}}\\&amp;\text{are fixed parameters.}\end{aligned}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DecisionMakingRateModel</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">NeuGroup</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">coherence</span><span class="p">,</span> <span class="n">JE</span><span class="o">=</span><span class="mf">0.2609</span><span class="p">,</span> <span class="n">JI</span><span class="o">=</span><span class="mf">0.0497</span><span class="p">,</span> <span class="n">Jext</span><span class="o">=</span><span class="mf">5.2e-4</span><span class="p">,</span> <span class="n">I0</span><span class="o">=</span><span class="mf">0.3255</span><span class="p">,</span>
                 <span class="n">gamma</span><span class="o">=</span><span class="mf">6.41e-4</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">100.</span><span class="p">,</span> <span class="n">tau_n</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">sigma_n</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mf">270.</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="mf">108.</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mf">0.154</span><span class="p">,</span>
                 <span class="n">noise_freq</span><span class="o">=</span><span class="mf">2400.</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'exp_auto'</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DecisionMakingRateModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="c1"># åˆå§‹åŒ–å‚æ•°
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">coherence</span> <span class="o">=</span> <span class="n">coherence</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">JE</span> <span class="o">=</span> <span class="n">JE</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">JI</span> <span class="o">=</span> <span class="n">JI</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">Jext</span> <span class="o">=</span> <span class="n">Jext</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">I0</span> <span class="o">=</span> <span class="n">I0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tau_n</span> <span class="o">=</span> <span class="n">tau_n</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">sigma_n</span> <span class="o">=</span> <span class="n">sigma_n</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">a</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">d</span>
        
        <span class="c1"># åˆå§‹åŒ–å˜é‡
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">s1</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.15</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">s2</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.15</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">r1</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">r2</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mu0</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">I1_noise</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">I2_noise</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">))</span>
        
        <span class="c1"># å™ªå£°è¾“å…¥çš„ç¥žç»å…ƒ
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">noise1</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">PoissonGroup</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">freqs</span><span class="o">=</span><span class="n">noise_freq</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">noise2</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">dyn</span><span class="p">.</span><span class="n">PoissonGroup</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">freqs</span><span class="o">=</span><span class="n">noise_freq</span><span class="p">)</span>
        
        <span class="c1"># å®šä¹‰ç§¯åˆ†å‡½æ•°
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">integral</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">odeint</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">derivative</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">)</span>
        
    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">bp</span><span class="p">.</span><span class="n">JointEq</span><span class="p">([</span><span class="bp">self</span><span class="p">.</span><span class="n">ds1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">ds2</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dI1noise</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">dI2noise</span><span class="p">])</span>
        
    <span class="k">def</span> <span class="nf">ds1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">mu0</span><span class="p">):</span>
        <span class="n">I1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">Jext</span> <span class="o">*</span> <span class="n">mu0</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">coherence</span> <span class="o">/</span> <span class="mf">100.</span><span class="p">)</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">JE</span> <span class="o">*</span> <span class="n">s1</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">JI</span> <span class="o">*</span> <span class="n">s2</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">I0</span> <span class="o">+</span> <span class="n">I1</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">I1_noise</span>
        <span class="n">r1</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">d</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span><span class="p">)))</span>
        <span class="k">return</span> <span class="o">-</span> <span class="n">s1</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">s1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">r1</span>
    
    <span class="k">def</span> <span class="nf">ds2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">mu0</span><span class="p">):</span>
        <span class="n">I2</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">Jext</span><span class="o">*</span><span class="n">mu0</span><span class="o">*</span><span class="p">(</span><span class="mf">1.</span><span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">coherence</span> <span class="o">/</span> <span class="mf">100.</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">JE</span> <span class="o">*</span> <span class="n">s2</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">JI</span> <span class="o">*</span> <span class="n">s1</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">I0</span> <span class="o">+</span> <span class="n">I2</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">I2_noise</span>
        <span class="n">r2</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">d</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span><span class="p">)))</span> 
        <span class="k">return</span> <span class="o">-</span> <span class="n">s2</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">s2</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">r2</span>

    <span class="k">def</span> <span class="nf">dI1noise</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">I1_noise</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise1</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="o">-</span> <span class="n">I1_noise</span> <span class="o">+</span> <span class="n">noise1</span><span class="p">.</span><span class="n">spike</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">tau_n</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigma_n</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigma_n</span><span class="p">))</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau_n</span>
    
    <span class="k">def</span> <span class="nf">dI2noise</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">I2_noise</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise2</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="o">-</span> <span class="n">I2_noise</span> <span class="o">+</span> <span class="n">noise2</span><span class="p">.</span><span class="n">spike</span> <span class="o">*</span> <span class="n">bm</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">tau_n</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigma_n</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigma_n</span><span class="p">))</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">tau_n</span>
    
    
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tdi</span><span class="p">):</span>
        <span class="c1"># æ›´æ–°å™ªå£°ç¥žç»å…ƒä»¥äº§ç”Ÿæ–°çš„éšæœºå‘æ”¾ self.noise1.update(tdi) self.noise2.update(tdi)
</span>        <span class="c1"># æ›´æ–°s1ã€s2ã€I1_noiseã€I2_noise
</span>        <span class="n">integral</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">integral</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">s1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">s2</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I1_noise</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I2_noise</span><span class="p">,</span> <span class="n">tdi</span><span class="p">.</span><span class="n">t</span><span class="p">,</span> <span class="n">mu0</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">mu0</span><span class="p">,</span>
                             <span class="n">noise1</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">noise1</span><span class="p">,</span> <span class="n">noise2</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">noise2</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="n">tdi</span><span class="p">.</span><span class="n">dt</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">s1</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">s2</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I1_noise</span><span class="p">.</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">I2_noise</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">integral</span>
        
        <span class="c1"># ç”¨æ›´æ–°åŽçš„s1ã€s2è®¡ç®—r1ã€r2
</span>        <span class="n">I1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">Jext</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">mu0</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">coherence</span> <span class="o">/</span> <span class="mf">100.</span><span class="p">)</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">JE</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">s1</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">JI</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">s2</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">I0</span> <span class="o">+</span> <span class="n">I1</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">I1_noise</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">r1</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">d</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span><span class="p">)))</span>
        <span class="n">I2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">Jext</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">mu0</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">coherence</span> <span class="o">/</span> <span class="mf">100.</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">JE</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">s2</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">JI</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">s1</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">I0</span> <span class="o">+</span> <span class="n">I2</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">I2_noise</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">r2</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">d</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">a</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">b</span><span class="p">)))</span>
        
        <span class="c1"># é‡ç½®å¤–éƒ¨è¾“å…¥ 
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">mu0</span><span class="p">[:]</span> <span class="o">=</span> <span class="mf">0.</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># å®šä¹‰å„ä¸ªé˜¶æ®µçš„æ—¶é•¿
</span><span class="n">pre_stimulus_period</span><span class="p">,</span> <span class="n">stimulus_period</span><span class="p">,</span> <span class="n">delay_period</span> <span class="o">=</span> <span class="mf">100.</span><span class="p">,</span> <span class="mf">2000.</span><span class="p">,</span> <span class="mf">500.</span>

<span class="c1"># ç”Ÿæˆæ¨¡åž‹
</span><span class="n">dmnet</span> <span class="o">=</span> <span class="n">DecisionMakingRateModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">coherence</span><span class="o">=</span><span class="mf">25.6</span><span class="p">,</span> <span class="n">noise_freq</span><span class="o">=</span><span class="mf">2400.</span><span class="p">)</span>

<span class="c1"># å®šä¹‰ç”µæµéšæ—¶é—´çš„å˜åŒ–
</span><span class="n">inputs</span><span class="p">,</span> <span class="n">total_period</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">constant_input</span><span class="p">([(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">pre_stimulus_period</span><span class="p">),</span>
                                                 <span class="p">(</span><span class="mf">20.</span><span class="p">,</span> <span class="n">stimulus_period</span><span class="p">),</span>
                                                 <span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">delay_period</span><span class="p">)])</span>
<span class="c1"># è¿è¡Œæ•°å€¼æ¨¡æ‹Ÿ
</span><span class="n">runner</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">DSRunner</span><span class="p">(</span><span class="n">dmnet</span><span class="p">,</span>
                     <span class="n">monitors</span><span class="o">=</span><span class="p">[</span><span class="s">'s1'</span><span class="p">,</span> <span class="s">'s2'</span><span class="p">,</span> <span class="s">'r1'</span><span class="p">,</span> <span class="s">'r2'</span><span class="p">],</span>
                     <span class="n">inputs</span><span class="o">=</span><span class="p">(</span><span class="s">'mu0'</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="s">'iter'</span><span class="p">))</span>
<span class="n">runner</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">total_period</span><span class="p">)</span>

<span class="c1"># å¯è§†åŒ–
</span><span class="n">fig</span><span class="p">,</span> <span class="n">gs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="s">'all'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">s1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'s1'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">s2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'s2'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">axvline</span><span class="p">(</span><span class="n">pre_stimulus_period</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sa">u</span><span class="s">'#444444'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">axvline</span><span class="p">(</span><span class="n">pre_stimulus_period</span> <span class="o">+</span> <span class="n">stimulus_period</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sa">u</span><span class="s">'#444444'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'gating variable $s$'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">legend</span><span class="p">()</span>

<span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">r1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'r1'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">plot</span><span class="p">(</span><span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">ts</span><span class="p">,</span> <span class="n">runner</span><span class="p">.</span><span class="n">mon</span><span class="p">.</span><span class="n">r2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'r2'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">axvline</span><span class="p">(</span><span class="n">pre_stimulus_period</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sa">u</span><span class="s">'#444444'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">axvline</span><span class="p">(</span><span class="n">pre_stimulus_period</span> <span class="o">+</span> <span class="n">stimulus_period</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'dashed'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sa">u</span><span class="s">'#444444'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'t (ms)'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'firing rate $r$'</span><span class="p">)</span>
<span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="p">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="results-1">Results</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828112555018.png" alt="image-20230828112555018" /></p>

<h2 id="phase-plane-analysis-1">Phase Plane Analysis</h2>

<p>å› ä¸ºåªæœ‰ä¸¤ä¸ªvariable</p>

<h3 id="model-implementation">Model implementation</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">bp</span><span class="p">.</span><span class="n">odeint</span>
<span class="k">def</span> <span class="nf">int_s1</span><span class="p">(</span><span class="n">s1</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">coh</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mf">20.</span><span class="p">):</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">JE</span> <span class="o">*</span> <span class="n">s1</span> <span class="o">+</span> <span class="n">JI</span> <span class="o">*</span> <span class="n">s2</span> <span class="o">+</span> <span class="n">Ib</span> <span class="o">+</span> <span class="n">JAext</span> <span class="o">*</span> <span class="n">mu</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">coh</span><span class="o">/</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">r1</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">d</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">-</span> <span class="n">b</span><span class="p">)))</span>
    <span class="k">return</span> <span class="o">-</span> <span class="n">s1</span> <span class="o">/</span> <span class="n">tau</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">s1</span><span class="p">)</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">r1</span>

<span class="o">@</span><span class="n">bp</span><span class="p">.</span><span class="n">odeint</span>
<span class="k">def</span> <span class="nf">int_s2</span><span class="p">(</span><span class="n">s2</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">coh</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mf">20.</span><span class="p">):</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">JE</span> <span class="o">*</span> <span class="n">s2</span> <span class="o">+</span> <span class="n">JI</span> <span class="o">*</span> <span class="n">s1</span> <span class="o">+</span> <span class="n">Ib</span> <span class="o">+</span> <span class="n">JAext</span> <span class="o">*</span> <span class="n">mu</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">coh</span><span class="o">/</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">r2</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">bm</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">d</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">x2</span> <span class="o">-</span> <span class="n">b</span><span class="p">)))</span>
    <span class="k">return</span> <span class="o">-</span> <span class="n">s2</span> <span class="o">/</span> <span class="n">tau</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">s2</span><span class="p">)</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">r2</span>
</code></pre></div></div>

<h3 id="without--with-input">Without / with input</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828112709355.png" alt="image-20230828112709355" /></p>

<p>åªå—æ‰°åŠ¨å½±å“ï¼Œæœ‰inputåŽä¸­é—´å˜å¾—ä¸ç¨³å®šï¼Œä½†å¦‚æžœå·²ç»é€‰æ‹©ï¼Œç½‘ç»œä»ç»´æŒä¹‹å‰é€‰æ‹©çš„ç»“æžœ</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828112811394.png" alt="image-20230828112811394" /></p>

<h3 id="coherence">Coherence</h3>

<p>ç¨³å®šç‚¹å¯¹ç½‘ç»œçš„æ‹‰ä¼¸æ›´å¼º</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828113031946.png" alt="image-20230828113031946" /></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828113009219.png" alt="image-20230828113009219" /></p>

<h1 id="reservoir-computing">Reservoir Computing</h1>

<p>å¼•å…¥è®­ç»ƒ</p>

<p>å€¾å‘äºŽä½¿ç”¨RNN</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828140305956.png" alt="image-20230828140305956" /></p>

<p>Connecting different units</p>

\[\begin{aligned}
&amp;\textsf{Input to unit i from unit j:} \\
&amp;&amp;&amp;I_{j\rightarrow i}=J_{ij}r_{j}(t) \\
&amp;\textsf{Total input to unit i:} \\
&amp;&amp;&amp;I_{i}^{(tot)}=\sum_{j=1}^{N}J_{ij}r_{j}(t)+I_{i}^{(ext)} 
\end{aligned}\]

\[\textsf{Activation of unit i:}
\\
\tau\frac{dx_{i}}{dt}=-x_{i}+\sum_{j=1}^{N}J_{ij}\frac{\phi(x_{j})}{1}+I_{i}^{(ext)}(t)\]

<p>è®­ç»ƒèŒƒå¼</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828140707998.png" alt="image-20230828140707998" /></p>

<h2 id="echo-state-machine">Echo state machine</h2>

<h3 id="echo-state-machine-1">Echo state machine</h3>

<p>ç±»ä¼¼äººå·¥ç¥žç»ç½‘ç»œRNNï¼Œå¯ä»¥å¤„ç†temporalä¿¡æ¯</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828140937455.png" alt="image-20230828140937455" /></p>

\[\begin{aligned}
&amp;\mathbf{x}(n+1) =f(\mathbf{W}^{\mathrm{in}}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n)+\mathbf{W}^{\mathrm{back}}\mathbf{y}(n))  \\
&amp;\mathbf{y}(n+1) =\mathbf{W}^{\mathrm{out}}(\mathbf{u}(n+1),\mathbf{x}(n+1),\mathbf{y}(n)) 
\end{aligned}\]

<p>For an RNN, the state of its internal neurons reflects the historical information of the external inputs.</p>

<p>åæ˜ çš„echoçš„åŽ†å²ä¿¡æ¯ï¼Œå”¯ä¸€ä¾èµ–åŽ†å²ä¿¡æ¯</p>

<p>Assuming that the updates of the network are discrete, the external input at the ð‘›th moment is u(ð‘›) and the neuron state is x(ð‘›), then x(ð‘›) should be determined by u(ð‘›), u(ð‘› - 1), â€¦ uniquely determined. At this point, x(ð‘›) can be regarded as an â€œechoâ€ of the historical input signals.</p>

<p>ä¸éœ€è¦è®­ç»ƒconnection</p>

<h3 id="echo-state-machine-with-leaky-integrator">Echo state machine with leaky integrator</h3>

<p>æœ‰ä¸€ä¸ªleakyé¡¹ï¼Œå¼•å…¥decay</p>

<p>###</p>

\[\begin{aligned}\hat{h}(n)=\tanh(W^{in}x(n)+W^{rec}h(n-1)+W^{fb}y(n-1)+b^{rec})\\h(n)=(1-\alpha)x(n-1)+\alpha\hat{h}(n)\end{aligned}\]

<p>where $h(n)$ is a vector of reservoir neuron activations, $W^{in}$ and $W^{rec}$ are the input and recurrent weight matrices respectively, and $\alpha\in(0,1]$ is the leaking rate. The model is also sometimes used without the leaky integration, which is a special case of $\alpha=1$</p>

<p>The linear readout layer is defined as</p>

\[y(n)=W^{out}h(n)+b^{out}\]

<p>where $y(n)$ is network output, $W^{out}$ the output weight matrix, and $b^out$ is the output bias</p>

<h2 id="constraints-of-echo-state-machine">Constraints of echo state machine</h2>

<h3 id="echo-state-property">Echo state property</h3>

<h4 id="theorem-1">Theorem 1</h4>

<p>For the echo state network defined above, the network will be echoey as long as the maximum singular value  $\sigma_{max}&lt;1$ of the recurrent connectivity matrix W .</p>

<blockquote>
  <p>Provement:
\(\begin{aligned}
d(\mathbf{x}(n+1),\mathbf{x}^{\prime}(n+1))&amp; =d(T(\mathbf{x}(n),\mathbf{u}(n+1)),T(\mathbf{x}'(n),\mathbf{u}(n+1)))  \\
&amp;=d(f(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n)),f(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}'(n))) \\
&amp;\leq d(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n),\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}^{\prime}(n)) \\
&amp;=d(\mathbf{W}\mathbf{x}(n),\mathbf{W}\mathbf{x}'(n)) \\
&amp;=||\mathbf{W}(\mathbf{x}(n)-\mathbf{x}^{\prime}(n))|| \\
&amp;\leq\sigma_{\max}(\mathbf{W})d(\mathbf{x}(n),\mathbf{x}'(n))
\end{aligned}\)</p>
</blockquote>

<h4 id="theorem-2">Theorem 2</h4>

<table>
  <tbody>
    <tr>
      <td>For the echo state network defined above, as long as the spectral radius $</td>
      <td>\lambda_{max}</td>
      <td>$ of the recurrent connection matrix W &gt; 1, then the network must not be echogenic. The spectral radius of the matrix is the absolute value of the largest eigenvalue $\lambda_{max}$.</td>
    </tr>
  </tbody>
</table>

<h4 id="how-to-initialize">How to initialize</h4>

<p>Using these two theorems, how should we initialize W so that the network has an echo property?
If we scale W, i.e., multiply it by a scaling factor $\alpha$, then $\sigma_{max}&lt;1$ and $\lambda_{max}$ will also be scaled $\alpha$.</p>

\[\text{For any square matrix, we have}\sigma_{max}\geq|\lambda_{max}|.\\
\text{Therefore we set}\alpha_{min}=1/\sigma_{max}(W),\alpha_{max}=1/|\lambda_{max}|(W).\mathrm{Then},
\\
\begin{array}{ll}\bullet&amp;\text{if}\alpha&lt;\alpha_{min}\text{,the network must have the echo state.}\\\bullet&amp;\text{if}\alpha&gt;\alpha_{max}\text{,the network will not have the echo state.}\\\bullet&amp;\text{if}\alpha_{min}\le\alpha\le\alpha_{max}\text{,the network may have the echo state.}\end{array}\]

<p><strong>$\alpha$è®¾çš„ç•¥å°äºŽ1</strong></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828142516052.png" alt="image-20230828142516052" /></p>

<h3 id="global-parameters-of-reservoir">Global parameters of reservoir</h3>

<p>è¿™äº›è¶…å‚ä¼šå½±å“reservoir networkçš„æ€§èƒ½ï¼Œéœ€è¦æ‰‹åŠ¨è°ƒå‚ï¼Œå¾ˆéš¾è‡ªåŠ¨åŽ»è°ƒæ•´</p>

<ul>
  <li>The size $N_x$
    <ul>
      <li>General wisdom: the bigger the reservoir, the better the obtainable performance</li>
      <li>Select global parameters with smaller reservoirs, then scale to bigger ones.</li>
    </ul>
  </li>
  <li>Sparsity</li>
  <li>Distribution of nonzero elements:
    <ul>
      <li>Normal distribution</li>
      <li>Uniform distribution</li>
      <li>The width of the distributions does not matter</li>
    </ul>
  </li>
  <li>spectral radius of $W$
    <ul>
      <li>scales the width of the distribution of its nonzero elements</li>
      <li>determines how fast the influence of an input dies out in a reservoir with time, and how stable the reservoir activations are</li>
      <li>The spectral radius should be larger in tasks requiring longer memory of the input</li>
    </ul>
  </li>
  <li>Scaling(-s) to $W^{in}$:
    <ul>
      <li>For uniform distributed $W^{in}$, $\alpha$ in the range of the interval $[-a;a]$.</li>
      <li>For normal distributed $W^{in}$, one may take the standard deviation as a scaling measure.</li>
    </ul>
  </li>
</ul>

<p>The leaking rate $\alpha$</p>

<h2 id="training-of-echo-state-machine">Training of echo state machine</h2>

<h3 id="offline-learning">Offline learning</h3>

<p>The advantage of the echo state network is that it does not train recurrent connections within the reservoir, but only the readout layer from the reservoir to the output.</p>

<p>çº¿æ€§å±‚çš„ä¼˜åŒ–æ–¹æ³•æ˜¯ç®€å•çš„</p>

<p><strong>Ridge regression</strong></p>

\[\begin{aligned}\epsilon_{\mathrm{train}}(n)&amp;=\mathbf{y}(n)-\mathbf{\hat{y}}(n)
\\&amp;=\mathbf{y}(n)-\mathbf{W}^{\mathrm{out}}\mathbf{x}(n)
\\&amp;L_{\mathrm{ridge}}=\frac{1}{N}\sum_{i=1}^{N}\epsilon_{\mathrm{train}}^{2}(i)+\alpha||\mathbf{W^{out}}||^{2}
\\\\W^{out}&amp;=Y^{target}X^T(XX^T+\beta I)^{-1}\end{aligned}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trainer</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">OfflineTrainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">fit_method</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">algorithms</span><span class="p">.</span><span class="n">RidgeRegression</span><span class="p">(</span><span class="mf">1e-7</span><span class="p">),</span> <span class="n">dt</span><span class="o">=</span><span class="n">dt</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="online-learning">Online learning</h3>

<p>æ¥ä¸€ä¸ªsampleï¼Œè¿›è¡Œä¸€æ¬¡trainingï¼Œå¯¹è®­ç»ƒèµ„æºå¯ä»¥é¿å…ç“¶é¢ˆ</p>

<p>The training data is passed to the trainer in a certain sequence (e.g., time series), and the trainer continuously learns based on the new incoming data.</p>

<p><strong>Recursive Least Squares (RLS) algorithm</strong></p>

\[E(\mathbf{y},\mathbf{y}^\mathrm{target},n)=\frac{1}{N_\mathrm{y}}\sum_{i=1}^{N_\mathrm{y}}\sum_{j=1}^{n}\lambda^{n-j}\left(y_i(j)-y_i^\mathrm{target}(j)\right)^2,\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trainer</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">OnlineTrainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">fit_method</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">algorithms</span><span class="p">.</span><span class="n">RLS</span><span class="p">(),</span> <span class="n">dt</span><span class="o">=</span><span class="n">dt</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="dataset">Dataset</h3>

<p>ç»™å®štime sequenceï¼Œå¯ä»¥è®©ç½‘ç»œåŽ»é¢„æµ‹regression</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828144309742.png" alt="image-20230828144309742" /></p>

<p>ç”¨åˆ°BrainPyé›†æˆçš„<code class="language-plaintext highlighter-rouge">Neuromorphic and Cognitive Datasets</code></p>

<h3 id="other-tasks">Other tasks</h3>

<p><code class="language-plaintext highlighter-rouge">MNIST dataset</code> or <code class="language-plaintext highlighter-rouge">Fashion MNIST</code></p>

<p>Two aspect:</p>

<ul>
  <li>Running time</li>
  <li>Memory Usage</li>
</ul>

<h2 id="echo-state-machine-programming">Echo state machine programming</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">brainpy</span> <span class="k">as</span> <span class="n">bp</span>
<span class="kn">import</span> <span class="nn">brainpy.math</span> <span class="k">as</span> <span class="n">bm</span>
<span class="kn">import</span> <span class="nn">brainpy_datasets</span> <span class="k">as</span> <span class="n">bd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># enable x64 computation
</span><span class="n">bm</span><span class="p">.</span><span class="n">set_environment</span><span class="p">(</span><span class="n">x64</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">bm</span><span class="p">.</span><span class="n">batching_mode</span><span class="p">)</span>
<span class="n">bm</span><span class="p">.</span><span class="n">set_platform</span><span class="p">(</span><span class="s">'cpu'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="dataset-1">Dataset</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_mackey_glass_series</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">x_series</span><span class="p">,</span> <span class="n">x_tau_series</span><span class="p">,</span> <span class="n">num_sample</span><span class="p">):</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

  <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">"Timeserie - </span><span class="si">{</span><span class="n">num_sample</span><span class="si">}</span><span class="s"> timesteps"</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ts</span><span class="p">[:</span><span class="n">num_sample</span><span class="p">],</span> <span class="n">x_series</span><span class="p">[:</span><span class="n">num_sample</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"lightgrey"</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">ts</span><span class="p">[:</span><span class="n">num_sample</span><span class="p">],</span> <span class="n">x_series</span><span class="p">[:</span><span class="n">num_sample</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">ts</span><span class="p">[:</span><span class="n">num_sample</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s">"viridis"</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"$t$"</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"$P(t)$"</span><span class="p">)</span>

  <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">margins</span><span class="p">(</span><span class="mf">0.05</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">"Phase diagram: $P(t) = f(P(t-</span><span class="se">\\</span><span class="s">tau))$"</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_tau_series</span><span class="p">[:</span> <span class="n">num_sample</span><span class="p">],</span> <span class="n">x_series</span><span class="p">[:</span> <span class="n">num_sample</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"lightgrey"</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_tau_series</span><span class="p">[:</span><span class="n">num_sample</span><span class="p">],</span> <span class="n">x_series</span><span class="p">[:</span> <span class="n">num_sample</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">ts</span><span class="p">[:</span><span class="n">num_sample</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s">"viridis"</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"$P(t-</span><span class="se">\\</span><span class="s">tau)$"</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"$P(t)$"</span><span class="p">)</span>
  <span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">colorbar</span><span class="p">()</span>
  <span class="n">cbar</span><span class="p">.</span><span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'$t$'</span><span class="p">)</span>

  <span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dt</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">mg_data</span> <span class="o">=</span> <span class="n">bd</span><span class="p">.</span><span class="n">chaos</span><span class="p">.</span><span class="n">MackeyGlassEq</span><span class="p">(</span><span class="mi">25000</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="n">dt</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">17</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">ts</span> <span class="o">=</span> <span class="n">mg_data</span><span class="p">.</span><span class="n">ts</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">mg_data</span><span class="p">.</span><span class="n">xs</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">mg_data</span><span class="p">.</span><span class="n">ys</span>

<span class="n">plot_mackey_glass_series</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">num_sample</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mi">1000</span> <span class="o">/</span> <span class="n">dt</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828151451523.png" alt="image-20230828151451523" /></p>

<h3 id="prediction-of-mackey-glass-timeseries">Prediction of Mackey-Glass timeseries</h3>

<h4 id="prepare-the-data">Prepare the data</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_data</span><span class="p">(</span><span class="n">t_warm</span><span class="p">,</span> <span class="n">t_forcast</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="n">sample_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">warmup</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">t_warm</span> <span class="o">/</span> <span class="n">dt</span><span class="p">)</span>  <span class="c1"># warmup the reservoir
</span>    <span class="n">forecast</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">t_forcast</span> <span class="o">/</span> <span class="n">dt</span><span class="p">)</span>  <span class="c1"># predict 10 ms ahead
</span>    <span class="n">train_length</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">t_train</span> <span class="o">/</span> <span class="n">dt</span><span class="p">)</span>

    <span class="n">X_warm</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[:</span><span class="n">warmup</span><span class="p">:</span><span class="n">sample_rate</span><span class="p">]</span>
    <span class="n">X_warm</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">X_warm</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">X_train</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[</span><span class="n">warmup</span><span class="p">:</span> <span class="n">warmup</span><span class="o">+</span><span class="n">train_length</span><span class="p">:</span> <span class="n">sample_rate</span><span class="p">]</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">Y_train</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[</span><span class="n">warmup</span><span class="o">+</span><span class="n">forecast</span><span class="p">:</span> <span class="n">warmup</span><span class="o">+</span><span class="n">train_length</span><span class="o">+</span><span class="n">forecast</span><span class="p">:</span> <span class="n">sample_rate</span><span class="p">]</span>
    <span class="n">Y_train</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">Y_train</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">X_test</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[</span><span class="n">warmup</span> <span class="o">+</span> <span class="n">train_length</span><span class="p">:</span> <span class="o">-</span><span class="n">forecast</span><span class="p">:</span> <span class="n">sample_rate</span><span class="p">]</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">Y_test</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[</span><span class="n">warmup</span> <span class="o">+</span> <span class="n">train_length</span> <span class="o">+</span> <span class="n">forecast</span><span class="p">::</span><span class="n">sample_rate</span><span class="p">]</span>
    <span class="n">Y_test</span> <span class="o">=</span> <span class="n">bm</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">X_warm</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># First warmup the reservoir using the first 100 ms
# Then, train the network in 20000 ms to predict 1 ms chaotic series ahead
</span><span class="n">x_warm</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">20000</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sample</span> <span class="o">=</span> <span class="mi">3000</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="n">sample</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">"Training data"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_train</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="n">sample</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">"True prediction"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828151606545.png" alt="image-20230828151606545" /></p>

<h4 id="prepare-the-esn">Prepare the ESN</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ESN</span><span class="p">(</span><span class="n">bp</span><span class="p">.</span><span class="n">DynamicalSystemNS</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_in</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">,</span> <span class="n">num_out</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">leaky_rate</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
               <span class="n">Win_initializer</span><span class="o">=</span><span class="n">bp</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">Uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ESN</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">r</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Reservoir</span><span class="p">(</span>
        <span class="n">num_in</span><span class="p">,</span> <span class="n">num_hidden</span><span class="p">,</span>
        <span class="n">Win_initializer</span><span class="o">=</span><span class="n">Win_initializer</span><span class="p">,</span>
        <span class="n">spectral_radius</span><span class="o">=</span><span class="n">sr</span><span class="p">,</span>
        <span class="n">leaky_rate</span><span class="o">=</span><span class="n">leaky_rate</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">o</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">num_hidden</span><span class="p">,</span> <span class="n">num_out</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">bm</span><span class="p">.</span><span class="n">training_mode</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">&gt;&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">r</span> <span class="o">&gt;&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">o</span>
</code></pre></div></div>

<h4 id="train-and-test">Train and test</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">ESN</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">reset_state</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">bp</span><span class="p">.</span><span class="n">RidgeTrainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># warmup
</span><span class="n">_</span> <span class="o">=</span> <span class="n">trainer</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_warm</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># train
</span><span class="n">_</span> <span class="o">=</span> <span class="n">trainer</span><span class="p">.</span><span class="n">fit</span><span class="p">([</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">])</span>
</code></pre></div></div>

<h4 id="test-the-training-data">Test the training data</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ys_predict</span> <span class="o">=</span> <span class="n">trainer</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">6000</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">as_numpy</span><span class="p">(</span><span class="n">ys_predict</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
         <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"ESN prediction"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">as_numpy</span><span class="p">(</span><span class="n">y_train</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">"--"</span><span class="p">,</span>
         <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"True value"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">'Mean Square Error: </span><span class="si">{</span><span class="n">bp</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">ys_predict</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828151747954.png" alt="image-20230828151747954" /></p>

<h4 id="test-the-testing-data">Test the testing data</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ys_predict</span> <span class="o">=</span> <span class="n">trainer</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>

<span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">6000</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">as_numpy</span><span class="p">(</span><span class="n">ys_predict</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"ESN prediction"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bm</span><span class="p">.</span><span class="n">as_numpy</span><span class="p">(</span><span class="n">y_test</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">"--"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"True value"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s">'Mean Square Error: </span><span class="si">{</span><span class="n">bp</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">ys_predict</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828151824907.png" alt="image-20230828151824907" /></p>

<h3 id="jit-connection-operators">JIT connection operators</h3>

<ul>
  <li>Just-in-time randomly generated matrix.</li>
  <li>Support for Mat@Vec and Mat@Mat.</li>
  <li>Support different random generation methods.(homogenous, uniform, normal)</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">math</span><span class="p">,</span> <span class="n">random</span>

<span class="k">def</span> <span class="nf">jitconn_prob_homo</span><span class="p">(</span><span class="n">events</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">outs</span><span class="p">):</span>
    <span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">max_cdist</span><span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="n">ceil</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">prob</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span>  <span class="n">events</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">event</span><span class="p">:</span>
            <span class="n">post_i</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_cdist</span><span class="p">)</span>
            <span class="n">outs</span><span class="p">[</span><span class="n">post_i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">weight</span>
</code></pre></div></div>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828153353131.png" alt="image-20230828153353131" /></p>

<h2 id="applications">Applications</h2>

<h3 id="from-the-perspective-of-kernel-methods">From the perspective of kernel methods</h3>

<p>ç»´åº¦æ‰©å¼ æ€æƒ³</p>

<p>Non-linear SVMs: Kernel Mapping</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828153621843.png" alt="image-20230828153621843" /></p>

<p>Kernel methods in neural system? <strong>ä¸Žç»´åº¦æ‰©å¼ çš„æ€æƒ³ç›¸ä¼¼</strong></p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828153801285.png" alt="image-20230828153801285" /></p>

<h3 id="subcortical-pathway-for-rapid-motion-processing">Subcortical pathway for rapid motion processing</h3>

<p>The first two stages of subcortical visual pathway:
Retina -&gt; superior colliculus</p>

<p>The first two stages of primary auditory pathway:
Inner Ear -&gt; Cochlear Nuclei</p>

<p>ç»´åº¦æ‰©å¼ åœ¨subcortical pathwayä¸­ä½“çŽ°ï¼Œreservoir èƒ½å¤Ÿé«˜ç»´å¤„ç†çš„æ›´ç®€å•</p>

<h3 id="spatial-temporal-tasks">Spatial-temporal tasks</h3>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828154155803.png" alt="image-20230828154155803" /></p>

<p>æ—¢æœ‰æ—¶é—´ä¿¡æ¯ï¼Œåˆæœ‰ç©ºé—´ä¿¡æ¯çš„datasetï¼Œä½¿ç”¨reservoiræ¥å¤„ç†é«˜ç»´ä¿¡æ¯ï¼Œåä½ Dimension expansion</p>

<h3 id="gait-recognition">Gait recognition</h3>

<p>inputæ¥äº†å†åšè®¡ç®—</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828154352087.png" alt="image-20230828154352087" /></p>

<h3 id="spatial-temporal-tasks-1">Spatial-temporal tasks</h3>

<p>large-scaleï¼Œéšsizeå¢žå¤§ï¼Œaccuracyå¢žå¤§</p>

<p><img src="/BrainPy-course-notes/master_content/Notes.assets/image-20230828154428762.png" alt="image-20230828154428762" /></p>

<h3 id="liquid-state-machine">Liquid state machine</h3>

<p>A liquid state machine (LSM) is a type of reservoir computer that uses a spiking neural network.</p>

<p>ä¸ŽESNä¸€æ ·çš„èŒƒå¼ï¼Œéƒ½æ˜¯åŽ»åšdimension expansion</p>

<p>å¾ˆéš¾åŽ»åˆ†æžæ€Žä¹ˆworkçš„</p>

  
</div>


          </div>
          


          

        </div>
        
          

  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="https://github.com/brainpy.png"
               alt="Sichao He" />
          <p class="site-author-name" itemprop="name">Sichao He</p>
           
              <p class="site-description motion-element" itemprop="description">Notes from the first training course on Neural Computational Modeling</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">æ—¥å¿—</span>
              </a>
            </div>
          

          

          

        </nav>

        
        
        
          <div class="feed-link motion-element">
            <a href="/BrainPy-course-notes/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>

        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Sichao He</span>
</div>


<div class="powered-by">
  ç”± <a class="theme-link" href="https://jekyllrb.com">Jekyll</a> å¼ºåŠ›é©±åŠ¨
</div>

<div class="theme-info">
  ä¸»é¢˜ -
  <a class="theme-link" href="https://github.com/simpleyyt/jekyll-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>





















  
   
  
  
  
  
  
  <script type="text/javascript" src="/BrainPy-course-notes/assets/lib/jquery/index.js?v=2.1.3"></script>

  
  
  
  
  
  <script type="text/javascript" src="/BrainPy-course-notes/assets/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  
  
  
  
  <script type="text/javascript" src="/BrainPy-course-notes/assets/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  
  
  
  
  <script type="text/javascript" src="/BrainPy-course-notes/assets/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  
  
  
  
  <script type="text/javascript" src="/BrainPy-course-notes/assets/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  
  
  
  
  <script type="text/javascript" src="/BrainPy-course-notes/assets/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/BrainPy-course-notes/assets/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/BrainPy-course-notes/assets/js/src/motion.js?v=5.1.1"></script>



  
  

  <script type="text/javascript" src="/BrainPy-course-notes/assets/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/BrainPy-course-notes/assets/js/src/post-details.js?v=5.1.1"></script>


  


  <script type="text/javascript" src="/BrainPy-course-notes/assets/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  











  




  

    

  







  






  

  

  
  


  

  

  

</body>
</html>


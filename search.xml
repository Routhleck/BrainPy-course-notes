<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title><![CDATA[BrainPy course notes]]></title>
      <url>/BrainPy-course-notes/2023/08/27/BrainPy-course-notes/</url>
      <content type="text"><![CDATA[[TOC]ç¥žç»è®¡ç®—å»ºæ¨¡ç®€ä»‹è®¡ç®—ç¥žç»ç§‘å­¦çš„èƒŒæ™¯ä¸Žä½¿å‘½è®¡ç®—ç¥žç»ç§‘å­¦æ˜¯è„‘ç§‘å­¦å¯¹ç±»è„‘æ™ºèƒ½çš„æ¡¥æ¢ä¸¤å¤§ç›®æ ‡  ç”¨è®¡ç®—å»ºæ¨¡çš„æ–¹æ³•æ¥é˜æ˜Žå¤§è„‘åŠŸèƒ½çš„è®¡ç®—åŽŸç†  å‘å±•ç±»è„‘æ™ºèƒ½çš„æ¨¡åž‹å’Œç®—æ³•Prehistory  1907 LIF model ç¥žç»è®¡ç®—çš„æœ¬è´¨  1950s HH model ç”µä½å®šé‡åŒ–æ¨¡åž‹ æœ€fundamentalçš„  1960s Rollâ€™s cable equation æè¿°ä¿¡å·åœ¨è½´çªå’Œæ ‘çªæ€Žä¹ˆä¼ é€’  1970s Amari, Wilson, Cowan et al.çŽ°ä»Šå»ºæ¨¡çš„åŸºç¡€  1982 Hopfield model(Amari-Hopfield model)å¼•å…¥ç‰©ç†å­¦æŠ€æœ¯ï¼Œå¸å¼•å­æ¨¡åž‹  1988 Sejnowski et al. â€œComputational Neuroscienceâ€(science)æå‡ºè®¡ç®—ç¥žç»ç§‘å­¦æ¦‚å¿µçŽ°åœ¨çš„è®¡ç®—ç¥žç»ç§‘å­¦å¯¹åº”äºŽç‰©ç†å­¦çš„ç¬¬è°·-ä¼½åˆ©ç•¥æ—¶ä»£ï¼Œå¯¹å¤§è„‘å·¥ä½œåŽŸç†è¿˜ç¼ºä¹æ¸…æ™°çš„ç†è®ºThree levels of Brain Science  å¤§è„‘åšä»€ä¹ˆComputational theory -&gt; Psychology &amp; Cognitive Science-&gt; Human-like Cognitive function  å¤§è„‘æ€Žä¹ˆåšRepresentation &amp; Algorithm -&gt; Computational Neuroscience-&gt; Brain-inspired model &amp; algorithm  å¤§è„‘æ€Žä¹ˆå®žçŽ°Implementation-&gt; Neuroscience-&gt; Neuromorphic computingMission of Computational Neuroscience  What I can not build a computational model, I do not understandç¥žç»è®¡ç®—å»ºæ¨¡çš„ç›®æ ‡ä¸ŽæŒ‘æˆ˜Limitation of Deep Learning  ä¸æ“…é•¿å¯¹æŠ—æ ·æœ¬  å¯¹å›¾åƒçš„ç†è§£æœ‰é™Brain is for Processing Dynamical InformationWe never â€œseeâ€ a static imageThe missing linka computational model of higher cognitive functiorçŽ°åœ¨åªæ˜¯åšçš„å±€éƒ¨çš„ç½‘ç»œï¼Œæ²¡æœ‰ä¸€ä¸ªæˆåŠŸçš„æ¨¡åž‹ï¼Œèƒ½ä»Žç¥žç»å…ƒå‡ºå‘æž„å»ºç½‘ç»œï¼Œåˆ°ç³»ç»Ÿå±‚é¢ä¸ŠåŽŸå› : å› ä¸ºç¥žç»ç§‘å­¦åº•å±‚æ•°æ®çš„ç¼ºå¤±ï¼Œå¯ä»¥è€ƒè™‘æ•°æ®é©±åŠ¨ã€å¤§æ•°æ®çš„æ–¹å¼æ¥åŠ å¿«å‘å±•ç¥žç»è®¡ç®—å»ºæ¨¡çš„å·¥å…·  å·¥æ¬²è¡Œå…¶äº‹ï¼Œå¿…å…ˆåˆ©å…¶å™¨We need â€œPyTorch/TensorFlowâ€ in Computational Neuroscience!Challenges in neural modellingæœ‰ä¸åŒçš„å°ºåº¦  Mutiple-scale  Large-scale  Multiple purposes  The modeling targets and methods are extremely complex, and we need a general framework.Limitations of Existing Brain SimulatorsçŽ°ä»Šçš„æ¡†æž¶ä¸èƒ½æ»¡è¶³ä»¥ä¸ŠWhat are needed for a brain simulator  EfficiencyHigh-speed simulation on parallel computing devices, etc.  IntegrationIntegrated modeling of simulation, training, and analysis  FlexibilityNew models at all scales can be accommodated  ExtensibilityExtensible to new modeling methods(machine learning)éœ€è¦æ–°çš„èŒƒå¼Our solution: BrainPy4 levelsç¥žç»è®¡ç®—å»ºæ¨¡ä¸¾ä¾‹Image understanding: an ill-posed problemImage Understanding = image segmentation + image object recognition  Chicken vs. Egg dilemma      Without segmentation, how to recognize    Without recognition, how to segment  The solution of brain: Analysis-by-synthesis çŒœæµ‹ä¸ŽéªŒè¯æ–¹æ³•Reverse Hierarchy Theoryäººçš„æ„ŸçŸ¥æ˜¯æ•´ä½“åˆ°å±€éƒ¨Two pathways for visual information processingKey Computational Issues for Global-to-local Neural Information Processing  What are global and local features  How to rapidly extract global features  How to generate global hypotheses  How to implement from global to local processing  The interplay between global and local features  OthersHow to extract global featuresGlobal first = Topology first(å¤§èŒƒå›´é¦–å…ˆï¼Œé™ˆéœ–)è§†è§‰ç³»ç»Ÿæ›´æ•æ„ŸäºŽæ‹“æ‰‘æ€§è´¨çš„å·®å¼‚  DNNs has difficulty to recognize topologyA retina-SC network for topology detectionè§†ç½‘è†œåˆ°ä¸Šä¸˜çš„æ£€æµ‹ï¼ŒGap junction coupling â€¦A Model for Motion Pattern RecognitionReservoir ModuleDecision-making ModuleHow to generate â€œglobalâ€ hypotheses in the representation spaceAttractor neural networkLevy Flight in Animal BehaviorsHow to process information from global to localPush-pull FeedbackA hierarchical Hopfield ModelInterplay between global and local featuresA two-pathway model for object recognitionModeling visual masking å¯ä»¥ç”¨two-pathwayå¾ˆå¥½è§£é‡ŠProgramming basicsPython BasicsValues  Boolean  String  Integer  Float  â€¦KeywordsNot allowed to use keywords, they define structure and rules of a language.help("keywords")Operatorsæ•°æ®ä¹‹é—´çš„æ“ä½œFor Integers and Floatsa=5b=3# addition +print("a+b=",atb)# subtraction -print("a-b=",a-b)# multiplication *print("axb="a*b)# division /print("a/b=",a/b)# power **print("a**b=",a**b)Booleans#Boolean experssions# equals: ==print("5==5",5==5) # do not equal: !=print("5!-5",5!=5)# greater than: &gt;print("5&gt;5",5&gt;5)# greater than or equal: &gt;=print("5&gt;=5â€5&gt;=5)# logica operatorsprint("True and False:", True and False)print("True or False:", True or False)print("not False:", not False)ModulesNot all functionality available comes automatically when starting python.import matchimport numpy as npprint(math.pi)print(np.pi)from numpy import piprint(pi)from numpy import *print(pi)Control statementsIfa = 5# In Python, blocks of code are defined using indentation.if a == 5:	print("ok")  okFor# range(5) means a list with integers, 0, 1, 2, 3, 4for i in range(5):    print(i)  01234Whilei = 1while i &lt;= 100:    print(i**3)    i += i**3 # a += b is short for a = a+b  181000Functions  Functions are used to abstract components of a program.  Much like a mathematical function, they take some input and then find the result. start a function definition with a keyword def  Then comes the function name, with arguments in braces, and then a colon.def func(args1, args2):    passData typesList  Group variables together  Specific order  Access item with brankets: [ ]  List can be sliced  List can be multiplied  List can be added  Lists are mutable  Copying a listmyList = [0, 1, 2, 0,"name"]print("myList[0]:", myList[0])print("myList[1]:", myList[1])print("myList[3]:", myList[3])print("myList[-1]:", myList[-1])print("myList[-2]:", myList[-2])  myList[0]: 0myList[1]: 1myList[3]: namemyList[-1]: namemyList[-2]: 2.0myList = [0, 1.0, "hello"]print("myList[0:2]:", mylist[0:2])print("myList*2:", myList*2)myList2 = [2,"yes"]print("myList+myList2:", myList+myList2)  myList[0:2]: [0ï¼Œ1.0]myList*2: [0ï¼Œ1.0ï¼Œ helloâ€™ï¼Œ0ï¼Œ1.0ï¼Œ helloâ€™]myList+myList2: [0ï¼Œ1.0ï¼Œâ€™helloâ€™ï¼Œ2ï¼Œyesâ€™]tupleTuples are immutable.dictionaryA dictionary is a collection of key-value pairsd = {}d[1] = 2d["a"] = 3print("d: ", d)c = {1:2, "a":3}print("c: ", c)print("c[1]: ", c[1])  d: {1: 2, â€˜aâ€™: 3}c: {1: 2, â€˜aâ€™: 3}c[1]: 2ClassIn Python, everything is an object. Classes are objects, instances ofclasses are objects, modules are objects, and functions are objects. 	1.  a type 	2.  an internal data representation (primitive or composite) 	3.  a set of procedures for interaction with the objecta simple example# define classclass Linear():    pass# instantiate objectlayer1 = Linear()print(layer1)  &lt;__main__.Linear object at 0x7f88ad6c61d0&gt;Initializing an object# define classclass Linear():    # It refers to the object (instance) itself    def __init__(self, n_input):        self.n_input = n_inputlayer1 = Linear(100)layer2 = Linear(1000)print("layer1 : ", layer1.n_input)print("layer2 : ", layer2.n_input)  layer1 : 100layer2 : 1000Class has methods (similar to functions)# define classclass Linear():    ### It refers to the the object (instance) itself    def __init__(self, n_input, n_output):	    self.n_input = n_input        self.n_output = n_output	def compute n params(self):        num_params = self.n_input * self.n_output        return num_paramslayerl = Linear(10,100)print(layerl.compute_n_params())  1000NumPy BasicNumpy Introduction  Fundamental package for scientific computing with Python  N-dimensional array object  Linear algebra, frontier transform, random number capacities  Building block for other packages (e.g. Scipy)Array  Arrays are mutable  Arrays attributes  â€¦A = np.zeros((2, 2))print(A)  [[0. 0.]	[0. 0.]]a.ndim		# 2 dimensiona.shape		# (2, 5) shape of arraya.size		# 10 $ of elementsa.T			# transposea.dtype		# data typeArray broadcastingWhen operating on two arrays, numpy compares shapes. Two dimensions are compatible when  They are of equal size  One of them is 1Vector operations  Inner product  Outer product  Dot product (matrix multiplication)u = [1, 2, 3]v = [1, 1, 1]np.inner(u, v)np.outer(u, v)np.dot(u, v)  6array([[1, 1, 1],			[2, 2, 2],			[3, 3, 3]])6Matrix operations  np.ones  .T  np.dot  np.eye  np.trace  np.row_stack  np.column_stackOperations along axesa = np.ones((2, 3))print(a)a.sum()a.sum(axis=0)a.cumsum()a.cumsum(axis=0)Slicing arraysa = np.random.random((2, 3))print(a)a[0,:] 	# first row, all columnsa[0:2] 	# first and second rows, al columnsa[:,1:3]# all rows, second and third columnsReshapea = np.ones((10,1))a.reshape(2,5)Linear algebraqr				# Computes the QR decompositioncholesky		# Computes the Cholesky decompositioninv(A)			# Inversesolve(A,b)		# Solves Ax = b for A full ranklstsq(A,b)		# Solves arg minx //Ax - b//2eig(A)			# Eigenvalue decompositioneigvals(A)		# Computes eigenvaluessvd(Aï¼Œfull)		# Sinqular value decompositionpinv(A)			# Computes pseudo-inverse of AFourier transformimport numpy.fftfft		# 1-dimensional DFTfft2	# 2-dimensional DFTfftn	# N-dimensional DFTifft	# 1-dimensional inverse DFT (etc.)rfft	# Real DFT (1-dim)Random samplingimport numpy.randomrand(d0, d1, ..., dn)		# Random values in a given shaperandn(d0, d1, ..., dn)		# Random standard normalrandint(lo, hi, size)		# Random integers [lo hi)choice(a, size, repl, p)	# Sample from ashuffle(a)					# Permutation (in-place)permutation(a)				# Permutation (new array)Distributions in randomimport numpy.randombetabinomialchisquareexponentialdirichletgammalaplacelognormal...Scipy  SciPy is a library of algorithms and mathematical tools built to work with NumPy  arrays.  scipy.linalg linear algebra  scipy.stats statistics  scipy.optimize optimization  scipy.sparse sparse matrices  scipy.signal signal processing  etc.BrainPy introductionModeling demands  Large-scale  Multi-scale  MethodsBrainPy Architecture  Infrastructure  Functions  Just-in-time compilation  DevicesMain featuresDense operators  Compatible with NumPy, TensorFlow, PyTorch and other dense matrix operator syntax.  Users do not need to learn and get started programming directly.Dedicated operatorsq  Applies brain dynamics sparse connectivity properties with event-driven computational features.  Reduce the complexity of brain dynamics simulations by several orders of magnitude.Numerical Integrators  Ordinary differential equations: brainpy.odeint  Stochastic differential equations: brainpy.sdeint  Fractional differential equations: brainpy.fdeint  Delayed differential equationsModular and composableä»Žå¾®è§‚åˆ°å®è§‚brainpy.DynamicalSystemJIT of object-orientedBrainPy provides object-oriented transformations:  brainpy.math.jit  brainpy.math.grad  brainpy.math.for_loop  brainpy.math.ifelseBrainPy Programming BasicsJust-in-Time compilationJust In Time Compilation (JIT, or Dynamic Translation), is compilation that is being done during the execution of a program.JIT compilation attempts to use the benefits of both. While the interpreted program is being run, the JIT compiler determines the most frequently used code and compiles it to machine code.The advantages of a JIT are due to the fact that since the compilation takes place in run time, a JIT compiler has access to dynamic runtime information enabling it to make better optimizations (such as inlining functions).def gelu(x):    sqrt = bm.sqrt(2 / bm.pi)    cdf = 0.5 * (1.0 + bm.tanh(sqrt * (x + 0.044715 * (x ** 3))))    y = x *cdf    return y&gt;&gt;&gt; gelu_jit = bm.jit(gelu) # ä½¿ç”¨JITObject-oriented JIT compilation  The class object must be inherited from brainpy.BrainPyObject, the base class of BrainPy, whose methods will be automatically JIT compiled.  All time-dependent variables must be defined as brainpy.math.Variable.class LogisticRegression(bp.BrainPyObject):    def __init__(self, dimension):        super(LogisticRegression, self).__init__()                # parameters        self.dimension = dimension                # variables        self.w = bm.Variable(2.0 * bm.ones(dimension) - 1.3)        	def __call__(self, X, Y):        u = bm.dot(((1.0 / (1.0 + bm.exp(-Y * bm.dot(X, self.w))) - 1.0) * Y), X)        self.w.value = self.w - u # in-place updateExampleL Run a neuron modelmodel = bp.neurons.HH(1000) #ä¸€å…±1000ä¸ªç¥žç»å…ƒrunner = bp.DSRunner(target=model, inputs=('input', 10.)) # jité»˜è®¤ä¸ºTruerunner(duration=1000, eval_time=True) #æ¨¡æ‹Ÿ 1000msç¦ç”¨JITæ¥debugData operationsArrayç­‰ä»·äºŽnumpyçš„arrayBrainPy arrays &amp; JAX arrayst1 = bm.arange(3)print(t1)print(t1.value)  JaxArray([0, 1, 2], dtype=int32)DeviceArray([0, 1, 2], dtype=int32)VariablesArrays that are not marked as dynamic variables will be JIT-compiled as static arrays, and modifications to static arrays will not be valid in the JIT compilation environment.t = bm.arange(4)v = bm.Variable(t)print(v)print(v.value)  Variable([0, 1, 2, 3], dtype=int32)DeviceArray([0, 1, 2, 3], dtype=int32)VariablesIn-place updating å°±åœ°æ›´æ–°Indexing and slicing  Indexing: v[i] = a or v[(1, 3)] = c  Slicing: v[i:j] = b  Slicing all values v[:] = d, v[...] = eAugmented assignment  add  subtract  divide  multiply  floor divide  modulo  power  and  or  xor  left shift  right shiftValue assignmentv.value = bm.arange(10)check_no_change(v)Update assignmentv.update(bm.random.randint(0, 20, size=10))Control flowsIf-elsebrainpy.math.wherea = 1.bm.where(a &lt; 0, 0., 1.)  DeviceArray(1., dtype=float32, weak_type=True)brainpy.math.ifelsedef ifelse(condition, branches, operands):	true_fun, false_fun = branches    if condition:        return true_fun(operands)    else:        return false_fun(operands)For loopimport brainpy.mathhist_of_out_vars = brainpy.math.for_loop(body_fun, operands)While loopi = bm.Variable(bm.zeros(1))counter = bm.Variable(bm.zeros(1))def cond_f():    return i[0] &lt; 10def body_f():    i.value += 1    counter.value += ibm.while_loop(body_f, cond_f, operands=())print(counter, i)Single Neuron Modeling: Conductance-Based ModelsNeuronal structure,ing potential, and equivalent circuitsNeuronal structure  Cell body/soma  Axon  Dendrites  SynapsesResting potentialTransport proteins for ions in neuron cell membranes:  Ion channels: Na + channels, K + channels, â€¦ (gated/non-gated)  Ion pumps: the Na + -K + pumpç¦»å­æµ“åº¦åœ¨èƒžå†…å¤–çš„å·®å¼‚äº§ç”Ÿçš„ç”µåŠ¿å·®      Ion concentration difference â†’ chemical gradient â†’ electrical gradient        Nernst Equation:\(E=\dfrac{RT}{zF}\ln\dfrac{[\mathrm{ion}]_{\mathrm{out}}}{[\mathrm{ion}]_{\mathrm{in}}}\)        Goldman-Hodgkin-Katz (GHK) Equation:\(V_m=\frac{RT}{F}\ln\left(\frac{P_{\mathrm{Na}}[\mathrm{Na}^+]_{\mathrm{out}}+P_{\mathrm{K}}[\mathrm{K}^+]_{\mathrm{out}}+P_{\mathrm{Cl}}[\mathrm{Cl}^-]_{\mathrm{in}}}{P_{\mathrm{Na}}[\mathrm{Na}^+]_{\mathrm{in}}+P_{\mathrm{K}}[\mathrm{K}^+]_{\mathrm{in}}+P_{\mathrm{Cl}}[\mathrm{Cl}^-]_{\mathrm{out}}}\right)\)  Equivalent circuitsComponents of an equivalent circuit:  Battery  Capacitor  ResistorConsidering the potassium channel ONLY:\(\begin{gathered}0=I_{\mathrm{cap}}+I_{K}=c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+{\frac{V_{\mathrm{M}}-E_{\mathrm{K}}}{R_{\mathrm{K}}}}, \\c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}=-{\frac{V_{\mathrm{M}}-E_{\mathrm{K}}}{R_{\mathrm{K}}}}=-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}}). \end{gathered}\)Considering the Na + , K + , and Cl - channels and the external current I(t):\(\begin{aligned}\frac{I(t)}{A} &amp; = c_{\mathrm{M}}\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t} + i_{\mathrm{ion}} \\\Rightarrow c_{\mathrm{M}}\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t} &amp; = -g_{\mathrm{Cl}}(V_{\mathrm{M}}-E_{\mathrm{Cl}}) - g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}}) - g_{\mathrm{Na}}(V_{\mathrm{M}}-E_{\mathrm{Na}}) + \frac{I(t)}{A}\end{aligned}\)Steady-state membrane potential given a constant current input I:\(\begin{array}{rcl}\Rightarrow&amp;c_{M}\frac{\mathrm{d}V_{M}}{\mathrm{d}t}=-(g_{C1}+g_{K}+g_{Na})V_{M}+g_{C1}E_{C1}+g_{K}E_{K}+g_{Na}E_{Na}+\frac{I(t)}{A}\\\\V_{sS}=\frac{g_{CM}E_{C1}+g_{K}E_{K}+g_{Na}E_{Na}+I/A}{g_{C1}+g_{K}+g_{Na}}&amp;\xrightarrow{I=0}&amp;V_{sN,I=0}=E_{R}=\frac{g_{CC}E_{C1}+g_{K}E_{K}+g_{Na}E_{Na}}{g_{C1}+g_{K}+g_{Na}}\end{array}\)Cable Theory &amp; passive conductionConsidering the axon as a long cylindrical cable:\(I_{\mathrm{cross}}(x,t)={I_{\mathrm{cross}}(x+\Delta x,t)}+I_{\mathrm{ion}}(x,t)+I_{\mathrm{cap}}(x,t)\)\[V(x+\Delta x,t)-V(x,t)=-I_{\mathrm{cross}}(x,t)R_{\mathrm{L}}=-I_{\mathrm{cross}}(x,t)\frac{\Delta x}{\pi a^{2}}\rho_{\mathrm{L}} \\{I_{\mathrm{cross}}(x,t)} =-\frac{\pi a^{2}}{\rho_{\mathrm{L}}}\frac{\partial V(x,t)}{\partial x}  \\{I_{\mathrm{ion}}} =(2\pi a\Delta x)i_{\mathrm{ion}}  \\I_{\mathrm{cap}}(x,t) =(2\pi a\Delta x)c_{\mathrm{M}}\frac{\partial V(x,t)}{\partial t}\]-&gt; \((2\pi a\Delta x)c_{\mathrm{M}}\frac{\partial V(x,t)}{\partial t}+(2\pi a\Delta x)i_{\mathrm{ion}}=\frac{\pi a^{2}}{\rho_{\mathrm{L}}}\frac{\partial V(x+\Delta x,t)}{\partial x}-\frac{\pi a^{2}}{\rho_{\mathrm{L}}}\frac{\partial V(x,t)}{\partial x}\)Cable Equation\(c_\mathrm{M}\frac{\partial V(x,t)}{\partial t}=\frac{a}{2\rho_\mathrm{L}}\frac{\partial^2V(x,t)}{\partial x^2}-i_\mathrm{ion}\)ç”µæµåœ¨é€šè¿‡é•¿ç›´å¯¼ä½“æ—¶ä¼šæ³„éœ²ç”µæµï¼Œå¦‚ä½•è®°å½•è†œç”µä½ï¼Œå¯ä»¥ä½¿ç”¨æ­¤æ–¹ç¨‹æ¥æè¿°Passive conduction: ion currents are caused by leaky channels exclusively\(i_{\mathrm{ion}}=V(x,t)/r_{\mathrm{M}}\)-&gt;\(\begin{aligned}c_\mathrm{M}\frac{\partial V(x,t)}{\partial t}&amp;=\frac{a}{2\rho_\mathrm{L}}\frac{\partial^2V(x,t)}{\partial x^2}-\frac{V(x,t)}{r_\mathrm{M}}\\\\\tau\frac{\partial V(x,t)}{\partial t}&amp;=\lambda^2\frac{\partial^2V(x,t)}{\partial x^2}-V(x,t)\quad\lambda=\sqrt{0.5ar_\mathrm{M}/\rho_\mathrm{L}}\end{aligned}\)æ²¡æœ‰åŠ¨ä½œç”µä½ï¼Œå•çº¯é€šè¿‡ç”µç¼†ä¼ è¾“If a constant external current is applied to ð‘¥ = 0  the steady-state membrane potential $ð‘‰_{ss}(ð‘¥)$ is\(\lambda^2\frac{\mathrm{d}^2V_{\mathrm{ss}}(x)}{\mathrm{d}x^2}-V_{\mathrm{ss}}(x)=0\longrightarrow V_{\mathrm{ss}}(x)=\frac{\lambda\rho_{\mathrm{L}}}{\pi a^2}I_0e^{-x/\lambda}\)ç”µä¿¡å·æ— è¡°å‡ä¼ æ’­: åŠ¨ä½œç”µä½Action potential &amp; active transportSteps of an action potential:  Depolarization  Repolarization  Hyperpolarization  RestingCharacteristics:  All-or-none  Fixed shape  Active electrical propertyHow to simulate an action potential?\(\begin{aligned}\frac{I(t)}{A}&amp; =c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+i_{\mathrm{ion}}  \\\Rightarrow\quad c_{\mathrm{M}}\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}&amp; =-g_{\mathrm{Cl}}(V_{\mathrm{M}}-E_{\mathrm{Cl}})-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}})-g_{\mathrm{Na}}(V_{\mathrm{M}}-E_{\mathrm{Na}})+\frac{I(t)}{A} \end{aligned}\)ç¦»å­é€šé“çš„å¼€é—­ä¼šéšç€ç”µåŽ‹è€Œå˜åŒ–ï¼Œç”µå¯¼ä¹Ÿéšç€ç”µåŽ‹è€Œå˜åŒ–Mechanism: voltage-gated ion channelsHHå»ºæ¨¡æ€è·¯ï¼šé€šè¿‡ç”µå¯¼Nodes of RanvierSaltatory conduction with a much higher speed and less energy consumptionä¸¤ä¸ªéƒŽé£žç»“ä¹‹é—´ä¼šæœ‰ç¦»å­é€šé“ï¼Œæ—¢æœ‰è¢«åŠ¨ä¼ å¯¼ï¼Œä¹Ÿæœ‰ä¸»åŠ¨çš„é˜²æ­¢è¡°å‡The Hodgkin-Huxley ModelModeling of each ion channelModeling of each ion channel:\(g_m=\bar{g}_mm^x\)Modeling of each ion gate:$$\mathcal{C}\underset{}{\operatorname{\overset{\alpha(\mathrm{V})}{\underset{\beta(\mathrm{V})}{\operatorname{\longrightarrow}}}}\mathcal{O}}\Rightarrow\begin{aligned}\frac{\mathrm{d}m}{\mathrm{d}t}&amp; =\alpha(V)(1-m)-\beta(V)m  &amp;=\frac{m_{\infty}(V)-m}{\tau_{m}(V)}\end{aligned}\\begin{aligned}m_\infty(V)&amp;=\frac{\alpha(V)}{\alpha(V)+\beta(V)}.\\tau_m(V)&amp;=\frac{1}{\alpha(V)+\beta(V)}\end{aligned}$$\[\text{If}\ V\text{ is constant:}m(t)=m_\infty(V)+(m_0-m_\infty(V))\mathrm{e}^{-t/\tau_m(V)}\]Voltage clamp\[\begin{aligned}\frac{I(t)}{A}&amp; =c_{\mathrm{M}}{\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}}+i_{\mathrm{ion}}  \\\Rightarrow\quad c_{\mathrm{M}}\frac{\mathrm{d}V_{\mathrm{M}}}{\mathrm{d}t}&amp; =-g_{\mathrm{Cl}}(V_{\mathrm{M}}-E_{\mathrm{Cl}})-g_{\mathrm{K}}(V_{\mathrm{M}}-E_{\mathrm{K}})-g_{\mathrm{Na}}(V_{\mathrm{M}}-E_{\mathrm{Na}})+\frac{I(t)}{A} \end{aligned}\]  The membrane potential is kept constant  The current from capacitors is excluded  Currents must come from leaky/voltage-gated ion channels\[\begin{aligned}I_{\mathrm{cap}}&amp;=c\frac{dV}{dt}=0\\I_{\mathrm{fb}}&amp;=\quad i_{\mathrm{ion}}=g_{\mathrm{Na}}(V-E_{\mathrm{Na}})+g_{\mathrm{K}}(V-E_{\mathrm{K}})+g_{\mathrm{L}}(V-E_{\mathrm{L}})\end{aligned}\]åªæµ‹é‡ä¸€ä¸ªç¦»å­é€šé“å°±å¯ä»¥å¾ˆå®¹æ˜“å¾—åˆ°ç”µå¯¼Leaky channelHyperpolarization â†’ the sodium and potassium channels are closed\(I_{\mathrm{fb}}=g_{\mathrm{Na}}(V-E_{\mathrm{Na}})+g_{\mathrm{K}}(V-E_{\mathrm{K}})+g_{\mathrm{L}}(V-E_{\mathrm{L}})\)\[\Rightarrow I_{\mathrm{fb}}=g_L(V-E_L)\]\[g_\mathrm{L}=0.3\mathrm{mS/cm}^2,E_\mathrm{L}=-54.4\mathrm{mV}\]Potassium and sodium channelsPotassium channels: Use choline to eliminate the inward current of Na +Na + current: $I_{fb} - I_{K}$è½¬åŒ–é€ŸçŽ‡å’Œç”µå¯¼çŽ‡ä¸¤ä¸ªå› ç´ Potassium channels  Resting state (gate closed)  Activated state (gate open)â†’ Activation gate: $g_{\mathrm{K}}=\bar{g}_{K}n^{x}$Sodium channels  Resting state (gate closed)  Activated state (gate open)  Inactivated state (gate blocked)â†’ Activation gate + inactivation gate: $g_{\mathrm{Na}}=\bar{g}_\text{Na}m^3h$The gates of sodium channelsModeling of each ion gate:\(\begin{aligned}&amp;\text{gk}&amp;&amp; =\bar{g}_{K}n^{x}  \\&amp;\text{gNa}&amp;&amp; =\bar{g}_{\mathrm{Na}}m^{3}h  \\&amp;\frac{\mathrm{d}n}{\mathrm{d}t}&amp;&amp; =\alpha_{n}(V)(1-n)-\beta_{n}(V)n  \\&amp;\frac{\mathrm{d}m}{\mathrm{d}t}&amp;&amp; =\alpha_{m}(V)(1-m)-\beta_{m}(V)m  \\&amp;\frac{\mathrm{d}h}{\mathrm{d}t}&amp;&amp; =\alpha_{h}(V)(1-h)-\beta_{h}(V)h \end{aligned}\)\[\begin{aligned}\frac{\mathrm{d}m}{\mathrm{d}t}&amp; =\alpha(V)(1-m)-\beta(V)m  \\&amp;=\frac{m_{\infty}(V)-m}{\tau_{m}(V)}\end{aligned}\]\[\begin{aligned}m_\infty(V)&amp;=\frac{\alpha(V)}{\alpha(V)+\beta(V)}\\\tau_m(V)&amp;=\frac{1}{\alpha(V)+\beta(V)}\end{aligned}.\]\[m(t)=m_\infty(V)+(m_0-m_\infty(V))\mathrm{e}^{-t/\tau_m(V)}\]The Hodgkin-Huxley(HH) Model\[c_\mathrm{M}\frac{\mathrm{d}V_\mathrm{M}}{\mathrm{d}t}=-g_\mathrm{Cl}(V_\mathrm{M}-E_\mathrm{Cl})-g_\mathrm{K}(V_\mathrm{M}-E_\mathrm{K})-g_\mathrm{Na}(V_\mathrm{M}-E_\mathrm{Na})+\frac{I(t)}{A}\]æœ¬è´¨æ˜¯4ä¸ªå¾®åˆ†æ–¹ç¨‹è”ç«‹åœ¨ä¸€èµ·\(\left\{\begin{aligned}&amp;c\frac{\mathrm{d}V}{\mathrm{d}t}=-\bar{g}_\text{Na}m^3h(V-E_\text{Na})-\bar{g}_\text{K}n^4(V-E_\text{K})-\bar{g}_\text{L}(V-E_\text{L})+I_\text{ext},\\&amp;\frac{\mathrm{d}n}{\mathrm{d}t}=\phi\left[\alpha_n(V)(1-n)-\beta_n(V)n\right]\\&amp;\frac{\mathrm{d}m}{\mathrm{d}t}=\phi\left[\alpha_m(V)(1-m)-\beta_m(V)m\right],\\&amp;\frac{\mathrm{d}h}{\mathrm{d}t}=\phi\left[\alpha_h(V)(1-h)-\beta_h(V)h\right],\end{aligned}\right.\)\[\begin{aligned}\alpha_n(V)&amp;=\frac{0.01(V+55)}{1-\exp\left(-\frac{V+55}{10}\right)},\quad\beta_n(V)&amp;=0.125\exp\left(-\frac{V+65}{80}\right),\\\alpha_h(V)&amp;=0.07\exp\left(-\frac{V+65}{20}\right),\quad\beta_n(V)&amp;=\frac{1}{\left(\exp\left(-\frac{V+55}{10}\right)+1\right)},\\\alpha_m(V)&amp;=\frac{0.1(V+40)}{1-\exp\left(-(V+40)/10\right)},\quad\beta_m(V)&amp;=4\exp\left(-(V+65)/18\right).\end{aligned}\]\[\phi=Q_{10}^{(T-T_{\mathrm{base}})/10}\]æ¯ä¸€æ­¥ç¬¦åˆç”Ÿç‰©å­¦How to fit each gating variable?Fitting n: $g_{\mathbf{K}}=\bar{g}{K}n^{x}\quad m(t)=m{\infty}(V)+(m_{0}-\color{red}{\boxed{m_{\infty}(V)}})\mathrm{e}^{-t/\pi_{m}(V)}$â†’ $g_\mathrm{K}(V,t)=\bar{g}\mathrm{K}\left[n\infty(V)-(n_\infty(V)-n_0(V))\mathrm{e}^{-\frac{t}{\tau_n(V)}}\right]^x$by $g_{\mathrm{K}\infty}=\bar{g}{\mathrm{K}}n{\infty}^{x},g_{\mathrm{K}0}=\bar{g}{\mathrm{K}}n{0}^{x}$â†’ $g_{\mathrm{K}}(V,t)=\left[g_{\mathrm{K}\infty}^{1/x}-(g_{\mathrm{K}\infty}^{1/x}-g_{\mathrm{K}0}^{1/x})\mathrm{e}^{-\frac{t}{\tau_{n}(V)}}\right]^{x}$Hodgkin-Huxley brain dynamics programmingDynamics Programming BasicsIntegratorså¾®åˆ†å™¨exampleFitzHugh-Nagumo equation\(\begin{aligned}\tau\dot{w}&amp;=v+a-bw,\\\dot{v}&amp;=v-\frac{\nu^3}{3}-w+I_{\mathrm{ext}}.\end{aligned}\)@bp.odeint(method='Euler', dt=0.01)def integral(V, w, t, Iext, a, b, tau):    dw = (V + a - b * w) / tau    dV = V - V * V * V / 3 - w + Iext    return dV, dwJointEqIn a dynamical system, there may be multiple variables that change dynamically over time. Sometimes these variables are interrelated, and updating one variable requires other variables as inputs. For better integration accuracy, we recommend that you use brainpy.JointEq to jointly solve interrelated differential equations.a, b = 0.02, 0.20dV = lambda V, t, w, Iext: 0.04 * V * V + 5 * V + 140 - w + Iext	# ç¬¬ä¸€ä¸ªæ–¹ç¨‹dw = lambda w, t, V: a * (b * V - w)								# ç¬¬äºŒä¸ªæ–¹ç¨‹joint_eq = bp.JointEq(dV, dw)										# è”åˆå¾®åˆ†æ–¹ç¨‹integral2 = bp.odeint(joint_eq, method='rk2')						# å®šä¹‰è¯¥è”åˆå¾®åˆ†æ–¹ç¨‹çš„æ•°å€¼ç§¯åˆ†æ–¹æ³•# å£°æ˜Žç§¯åˆ†è¿è¡Œå™¨runner = bp.integrators.IntegratorRunner(	integral,    monitors=['V']    inits=dict(V=0., w=0.)    args=dict(a=a, b=b, tau=tau, Iext=Iext),    dt=0.01)# ä½¿ç”¨ç§¯åˆ†è¿è¡Œå™¨æ¥è¿›è¡Œæ¨¡æ‹Ÿ100msï¼Œç»“åˆæ­¥é•¿dt=0.01runner.run(100.)plt.plot(runner.mon.ts, runner.mon.V)plt.show()DynamicalSystemBrainPy provides a generic SynamicalSystem class to define various types of dynamical models.BrainPy supports modelings in brain simulation and brain-inspired computing.All these supports are based on one common concept: Dynamical System via brainpy.DynamicalSystem.What is DynamicalSystemA DynamicalSystem defines the updating rule of the model at single time step.  For models with state, DynamicalSystem defines the state transition from $t$ to $t + dt$, i.e., $S(t+dt)=F(S(t),x,t,dt)$, where $S$ is the state, $x$ is input, $t$ is the time, and $dt$ is the time step. This is the case for recurrent neural networks (like GRU, LSTM), neuron models (like HH, LIF), or synapse models which are widely used in brain simulation.  However, for models in deep learning, like convolution and fully-connected linear layers, DynamicalSystem defines the input-to-output mapping, i.e., $y=F(x,t)$.How to define DynamicalSystemclass YourDynamicalSystem(bp.DynamicalSystem):    def update(self, x):        ...Instead of input x, there are shared arguments across all nodes/layers in the network:  the current time t, or  the current running index i, or  the current time step dt, or  the current phase of training or testing fit=True/False.Here, it is necessary to explain the usage of bp.share.  bp.share.save( ): The function saves shared arguments in the global context. User can save shared arguments in tow ways, for example, if user want to set the current time t=100, the current time step dt=0.1,the user can use bp.share.save("t",100,"dt",0.1) or bp.share.save(t=100,dt=0.1).  bp.share.load( ): The function gets the shared data by the key, for example, bp.share.load("t").  bp.share.clear_shargs( ): The function clears the specific shared arguments in the global context, for example, bp.share.clear_shargs("t").  bp.share.clear( ): The function clears all shared arguments in the global context.How to run DynamicalSystemAs we have stated above that DynamicalSystem only defines the updating rule at single time step, to run a DynamicalSystem instance over time, we need a for loop mechanism.brainpy.math.for_loopfor_loop is a structural control flow API which runs a function with the looping over the inputs. Moreover, this API just-in-time compile the looping process into the machine code.inputs = bp.inputs.section_input([0., 6.0, 0.], [100., 200., 100.])indices = np.arange(inputs.size)def run(i, x):    neu.step_run(i, x)    return neu.V.valuevs = bm.for_loop(run, (indices, inputs), progress_bar=True)brainpy.LoopOverTimeDifferent from for_loop, brainpy.LoopOverTime is used for constructing a dynamical system that automatically loops the model over time when receiving an input.for_loop runs the model over time. While brainpy.LoopOverTime creates a model which will run the model over time when calling it.net2.reset_state(batch_size=10)looper = bp.LoopOverTime(net2)out = looper(currents)brainpy.DSRunnerInitializing a DSRunnerGenerally, we can initialize a runner for dynamical systems with the format of:runner = DSRunner(target=instance_of_dynamical_system,                  inputs=inputs_for_target_DynamicalSystem,                  monitors=interested_variables_to_monitor,                  dyn_vars=dynamical_changed_variables,                  jit=enable_jit_or_not,                  progress_bar=report_the_running_progress,                  numpy_mon_after_run=transform_into_numpy_ndarray                  )  target specifies the model to be simulated. It must an instance of brainpy.DynamicalSystem.  inputs is used to define the input operations for specific variables.          It should be the format of [(target, value, [type, operation])], where target is the input target, value is the input value, type is the input type (such as â€œfixâ€, â€œiterâ€, â€œfuncâ€), operation is the operation for inputs (such as â€œ+â€, â€œ-â€, â€œ*â€, â€œ/â€, â€œ=â€). Also, if you want to specify multiple inputs, just give multiple (target, value, [type, operation]), such as [(target1, value1), (target2, value2)].      It can also be a function, which is used to manually specify the inputs for the target variables. This input function should receive one argument tdi which contains the shared arguments like time t, time step dt, and index i.        monitors is used to define target variables in the model. During the simulation, the history values of the monitored variables will be recorded. It can also to monitor variables by callable functions and it should be a dict. The key should be a string for later retrieval by runner.mon[key]. The value should be a callable function which receives an argument: tdt.  dyn_vars is used to specify all the dynamically changed variables used in the target model.  jit determines whether to use JIT compilation during the simulation.  progress_bar determines whether to use progress bar to report the running progress or not.  numpy_mon_after_run determines whether to transform the JAX arrays into numpy ndarray or not when the network finishes running.Running a DSRunnerAfter initialization of the runner, users can call .run() function to run the simulation. The format of function .run() is showed as follows:runner.run(duration=simulation_time_length,           inputs=input_data,           reset_state=whether_reset_the_model_states,           shared_args=shared_arguments_across_different_layers,           progress_bar=report_the_running_progress,           eval_time=evaluate_the_running_time           )  duration is the simulation time length.  inputs is the input data. If inputs_are_batching=True, inputs must be a PyTree of data with two dimensions: (num_sample, num_time, ...). Otherwise, the inputs should be a PyTree of data with one dimension: (num_time, ...).  reset_state determines whether to reset the model states.  shared_args is shared arguments across different layers. All the layers can access the elements in shared_args.  progress_bar determines whether to use progress bar to report the running progress or not.  eval_time determines whether to evaluate the running time.Monitors# initialize monitor through a list of stringsrunner1 = bp.DSRunner(target=net,                      monitors=['E.spike', 'E.V', 'I.spike', 'I.V'],  # 4 elements in monitors                      inputs=[('E.input', 20.), ('I.input', 20.)],                      jit=True)Once we call the runner with a given time duration, the monitor will automatically record the variable evolutions in the corresponding models. Afterwards, users can access these variable trajectories by using .mon.[variable_name]. The default history times .mon.ts will also be generated after the model finishes its running. Letâ€™s see an example.runner1.run(100.)bp.visualize.raster_plot(runner1.mon.ts, runner1.mon['E.spike'], show=True)Initialization with index specificationmonitors=[('E.spike', [1, 2, 3]),  # monitor values of Variable at index of [1, 2, 3]                                'E.V'],  # monitor all values of Variable 'V'  The monitor shape of â€œE.Vâ€ is (run length, variable size) = (1000, 3200)The monitor shape of â€œE.spikeâ€ is (run length, index size) = (1000, 3)Explicit monitor targetmonitors={'spike': net.E.spike, 'V': net.E.V},  The monitor shape of â€œVâ€ is = (1000, 3200)The monitor shape of â€œspikeâ€ is = (1000, 3200)Explicit monitor target with index specificationmonitors={'E.spike': (net.E.spike, [1, 2]),  # monitor values of Variable at index of [1, 2]                                'E.V': net.E.V},  # monitor all values of Variable 'V'  The monitor shape of â€œE.Vâ€ is = (1000, 3200)The monitor shape of â€œE.spikeâ€ is = (1000, 2)InputsIn brain dynamics simulation, various inputs are usually given to different units of the dynamical system. In BrainPy, inputs can be specified to runners for dynamical systems. The aim of inputs is to mimic the input operations in experiments like Transcranial Magnetic Stimulation (TMS) and patch clamp recording.inputs should have the format like (target, value, [type, operation]), where  target is the target variable to inject the input.  value is the input value. It can be a scalar, a tensor, or a iterable object/function.  type is the type of the input value. It support two types of input: fix and iter. The first one means that the data is static; the second one denotes the data can be iterable, no matter whether the input value is a tensor or a function. The iter type must be explicitly stated.  operation is the input operation on the target variable. It should be set as one of { + , - , * , / , = }, and if users do not provide this item explicitly, it will be set to â€˜+â€™ by default, which means that the target variable will be updated as val = val + input.Static inputsrunner6 = bp.DSRunner(target=net,                      monitors=['E.spike'],                      inputs=[('E.input', 20.), ('I.input', 20.)],  # static inputs                      jit=True)runner6.run(100.)bp.visualize.raster_plot(runner6.mon.ts, runner6.mon['E.spike'])Iterable inputsI, length = bp.inputs.section_input(values=[0, 20., 0],                                    durations=[100, 1000, 100],                                    return_length=True,                                    dt=0.1)runner7 = bp.DSRunner(target=net,                      monitors=['E.spike'],                      inputs=[('E.input', I, 'iter'), ('I.input', I, 'iter')],  # iterable inputs                      jit=True)runner7.run(length)bp.visualize.raster_plot(runner7.mon.ts, runner7.mon['E.spike'])Run a built-in HH modelUsing Built-in Models â€” BrainPy documentationimport brainpy as bpimport brainpy.math as bmcurrent, length = bp.inputs.section_input(values=[0., bm.asarray([1., 2., 4., 8., 10., 15.]), 0.],                                         durations=[10, 2, 25],                                         return_length=True)hh_neurons = bp.neurons.HH(current.shape[1])runner = bp.DSRunner(hh_neurons, monitors=['V', 'm', 'h', 'n'], inputs=('input', current, 'iter'))runner.run(length)Run a HH model from scratchThe mathematic expression of the HH model\[\left\{\begin{aligned}&amp;c\frac{\mathrm{d}V}{\mathrm{d}t}=-\bar{g}_\text{Na}m^3h(V-E_\text{Na})-\bar{g}_\text{K}n^4(V-E_\text{K})-\bar{g}_\text{L}(V-E_\text{L})+I_\text{ext},\\&amp;\frac{\mathrm{d}n}{\mathrm{d}t}=\phi\left[\alpha_n(V)(1-n)-\beta_n(V)n\right]\\&amp;\frac{\mathrm{d}m}{\mathrm{d}t}=\phi\left[\alpha_m(V)(1-m)-\beta_m(V)m\right],\\&amp;\frac{\mathrm{d}h}{\mathrm{d}t}=\phi\left[\alpha_h(V)(1-h)-\beta_h(V)h\right],\end{aligned}\right.\]\[\begin{aligned}\alpha_n(V)&amp;=\frac{0.01(V+55)}{1-\exp\left(-\frac{V+55}{10}\right)},\quad\beta_n(V)&amp;=0.125\exp\left(-\frac{V+65}{80}\right),\\\alpha_h(V)&amp;=0.07\exp\left(-\frac{V+65}{20}\right),\quad\beta_n(V)&amp;=\frac{1}{\left(\exp\left(-\frac{V+55}{10}\right)+1\right)},\\\alpha_m(V)&amp;=\frac{0.1(V+40)}{1-\exp\left(-(V+40)/10\right)},\quad\beta_m(V)&amp;=4\exp\left(-(V+65)/18\right).\end{aligned}\]\[\phi=Q_{10}^{(T-T_{\mathrm{base}})/10}\]V: the membrane potentialn: activation variable of the Kt channelm: activation variable of the Nat channelh; inactivation variable of the Nat channeDefine HH model class  Inherit bp.dyn.NeuDynimport brainpy as bpimport brainpy.math as bmclass HH(bp.dyn.NeuDyn):    def __init__(self, size,                ENa=50., gNa=120.,                Ek=-77., gK=36.,                EL=-54.387, gL=0.03,                V_th=0., C=1.0, T=6.3):        super(HH, self).__init__(size=size)Initializationimport brainpy as bpimport brainpy.math as bmclass HH(bp.dyn.NeuDyn):    def __init__(self, size,                ENa=50., gNa=120.,                Ek=-77., gK=36.,                EL=-54.387, gL=0.03,                V_th=0., C=1.0, T=6.3):        super(HH, self).__init__(size=size)                # parameters        self.ENa = ENa        self.EK = EK        self.EL = EL        self.gNA = gNa        self.gK = gK        self.gL = gL        self.C = C        self.V_th = V_th        self.T_base = 6.3        self.phi = 3.0 ** ((T - self.T_base) / 10.0)                # variable        self.V = bm.Variable(-70.68 * bm.ones(self.num))        self.m = bm.Variable(0.0266 * bm.ones(self.num))        self.h = bm.Variable(0.772 * bm.ones(self.num))        self.n = bm.Variable(0.235 * bm.ones(self.num))        self.input = bm.Variable(bm.zeros(self.num))        self.spike = bm.Variable(bm.zeros(self.num, dtype=bool))        self.t_last_spike = bm.Variable(bm.ones(self.num) * -1e7)                # å®šä¹‰ç§¯åˆ†å‡½æ•°    	self.integral = bp.odeint(f=self.derivative, method='exp_auto')Define the derivative function@propertydef derivative(self):    return bp.JointEq(self.dV, self.dm, self.dh, self.dn)def dV(self, V, t, m, h, n, Iext):    I_Na = (self.gNa * m ** 3.0 * h) * (V - self.ENa)    I_K = (self.gK * n ** 4.0) * (V - self.EK)    I_leak = self.gL * (V - self.EL)    dVdt = (- I_Na - I_K - I_leak + Iext) / self.C    return dVdtdef dm(self, m, t, V):    alpha = 0.1 * (V + 40) / (1 - bm.exp(-(V + 40) / 10))    beta = 4.0 * bm.exp(-(V + 65) / 18)    dmdt = alpha * (1 - m) - beta * m    return self.phi * dmdtdef dh(self, h, t, V):    alpha = 0.07 * bm.exp(-(V + 65) / 20.)    beta = 1 / (1 + bm.exp(-(V + 35) / 10))    dhdt = alpha * (1 - h) - beta * h    return self.phi * dhdtdef dn(self, n, t, V):    alpha = 0.01 * (V + 55) / (1 - bm.exp(-(V + 55) / 10))    beta = 0.125 * bm.exp(-(V + 65) / 80)    dndt = alpha * (1 - n) - beta * n    return self.phi * dndtComplete the update() functiondef update(self, x=None):    t = bp.share.load('t')    dt = bp.share.load('dt')    # TODO: æ›´æ–°å˜é‡V, m, h, n, æš‚å­˜åœ¨V, m, h, nä¸­    V, m, h, n = self.integral(self.V, self.m, self.h, self.n, t, self.input, dt=dt)    #åˆ¤æ–­æ˜¯å¦å‘ç”ŸåŠ¨ä½œç”µä½    self.spike.value = bm.logical_and(self.V &lt; self.V_th, V &gt;= self.V_th)    # æ›´æ–°æœ€åŽä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´    self.t_last_spike.value = bm.where(self.spike, t, self.t_last_spike)    # TODO: æ›´æ–°å˜é‡V, m, h, nçš„å€¼    self.V.value = V    self.m.value = m    self.h.value = h    self.n.value = n    #é‡ç½®è¾“å…¥    self.input[:] = 0Simulationcurrent, length = bp.inputs.section_input(values=[0., bm.asarray([1., 2., 4., 8., 10., 15.]), 0.],                                          durations=[10, 2, 25],                                          return_length=True)hh_neurons = HH(current.shape[1])runner = bp.DSRunner(hh_neurons, monitors=['V', 'm', 'h', 'n'], inputs=('input', current, 'iter'))runner.run(length)Visualizationimport numpy as npimport matplotlib.pyplot as pltbp.visualize.line_plot(runner.mon.ts, runner.mon.V, ylabel='V (mV)', plot_ids=np.arange(current.shape[1]))plt.plot(runner.mon.ts, bm.where(current[:, -1]&gt;0, 10, 0) - 90.)plt.figure()plt.plot(runner.mon.ts, runner.mon.m[:, -1])plt.plot(runner.mon.ts, runner.mon.h[:, -1])plt.plot(runner.mon.ts, runner.mon.n[:, -1])plt.legend(['m', 'h', 'n'])plt.xlabel('Time (ms)')Customize a conductance-based modelç”µè·¯æ¨¡æ‹Ÿï¼Œå†™æˆç”µå¯¼å½¢å¼\(\begin{aligned}\text{gK}&amp; =\bar{g}_\text{K}n^4,  \\\frac{\mathrm{d}n}{\mathrm{d}t}&amp; =\phi[\alpha_n(V)(1-n)-\beta_n(V)n], \end{aligned}\)åŠ¨åŠ›å­¦å½¢å¼æè¿°ï¼Œå¼•å…¥é—¨æ¡†å˜é‡$n$\(\begin{aligned}&amp;\alpha_{n}(V) =\frac{0.01(V+55)}{1-\exp(-\frac{V+55}{10})},  \\&amp;\beta_{n}(V) =0.125\exp\left(-\frac{V+65}{80}\right). \end{aligned}\)ç”±æ­¤å¼æ¥å»ºæ¨¡é’¾ç¦»å­é€šé“Programming an ion channelThree ion channelimport brainpy as bpimport brainpy.math as bmclass IK(bp.dyn.IonChannel):  def __init__(self, size, E=-77., g_max=36., phi=1., method='exp_auto'):    super(IK, self).__init__(size)    self.g_max = g_max    self.E = E    self.phi = phi    self.n = bm.Variable(bm.zeros(size))  # variables should be packed with bm.Variable        self.integral = bp.odeint(self.dn, method=method)  def dn(self, n, t, V):    alpha_n = 0.01 * (V + 55) / (1 - bm.exp(-(V + 55) / 10))    beta_n = 0.125 * bm.exp(-(V + 65) / 80)    return self.phi * (alpha_n * (1. - n) - beta_n * n)  def update(self, V):    t = bp.share.load('t')    dt = bp.share.load('dt')    self.n.value = self.integral(self.n, t, V, dt=dt)  def current(self, V):    return self.g_max * self.n ** 4 * (self.E - V)class INa(bp.dyn.IonChannel):  def __init__(self, size, E= 50., g_max=120., phi=1., method='exp_auto'):    super(INa, self).__init__(size)    self.g_max = g_max    self.E = E    self.phi = phi    self.m = bm.Variable(bm.zeros(size))  # variables should be packed with bm.Variable    self.h = bm.Variable(bm.zeros(size))        self.integral_m = bp.odeint(self.dm, method=method)    self.integral_h = bp.odeint(self.dh, method=method)  def dm(self, m, t, V):    # TODO: è®¡ç®—dm/dt    alpha_m = 0.11 * (V + 40) / (1 - bm.exp(-(V + 40) / 10))    beta_m = 4 * bm.exp(-(V + 65) / 18)    return self.phi * (alpha_m * (1. - m) - beta_m * m)  def dh(self, h, t, V):    # TODO: è®¡ç®—dh/dt    alpha_h = 0.07 * bm.exp(-(V + 65) / 20)    beta_h = 1. / (1 + bm.exp(-(V + 35) / 10))    return self.phi * (alpha_h * (1. - h) - beta_h * h)  def update(self, V):    t = bp.share.load('t')    dt = bp.share.load('dt')    # TODO: æ›´æ–°self.m, self.h    self.m.value = self.integral_m(self.m, t, V, dt=dt)    self.h.value = self.integral_h(self.h, t, V, dt=dt)  def current(self, V):    return self.g_max * self.m ** 3 * self.h * (self.E - V)class IL(bp.dyn.IonChannel):  def __init__(self, size, E=-54.39, g_max=0.03):    super(IL, self).__init__(size)    self.g_max = g_max    self.E = E  def current(self, V):    return self.g_max * (self.E - V)  def update(self, V):    passBuild a HH model with ion channelsUsing customized ion channelsclass HH(bp.dyn.CondNeuGroup):  def __init__(self, size):    super(HH, self).__init__(size, V_initializer=bp.init.Uniform(-80, -60.))    # TODO: åˆå§‹åŒ–ä¸‰ä¸ªç¦»å­é€šé“    self.IK = IK(size, E=-77., g_max=36.)    self.INa = INa(size, E=50., g_max=120.)    self.IL = IL(size, E=-54.39, g_max=0.03)Using built-in ion channelsclass HH(bp.dyn.CondNeuGroup):    def __init__(self, size):        super().__init__(size)                self.INa = bp.channels.INa_HH1952(size)        self.IK = bp.channels.IK_HH1952(size)        self.IL = bp.cahnnels.IL(size, E=-54.387, g_max=0.03)Simulationneu = HH(1)runner = bp.DSRunner(    neu,     monitors=['V', 'IK.n', 'INa.m', 'INa.h'],     inputs=('input', 1.698)  # near the threshold current)runner.run(200)  # the running time is 200 msimport matplotlib.pyplot as pltplt.plot(runner.mon['ts'], runner.mon['V'])plt.xlabel('t (ms)')plt.ylabel('V (mV)')plt.savefig("HH.jpg")plt.show()plt.figure(figsize=(6, 2))plt.plot(runner.mon['ts'], runner.mon['IK.n'], label='n')plt.plot(runner.mon['ts'], runner.mon['INa.m'], label='m')plt.plot(runner.mon['ts'], runner.mon['INa.h'], label='h')plt.xlabel('t (ms)')plt.legend()plt.savefig("HH_channels.jpg")plt.show()Simple Neuron Modeling: Simplified ModelsThe Leaky Integrate-and-Fire(LIF) Neuron ModelThe LIF neuron model\[\begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-(V-V_{\mathrm{rest}})+RI(t)\\\\\mathrm{if}V&amp;&gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\text{last}\ {t_{ref}}\end{aligned}\]åªæœ‰ä¸€ä¸ªå¾®åˆ†æ–¹ç¨‹ï¼Œè¦åŠ ä¸€ä¸ªä¸åº”æœŸ(t refractory period)ï¼Œè†œç”µä½ä¸å‘ç”Ÿä»»ä½•æ”¹å˜ï¼Œè®¤ä¸ºç¦»å­é€šé“åªæœ‰æ³„éœ²é€šé“Given a constant current input:æ²¡æœ‰å»ºæ¨¡å‡†ç¡®å˜åŒ–ï¼Œåªæä¾›ä»€ä¹ˆæ—¶å€™è†œç”µä½çš„å˜åŒ–The dynamic features of the LIF modelGeneral solution (constant input):$V(t)=V_{\text{reset}}+RI_{\text{c}}(1-\mathrm{e}^{-\frac{t-t_0}{\tau}})$Firing frequency:\(\begin{aligned}T&amp;=-\tau\ln\left(1-\frac{V_{\phi h}-V_{\mathrm{rest}}}{RI_{\varsigma}}\right)\\f&amp;=\frac{1}{T+t_{\mathrm{ref}}}=\frac{1}{t_{\mathrm{ref}}-\tau\ln\left(1-\frac{V_{0}-V_{\mathrm{rest}}}{RI_{\varsigma}}\right)}\end{aligned}\)Rheobase current (minimal current):\(I_{\theta}=\frac{V_{\mathrm{th}}-V_{\mathrm{reset}}}{R}\)åŸºå¼ºç”µæµï¼Œå¦‚æžœå°äºŽå®ƒå°†æ— æ³•å‘æ”¾Strengths &amp; weaknesses of the LIF modelStrengths  Simple, high simulation efficiency  Intuitive  Fits well the subthreshold membrane potentialWeaknesses  The shape of action potentials is over-simplified  Has no memory of the spiking history  Cannot reproduce diverse firing patternsOther Univariate neuron modelsThe Quadratic Integrate-and-Fire (QOF) model:\[\begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=a_{0}(V-V_{\mathrm{re}t})(V-V_{\mathrm{c}})+RI(t)\\&amp;\text{if }V&gt;\theta,\quad V\leftarrow V_{\mathrm{re}set}\quad\text{last}\quad t_{\mathrm{ref}}\end{aligned}\]è†œç”µä½ä»éœ€è¦æ‰‹åŠ¨é‡ç½®The Theta neuron model\[\frac{\mathrm{d}\theta}{\mathrm{d}t}=1-\cos\theta+(1+\cos\theta)(\beta+I(t))\]éšå¼è¡¨è¾¾ï¼Œä¸å…·æœ‰ç‰©ç†æ„ä¹‰ï¼Œä½†ä¹Ÿä¼šè¿›è¡Œæ•´åˆå‘æ”¾The Exponential Integrate-and-Fire (ExpIF) model\[\begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-\left(V-V_{\mathrm{res}t}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{3T}}+RI(t)\\\mathrm{if~}V&amp;&gt;\theta,\quad V\leftarrow V_{\mathrm{res}t}\mathrm{last}t_{\mathrm{ref}}\end{aligned}\]ä»éœ€è¦æ‰‹åŠ¨é‡ç½®è†œç”µä½The Adaptive Exponential Integrate-and-Fire(AdEx) Neuron ModelThe AdEx neuron modelTwo variables:  ð‘‰: membrane potential  ð‘¤: adaptation variable\[\begin{aligned}\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{S_{T}}}-Rw+RI(t)  \\\tau_{w}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{\mathrm{w}}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right)  \\\mathrm{if}V&amp; &gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} \end{aligned}\]ä¸ä¸ºé›¶ï¼Œå°±ä¼šè¡°å‡åˆ°$-w$  A larger ð‘¤ suppresses ð‘‰ from increasing  ð‘¤ decays exponentially while having a sudden increase when the neuron firesFiring patterns of the AdEx modelCategorization of firing patternsAccording to the steady-state firing time intervals:  Tonic/regular spiking  Adapting  Bursting  Irregular spikingAccording to the initial-state features:  Tonic/classic spiking  Initial burst  Delayed spikingOther multivariate neuron modelsThe Izhikevich model\[\begin{aligned}&amp;\frac{dV}{dt} =0.04V^{2}+5V+140-u+I  \\&amp;\frac{\mathrm{d}u}{\mathrm{d}t} =a\left(bV-u\right)  \\&amp;\operatorname{if}V &gt;\theta,\quad V\leftarrow c,u\leftarrow u+d\text{ last }t_{\mathrm{ref}} \end{aligned}\]äºŒæ¬¡æ•´åˆå‘æ”¾å¤šåŠ äº†ä¸€ä¸ª$u$The FitzHughâ€“Nagumo (FHN) model\[\begin{aligned}\dot{v}&amp;=v-\frac{v^3}3-w+RI_{\mathrm{ext}}\\\tau\dot{w}&amp;=v+a-bw.\end{aligned}\]æ²¡æœ‰å¯¹è†œç”µä½è¿›è¡Œäººä¸ºçš„é‡ç½®ï¼Œå¯ä»¥æ›´å¥½çš„è¿›è¡ŒåŠ¨åŠ›å­¦åˆ†æžï¼Œæ²¡æœ‰æ‰“ç ´å¾®åˆ†æ–¹ç¨‹çš„è¿žç»­æ€§The Generalized Integrate-and-Fire (GIF) modeln+2ä¸ªå˜é‡\(\begin{aligned}&amp;\tau{\frac{\mathrm{d}V}{\mathrm{d}t}} =-\left(V-V_{\mathrm{rest}}\right)+R\sum_{j}I_{j}+RI  \\&amp;\frac{\mathrm{d}\Theta}{\mathrm{d}t} =a\left(V-V_{\mathrm{rest}}\right)-b\left(\Theta-\Theta_{\infty}\right)  \\&amp;\frac{\mathrm{d}l_{j}}{\mathrm{d}t} =-k_{j}I_{j},\quad j=1,2,...,n  \\&amp;\operatorname{if}V &gt;\Theta,\quad I_{j}\leftarrow R_{j}I_{j}+A_{j},V\leftarrow V_{\mathrm{reset}},\Theta\leftarrow max(\Theta_{\mathrm{reset}},\Theta) \end{aligned}\)æ¯ä¸ªå˜é‡éƒ½æ˜¯çº¿æ€§çš„ï¼Œæ³›åŒ–æ€§ä½“çŽ°åœ¨é‡ç½®æ¡ä»¶ä¸ŠDynamic analysis: phase-plane analysisPhase plane analysiså¯¹åŠ¨åŠ›å­¦ç³»ç»Ÿçš„è¡Œä¸ºæ¥åˆ†æžï¼Œæ™®éå¯¹ä¸¤ä¸ªå˜é‡æ¥è¿›è¡Œåˆ†æžAnalyzes the behavior of a dynamical system with (usually two) variables described by ordinary differential equations\(\begin{aligned}&amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp;&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{S_{T}}}-Rw+RI(t)  \\&amp;\tau_{W}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp;&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right)  \\&amp;\mathrm{if}V&amp;&amp; &gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} \end{aligned}\)Elements:  Nullclines: $\mathrm{d}V/\mathrm{d}t=0;\mathrm{d}w/\mathrm{d}t=0$  Fixed points: $\mathrm{d}V/\mathrm{d}t=0\mathrm{~and~}\mathrm{d}w/\mathrm{d}t=0$  The vector field  The trajectory of variableså‡è®¾å¤–éƒ¨ç”µæµæ’å®šPhase plane analysis for the AdEx neuron model\[\begin{aligned}&amp;\tau_{m}{\frac{\mathrm{d}V}{\mathrm{d}t}}&amp;&amp; =-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{\Lambda_{T}}}-Rw+RI(t)  \\&amp;\tau_{w}{\frac{\mathrm{d}w}{\mathrm{d}t}}&amp;&amp; =a\left(V-V_{\mathrm{rest}}\right)-w+b\tau_{w}\sum_{t^{(f)}}\delta\left(t-t^{(f)}\right)  \\&amp;\text{ifV}&amp;&amp; &gt;\theta,\quad V\leftarrow V_\mathrm{reset}\text{ last }t_\mathrm{ref} \end{aligned}\]TonicAdaptationBurstingTransient spikingDynamic analysis: bifurcation analysisBifurcation analysisQuantitative analysis of the existence and the properties of fixed points in a dynamical system with a changing parameteræŸä¸ªå¤–ç•Œæ¡ä»¶å˜åŒ–æ—¶ï¼Œå›ºå®šç‚¹çš„å˜åŒ–Elements:  Lines of fixed points  Stability properties of fixed pointsBifurcation analysis for the AdEx Neuron modelbifurcation analysis for 2 variablesVariables: ð‘‰ and ð‘¤Parameters: $I_{ext}$\(\begin{align}\tau_{m} \frac{\mathrm{d}V}{\mathrm{d}t} &amp;= -\left(V-V_{\mathrm{rest}}\right) + \Delta_{T} \mathrm{e}^{\frac{V-V_{T}}{ST}} - Rw + RI(t) \\-\frac{\mathrm{d}w}{\mathrm{d}t} &amp;= a(V-V_{\mathrm{rest}}) - w + b\tau_{w} \sum_{t^{(f)}} \delta(t-t^{(f)}) \\\text{if } V &gt; \theta, \quad V &amp;\leftarrow V_{\mathrm{reset}}, \quad \text{last } t_{\mathrm{ref}}\end{align}\)Subjects: two variables (ð‘‰ and ð‘¤)Extended: The limit cycleThe FitzHughâ€“Nagumo (FHN) model\(\begin{aligned}\dot{v}&amp;=v-\frac{v^3}3-w+RI_\mathrm{ext}\\\tau\dot{w}&amp;=v+a-bw.\end{aligned}\)This dynamical system, in certain conditions, exhibits a cyclic pattern of variable changes which can be visualized as a closed trajectory in the phase plane.å˜åŒ–é”å®šåˆ°çŽ¯ä¸­Reduced Models - brain dynamics programmingLIF neuron models programmingDefine LIF class\[\begin{aligned}&amp;\tau\frac{\mathrm{d}V}{\mathrm{d}t}=-(V-V_{\mathrm{rest}})+RI(t)\\&amp;\text{if }V&gt;V_{\mathrm{th}},\quad V\leftarrow V_{\mathrm{reset}}\text{last}t_{\mathrm{ref}}\end{aligned}\]class LIF(bp.dyn.NeuDyn):    def __init__(self, size, V_rest=0, V_reset=-5, V_th=20, R=1, tau=10, t_ref=5., **kwargs):        # åˆå§‹åŒ–çˆ¶ç±»        super(LIF, self).__init__(size=size, **kwargs)Initializationclass LIF(bp.dyn.NeuDyn):    def __init__(self, size, V_rest=0, V_reset=-5, V_th=20, R=1, tau=10, t_ref=5., **kwargs):        # åˆå§‹åŒ–çˆ¶ç±»        super(LIF, self).__init__(size=size, **kwargs)                # åˆå§‹åŒ–å‚æ•°        self.V_rest = V_rest        self.V_reset = V_reset        self.V_th = V_th        self.R = R        self.tau = tau        self.t_ref = t_ref  # ä¸åº”æœŸæ—¶é•¿                # åˆå§‹åŒ–å˜é‡        self.V = bm.Variable(bm.random.randn(self.num) + V_reset)        self.input = bm.Variable(bm.zeros(self.num))        self.t_last_spike = bm.Variable(bm.ones(self.num) * -1e7)  # ä¸Šä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´        self.refractory = bm.Variable(bm.zeros(self.num, dtype=bool))  # æ˜¯å¦å¤„äºŽä¸åº”æœŸ        self.spike = bm.Variable(bm.zeros(self.num, dtype=bool))  # è„‰å†²å‘æ”¾çŠ¶æ€                # ä½¿ç”¨æŒ‡æ•°æ¬§æ‹‰æ–¹æ³•è¿›è¡Œç§¯åˆ†        self.integral = bp.odeint(f=self.derivative, method='exponential_euler')Define the derivative function# å®šä¹‰è†œç”µä½å…³äºŽæ—¶é—´å˜åŒ–çš„å¾®åˆ†æ–¹ç¨‹def derivative(self, V, t, Iext):    dVdt = (-V + self.V_rest + self.R * Iext) / self.tau    return dVdtComplete the update() functiondef update(self):    t, dt = bp.share['t'], bp.share['dt']    # ä»¥æ•°ç»„çš„æ–¹å¼å¯¹ç¥žç»å…ƒè¿›è¡Œæ›´æ–°    refractory = (t - self.t_last_spike) &lt;= self.t_ref  # åˆ¤æ–­ç¥žç»å…ƒæ˜¯å¦å¤„äºŽä¸åº”æœŸ    V = self.integral(self.V, t, self.input, dt=dt)  # æ ¹æ®æ—¶é—´æ­¥é•¿æ›´æ–°è†œç”µä½    V = bm.where(refractory, self.V, V)  # è‹¥å¤„äºŽä¸åº”æœŸï¼Œåˆ™è¿”å›žåŽŸå§‹è†œç”µä½self.Vï¼Œå¦åˆ™è¿”å›žæ›´æ–°åŽçš„è†œç”µä½V    spike = V &gt; self.V_th  # å°†å¤§äºŽé˜ˆå€¼çš„ç¥žç»å…ƒæ ‡è®°ä¸ºå‘æ”¾äº†è„‰å†²    self.spike[:] = spike  # æ›´æ–°ç¥žç»å…ƒè„‰å†²å‘æ”¾çŠ¶æ€    self.t_last_spike[:] = bm.where(spike, t, self.t_last_spike)  # æ›´æ–°æœ€åŽä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´    self.V[:] = bm.where(spike, self.V_reset, V)  # å°†å‘æ”¾äº†è„‰å†²çš„ç¥žç»å…ƒè†œç”µä½ç½®ä¸ºV_resetï¼Œå…¶ä½™ä¸å˜    self.refractory[:] = bm.logical_or(refractory, spike)  # æ›´æ–°ç¥žç»å…ƒæ˜¯å¦å¤„äºŽä¸åº”æœŸ    self.input[:] = 0.  # é‡ç½®å¤–ç•Œè¾“å…¥Simulationdef run_LIF():  # è¿è¡ŒLIFæ¨¡åž‹  group = LIF(1)  runner = bp.DSRunner(group, monitors=['V'], inputs=('input', 22.))  runner(200)  # è¿è¡Œæ—¶é•¿ä¸º200ms  # ç»“æžœå¯è§†åŒ–  fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6)  ax = fig.add_subplot(gs[0, 0])  plt.plot(runner.mon.ts, runner.mon.V)  plt.xlabel(r'$t$ (ms)')  plt.ylabel(r'$V$ (mV)')  ax.spines['top'].set_visible(False)  ax.spines['right'].set_visible(False)  plt.show()Input current &amp; firing frequency\[\begin{gathered}V(t)=V_{\mathrm{reset}}+RI_{\mathrm{c}}(1-\mathrm{e}^{-\frac{t-t_{0}}{\tau}}). \\T=-\tau\ln\left[1-\frac{V_{\mathrm{th}}-V_{\mathrm{rest}}}{RI_{\mathrm{c}}}\right] \\f={\frac{1}{T+t_{\mathrm{ref}}}}={\frac{1}{t_{\mathrm{ref}}-\tau\ln\left[1-{\frac{V_{\mathrm{th}}-V_{\mathrm{rest}}}{RI_{c}}}\right]}} \end{gathered}\]# è¾“å…¥ä¸Žé¢‘çŽ‡çš„å…³ç³»current = bm.arange(0, 600, 2)duration = 1000LIF_neuron = LIF(current.shape[0])runner_2 = bp.dyn.DSRunner(LIF_neurons, monitors=['spike'], inputs={'input', current}, dt=0.01)runner_2.run(duration)freqs = runner_2.mon.spike.sum(axis=0) / (duration/1000)plt.figure()plt.plot(current, freqs)plt.xlabel('inputs')plt.ylabel('frequencies')Other Univariate neuron modelsThe Quadratic Integrate-and-Fire (QIF) model\(\begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=a_{0}(V-V_{\mathrm{res}t})(V-V_{c})+RI(t)\\\mathrm{if~}V&amp;&gt;\theta,\quad V\leftarrow V_{\mathrm{reset~last~}t_{\mathrm{ref}}}\end{aligned}\)def derivative(self, V, t, I):    dVdt = (self.c * (V - self.V_reset) * (V - self.V_c) + self.R * I) / self.tau    return dVdtThe Exponential Integrate-and-Fire (ExpIF) model\(\begin{aligned}\tau\frac{\mathrm{d}V}{\mathrm{d}t}&amp;=-\left(V-V_{\mathrm{rest}}\right)+\Delta_{T}\mathrm{e}^{\frac{V-V_{T}}{\delta_{T}}}+RI(t)\\&amp;\mathrm{if~}V&gt;\theta,\quad V\leftarrow V_{\mathrm{reset}}\mathrm{last}t_{\mathrm{ref}}\end{aligned}\)def derivative(self, V, t, I):    exp_v = self.delta_T * bm.exp((V - self.V_T) / self.delta_T)    dvdt = (- (V - self.V_rest) + exp_v + self.R * I) / self.tau    return dvdtAdEx neuron models programming\[\begin{gather}\tau_{m} \frac{\mathrm{d}V}{\mathrm{d}t} = -(V-V_{\mathrm{rest}}) + \Delta_{T} \mathrm{e}^{\frac{V-V_{T}}{\Delta T}} - Rw + RI(t), \\\tau_{w} \frac{\mathrm{d}w}{\mathrm{d}t} = a(V-V_{\mathrm{rest}}) - w + b\tau_{w} \sum_{t^{(f)}} \delta(t-t^{(f)}), \\\text{if } V &gt; V_{\mathrm{th}}, \quad V \leftarrow V_{\mathrm{reset}}, \quad \text{last } t_{\mathrm{ref}}. \end{gather}\]Define AdEx classclass AdEx(bp.dyn.NeuDyn):    def __init__(self, size,                V_rest=-65, V_reset=-68, V_th=-30, V_T=-59.9, delta_T=3.48                a=1., b=1., R=1., tau=10., tau_w=30., tau_ref=0.,                **kwargs):        # åˆå§‹åŒ–çˆ¶ç±»        super(AdEx, self).__init__(size=size, **kwargs)Initializationclass AdEx(bp.dyn.NeuDyn):    def __init__(self, size,                V_rest=-65, V_reset=-68, V_th=-30, V_T=-59.9, delta_T=3.48                a=1., b=1., R=1., tau=10., tau_w=30., tau_ref=0.,                **kwargs):        # åˆå§‹åŒ–çˆ¶ç±»        super(AdEx, self).__init__(size=size, **kwargs)                # åˆå§‹åŒ–å‚æ•°        self.V_rest = V_rest        self.V_reset = V_reset        self.V_th = V_th        self.V_T = V_T        self.delta_T = delta_T        self.a = a        self.b = b        self.R = R        self.tau = tau        self.tau_w = tau_w                self.tau_ref = tau_ref                # åˆå§‹åŒ–å˜é‡        self.V = bm.Variable(bm.random.randn(self.num) - 65.)        self.w = bm.Variable(bm.zeros(self.num))        self.input = bm.Variable(bm.zeros(self.num))        self.t_last_spike = bm.Variable(bm.ones(self.num) * -1e7)  # ä¸Šä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´        self.refractory = bm.Variable(bm.zeros(self.num, dtype=bool))  # æ˜¯å¦å¤„äºŽä¸åº”æœŸ        self.spike = bm.Variable(bm.zeros(self.num, dtype=bool))  # è„‰å†²å‘æ”¾çŠ¶æ€                # å®šä¹‰ç§¯åˆ†å™¨        self.integral = bp.odeint(f=self.derivative, method='exp_auto')Define the derivative functiondef dV(self, V, t, w, I):	exp = self.delta_T * bm.exp((V - self.V_T) / self.delta_T)    dVdt = (-V + self.V_rest + exp - self.R * w + self.R * I) / self.tau    return dVdtdef dw(self, w, t, V):    dwdt = (self.a * (V - self.V_rest) - w) / self.tau_w    return dwdt@propertydef derivative(self):    return bp.JointEq([self.dV, self.dw])Complete the update() functiondef update(self):    t, dt = bp.share['t'], bp.share['dt']    V, w = self.integral(self.V.value, self.w.value, t, self.input, dt=dt)    # ä»¥æ•°ç»„çš„æ–¹å¼å¯¹ç¥žç»å…ƒè¿›è¡Œæ›´æ–°    refractory = (t - self.t_last_spike) &lt;= self.t_ref  # åˆ¤æ–­ç¥žç»å…ƒæ˜¯å¦å¤„äºŽä¸åº”æœŸ    V = bm.where(refractory, self.V, V)  # è‹¥å¤„äºŽä¸åº”æœŸï¼Œåˆ™è¿”å›žåŽŸå§‹è†œç”µä½self.Vï¼Œå¦åˆ™è¿”å›žæ›´æ–°åŽçš„è†œç”µä½V    spike = V &gt; self.V_th  # å°†å¤§äºŽé˜ˆå€¼çš„ç¥žç»å…ƒæ ‡è®°ä¸ºå‘æ”¾äº†è„‰å†²    self.spike[:] = spike  # æ›´æ–°ç¥žç»å…ƒè„‰å†²å‘æ”¾çŠ¶æ€    self.t_last_spike[:] = bm.where(spike, t, self.t_last_spike)  # æ›´æ–°æœ€åŽä¸€æ¬¡è„‰å†²å‘æ”¾æ—¶é—´    self.V[:] = bm.where(spike, self.V_reset, V)  # å°†å‘æ”¾äº†è„‰å†²çš„ç¥žç»å…ƒè†œç”µä½ç½®ä¸ºV_resetï¼Œå…¶ä½™ä¸å˜    self.w[:] = bm.where(spike, w + self.b, w)  #æ›´æ–°è‡ªé€‚åº”ç”µæµ    self.refractory[:] = bm.logical_or(refractory, spike)  # æ›´æ–°ç¥žç»å…ƒæ˜¯å¦å¤„äºŽä¸åº”æœŸ    self.input[:] = 0.  # é‡ç½®å¤–ç•Œè¾“å…¥SimulationOther multivariate neuron modelsThe Izhikevich model\(\begin{aligned}&amp;\frac{dV}{dt} =0.04V^{2}+5V+140-u+I  \\&amp;\frac{\mathrm{d}u}{\mathrm{d}t} =a\left(bV-u\right)  \\&amp;\operatorname{if}V &gt;\theta,\quad V\leftarrow c,u\leftarrow u+d\mathrm{last}t_{\mathrm{ref}} \end{aligned}\)def dV(self, V, t, u, I):    dVdt = 0.04 * V * V + 5 * V + 140 - u + I    return dVdtdef du(self, u, t, V):    dudt = self.a * (self.b * V - u)    return dudtThe Generalized Integrate-and-Fire (GIF) model\(\begin{aligned}&amp;\tau{\frac{\mathrm{d}V}{\mathrm{d}t}} =-\left(V-V_{\mathrm{rest}}\right)+R\sum_{j}I_{j}+RI  \\&amp;\frac{\mathrm{d}\Theta}{\mathrm{d}t} =a\left(V-V_{\mathrm{est}}\right)-b\left(\Theta-\Theta_{\infty}\right)  \\&amp;\frac{\mathrm{d}I_j}{\mathrm{d}r} =-k_jI_j,\quad j=1,2,\ldots,n  \\&amp;\text{if V} &gt;\Theta,\quad I_{j}\leftarrow R_{j}I_{j}+A_{j},V\leftarrow V_{\mathrm{reset}},\Theta\leftarrow max\left(\Theta_{\mathrm{reset}},\Theta\right) \end{aligned}\)def dI1(self, I1, t):    return - self.k1 * I1def dI2(self, I2, t):    return - self.k2 * I2def dVth(self, V_th, t, V):    return self.a * (V - self.v_rest) - self.b * (V_th - self.V_th_inf)def dV(self, V, t, I1, I2, I):    return (- (V - self.V_rest) + self.R * (I + I1 + I2)) / self.tauBuilt-in reduced neuron modelsDynamic analysis: phase-plane analysisSimple case\[\frac{dx}{dt}=\sin(x)+I,\]@bp.odeintdef int_x(x, t, Iext):	return bp.math.sin(x) + Iextpp = bp.analysis.PhasePlane1D(	model=int_x,	target_vars={'x': [-10, 10]},	pars_update={'Iext': 0.},    resolutions={'x': 0.01})pp.plot_vector_field()pp.plot_fixed_point(show=True)  Nullcline: The zero-growth isoclines, such as $f(x,y) = 0$ and $g(x,y) = 0$  Fixed points: The equilibrium points of the system, which are located at all the nullclines intersect.  Vector field: The vector field of the system.  Limit cycles: The limit cycles.  Trajectories: A simulation trajectory with the given initial valuesPhase plane analysis for AdExdef ppa_AdEx(group):    bm.enable_x64()        v_range = [-70., -40.]    w_range = [-10., 50.]        phase_plane_analyzer = bp.analysis.PhasePlane2D(        model=group,        target_vars={'V': v_range, 'w': w_range, },  # å¾…åˆ†æžå˜é‡        pars_update={'I': Iext},  # éœ€è¦æ›´æ–°çš„å˜é‡        resolutions=0.05    )    # ç”»å‡ºV, wçš„é›¶å¢žé•¿æ›²çº¿    phase_plane_analyzer.plot_nullcline()    # ç”»å‡ºå¥‡ç‚¹    phase_plane_analyzer.plot_fixed_point()    # ç”»å‡ºå‘é‡åœº    phase_plane_analyzer.plot_vector_field()        # åˆ†æ®µç”»å‡ºV, wçš„å˜åŒ–è½¨è¿¹    group.V[:], group.w[:] = group.V_reset, 0    runner = bp.DSRunner(group, monitors=['V', 'w', 'spike'], inputs=('input', Iext))    runner(500)    spike = runner.mon.spike.squeeze()    s_idx = np.where(spike)[0]  # æ‰¾åˆ°æ‰€æœ‰å‘æ”¾åŠ¨ä½œç”µä½å¯¹åº”çš„index    s_idx = np.concatenate(([0], s_idx, [len(spike) - 1]))  # åŠ ä¸Šèµ·å§‹ç‚¹å’Œç»ˆæ­¢ç‚¹çš„index    for i in range(len(s_idx) - 1):        vs = runner.mon.V[s_idx[i]: s_idx[i + 1]]        ws = runner.mon.w[s_idx[i]: s_idx[i + 1]]        plt.plot(vs, ws, color='darkslateblue')            # ç”»å‡ºè™šçº¿ x = V_reset    plt.plot([group.V_reset, group.V_reset], w_range, '--', color='grey', zorder=-1)        plt.show()Dynamic analysis: bifurcation analysisSimple case\[\frac{dx}{dt}=\sin(x)+I,\]bif = bp.analysis.Bifurcation1D(	model=int_x,	target_vars={'x': [-10, 10]},	target_pars={'Iext': [0., 1.5]},	resolutions={'Iext': 0.005, 'x': 0.05})bif.plot_bifurcation(show=True)Synapse models and their programmingThe biology of synapsesNeurotransmitter &amp; SynapseWhen the action potential invades the axon terminals, it causes voltage-gated ð¶ð¶ð‘Žð‘Ž 2+ channels to open (1), which triggers vesicles to bind to the presynaptic membrane (2). Neurotransmitter is released into the synaptic cleft by exocytosis and diffuses across the cleft (3). Binding of the neurotransmitter to receptor molecules in the postsynaptic membrane completes the process of transmission (4).åŽ»æžåŒ–æ—¶é’™ç¦»å­å†…æµï¼Œä¸Žå›Šæ³¡ç›¸ç»“åˆï¼Œâ€¦ï¼Œä¸Žå—ä½“ç»“åˆï¼Œæ‰“å¼€ç¦»å­é€šé“ï¼Œè¶…æžåŒ–ã€åŽ»æžåŒ–çŽ°è±¡Neurotransmitter leading to postsynaptic potential.The binding of neurotransmitter to the postsynaptic membrane receptors changes the membrane potential ($V_m$). These postsynaptic potentials can be either excitatory (depolarizing the membrane), as shown here, or inhibitory (hyperpolarizing the membrane).Neurotransmitterså…´å¥‹æ€§ç¥žç»é€’è´¨ï¼š  ä¹™é…°èƒ†ç¢± (ACh)  å„¿èŒ¶é…šèƒº (catecholamines)  è°·æ°¨é…¸ (glutamate)  ç»„èƒº (histamine)  5-ç¾Ÿè‰²èƒº (serotonin)  æŸäº›ç¥žç»è‚½ç±» (some of neuropeptides)æŠ‘åˆ¶æ€§ç¥žç»é€’è´¨ï¼š  GABA  ç”˜æ°¨é…¸ (glycine)  æŸäº›ç¥žç»è‚½ç±» (some of peptides)The postsynaptic responseThe aim of a synapse model is to describe accurately the postsynaptic response generated by the arrival of an action potential at a presynaptic terminal.  The fundamental quantity to be modelled is the time course of the postsynaptic receptor conductance  The models:          Simple phenomenological waveforms      More complex kinetic schemes that are analogous to the models of membrane- bound ion channels      å»ºæ¨¡è¿™ç§å“åº”æ¨¡å¼ï¼Œæ‰“å¼€å…³é—­çš„æ¦‚çŽ‡â€¦Phenomenological synapse modelsExponential ModelAssumption:  The release of neurotransmitter, its diffusion across the cleft, the receptor binding, and channel opening all happen very quickly, so that the channels instantaneously jump from the closed to the open state. channelä¼šçž¬é—´å¢žåŠ ç„¶åŽé€æ¸å…³é—­\[g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}e^{-(t-t_{0})/\tau}\\\begin{matrix}\bullet&amp;\tau \ \text{is the time constant}\\\bullet&amp;t_0 \ \text{is the time of the pre-synaptic spike}\\\bullet&amp;\bar{g_{syn}}\ \text{is the maximal conductance}\end{matrix}\]-&gt; corresponding differential equation\(\tau\frac{dg_{\mathrm{syn}}(t)}{dt}=-g_{\mathrm{syn}}(t)+\bar{g}_{\mathrm{syn}}\delta\left(t_{0}-t\right)\)  Can fit with experimental data.  A good approximation for GABA A and AMPA, because the rising phase is much shorter than their decay phase.Dual Exponential Modelexponential modelä¸Šå‡çš„å¤ªå¿«ï¼Œä¸å¤ªç¬¦åˆæŸäº›synapseDual exponential synapse provides a general way to describe the synaptic conductance with different rising and decay time constants.\(g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}\frac{\tau_{1}\tau_{2}}{\tau_{1}-\tau_{2}}\left(\exp\left(-\frac{t-t_{0}}{\tau_{1}}\right)-\exp\left(-\frac{t-t_{0}}{\tau_{2}}\right)\right)\\\begin{matrix}\bullet &amp;t_1\ \text{is the decay synaptic time constant} \\\bullet &amp;\tau_2\ \text{is the rise synaptic time constant} \\\bullet &amp;t_0\ \text{is the time of the pre-synaptic spike} \\\bullet &amp;\bar{g}_{syn}\ \text{is the maximal conductance}\end{matrix}\)-&gt;corresponding differential equation\(\begin{aligned}&amp;g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}}g \\&amp;\frac{dg}{dt}=-\frac{g}{\tau_{\mathrm{decay}}}+h \\&amp;\frac{dh}{dt}&amp; =-\frac{h}{\tau_{\mathrm{rise}}}+\delta\left(t_{0}-t\right), \end{aligned}\)The time course of most synaptic conductance can be well described by this sum of two exponentials.Synaptic time constantshttp://compneuro.uwaterloo.ca/research/constants-constraints/neurotransmitter-time-constants-pscs.htmlAMPA synapse  $t_{decay}$ = 0.18 ms in the auditory system of the chick nucleus magnocellularis (Trussell, 1999).  $t_{rise}$ 25 ms and $\tau_{decay}$ =0.77 ms in dentate gyrus basket cells (Geiger et al., 1997).  $t_{rise}$ = 0.2 ms and $\tau_{decay}$ =1.7 ms in in neocortical layer 5 pyramidal neurons (Hausser and Roth, 1997b).  Reversal potential is nearly 0 mV.NMDA synapse  The decay time constants (at near-physiological temperature):          19 ms in dentate gyrus basket cells (Geiger et al., 1997),      26 ms in neocortical layer 2/3 pyramidal neurons (Feldmeyer et al., 2002),      89 ms in CA1 pyramidal cells (Diamond, 2001).        The rise time constants are about 2 ms (Feldmeyer et al., 2002).  Reversal potential is nearly 0 mV.GABA$_A$ synapse  GABAergic synapses from dentate gyrus basket cells onto other basket cells are faster: $t_{rise}$ = 0.3 ms and $t_{decay}$ = 2.5 ms (Bartos et al., 2001) than synapses from basket cells to granule cells: $t_{rise}$ = 0.26 ms and $t_{decay}$ = 6.5 ms (Kraushaar and Jonas, 2000).  Reversal potential is nearly -80 mV.GABA$_B$ synapse  Common models use models with a rise time of about 25-50 ms, a fast decay time in the range of 100-300ms and a slow decay time of 500-1000 ms.  Reversal potential is nearly -90 mV.General property of synaptic time constants  The time constants of synaptic conductance vary widely among synapse types.  The synaptic kinetics tends to accelerate during development (T. Takahashi, Neuroscience Research, 2005) .  The synaptic kinetics becomes substantially faster with increasing temperature.Current- and Conductance-based ResponseConductance-based ResponseMost synaptic ion channels, such as AMPA and GABA, display an approximately linear current-voltage relationship when they open.For example:The synapse is located on a thin dendrite, because the local membrane potential V changes considerably when the synapse is activated.Current-based ResponseIn some case, we can also approximate the synapses as sources of current and not a conductance.For example:The excitatory synapse on a large compartment, because the depolarization of the membrane is small.Programming of phenomenological synapse modelsProjAlignPostMg2brainpy.dyn.ProjAlignPostMg2(     pre,     delay,     comm,     syn,     out,     post  )    pre (JointType[DynamicalSystem, AutoDelaySupp]): The pre-synaptic neuron group.  delay (Union[None, int, float]): The synaptic delay.  comm (DynamicalSystem): The synaptic communication.  syn (ParamDescInit): The synaptic dynamics.  out (ParamDescInit): The synaptic output.  post (DynamicalSystem) The post-synaptic neuron group.åªéœ€è¦å»ºæ¨¡æ‰€æœ‰postçš„neuronsCSR matrixExponential ModelThe single exponential decay synapse model assumes the release of neurotransmitter, its diffusion across the cleft, the receptor binding, and channel opening all happen very quickly, so that the channels instantaneously jump from the closed to the open state. Therefore, its expression is given by \(g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} e^{-\left(t-t_{0}\right) / \tau}\)where $\tau$ is the time constant, $t_0$ is the time of the pre-synaptic spike, $\bar{g}_{\mathrm{syn}}$ is the maximal conductance.The corresponding differential equation:\(\frac{d g}{d t} = -\frac{g}{\tau_{decay}}+\sum_{k} \delta(t-t_{j}^{k}).\)COBAGiven the synaptic conductance, the COBA model outputs the post-synaptic current with\(I_{syn}(t) = g_{\mathrm{syn}}(t) (E - V(t))\)class ExponSparseCOBA(bp.Projection):  def __init__(self, pre, post, delay, prob, g_max, tau, E):    super().__init__()        self.proj = bp.dyn.ProjAlignPostMg2(      pre=pre,       delay=delay,       comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max),      syn=bp.dyn.Expon.desc(post.num, tau=tau),      out=bp.dyn.COBA.desc(E=E),      post=post,     )class SimpleNet(bp.DynSysGroup):  def __init__(self, E=0.):    super().__init__()    self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.))    self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5.,                              V_initializer=bp.init.Constant(-60.))    self.syn = ExponSparseCOBA(self.pre, self.post, delay=None, prob=1., g_max=1., tau=5., E=E)      def update(self):    self.pre()    self.syn()    self.post()        # monitor the following variables    conductance = self.syn.proj.refs['syn'].g    current = self.post.sum_inputs(self.post.V)    return conductance, current, self.post.Vdef run_a_net(net):  indices = np.arange(1000)  # 100 ms  conductances, currents, potentials = bm.for_loop(net.step_run, indices, progress_bar=True)  ts = indices * bm.get_dt()    # --- similar to:   # runner = bp.DSRunner(net)  # conductances, currents, potentials = runner.run(100.)    fig, gs = bp.visualize.get_figure(1, 3, 3.5, 4)  fig.add_subplot(gs[0, 0])  plt.plot(ts, conductances)  plt.title('Syn conductance')  fig.add_subplot(gs[0, 1])  plt.plot(ts, currents)  plt.title('Syn current')  fig.add_subplot(gs[0, 2])  plt.plot(ts, potentials)  plt.title('Post V')  plt.show()CUBAGiven the conductance, this model outputs the post-synaptic current with a identity function:\(I_{\mathrm{syn}}(t) = g_{\mathrm{syn}}(t)\)class ExponSparseCUBA(bp.Projection):  def __init__(self, pre, post, delay, prob, g_max, tau):    super().__init__()        self.proj = bp.dyn.ProjAlignPostMg2(      pre=pre,       delay=delay,       comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max),      syn=bp.dyn.Expon.desc(post.num, tau=tau),      out=bp.dyn.CUBA.desc(),      post=post,     )class SimpleNet2(bp.DynSysGroup):  def __init__(self, g_max=1.):    super().__init__()        self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.))    self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5.,                              V_initializer=bp.init.Constant(-60.))    self.syn = ExponSparseCUBA(self.pre, self.post, delay=None, prob=1., g_max=g_max, tau=5.)      def update(self):    self.pre()    self.syn()    self.post()        # monitor the following variables    conductance = self.syn.proj.refs['syn'].g    current = self.post.sum_inputs(self.post.V)    return conductance, current, self.post.VDense connectionsExponential synapse model with the conductance-based (COBA) output current and dense connections.class ExponDenseCOBA(bp.Projection):  def __init__(self, pre, post, delay, prob, g_max, tau, E):    super().__init__()        self.proj = bp.dyn.ProjAlignPostMg2(      pre=pre,       delay=delay,       comm=bp.dnn.MaskedLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max),      syn=bp.dyn.Expon.desc(post.num, tau=tau),      out=bp.dyn.COBA.desc(E=E),      post=post,     )Exponential synapse model with the current-based (COBA) output current and dense connections.class ExponDenseCUBA(bp.Projection):  def __init__(self, pre, post, delay, prob, g_max, tau, E):    super().__init__()        self.proj = bp.dyn.ProjAlignPostMg2(      pre=pre,       delay=delay,       comm=bp.dnn.MaskedLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max),      syn=bp.dyn.Expon.desc(post.num, tau=tau),      out=bp.dyn.CUBA.desc(),      post=post,     )ProjAlignPreMg2Synaptic projection which defines the synaptic computation with the dimension of presynaptic neuron group.brainpy.dyn.ProjAlignPreMg2(     pre,     delay,     syn,     comm,     out,     post  )    pre (JointType[DynamicalSystem, AutoDelaySupp]): The pre-synaptic neuron group.  delay (Union[None, int, float]): The synaptic delay.  syn (ParamDescInit): The synaptic dynamics.  comm (DynamicalSystem): The synaptic communication.  out (ParamDescInit): The synaptic output.  post (DynamicalSystem) The post-synaptic neuron group.Dual Exponential ModelThe dual exponential synapse model, also named as difference of two exponentials model, is given by:\(g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} \frac{\tau_{1} \tau_{2}}{\tau_{1}-\tau_{2}}\left(\exp \left(-\frac{t-t_{0}}{\tau_{1}}\right)-\exp \left(-\frac{t-t_{0}}{\tau_{2}}\right)\right)\)where $\tau_1$ is the time constant of the decay phase, $\tau_2$ is the time constant of the rise phase, $t_0$ is the time of the pre-synaptic spike, $\bar{g}_{\mathrm{syn}}$ is the maximal conductance.The corresponding differential equation:\[\begin{aligned}  &amp;g_{\mathrm{syn}}(t)=\bar{g}_{\mathrm{syn}} g \\  &amp;\frac{d g}{d t}=-\frac{g}{\tau_{\mathrm{decay}}}+h \\  &amp;\frac{d h}{d t}=-\frac{h}{\tau_{\text {rise }}}+ \delta\left(t_{0}-t\right),  \end{aligned}\]The alpha function is retrieved in the limit when both time constants are equal.class DualExpSparseCOBA(bp.Projection):  def __init__(self, pre, post, delay, prob, g_max, tau_decay, tau_rise, E):    super().__init__()        self.proj = bp.dyn.ProjAlignPreMg2(      pre=pre,       delay=delay,       syn=bp.dyn.DualExpon.desc(pre.num, tau_decay=tau_decay, tau_rise=tau_rise),      comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max),      out=bp.dyn.COBA(E=E),      post=post,     )class SimpleNet4(bp.DynSysGroup):  def __init__(self, E=0.):    super().__init__()        self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.))    self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5.,                              V_initializer=bp.init.Constant(-60.))    self.syn = DualExpSparseCOBA(self.pre, self.post, delay=None, prob=1., g_max=1.,                                  tau_decay=5., tau_rise=1., E=E)      def update(self):    self.pre()    self.syn()    self.post()        # monitor the following variables    conductance = self.syn.proj.refs['syn'].g    current = self.post.sum_inputs(self.post.V)    return conductance, current, self.post.VBiophysical synapse modelsLimitations of phenomenological modelsæ‰“å¼€çš„æ•°é‡æ˜¯æœ‰é™çš„ï¼Œè€Œä¸”æœ‰é¥±å’ŒæœŸ  Saturation of postsynaptic receptors by previously released transmitter.  Certain receptor types also exhibit desensitization that prevents them (re)opening for a period after transmitter-binding, like sodium channels underlying action potential.Linetic/Markov models  The simplest kinetic model is a two-state scheme in which receptors can be either closed, ð¶, or open, ð‘‚, and the transition between states depends on transmitter concentration, [ð‘‡], in the synaptic cleft:  ð›¼ and ð›½ are voltage-independent forward and backward rate constants.  ð¶ and ð‘‚ can range from 0 to 1, and describe the fraction of receptors in the closed and open states, respectively.  The synaptic conductance is: $g_{syn}(t)=\bar{g}_{max}g(t)$AMPA/GABA$_A$ synapse model\(\begin{aligned}\frac{ds}{dt}&amp;=\alpha[T](1-s)-\beta s\\I&amp;=\tilde{g}s(V-E)\end{aligned}\)  ð›¼[ð‘‡] denotes the transition probability from state (1âˆ’ð‘ ) to state (ð‘ )  ð›½ represents the transition probability of the other direction  ð¸ is a reverse potential, which can determine whether the direction of ð¼ is inhibition or excitation.  ð¸ = 0 ð‘šð‘šð‘‰ð‘‰ =&gt; Excitatory synapse [AMPA]  ð¸ = âˆ’80 ð‘šð‘šð‘‰ð‘‰ =&gt; Inhibitory synapse [GABA A ]ComparisonNMDA synapse model\(\begin{aligned}&amp;\frac{ds}{dt} =\alpha[T](1-s)-\beta s  \\&amp;I=\tilde{g}sB(V)(V-E) \\&amp;B(V )=\frac{1}{1+\exp(-0.062V)[Mg^{2+}]_{o}/3.57} \end{aligned}\)The magnesium block of the NMDA receptor channel is an extremely fast process compared to the other kinetics of the receptor (Jahr and Stevens 1990a, 1990b). The block can therefore be accurately modeled as an instantaneous function of voltage(Jahr and Stevens 1990b).where $[Mg^{2+}]$ is the external magnesium concentration (1 to 2mM inphysiological conditions)Programming of biophysical synapse modelsAMPA synapse modelclass AMPA(bp.Projection):    def __init__(self, pre, post, delay, prob, g_max, E=0.):        super().__init__()        self.proj = bp.dyn.ProjAlignPreMg2(          pre=pre,           delay=delay,           syn=bp.dyn.AMPA.desc(pre.num, alpha=0.98, beta=0.18, T=0.5, T_dur=0.5),          comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max),          out=bp.dyn.COBA(E=E),          post=post,         )class SimpleNet(bp.DynSysGroup):  def __init__(self, syn_cls):    super().__init__()    self.pre = bp.dyn.SpikeTimeGroup(1, indices=(0, 0, 0, 0), times=(10., 30., 50., 70.))    self.post = bp.dyn.LifRef(1, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5.,                              V_initializer=bp.init.Constant(-60.))    self.syn = syn_cls(self.pre, self.post, delay=None, prob=1., g_max=1.)      def update(self):    self.pre()    self.syn()    self.post()        # monitor the following variables    conductance = self.syn.proj.refs['syn'].g    current = self.post.sum_inputs(self.post.V)    return conductance, current, self.post.Vdef run_a_net(net, duration=100):  indices = np.arange(int(duration/bm.get_dt()))  # duration ms  conductances, currents, potentials = bm.for_loop(net.step_run, indices, progress_bar=True)  ts = indices * bm.get_dt()    # --- similar to:   # runner = bp.DSRunner(net)  # conductances, currents, potentials = runner.run(100.)    fig, gs = bp.visualize.get_figure(1, 3, 3.5, 4)  fig.add_subplot(gs[0, 0])  plt.plot(ts, conductances)  plt.title('Syn conductance')  fig.add_subplot(gs[0, 1])  plt.plot(ts, currents)  plt.title('Syn current')  fig.add_subplot(gs[0, 2])  plt.plot(ts, potentials)  plt.title('Post V')  plt.show()$\text{GABA}_A$ synapse modelclass GABAa(bp.Projection):    def __init__(self, pre, post, delay, prob, g_max, E=-80.):        super().__init__()        self.proj = bp.dyn.ProjAlignPreMg2(          pre=pre,           delay=delay,           syn=bp.dyn.GABAa.desc(pre.num, alpha=0.53, beta=0.18, T=1.0, T_dur=1.0),          comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max),          out=bp.dyn.COBA(E=E),          post=post,         )run_a_net(SimpleNet(syn_cls=GABAa))NMDA synapse modelclass NMDA(bp.Projection):    def __init__(self, pre, post, delay, prob, g_max, E=0.0):        super().__init__()        self.proj = bp.dyn.ProjAlignPreMg2(          pre=pre,           delay=delay,           syn=bp.dyn.NMDA.desc(pre.num, a=0.5, tau_decay=100., tau_rise=2.),           comm=bp.dnn.CSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max),           out=bp.dyn.MgBlock(E=E),           post=post,         )run_a_net(SimpleNet(NMDA))Kinetic synapse models are more realisticclass SimpleNet5(bp.DynSysGroup):  def __init__(self, freqs=10.):    super().__init__()        self.pre = bp.dyn.PoissonGroup(1, freqs=freqs)    self.post = bp.dyn.LifRef(1,  V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5.,                              V_initializer=bp.init.Constant(-60.))    self.syn = NMDA(self.pre, self.post, delay=None, prob=1., g_max=1., E=0.)      def update(self):    self.pre()    self.syn()    self.post()        # monitor the following variables    return self.syn.proj.refs['syn'].g, self.post.Vdef compare_freqs(freqs):  fig, _ = bp.visualize.get_figure(1, 1, 4.5, 6.)  for freq in freqs:      net = SimpleNet5(freqs=freq)      indices = np.arange(1000)  # 100 ms      conductances, potentials = bm.for_loop(net.step_run, indices, progress_bar=True)      ts = indices * bm.get_dt()      plt.plot(ts, conductances, label=f'{freq} Hz')  plt.legend()  plt.ylabel('g')  plt.show()compare_freqs([10., 100., 1000., 10000.])How to customize a synapsePreparationsProjAlignPostMg2 and ProjAlignPreMg2Exponential Modelclass Exponen(bp.dyn.SynDyn, bp.mixin.AlignPost):  def __init__(self, size, tau):    super().__init__(size)        # parameters    self.tau = tau        # variables    self.g = bm.Variable(bm.zeros(self.num))        # integral    self.integral = bp.odeint(lambda g, t: -g/tau, method='exp_auto')      def update(self, pre_spike=None):    self.g.value = self.integral(g=self.g.value, t=bp.share['t'], dt=bp.share['dt'])    if pre_spike is not None:      self.add_current(pre_spike)    return self.g.value        def add_current(self, x):  # specical for bp.mixin.AlignPost    self.g += x      def return_info(self):    return self.gAMPA Modelclass AMPA(bp.dyn.SynDyn):  def __init__(self, size, alpha= 0.98, beta=0.18, T=0.5, T_dur=0.5):    super().__init__(size=size)    # parameters    self.alpha = alpha    self.beta = beta    self.T = T    self.T_duration = T_dur    # functions    self.integral = bp.odeint(method='exp_auto', f=self.dg)        # variables    self.g = bm.Variable(bm.zeros(self.num))    self.spike_arrival_time = bm.Variable(bm.ones(self.num) * -1e7)  def dg(self, g, t, TT):    return self.alpha * TT * (1 - g) - self.beta * g    def update(self, pre_spike):    self.spike_arrival_time.value = bm.where(pre_spike, bp.share['t'], self.spike_arrival_time)    TT = ((bp.share['t'] - self.spike_arrival_time) &lt; self.T_duration) * self.T    self.g.value = self.integral(self.g, bp.share['t'], TT, bp.share['dt'])    return self.g.value  def return_info(self):    return self.gSynapse outputsCOBAclass COBA(bp.dyn.SynOut):  def __init__(self, E):    super().__init__()    self.E = E  def update(self, conductance, potential):    return conductance * (self.E - potential)CUBAclass CUBA(bp.dyn.SynOut):  def __init__(self, E):    super().__init__()    self.E = E  def update(self, conductance, potential):    return conductanceMg BlockingThe voltage dependence is due to the blocking of the pore of the NMDA receptor from the outside by a positively charged magnesium ion. The channel is nearly completely blocked at resting potential, but the magnesium block is relieved if the cell is depolarized. The fraction of channels $B(V)$ that are not blocked by magnesium can be fitted to\[B(V) = {1 \over 1 + \exp(-0.062V) [Mg^{2+}]_o/3.57}\]Here, $[{Mg}^{2+}]_{o}$ is the extracellular magnesium concentration, usually 1 mM.If we make the approximation that the magnesium block changes instantaneously with voltage and is independent of the gating of the channel, the net NMDA receptor-mediated synaptic current is given by\[I=\bar{g}sB(V)(V-E)\]where $V(t)$ is the post-synaptic neuron potential, $E$ is the reversal potential.class MgBlock(bp.dyn.SynOut):  def __init__(self, E= 0., cc_Mg= 1.2, alpha= 0.062, beta= 3.57):    super().__init__()    self.E = E    self.cc_Mg = cc_Mg    self.alpha = alpha    self.beta = beta  def update(self, conductance, potential):    return conductance * (self.E - potential) / (1 + self.cc_Mg / self.beta * bm.exp(-self.alpha * potential))Masked matrixclass MaskedLinear(bp.dnn.Layer):  def __init__(self, conn, weight):    super().__init__()        # connection and weight    weight = bp.init.parameter(weight, (conn.pre_num, conn.post_num))    if isinstance(self.mode, bm.TrainingMode):      weight = bm.TrainVar(weight)    self.weight = weight    # connection    self.conn = conn    self.mask = bm.sharding.partition(self.conn.require('conn_mat'))  def update(self, x):    return x @ (self.weight * self.mask)Short-term Synaptic PlasticitySynaptic transmission and plasticityProcess of Chemical synaptic transmissionEPSP and EPSCEPSP: Excitatory Post Synaptic PotentialEPSC: Excitatory Post Synaptic CurrentPost synaptic current: $I(t)=g(t)\bigl[V_{\mathrm{post}}(\mathrm{t})-E_{reversal}\bigr]$Dynamics of post-synaptic conductance (exponential model): $\frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{S}}+A\delta(t-t_{sp})$The synaptic strength is characterized as EPSC, which refers to the post synaptic current increment at each spike, $EPSC_{n}=A(V_{\mathrm{rest}}-E_{reversal})$Synaptic plasticityå®žé™…ä¸Šçªè§¦å¼ºåº¦ä¼šä¸€ç›´å˜æ¢Phenomenological model of STPShort-term depression observed between pyramidal cellsè†œç‰‡å‰æŠ€æœ¯ï¼Œè®°å½•è†œç”µä½å˜åŒ–Modeling neuro-transmitter consumptionDynamics of three-factor STD:\(\begin{gathered}\frac{dx(t)}{dt}=\frac{z(t)}{\tau_{rec}}-U_{SE}x(t)\delta\big(t-t_{sp}\big), \\\frac{dy(t)}{dt}=-\frac{y(t)}{\tau_{in}}+U_{SE}x(t)\delta\big(t-t_{sp}\big), \\x(t)+y(t)+z(t)=1, \\\frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{s}}+g_{max}y(t), \end{gathered}\)$x$: Fraction of available neuro-transmitter$y$: Fraction of active neuro-transmitter$z$: Fraction of inactive neuro-transmitter$U_{se}$: Release probability of active neuro-transmitter$t_{sp}$: Pre-synaptic spike time$g(t)$:  Post-synaptic conductance$A$: total amount of neuro-transmitter$\tau_{in}$ &amp; $\tau_{rec}$ &amp; $\tau_s$: Time constantsSimulate the three-factor STD\[\begin{gathered}\frac{dx(t)}{dt}=\frac{z(t)}{\tau_{rec}}-U_{SE}x(t)\delta\big(t-t_{sp}\big), \\\frac{dy(t)}{dt}=-\frac{y(t)}{\tau_{in}}+U_{SE}x(t)\delta\big(t-t_{sp}\big), \\x(t)+y(t)+z(t)=1, \\\frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{s}}+Ay(t), \end{gathered}\]\[\tau_{rec}=500ms,\quad\tau_{in}=3ms,\quad fr=20hz\]Simplify the dynamics of neuro-transmitter consumptionå˜é‡å¤ªå¤šäº†ï¼Œæ¨¡åž‹å¤æ‚In general, the inactivation time constants is much shorter (3ms) than the spike time interval, i.e., ðœ 2- â‰ª Î”ð‘¡, so the formulation can be approximately simplified,\(\begin{aligned}\frac{dy(t)}{dt}&amp;=-\frac{y(t)}{\tau_{in}}+U_{SE}x(t)\delta\big(t-t_{sp}\big)\\&amp;\Longrightarrow\color{red}{\left\{\begin{array}{c}y(t)=U_{SE}x^-\delta_1(t-t_{sp}),\\x^-=\lim_{t-t_{sp}\to0^-}x(t)\end{array}\right.}\end{aligned}\)Simplified model:\(\begin{gathered}\frac{dx(t)}{dt} =\frac{1-x(t)}{\tau_{rec}}-U_{SE}x^{-}\delta\big(t-t_{sp}\big), \\\frac{dg(t)}{dt} =-\frac{g(t)}{\tau_{s}}+AU_{SE}x^{-}\delta\big(t-t_{sp}\big), \\EPSC=AU_{SE}x^{-}, \end{gathered}\)Infer model parameters from experimental dataæŽ¨æ–­è¶…å‚ï¼ŒEPSCçš„ç†è®ºè§£Short term depression model:\(\begin{aligned}\frac{dx(t)}{dt}&amp;=\frac{1-x(t)}{\tau_{rec}}-U_{SE}x^{-}\delta(t-t_{sp}),\\EPSC&amp;=AU_{SE}x^{-},\end{aligned}\)Iterative expression for EPSCs:\(x_{1}^{-}=1, EPSC_{1}=AU_{SE},  \\x_{n+1}^{-}=1-x_{n}^{-}(1-U_{SE})\mathrm{e}^{-\frac{\Delta t}{\tau_{rec}}} \\EPSC_{n+1}=AU_{SE}-EPSC_{n}(1-U_{SE})e^{-\frac{\Delta t}{\tau_{rec}}}\)Short-term facilitation observed between pyramidal cells and interneuronsçŸ­æ—¶ç¨‹å¢žå¼ºModeling neuro-transmitter release probabilityå…ˆå‰æ¼æŽ‰é‡Šæ”¾æ¦‚çŽ‡çš„å»ºæ¨¡The release probability can also be modelled as a dynamical variable ð‘¢(ð‘¡),\(\begin{gathered}\frac{du(t)}{dt}=\frac{-u(t)}{\tau_{f}}+U_{SE}(1-u^{-})\delta\big(t-t_{sp}\big), \\\frac{dx(t)}{dt}=\frac{1-x(t)}{\tau_{d}}-u(t)x^{-}\delta\big(t-t_{sp}+\delta t\big), \\\frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{S}}+Au(t)x^{-}\delta\big(t-t_{sp}+\delta t\big), \\EPSC=Au(t)x^{-}, \end{gathered}\)$U_{SE}$ might reflect the concentration of $Ca^{2+}$The release probability can also be modelled as a dynamical variable ð‘¢(ð‘¡),\(\begin{gathered}\frac{du(t)}{dt}=\frac{-u(t)}{\tau_{f}}+U_{SE}(1-u^{-})\delta\big(t-t_{sp}\big), \\\frac{dx(t)}{dt}=\frac{1-x(t)}{\tau_{d}}-u^{+}x^{-}\delta\big(t-t_{sp}\big), \\\frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{s}}+Au^{+}x^{-}\delta\big(t-t_{sp}\big), \\{EPSC=Au^{+}x^{-},\quad u^{+}=\lim_{t-t_{sp}\to0^{+}}u(t),} \end{gathered}\)STD and STF under different parameter regimeDerivation of iterative expressions for EPSCs\[\begin{gathered}\mathrm{Iterative~expression~for~}x_{n},u_{n},EPSC_{n}; \\u_{1}^{+}=U_{SE},\quad x_{1}^{-}=1, \\x_{n+1}^{-}=1-x_{n}^{-}(1-u_{n}^{+})\mathrm{e}^{-\frac{\Delta t}{\tau_{rec}}}, \\u_{n+1}^{+}=u_{n}^{+}e^{-\frac{\Delta t}{\tau_{f}}}+U_{SE}\left(1-u_{n}^{+}e^{-\frac{\Delta t}{\tau_{f}}}\right), \\EPSC_{n+1}=Au_{n}^{+}x_{n}^{-}, \end{gathered}\]\[\begin{gathered}\mathrm{Steady~state~of~}x_{n},u_{n},EPSC_{n}; \\u_{st}^{+}=\frac{U_{SE}}{1-(1-U_{SE})e^{-\frac{\Delta t}{\tau_{f}}}}\geq U_{SE}=u_{1}^{+}, \\x_{st}^{-}=\frac{1}{1+(1-u_{st}^{+})\mathrm{e}^{-\frac{\Delta t}{\tau_{rec}}}}\leq1=x_{1}^{-}, \\EPSC_{st}=Au_{st}^{+}x_{st}^{-} \end{gathered}\]Prediction for complex post-synaptic patternsInfer model parameters by fitting experiments:\(EPSC_{n+1} = Au_nx_n\)Simulate with complex pre-synaptic spike trains and compare with vitro experimental results (patch-clamp)Effects on information transmissionMean-field Analysis of STP modelSTP based on spiking time \(\begin{gathered}\frac{du(t)}{dt}=\frac{-u(t)}{\tau_{f}}+U_{sE}(1-u^{-})\delta\big(t-t_{sp}\big), \\\frac{dx(t)}{dt}=\frac{1-x(t)}{\tau_{d}}-u^{+}x^{-}\delta\big(t-t_{sp}\big), \\\frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{s}}+Au^{+}x^{-}\delta\big(t-t_{sp}\big), \\u^{+}=\lim_{t-t_{sp\rightarrow0^{+}}}u(t), \end{gathered}\)-&gt;åšæ—¶é—´å¹³å‡STP based on firing rate\(\begin{gathered}\frac{du(t)}{dt}=\frac{-u(t)}{\tau_{f}}+U_{sE}(1-u^{-})\delta\big(t-t_{sp}\big), \\\frac{dx(t)}{dt}=\frac{1-x(t)}{\tau_{d}}-u^{+}x^{-}\delta\big(t-t_{sp}\big), \\\frac{dg(t)}{dt}=-\frac{g(t)}{\tau_{s}}+Au^{+}x^{-}\delta\big(t-t_{sp}\big), \\u^{+}=\lim_{t-t_{sp\rightarrow0^{+}}}u(t), \end{gathered}\)ä¸¢æŽ‰æ—¶é—´å˜åŒ–çš„å…·ä½“ç»†èŠ‚ï¼ŒæŠ“ä½äº†é‡è¦è¶‹åŠ¿Theoretical analysis of the rate modelSuppose the pre-synaptic firing rate keeps as constant, we can calculate the stationary response\(u_{st}=\frac{U_{SE}R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}},\quad u_{st}^{+}=U_{SE}\frac{1+R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}},\quad x_{st}=\frac{1}{1+u_{st}^{+}\tau_{d}R_{0}},\)\[EPSC_{st}=Au_{st}^{+}x_{st}=A\frac{u_{st}^{+}}{1+u_{st}^{+}\tau_{d}R_{0}},\quad PSV_{st}\propto g_{st}=\tau_{s}Au_{st}^{+}x_{st}R_{0}=A\frac{u_{st}^{+}R_{0}}{1+u_{st}^{+}\tau_{d}R_{0}},\]Frequency-dependent Gain control of spike information\[\begin{gathered}u_{st}^{+}=U_{SE}\frac{1+R_{0}\tau_{f}}{1+U_{SE}R_{0}\tau_{f}}, \\x_{st}=\frac{1}{1+u_{st}^{+}\tau_{d}R_{0}}, \\EPSC_{st}=Au_{st}^{+}x_{st}=A\frac{u_{st}^{+}}{1+u_{st}^{+}\tau_{d}R_{0}}, \end{gathered}\]Peak frequency: $\theta\sim\frac{1}{\sqrt{U\tau_{f}\tau_{d}}}$Simulation of Frequency-dependent Gain controlEffects on network dynamicsSTP modeling Working memoryE-I Balanced Neural NetworkIrregular Spiking of NeuronsSignal process of single neuronExternal Stimulus -&gt;Single neuron model\(\begin{aligned}\tau&amp;\frac{\mathrm{d}V}{\mathrm{d}t}=-(V-V_\text{rest })+RI(t)\\\\&amp;\text{if}V&gt;V_\text{th},\quad V\leftarrow V_\text{reset }\text{last}t_\text{ref}\end{aligned}\)-&gt; â€¦ -&gt; Perception or actionçœŸæ­£çš„ç¥žç»å…ƒå¹¶ä¸æ˜¯LIF modelçš„è¾“å‡ºSimulationNeuron recorded in vivoIrregular Spiking of NeuronsStatistical Description of Spikesç”¨ä»¥ä¸‹çš„å˜é‡æ¥è¿›è¡Œç»Ÿè®¡æè¿°  Firing RateRate = average over time(single neuron, single run)Spike count $v=\frac{n_{sp}}{T}$  ISI(Interspike interval distributions)average ISI $\overline{\Delta t}=\frac{1}{n_{sp}-1}\sum_{i=1}^{n_{sp}-1}\Delta t_{i}$standard deviation ISI: $\sigma_{\Delta t}^{2}=\frac{1}{n_{sp}-1}\sum_{i=1}^{n_{sp}-1}(\Delta t_{i}-\overline{\Delta t})^{2}$  $C_V$(Coefficient of variation, Fano factor) çª„è¿˜æ˜¯å®½çš„åˆ†å¸ƒ ä¿¡æ¯è¡¨å¾æœ‰å¤šå¼ºçš„ä¸ç¨³å®šæ€§$C_{V}=\sigma_{\Delta t}^{2}/\overline{\Delta t}$Poisson ProcessIn probability theory and statistics, the Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last eventã€‚\(\begin{aligned}&amp;P(X=k\mathrm{~events~in~interval~}t)=e^{-rt}\frac{(rt)^{k}}{k!} \\&amp;\mathrm{mean:}\quad\overline{X}=rt \\&amp;\mathrm{variance}:\quad\sigma^{2}=rt\\&amp;\mathrm{Fano factor:}\quad\frac{\sigma^{2}}{X}=1\end{aligned}\)Fano factor -&gt; noise-to-signal ratioIrregular Spiking of NeuronsLIFåœ¨å•ä¸ªç¥žç»å…ƒçš„æƒ…å†µä¸‹æ˜¯åŸºæœ¬æ²¡æœ‰å¤ªå¤§é—®é¢˜çš„ï¼Œåœ¨æ•´ä¸ªç½‘ç»œä¸­ä¼šå—ç½‘ç»œä¿¡æ¯è°ƒæŽ§Why Irregular?  ä¸å®Œå…¨æ˜¯inputå½±å“çš„  ä¸èƒ½ç®€å•æ¥è¡¡é‡On average, a cortical neuron receives inputs from 1000~10000 connected neurons. -&gt; averaged noise ~ 0E-I Balanced Network\[\begin{gathered}\tau\frac{du_{i}^{E}}{dt}=-u_{i}^{E}+\sum_{j=1}^{K_{E}}J_{EE}r_{j}^{E}+\sum_{j=1}^{K_{I}}J_{EI}r_{j}^{I}+I_{i}^{E} \\\tau\frac{du_{i}^{I}}{dt}=-u_{i}^{I}+\sum_{j=1}^{K_{I}}J_{II}r_{j}^{I}+\sum_{j=1}^{K_{E}}J_{IE}r_{j}^{E}+I_{i}^{I} \end{gathered}\]Sparse &amp; random connections:$1\ll K_{\mathrm{E}},K_{1}\ll N_{\mathrm{E}},N_{\mathrm{I}}$ . Neurons fire largely independently to each other.\(\begin{gathered}\text{Single neuron fires irregularly } r_j^E, r_j^{\prime} \text{with mean rate } \mu \text{and variance } \sigma^2.\\\text{The mean of recurrent input received by E neuron:} \\\sim K_{E}J_{EE}\mu-K_{I}J_{EI}\mu  \\\text{The variance of recurrent input received by E neuron:} \\\sim K_{E}(J_{EE})^{2}\sigma^{2}+K_{I}(J_{EI})^{2}\sigma^{2} \\\begin{gathered} \\\text{The balanced condition:} \\K_{E}J_{EE}-K_{l}J_{El}{\sim}0(1) \\J_{EE}=\frac{1}{\sqrt{K_{E}}},J_{EI}=\frac{1}{\sqrt{K_{I}}},K_{E}(J_{EE})^{2}\sigma^{2}+K_{I}(J_{EI})^{2}\sigma^{2}\sim O(1) \end{gathered}\end{gathered}\)\[\begin{aligned}\frac{I_E}{I_I}&amp;&gt;\frac{J_E}{J_I}&amp;&gt;1\\\\J_E&amp;&gt;1\\\\\text{r not too big}\end{aligned}\]\[\overline{I_a}=\overline{F_a}+\overline{R_a}=\sqrt{N}(f_a\mu_0+w_{aE}r_E+w_{aI}r_I),\quad a=E,I,\\\begin{gathered}w_{ab}~=~p_{ab}j_{ab}q_{b} \\J_{ij}^{ab}~=~j_{ab}/\sqrt{N}; \\\frac{f_{E}}{f_{I}}&gt;\frac{w_{EI}}{w_{II}}&gt;\frac{w_{EE}}{w_{IE}}. \end{gathered}\]BrainPy SimulationSimulationLIF neuron 4000 (E/I=4/1, P=0.02)ðœ = 20 msð‘‰ð‘Ÿð‘’ð‘ ð‘¡ = -60 mVSpiking threshold: -50 mVRefractory period: 5 ms\(\begin{gathered}\tau\frac{dV}{dt}=(V_{\mathrm{rest}}-V)+I \\I=g_{exc}(E_{exc}-V)+g_{inh}(E_{inh}-V)+I_{\mathrm{ext}} \end{gathered} \ \ \ \ \ \\begin{aligned}\tau_{exc}&amp;\frac{dg_{exc}}{dt}=-g_{exc}\\\tau_{inh}&amp;\frac{dg_{inh}}{dt}=-g_{inh}\end{aligned}\)\[\begin{array}{l}E_\mathrm{exc}=0\text{mV}\mathrm{and}E_\mathrm{inh}=-80\text{mV},I_\mathrm{ext}=20.\\\tau_\mathrm{exc}=5\text{ ms},\tau_\mathrm{inh}=10\text{ ms},\Delta g_\mathrm{exc}=0.6\text{ and}\Delta g_\mathrm{inh}=6.7.\end{array}\]Synaptic Computation# åŸºäºŽ align post Exponential synaptic computationclass Exponential(bp.Projection):    def __init__(self, pre, post, delay, prob, g_max, tau, E, label=None):        super().__init__()        self.pron = bp.dyn.ProjAlignPost2(        	pre=pre,            delay=delay,            comm=bp.dnn.EventCSRLinear(bp.conn.FixedProb(prob, pre=pre.num, post=post.num), g_max), # éšæœºè¿žæŽ¥            syn=bp.dyn.Expon(size=post.num, tau=tau), # Exponential synapse            out=bp.dyn.COBA(E=E), # COBA network            post=post,            out_label=label        )E-I Balanced Network# æž„å»º E-I Balanced Networkclass EINet(bp.DynamicalSystem):    def __init__(self, ne=3200, ni=800):        super().__init__()                # bp.neurons.LIF()        self.E = bp.dyn.LifRef(ne, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5.,                              V_initializer=bp.init.Normal(-55., 2.))        self.I = bp.dyn.LifRef(ni, V_rest=-60., V_th=-50., V_reset=-60., tau=20., tau_ref=5.,                              V_initializer=bp.init.Normal(-55., 2.))    	#### E2E, E2I, I2E, I2I Exponential synaptic computation        # delay=0, prob=0.02, g_max_E=0.6, g_max_I=6.7, tau_E=5, tau_I=10,        # reversal potentials E_E=0, E_E=-80, label=EE,EI,IE,II        self.E2E = Exponential(self.E, self.E, 0., 0.02, 0.6, 5., 0., 'EE')        self.E2I = Exponential(self.E, self.I, 0., 0.02, 0.6, 5., 0., 'EI')        self.I2E = Exponential(self.I, self.E, 0., 0.02, 6.7, 5., -80., 'IE')        self.I2I = Exponential(self.I, self.I, 0., 0.02, 6.7, 5., -80., 'II')def update(self, inp=0.):    # æ›´æ–°çªè§¦ä¼ å…¥ç”µæµ    self.E2E()    self.E2I()    self.I2E()    self.I2I()        # æ›´æ–°ç¥žç»å…ƒç¾¤ä½“    self.E(inp)    self.I(inp)        # è®°å½•éœ€è¦ monitorçš„å˜é‡    E_E_inp = self.E.sum_inputs(self.E.V, label='EE') #E2Eçš„è¾“å…¥    I_E_inp = self.E.sum_inputs(self.E.V, label='IE') # I2Eçš„è¾“å…¥    return self.E.spike, self.I.spike, E_E_inp, I_E_inpProperties of E-I Balanced Network  Linear encodingExternal input strength is â€œlinearlyâ€ encoded by the mean firing rate of the neural population  Fast ResponseThe network responds rapidly to abrupt changes of the inputNoise speeds up computationå¿«é€Ÿç›¸åº”çš„åŽŸç†ï¼Œå‡åŒ€åˆ†å¸ƒåœ¨é˜ˆå€¼ä¸‹é¢çš„ç©ºé—´  A neural ensemble jointly encodes stimulus information;  Noise randomizes the distribution of neuronal membrane potentials;  Those neurons (red circle) whose potentials are close to the threshold will fire rapidly;  If the noisy environment is proper, even for a small input, a certain number of neurons will fire instantly to report the presence of a stimulus.Continuous Attractor Neural NetworkAttractor ModelsThe concept of attractor dynamicsDifferent types of attractors:Point attractors, Line attractors, Ring attractors, Plane attractors, Cyclic attractors, Chaotic attractorsç¨³æ€ï¼Œèƒ½é‡æ¢¯åº¦å¸å¼•åˆ°attractorDiscrete attractor Network Model: Hopfield Model$S_i=\pm1$: the neuronal state$W_{ij}$ : the neuronal connectionThe network dynamics:\(S_{i}=\mathrm{sign}\bigg(\sum_{j}w_{ij}S_{j}-\theta\bigg),\quad\mathrm{sign}(x)=1,\mathrm{for}x&gt;0;-1,\mathrm{otherwise}\)Updating rule: synchronous or asynchronousConsider the network stores $p$ pattern, $\xi_{i}^{\mu},\mathrm{for}\mu=1,\ldots p;i=1,\ldots N$Setting $w_{ij}=\frac{1}{N}\sum_{\mu=1}^{p}\xi_{i}^{\mu}\xi_{j}^{\mu}$Energy space of Hopfield network\[\begin{aligned}&amp;\text{Energy function: }E=-\frac{1}{2}\sum_{i,j}w_{ij}S_{i}S_{j}+\theta\sum_{i}S_{i} \\&amp;\mathrm{Consider}S_{i}\mathrm{~is~updated},S_{i}(t+1)=sign[\sum_{j}w_{ij}S_{j}(t)-\theta] \\&amp;\Delta E=E(t+1)-E(t)\\&amp;=-[S_{i}(t+1)-S_{i}(t)]\sum_{j}w_{ij}S_{j}(t)+\theta\left[S_{i}(t+1)-S_{i}(t)\right] \\&amp;=-[S_{i}(t+1)-S_{i}(t)][\sum_{j}w_{ij}S_{j}(t)-\theta] \\&amp;\leq0\end{aligned}\]åŒæ ·æ¿€æ´»åŒæ ·patternçš„ç¥žç»å…ƒï¼Œ~å¸å¼•å­Auto-associative memory in Hopfield NetworkA partial/noisy input can retrieve the related memory patternPersistent activity in working memoryAfter the removal of external input, the neurons in the network encoding the stimulus continue to fire persistently.Continuous Attractor Neural NetworkNeural codingLow-dimensional continuous featureContinuous Attractor neural networkCANN: A rate-based recurrent circuit model\(\begin{aligned}\tau\frac{\partial U(x,t)}{\partial t}&amp;=-U(x,t)+\rho\int f(x,x')r(x',t)dx'+l^{ext}(1)\\r(x,t)&amp;=\frac{U^2(x,t)}{1+k\rho\int U^2(x,t)dx}\quad(2)\\J(x,x')&amp;=\frac{J_0}{\sqrt{2\pi}a}\exp\left[-\frac{(x-x')^2}{2a^2}\right](3)\end{aligned}\)ré¢‘çŽ‡ï¼ŒJå¼ºåº¦ï¼ŒU decayA Continuous family of attractor statesåšå¹³ç§»çš„æ”¹å˜ï¼Œå˜åŒ–ä¼šè¢«ä¿ç•™ï¼Œline attractorï¼Œå—åˆ°ç¼–ç è¿žç»­åˆºæ¿€Stability analysis derive continuous attractor dynamicsåªéœ€è¦çœ‹åœ¨åŽŸå§‹çŠ¶æ€åŠ å…¥ä¸€ä¸ªå°é‡é¡¹ï¼Œå†ä»£å…¥å›žConsider small fluctuations around a stationary state at z:Projecting $\delta U$ on the $i$th right eigenvector of $F(\delta U)_i(t)=(\delta U)_i(0)e^{-(1-\lambda _i)t/\tau}$Two cases:  If $\lambda _i &lt; 1$, the projection decays exponentially  If $\lambda _i$ = 1, the projection is sustainedSpectra of the kernel F\[\begin{aligned}\bullet&amp;\lambda_0=1-2k\rho A\sqrt{2\pi}a&lt;1,\quad&amp;\mathbf{u}_0(x\mid z)=\overline{\mathbf{U}}(x\mid z);\\\bullet&amp;\lambda_1=1,\quad&amp;\mathbf{u}_1(x\mid z)=\frac{d\overline{\mathbf{U}}(x\mid z)}{dz},\text{the tangent of the valley}\\\bullet&amp;\lambda_n=\frac1{2^{n-2}},\quad&amp;\mathbf{u}_n(z)=\text{Combination of }\mathbf{v}_n(z)\end{aligned}\]$\mathbf{v}_{n}(z)\sim e^{-(c-z)^{2}/4a^{2}}(\frac{d}{dc})^{n}e^{-(c-z)^{2}/2a^{2}},$ the wave functions of quantumn harmonic osscilatorNote the decay time constant is: $\frac{\tau}{1-\lambda _n}$Only bump position shift survivesRing attractor network for head-direction cell in fruit flyComputation with CANNPersistent activity for working memoryWhen the global inhibition is not too strong, the network spontaneously hold bump activity:\(k&lt;\frac{\rho J_{0}^{2}}{8\sqrt{2\pi}a}\)\[\begin{aligned}&amp;\tilde{U}(x|z) =\quad U_{0}\exp\left[-\frac{(x-z)^{2}}{4a^{2}}\right],  \\&amp;\tilde{r}(x|z) =\quad r_{0}\exp\left[-\frac{(x-z)^{2}}{2a^{2}}\right],  \\&amp;U_{0}=[1+(1-k/k_{c})^{1/2}]A/(4\sqrt{\pi}ak) \\&amp;r_0=[1+(1-k/k_{c})^{1/2}]/(2\sqrt{2\pi}ak\rho).\end{aligned}\]Smooth tracking by CANNProject the network dynamics on $v_1(t)$$\tau{\frac{\partial\mathbf{U}\mathbf{v}_{1}}{\partial t}}=-\mathbf{U}\mathbf{v}{1}+(\mathbf{J}*\mathbf{r})*\mathbf{v}{1}+\mathbf{I}^{ext}*\mathbf{v}_{1}$Consider\(\begin{aligned}&amp;I^{ext}(t)=\alpha\overline{U}(x\mid z_0)+\sigma\xi_c(t)\\&amp;\mathbf{U}*\mathbf{v}_1\equiv\int dxU(x\mid z)\nu_1(x\mid z)\\\\&amp;\tau\frac{dz}{dt}=-\alpha(z-z_0)e^{-(z-z_0)^2/8a^2}+\beta\xi(t)\end{aligned}\)1st term: the force of the signal that pulls the bump back to the stimulus position2nd term: random shiftPopulation decoding via template matching\[\hat{x}=\max_{z}\sum_{i}r_{i}f_{i}(z)\]  The noisy bump is the population activity when the stimulus $x=0$  Among three positions, the red one($z=0$) has the maximum overlap with the observed data.Computation and Dynamics of Adaptive CANNAdaptive Continuous Attractor neural network\[\begin{aligned}&amp;\tau{\frac{dU(x,t)}{dt}} =-U(x,t)+\rho\int dx'J(x-x^{\prime})r(x',t)-V(x,t)+I^{ext}(x,t)  \\&amp;\tau_{_{\nu}}\frac{dV(x,t)}{dt} =-V(x,t)+mU(x,t) \end{aligned}\]$V(x,t)$ represents the SFA effect,$V(x,t)=\frac{m}{\tau_{\nu}}\int_{-\infty}^{â€˜}e^{-\frac{t-tâ€™}{\tau_{\nu}}}U(x,tâ€™)dtâ€™$SFA(Spike frequency Adaptation):  Neuronal response attenuates after experiencing prolonged firing.  Slow negative feedback modulation to neuronal response.Intrinsic mobility of A-CANNTraveling Wave: a moving bump in the network without relying on external driveThe mechanism: SFA suppresses localized neural activity and triggers$m&gt;\frac{\tau}{\tau _v}$, Travelling waveLevy flights vs. Brownian motionLÃ©vy flights in ecology and human cogniDve behaviorsç”Ÿç‰©å­¦å¤§å¤šè¿åŠ¨æœä»Žlevy flightsNoisy adaptation generates Levy flight in CANNTime Delay in Neural Signal TransmissionAnticipatory Head Direction Signals in Anterior Thalamusæœ‰é¢„æµ‹ç­–ç•¥ï¼Œå®žçŽ°æŠµæ¶ˆä¿¡æ¯ä¼ é€’çš„delayCANNåŠ å…¥è´Ÿåé¦ˆæœºåˆ¶æ˜¯å¯ä»¥å®žçŽ°é¢„æµ‹çš„CANN with STP\[\begin{gathered}\tau{\frac{\mathrm{d}U(x,t)}{\mathrm{d}t}} {\cal O}=-U(x,t)+\rho\int g^{+}(x)h(x^{\prime},t)J(x,x^{\prime})r(x^{\prime},t)dx^{\prime}+I^{ext}(x,t)(1) \\\frac{dg(x,t)}{dt}=-\frac{g(x,t)}{\tau_{f}}+G(1-g^{-}(x))r(x^{\prime},t)\quad(2) \\\frac{dh(x,t)}{dt}=\frac{1-h(x,t)}{\tau_{d}}-g^{+}(x)h(x,t)r(x^{\prime},t)\quad(3) \\r(x,t)={\frac{U^{2}(x,t)}{1+k\rho\int U^{2}(x,t)dx}}\quad(4) \end{gathered}\]Programming in BrainPyCustomize a ring CANN in brainpyIn simulations, we can not simulate a CANN encoding features ranging $(-\inf, \inf)$. Instead, we simulate a ring attractor network which encodes features ranging $(-\pi, \pi)$. Note that the distance on a ring should be:\(dist_{ring}(x,x') = min(|x-x'|,2\pi-|x-x'|)\)class CANN1D(bp.NeuGroupNS):  def __init__(self, num, tau=1., k=8.1, a=0.5, A=10., J0=4.,               z_min=-bm.pi, z_max=bm.pi, **kwargs):    super(CANN1D, self).__init__(size=num, **kwargs)    # åˆå§‹åŒ–å‚æ•°    self.tau = tau    self.k = k    self.a = a    self.A = A    self.J0 = J0    # åˆå§‹åŒ–ç‰¹å¾ç©ºé—´ç›¸å…³å‚æ•°    self.z_min = z_min    self.z_max = z_max    self.z_range = z_max - z_min    self.x = bm.linspace(z_min, z_max, num)    self.rho = num / self.z_range    self.dx = self.z_range / num    # åˆå§‹åŒ–å˜é‡    self.u = bm.Variable(bm.zeros(num))    self.input = bm.Variable(bm.zeros(num))    self.conn_mat = self.make_conn(self.x)  # è¿žæŽ¥çŸ©é˜µ    # å®šä¹‰ç§¯åˆ†å‡½æ•°    self.integral = bp.odeint(self.derivative)  # å¾®åˆ†æ–¹ç¨‹  @property  def derivative(self):    du = lambda u, t, Irec, Iext: (-u + Irec + Iext) / self.tau    return du  # å°†è·ç¦»è½¬æ¢åˆ°[-z_range/2, z_range/2)ä¹‹é—´  def dist(self, d):    d = bm.remainder(d, self.z_range)    d = bm.where(d &gt; 0.5 * self.z_range, d - self.z_range, d)    return d  # è®¡ç®—è¿žæŽ¥çŸ©é˜µ  def make_conn(self, x):    assert bm.ndim(x) == 1    d = self.dist(x - x[:, None])  # è·ç¦»çŸ©é˜µ    Jxx = self.J0 * bm.exp(      -0.5 * bm.square(d / self.a)) / (bm.sqrt(2 * bm.pi) * self.a)     return Jxx  # èŽ·å–å„ä¸ªç¥žç»å…ƒåˆ°poså¤„ç¥žç»å…ƒçš„è¾“å…¥  def get_stimulus_by_pos(self, pos):    return self.A * bm.exp(-0.25 * bm.square(self.dist(self.x - pos) / self.a))  def update(self, x=None):    _t = bp.share['t']    u2 = bm.square(self.u)    r = u2 / (1.0 + self.k * bm.sum(u2))    Irec = bm.dot(self.conn_mat, r)    self.u[:] = self.integral(self.u, _t,Irec, self.input)    self.input[:] = 0.  # é‡ç½®å¤–éƒ¨ç”µæµSimulate the persistent activity of CANN after the removal of external inputdef Persistent_Activity(k=0.1,J0=1.):    # ç”ŸæˆCANN    cann = CANN1D(num=512, k=k,J0=J0)    # ç”Ÿæˆå¤–éƒ¨åˆºæ¿€ï¼Œä»Žç¬¬2åˆ°12msï¼ŒæŒç»­10ms    dur1, dur2, dur3 = 2., 10., 10.    I1 = cann.get_stimulus_by_pos(0.)    Iext, duration = bp.inputs.section_input(values=[0., I1, 0.],                                             durations=[dur1, dur2, dur3],                                             return_length=True)    noise_level = 0.1    noise = bm.random.normal(0., noise_level, (int(duration / bm.get_dt()), len(I1)))    Iext += noise    # è¿è¡Œæ•°å€¼æ¨¡æ‹Ÿ    runner = bp.DSRunner(cann, inputs=['input', Iext, 'iter'], monitors=['u'])    runner.run(duration)    # å¯è§†åŒ–    def plot_response(t):        fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6)        ax = fig.add_subplot(gs[0, 0])        ts = int(t / bm.get_dt())        I, u = Iext[ts], runner.mon.u[ts]        ax.plot(cann.x, I, label='Iext')        ax.plot(cann.x, u, linestyle='dashed', label='U')        ax.set_title(r'$t$' + ' = {} ms'.format(t))        ax.set_xlabel(r'$x$')        ax.spines['top'].set_visible(False)        ax.spines['right'].set_visible(False)        ax.legend()        # plt.savefig(f'CANN_t={t}.pdf', transparent=True, dpi=500)    plot_response(t=10.)    plot_response(t=20.)    bp.visualize.animate_1D(        dynamical_vars=[{'ys': runner.mon.u, 'xs': cann.x, 'legend': 'u'},                        {'ys': Iext, 'xs': cann.x, 'legend': 'Iext'}],        frame_step=1,        frame_delay=40,        show=True,    )    plt.show()Persistent_Activity(k=0.1)Simulate the tracking behavior of CANNdef smooth_tracking():    cann = CANN1D(num=512, k=8.1)    # å®šä¹‰éšæ—¶é—´å˜åŒ–çš„å¤–éƒ¨åˆºæ¿€    v_ext = 1e-3    dur1, dur2, dur3 = 10., 10., 20    num1 = int(dur1 / bm.get_dt())    num2 = int(dur2 / bm.get_dt())    num3 = int(dur3 / bm.get_dt())    position = bm.zeros(num1 + num2 + num3)    position[num1: num1 + num2] = bm.linspace(0., 1.5 * bm.pi, num2)    position[num1 + num2: ] = 1.5 * bm.pi    position = position.reshape((-1, 1))    Iext = cann.get_stimulus_by_pos(position)    # è¿è¡Œæ¨¡æ‹Ÿ    runner = bp.DSRunner(cann,                         inputs=['input', Iext, 'iter'],                         monitors=['u'])    runner.run(dur1 + dur2 + dur3)    # å¯è§†åŒ–    def plot_response(t, extra_fun=None):        fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6)        ax = fig.add_subplot(gs[0, 0])        ts = int(t / bm.get_dt())        I, u = Iext[ts], runner.mon.u[ts]        ax.plot(cann.x, I, label='Iext')        ax.plot(cann.x, u, linestyle='dashed', label='U')        ax.set_title(r'$t$' + ' = {} ms'.format(t))        ax.set_xlabel(r'$x$')        ax.spines['top'].set_visible(False)        ax.spines['right'].set_visible(False)        ax.legend()        if extra_fun: extra_fun()        # plt.savefig(f'CANN_tracking_t={t}.pdf', transparent=True, dpi=500)    plot_response(t=10.)    def f():        plt.annotate('', xy=(1.5, 10), xytext=(0.5, 10), arrowprops=dict(arrowstyle="-&gt;"))    plot_response(t=15., extra_fun=f)    def f():        plt.annotate('', xy=(-2, 10), xytext=(-3, 10), arrowprops=dict(arrowstyle="-&gt;"))    plot_response(t=20., extra_fun=f)    plot_response(t=30.)    bp.visualize.animate_1D(        dynamical_vars=[{'ys': runner.mon.u, 'xs': cann.x, 'legend': 'u'},                        {'ys': Iext, 'xs': cann.x, 'legend': 'Iext'}],        frame_step=5,        frame_delay=50,        show=True,    )    plt.show()smooth_tracking()Customize a CANN with SFASimulate the spontaneous traveling waveclass CANN1D_SFA(bp.NeuGroupNS):  def __init__(self, num, m = 0.1, tau=1., tau_v=10., k=8.1, a=0.5, A=10., J0=4.,               z_min=-bm.pi, z_max=bm.pi, **kwargs):    super(CANN1D_SFA, self).__init__(size=num, **kwargs)    # åˆå§‹åŒ–å‚æ•°    self.tau = tau    self.tau_v = tau_v #time constant of SFA    self.k = k    self.a = a    self.A = A    self.J0 = J0    self.m = m #SFA strength          # åˆå§‹åŒ–ç‰¹å¾ç©ºé—´ç›¸å…³å‚æ•°    self.z_min = z_min    self.z_max = z_max    self.z_range = z_max - z_min    self.x = bm.linspace(z_min, z_max, num)    self.rho = num / self.z_range    self.dx = self.z_range / num    # åˆå§‹åŒ–å˜é‡    self.u = bm.Variable(bm.zeros(num))    self.v = bm.Variable(bm.zeros(num)) #SFA current    self.input = bm.Variable(bm.zeros(num))    self.conn_mat = self.make_conn(self.x)  # è¿žæŽ¥çŸ©é˜µ    # å®šä¹‰ç§¯åˆ†å‡½æ•°    self.integral = bp.odeint(self.derivative)  # å¾®åˆ†æ–¹ç¨‹  @property  def derivative(self):    du = lambda u, t, v, Irec, Iext: (-u + Irec + Iext-v) / self.tau    dv = lambda v, t, u: (-v + self.m*u) / self.tau_v    return bp.JointEq([du, dv])  # å°†è·ç¦»è½¬æ¢åˆ°[-z_range/2, z_range/2)ä¹‹é—´  def dist(self, d):    d = bm.remainder(d, self.z_range)    d = bm.where(d &gt; 0.5 * self.z_range, d - self.z_range, d)    return d  # è®¡ç®—è¿žæŽ¥çŸ©é˜µ  def make_conn(self, x):    assert bm.ndim(x) == 1    d = self.dist(x - x[:, None])  # è·ç¦»çŸ©é˜µ    Jxx = self.J0 * bm.exp(      -0.5 * bm.square(d / self.a)) / (bm.sqrt(2 * bm.pi) * self.a)     return Jxx  # èŽ·å–å„ä¸ªç¥žç»å…ƒåˆ°poså¤„ç¥žç»å…ƒçš„è¾“å…¥  def get_stimulus_by_pos(self, pos):    return self.A * bm.exp(-0.25 * bm.square(self.dist(self.x - pos) / self.a))  def update(self, x=None):    u2 = bm.square(self.u)    r = u2 / (1.0 + self.k * bm.sum(u2))    Irec = bm.dot(self.conn_mat, r)    u, v = self.integral(self.u, self.v, bp.share['t'],Irec, self.input)    self.u[:] = bm.where(u&gt;0,u,0)    self.v[:] = v    self.input[:] = 0.  # é‡ç½®å¤–éƒ¨ç”µæµSimulate the spontaneous traveling wavedef traveling_wave(num=512,m=0.1,k=0.1):    # ç”ŸæˆCANN    cann_sfa = CANN1D_SFA(num=num, m=m,k=k)    # ç”Ÿæˆå¤–éƒ¨åˆºæ¿€    dur = 1000.    noise_level = 0.1    Iext = bm.random.normal(0., noise_level, (int(dur / bm.get_dt()), num))    duration = dur    # è¿è¡Œæ•°å€¼æ¨¡æ‹Ÿ    runner = bp.DSRunner(cann_sfa, inputs=['input', Iext, 'iter'], monitors=['u'])    runner.run(duration)    # å¯è§†åŒ–    def plot_response(t):        fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6)        ax = fig.add_subplot(gs[0, 0])        ts = int(t / bm.get_dt())        I, u = Iext[ts], runner.mon.u[ts]        ax.plot(cann_sfa.x, I, label='Iext')        ax.plot(cann_sfa.x, u, linestyle='dashed', label='U')        ax.set_title(r'$t$' + ' = {} ms'.format(t))        ax.set_xlabel(r'$x$')        ax.spines['top'].set_visible(False)        ax.spines['right'].set_visible(False)        ax.legend()        # plt.savefig(f'CANN_t={t}.pdf', transparent=True, dpi=500)    plot_response(t=100.)    plot_response(t=150.)    plot_response(t=200.)    bp.visualize.animate_1D(        dynamical_vars=[{'ys': runner.mon.u, 'xs': cann_sfa.x, 'legend': 'u'},                        {'ys': Iext, 'xs': cann_sfa.x, 'legend': 'Iext'}],        frame_step=1,        frame_delay=40,        show=True,    )    plt.show()    traveling_wave(num=512,m=0.5,k=0.1)Simulate the anticipative trackingdef anticipative_tracking(m=10,v_ext=6*1e-3):    cann_sfa = CANN1D_SFA(num=512, m=m)        # å®šä¹‰éšæ—¶é—´å˜åŒ–çš„å¤–éƒ¨åˆºæ¿€    v_ext = v_ext    dur1, dur2, = 10., 1000.    num1 = int(dur1 / bm.get_dt())    num2 = int(dur2 / bm.get_dt())    position = np.zeros(num1 + num2)    for i in range(num2):        pos = position[i+num1-1]+v_ext*bm.dt        # the periodical boundary        pos = np.where(pos&gt;np.pi, pos-2*np.pi, pos)        pos = np.where(pos&lt;-np.pi, pos+2*np.pi, pos)        # update        position[i+num1] = pos    position = position.reshape((-1, 1))    Iext = cann_sfa.get_stimulus_by_pos(position)    # è¿è¡Œæ¨¡æ‹Ÿ    runner = bp.DSRunner(cann_sfa,                         inputs=['input', Iext, 'iter'],                         monitors=['u'],                         dyn_vars=cann_sfa.vars())    runner.run(dur1 + dur2)    # å¯è§†åŒ–    def plot_response(t, extra_fun=None):        fig, gs = bp.visualize.get_figure(1, 1, 4.5, 6)        ax = fig.add_subplot(gs[0, 0])        ts = int(t / bm.get_dt())        I, u = Iext[ts], runner.mon.u[ts]        ax.plot(cann_sfa.x, I, label='Iext')        ax.plot(cann_sfa.x, 10*u, linestyle='dashed', label='U')        ax.set_title(r'$t$' + ' = {} ms'.format(t))        ax.set_xlabel(r'$x$')        ax.spines['top'].set_visible(False)        ax.spines['right'].set_visible(False)        ax.legend()    plot_response(t=10.)    plot_response(t=200.)    plot_response(t=400.)    bp.visualize.animate_1D(        dynamical_vars=[{'ys': runner.mon.u, 'xs': cann_sfa.x, 'legend': 'u'},                        {'ys': Iext, 'xs': cann_sfa.x, 'legend': 'Iext'}],        frame_step=5,        frame_delay=50,        show=True,    )    plt.show()anticipative_tracking()Customize a CANN with STPclass CANN1D_STP(bp.NeuGroupNS):  def __init__(self, num, tau=1., tau_f=1., tau_d=30., G=0.2, k=8.1, a=0.5, A=10., J0=12.,               z_min=-bm.pi, z_max=bm.pi, **kwargs):    super(CANN1D_STP, self).__init__(size=num, **kwargs)    # åˆå§‹åŒ–å‚æ•°    self.tau = tau    self.tau_f = tau_f #time constant of u    self.tau_d = tau_d #time constant of h    self.G = G    self.k = k    self.a = a    self.A = A    self.J0 = J0          # åˆå§‹åŒ–ç‰¹å¾ç©ºé—´ç›¸å…³å‚æ•°    self.z_min = z_min    self.z_max = z_max    self.z_range = z_max - z_min    self.x = bm.linspace(z_min, z_max, num)    self.rho = num / self.z_range    self.dx = self.z_range / num    # åˆå§‹åŒ–å˜é‡    self.u = bm.Variable(bm.zeros(num))    self.g = bm.Variable(bm.zeros(num)) #neuro-transmitter release probability    self.h = bm.Variable(bm.ones(num)) #neuro-transmitter available fraction    self.input = bm.Variable(bm.zeros(num))    self.conn_mat = self.make_conn(self.x)  # è¿žæŽ¥çŸ©é˜µ    # å®šä¹‰ç§¯åˆ†å‡½æ•°    self.integral = bp.odeint(self.derivative)  # å¾®åˆ†æ–¹ç¨‹  @property  def derivative(self):    du = lambda u, t, Irec, Iext: (-u + Irec + Iext) / self.tau    dg = lambda g, t, r: -g / self.tau_f + self.G * (1 - g) * r     dh = lambda h, t, g, r:  (1 - h) / self.tau_d - (g + self.G * (1 - g)) * h *r    return bp.JointEq([du, dg, dh])  # å°†è·ç¦»è½¬æ¢åˆ°[-z_range/2, z_range/2)ä¹‹é—´  def dist(self, d):    d = bm.remainder(d, self.z_range)    d = bm.where(d &gt; 0.5 * self.z_range, d - self.z_range, d)    return d  # è®¡ç®—è¿žæŽ¥çŸ©é˜µ  def make_conn(self, x):    assert bm.ndim(x) == 1    d = self.dist(x - x[:, None])  # è·ç¦»çŸ©é˜µ    Jxx = self.J0 * bm.exp(      -0.5 * bm.square(d / self.a)) / (bm.sqrt(2 * bm.pi) * self.a)     return Jxx  # èŽ·å–å„ä¸ªç¥žç»å…ƒåˆ°poså¤„ç¥žç»å…ƒçš„è¾“å…¥  def get_stimulus_by_pos(self, pos):    return self.A * bm.exp(-0.25 * bm.square(self.dist(self.x - pos) / self.a))  def update(self, x=None):    u2 = bm.square(self.u)    r = u2 / (1.0 + self.k * bm.sum(u2))     Irec = bm.dot(self.conn_mat, (self.g + self.G * (1 - self.g))*self.h*r)    u, g, h = self.integral(u=self.u, g=self.g, h=self.h, t=bp.share['t'], Irec=Irec, Iext=self.input, r=r, dt=bm.dt)    self.u[:] = bm.where(u&gt;0,u,0)    self.g.value = g    self.h.value = h    self.input[:] = 0.  # é‡ç½®å¤–éƒ¨ç”µæµSimulate traveling wave in CANN with STPdef traveling_wave_STP(num=512,k=0.1,J0=12.,tau_d=1000,tau_f=1.,G=0.9):    # ç”ŸæˆCANN    cann_stp = CANN1D_STP(num=num, k=k,tau_d=tau_d,tau_f=tau_f,G=G, J0=J0)    # ç”Ÿæˆå¤–éƒ¨åˆºæ¿€    dur = 1000.    noise_level = 0.1    Iext = bm.random.normal(0., noise_level, (int(dur / bm.get_dt()), num))    duration = dur    # è¿è¡Œæ•°å€¼æ¨¡æ‹Ÿ    runner = bp.DSRunner(cann_stp, inputs=['input', Iext, 'iter'], monitors=['u','g','h'])    runner.run(duration)    fig,ax = plt.subplots(figsize=(3,3))    u = bm.as_numpy(runner.mon.u)    max_index = np.argmax(u[1000,:])    print(max_index)    ax.plot(runner.mon.g[:,max_index],label='g')    ax.plot(runner.mon.h[:,max_index],label='h')    ax.legend()    # å¯è§†åŒ–    def plot_response(t):        fig, gs = bp.visualize.get_figure(1, 1, 3, 3)        ax = fig.add_subplot(gs[0, 0])        ts = int(t / bm.get_dt())        I, u = Iext[ts], runner.mon.u[ts]        ax.plot(cann_stp.x, I, label='Iext')        ax.plot(cann_stp.x, u, linestyle='dashed', label='U')        ax.set_title(r'$t$' + ' = {} ms'.format(t))        ax.set_xlabel(r'$x$')        ax.spines['top'].set_visible(False)        ax.spines['right'].set_visible(False)        ax.legend()    plot_response(t=100.)    plot_response(t=200.)    plot_response(t=300.)    bp.visualize.animate_1D(        dynamical_vars=[{'ys': runner.mon.u, 'xs': cann_stp.x, 'legend': 'u'},                        {'ys': Iext, 'xs': cann_stp.x, 'legend': 'Iext'}],        frame_step=1,        frame_delay=40,        show=True,    )    plt.show()    traveling_wave_STP(G=0.5,tau_d=50)Decision-Making NetworkLIP -&gt; Decision-MakingCoherent motion taskåˆ¤æ–­éšæœºç‚¹(å¤§éƒ¨åˆ†ç‚¹)çš„è¿åŠ¨æœå‘coherenceå½±å“ä»»åŠ¡çš„éš¾åº¦0%éš¾ï¼Œ100%ç®€å•ç¼–ç å†³ç­–çš„å“åº”ï¼Œä¸æ˜¯è¿åŠ¨Reaction Time vs. Fixed Durationcoherenceè¶Šé«˜ï¼Œååº”æ—¶é—´è¶ŠçŸ­Fixed Durationå¤šäº†Delay timeå®žéªŒè®¾è®¡çº¯ç²¹æŠŠdecision-makingç»™æå–å‡ºæ¥Effect of Difficultycoherenceè¶Šå¤§ï¼Œååº”æ—¶é—´æ˜¯è¶ŠçŸ­ï¼Œsingle neuronå¾ˆéš¾åšåˆ°è¿™ä¹ˆçŸ­çš„decision-makingï¼Œè€ƒè™‘è¦å»ºæ¨¡çš„å› ç´ Response of MT Neuronsè®°å½•MTçš„ç¥žç»å…ƒï¼Œå¯¹è¿™ç§è¿åŠ¨çš„æœå‘åˆºæ¿€è¿›è¡Œç¼–ç çº¿æ€§ç¼–ç coherenceè¿åŠ¨å¼ºåº¦çš„æ–¹å‘åšå†³ç­–åœ¨å®ƒçš„ä¸‹æ¸¸Response of LIP NeuronsMTçš„ä¸‹æ¸¸æ‰¾åˆ°LIPçš„ç¥žç»å…ƒçˆ¬å‡åˆ°ä¸€å®šé«˜åº¦å†åšé€‰æ‹©coherenceä¸Žçˆ¬å‡çš„æ–œçŽ‡ä¹Ÿä¼šæœ‰å½±å“ï¼Œä»»åŠ¡è¶Šéš¾ï¼Œçˆ¬å‡æ–œçŽ‡è¶Šå°Ramping-to-threshold(perfect integrator) Model\(\begin{aligned}\frac{dR}{dt}=I_A-I_B+\text{noise},\quad R(t)&amp;=(I_A-I_B)t+\int_0^tdt\text{noise}.\\\tau_\text{network}&amp;=\infty!\end{aligned}\)ä¸¤ç§é€‰æ‹©ç§¯åˆ†æ±‚å’Œåšç§¯ç´¯ï¼Œç­‰åˆ°é˜ˆå€¼åšå†³ç­–Accumulates information (evidence) -&gt; Rampingç›´æŽ¥ä¿å­˜ä¿¡æ¯ï¼Œæ²¡æœ‰ç‰¹åˆ«å¥½çš„ç”Ÿç‰©å¯¹åº”A Spiking Network of DMA cortical microcircuit modelA=Upward motion B=Downward motion2-population excitatory neurons (integrate-and-fire neurons driven by Poisson input)Slow reverberatory excitation mediated by the NMDA receptors at recurrent synapsesAMPA receptors ($\tau _{syn}=$1 - 3 ms)NMDA receptors ($\tau _{syn}=$ 50 - 100 ms).ä¸¤ç¾¤ç¥žç»å…ƒåˆ†åˆ«åšä¸åŒçš„é€‰æ‹©ï¼Œä¸Žè‡ªå·±å¯¹æ–¹éƒ½æœ‰è¿žæŽ¥NMDA ç¼“æ…¢çš„ä¿¡å·ä½¿å¾—æœ‰æ…¢æ…¢å¢žé•¿çš„rampingçš„è¿‡ç¨‹interneuronsçš„backwardæœ‰æŠ‘åˆ¶ä½œç”¨Coherence-Dependent Inputçº¿æ€§ç¼–ç è¿åŠ¨æœå‘çš„ä¿¡æ¯ï¼Œcoherenceå¼ºåº¦å½±å“firing rateï¼Œä¸€ç³»åˆ—æ³Šæ¾è¿‡ç¨‹ï¼ŒåŒæ—¶è¿˜æœ‰noiseã€‚æœ¬èº«ä¸¤ç§ä¿¡æ¯è¿˜æ˜¯æœ‰å·®å¼‚Duality of this modelä¸åŒcoherenceçš„ç¥žç»å…ƒå“åº”ä¸¤ä¸ªgroupä¼šç«žäº‰ï¼Œå½“æœ‰ä¸€ä¸ªgroupè¾¾åˆ°20%ï¼Œè¿›å…¥è¿™ä¸ªçª—å£ï¼Œå°±ä¼šç›´æŽ¥å‘æ”¾ä¸ŠåŽ»Spontaneous symmetry breaking and stochastic decision makingSimulation of Spiking DMA Cortical Microcircuit Modelç”¨ä¸¤ä¸ªcoherenceç”Ÿæˆå‡ºæ¥çš„åºåˆ—\(\begin{gathered}C_m\frac{dV(t)}{dt}=-g_L(V(t)-V_L)-I_{syn}(t)\\I_{syn}(t)=I_{\mathrm{ext},\mathrm{AMPA}}\left(t\right)+I_{\mathrm{rec},AMPA}(t)+I_{\mathrm{rec},NMDA}(t)+I_{\mathrm{rec},\mathrm{GABA}}(t)\end{gathered}\)\[\begin{gathered}I_{\mathrm{ext},\mathrm{AMPA}}\left(t\right)=g_{\mathrm{ext},\mathrm{AMPA}}\left(V(t)-V_{E}\right)s^{\mathrm{ext},\mathrm{AMPA}}\left(t\right) \\I_{\mathrm{rec},\mathrm{AMP}\Lambda}\left(t\right)=g_{\mathrm{rec},\mathrm{AMP}\Lambda}\left(V(t)-V_{E}\right)\sum_{j=1}^{Ce}w_{j}s_{j}^{AMPA}(t) \\I_{\mathrm{rec},\mathrm{NMDA}}\left(t\right)=\frac{g_{\mathrm{NMDA}}(V(t)-V_{E})}{\left(1+\left[\mathrm{Mg}^{2+}\right]\exp(-0.062V(t))/3.57\right)}\sum_{j=1}^{\mathrm{C_E}}w_{j}s_{j}^{\mathrm{NMDA}}\left(t\right) \\I_\mathrm{rec,GABA}(t)=g_\mathrm{GABA}(V(t)-V_l)\sum_{j=1}^{C_1}s_j^\mathrm{GABA}(t) \end{gathered}\]\[w_j=\left\{\begin{matrix}w_+&gt;1,\\w_-&lt;1,\\others=1.\end{matrix}\right.\]å››ç±»ç¥žç»å…ƒï¼Œä¸‰ç±»ä¿¡å·å¤–ç•Œè¾“å…¥çš„ä¿¡å·ï¼Œrecurrentä¿¡å·ï¼Œå…¶å®ƒç¥žç»å…ƒçš„ä¿¡å·ï¼ŒæŠ‘åˆ¶ç¥žç»å…ƒçš„ä¿¡å·éƒ½æœ‰AMPAå’ŒNMDAè¿™ä¸¤ä¸ªsynapseï¼Œè¿˜æœ‰æŠ‘åˆ¶çš„GABAclass AMPA(bp.Projection):  def __init__(self, pre, post, conn, delay, g_max, tau, E):    super().__init__()    if conn == 'all2all':      comm = bp.dnn.AllToAll(pre.num, post.num, g_max)    elif conn == 'one2one':      comm = bp.dnn.OneToOne(pre.num, g_max)    else:      raise ValueError    syn = bp.dyn.Expon.desc(post.num, tau=tau)    out = bp.dyn.COBA.desc(E=E)    self.proj = bp.dyn.ProjAlignPostMg2(      pre=pre, delay=delay, comm=comm,      syn=syn, out=out, post=post    )class NMDA(bp.Projection):  def __init__(self, pre, post, conn, delay, g_max):    super().__init__()    if conn == 'all2all':      comm = bp.dnn.AllToAll(pre.num, post.num, g_max)    elif conn == 'one2one':      comm = bp.dnn.OneToOne(pre.num, g_max)    else:      raise ValueError    syn = bp.dyn.NMDA.desc(pre.num, a=0.5, tau_decay=100., tau_rise=2.)    out = bp.dyn.MgBlock(E=0., cc_Mg=1.0)    self.proj = bp.dyn.ProjAlignPreMg2(      pre=pre, delay=delay, syn=syn,      comm=comm, out=out, post=post    )class DecisionMakingNet(bp.DynSysGroup):  def __init__(self, scale=1., f=0.15):    super().__init__()    # ç½‘ç»œä¸­å„ç»„ç¥žç»å…ƒçš„æ•°ç›®    num_exc = int(1600 * scale)    num_I, num_A, num_B = int(400 * scale), int(f * num_exc), int(f * num_exc)    num_N = num_exc - num_A - num_B    self.num_A, self.num_B, self.num_N, self.num_I = num_A, num_B, num_N, num_I    poisson_freq = 2400.  # Hz    w_pos = 1.7    w_neg = 1. - f * (w_pos - 1.) / (1. - f)    g_ext2E_AMPA = 2.1  # nS    g_ext2I_AMPA = 1.62  # nS    g_E2E_AMPA = 0.05 / scale  # nS    g_E2I_AMPA = 0.04 / scale  # nS    g_E2E_NMDA = 0.165 / scale  # nS    g_E2I_NMDA = 0.13 / scale  # nS    g_I2E_GABAa = 1.3 / scale  # nS    g_I2I_GABAa = 1.0 / scale  # nS    neu_par = dict(V_rest=-70., V_reset=-55., V_th=-50., V_initializer=bp.init.OneInit(-70.))    # E neurons/pyramid neurons    self.A = bp.dyn.LifRef(num_A, tau=20., R=0.04, tau_ref=2., **neu_par)    self.B = bp.dyn.LifRef(num_B, tau=20., R=0.04, tau_ref=2., **neu_par)    self.N = bp.dyn.LifRef(num_N, tau=20., R=0.04, tau_ref=2., **neu_par)    # I neurons/interneurons    self.I = bp.dyn.LifRef(num_I, tau=10., R=0.05, tau_ref=1., **neu_par)    # poisson stimulus  # 'freqs' as bm.Variable    self.IA = bp.dyn.PoissonGroup(num_A, freqs=bm.Variable(bm.zeros(1)))    self.IB = bp.dyn.PoissonGroup(num_B, freqs=bm.Variable(bm.zeros(1)))    # noise neurons    self.noise_B = bp.dyn.PoissonGroup(num_B, freqs=poisson_freq)    self.noise_A = bp.dyn.PoissonGroup(num_A, freqs=poisson_freq)    self.noise_N = bp.dyn.PoissonGroup(num_N, freqs=poisson_freq)    self.noise_I = bp.dyn.PoissonGroup(num_I, freqs=poisson_freq)    # define external inputs    self.IA2A = AMPA(self.IA, self.A, 'one2one', None, g_ext2E_AMPA, tau=2., E=0.)    self.IB2B = AMPA(self.IB, self.B, 'one2one', None, g_ext2E_AMPA, tau=2., E=0.)    # define AMPA projections from N    self.N2B_AMPA = AMPA(self.N, self.B, 'all2all', 0.5, g_E2E_AMPA * w_neg, tau=2., E=0.)    self.N2A_AMPA = AMPA(self.N, self.A, 'all2all', 0.5, g_E2E_AMPA * w_neg, tau=2., E=0.)    self.N2N_AMPA = AMPA(self.N, self.N, 'all2all', 0.5, g_E2E_AMPA, tau=2., E=0.)    self.N2I_AMPA = AMPA(self.N, self.I, 'all2all', 0.5, g_E2I_AMPA, tau=2., E=0.)    # define NMDA projections from N    self.N2B_NMDA = NMDA(self.N, self.B, 'all2all', 0.5, g_E2E_NMDA * w_neg)    self.N2A_NMDA = NMDA(self.N, self.A, 'all2all', 0.5, g_E2E_NMDA * w_neg)    self.N2N_NMDA = NMDA(self.N, self.N, 'all2all', 0.5, g_E2E_NMDA)    self.N2I_NMDA = NMDA(self.N, self.I, 'all2all', 0.5, g_E2I_NMDA)    # define AMPA projections from B    self.B2B_AMPA = AMPA(self.B, self.B, 'all2all', 0.5, g_E2E_AMPA * w_pos, tau=2., E=0.)    self.B2A_AMPA = AMPA(self.B, self.A, 'all2all', 0.5, g_E2E_AMPA * w_neg, tau=2., E=0.)    self.B2N_AMPA = AMPA(self.B, self.N, 'all2all', 0.5, g_E2E_AMPA, tau=2., E=0.)    self.B2I_AMPA = AMPA(self.B, self.I, 'all2all', 0.5, g_E2I_AMPA, tau=2., E=0.)    # define NMDA projections from B    self.B2B_NMDA = NMDA(self.B, self.B, 'all2all', 0.5, g_E2E_NMDA * w_pos)    self.B2A_NMDA = NMDA(self.B, self.A, 'all2all', 0.5, g_E2E_NMDA * w_neg)    self.B2N_NMDA = NMDA(self.B, self.N, 'all2all', 0.5, g_E2E_NMDA)    self.B2I_NMDA = NMDA(self.B, self.I, 'all2all', 0.5, g_E2I_NMDA)    # define AMPA projections from A    self.A2B_AMPA = AMPA(self.A, self.B, 'all2all', 0.5, g_E2E_AMPA * w_neg, tau=2., E=0.)    self.A2A_AMPA = AMPA(self.A, self.A, 'all2all', 0.5, g_E2E_AMPA * w_pos, tau=2., E=0.)    self.A2N_AMPA = AMPA(self.A, self.N, 'all2all', 0.5, g_E2E_AMPA, tau=2., E=0.)    self.A2I_AMPA = AMPA(self.A, self.I, 'all2all', 0.5, g_E2I_AMPA, tau=2., E=0.)    # define NMDA projections from A    self.A2B_NMDA = NMDA(self.A, self.B, 'all2all', 0.5, g_E2E_NMDA * w_neg)    self.A2A_NMDA = NMDA(self.A, self.A, 'all2all', 0.5, g_E2E_NMDA * w_pos)    self.A2N_NMDA = NMDA(self.A, self.N, 'all2all', 0.5, g_E2E_NMDA)    self.A2I_NMDA = NMDA(self.A, self.I, 'all2all', 0.5, g_E2I_NMDA)    # define I-&gt;E/I conn    self.I2B = AMPA(self.I, self.B, 'all2all', 0.5, g_I2E_GABAa, tau=5., E=-70.)    self.I2A = AMPA(self.I, self.A, 'all2all', 0.5, g_I2E_GABAa, tau=5., E=-70.)    self.I2N = AMPA(self.I, self.N, 'all2all', 0.5, g_I2E_GABAa, tau=5., E=-70.)    self.I2I = AMPA(self.I, self.I, 'all2all', 0.5, g_I2I_GABAa, tau=5., E=-70.)    # define external projections    #### TO DO!!!!    self.noise2B = AMPA(self.noise_B, self.B, 'one2one', None, g_ext2E_AMPA, tau=2., E=0.)    self.noise2A = AMPA(self.noise_A, self.A, 'one2one', None, g_ext2E_AMPA, tau=2., E=0.)    self.noise2N = AMPA(self.noise_N, self.N, 'one2one', None, g_ext2E_AMPA, tau=2., E=0.)    self.noise2I = AMPA(self.noise_I, self.I, 'one2one', None, g_ext2I_AMPA, tau=2., E=0.)class Tool:  def __init__(self, pre_stimulus_period=100., stimulus_period=1000., delay_period=500.):    self.pre_stimulus_period = pre_stimulus_period    self.stimulus_period = stimulus_period    self.delay_period = delay_period    self.freq_variance = 10.    self.freq_interval = 50.    self.total_period = pre_stimulus_period + stimulus_period + delay_period  def generate_freqs(self, mean):    # stimulus period    n_stim = int(self.stimulus_period / self.freq_interval)    n_interval = int(self.freq_interval / bm.get_dt())    freqs_stim = np.random.normal(mean, self.freq_variance, (n_stim, 1))    freqs_stim = np.tile(freqs_stim, (1, n_interval)).flatten()    # pre stimulus period    freqs_pre = np.zeros(int(self.pre_stimulus_period / bm.get_dt()))    # post stimulus period    freqs_delay = np.zeros(int(self.delay_period / bm.get_dt()))    all_freqs = np.concatenate([freqs_pre, freqs_stim, freqs_delay], axis=0)    return bm.asarray(all_freqs)  def visualize_results(self, mon, IA_freqs, IB_freqs, t_start=0., title=None):    fig, gs = bp.visualize.get_figure(4, 1, 3, 10)    axes = [fig.add_subplot(gs[i, 0]) for i in range(4)]    ax = axes[0]    bp.visualize.raster_plot(mon['ts'], mon['A.spike'], markersize=1, ax=ax)    if title: ax.set_title(title)    ax.set_ylabel("Group A")    ax.set_xlim(t_start, self.total_period + 1)    ax.axvline(self.pre_stimulus_period, linestyle='dashed')    ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle='dashed')    ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle='dashed')    ax = axes[1]    bp.visualize.raster_plot(mon['ts'], mon['B.spike'], markersize=1, ax=ax)    ax.set_ylabel("Group B")    ax.set_xlim(t_start, self.total_period + 1)    ax.axvline(self.pre_stimulus_period, linestyle='dashed')    ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle='dashed')    ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle='dashed')    ax = axes[2]    rateA = bp.measure.firing_rate(mon['A.spike'], width=10.)    rateB = bp.measure.firing_rate(mon['B.spike'], width=10.)    ax.plot(mon['ts'], rateA, label="Group A")    ax.plot(mon['ts'], rateB, label="Group B")    ax.set_ylabel('Population activity [Hz]')    ax.set_xlim(t_start, self.total_period + 1)    ax.axvline(self.pre_stimulus_period, linestyle='dashed')    ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle='dashed')    ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle='dashed')    ax.legend()    ax = axes[3]    ax.plot(mon['ts'], IA_freqs, label="group A")    ax.plot(mon['ts'], IB_freqs, label="group B")    ax.set_ylabel("Input activity [Hz]")    ax.set_xlim(t_start, self.total_period + 1)    ax.axvline(self.pre_stimulus_period, linestyle='dashed')    ax.axvline(self.pre_stimulus_period + self.stimulus_period, linestyle='dashed')    ax.axvline(self.pre_stimulus_period + self.stimulus_period + self.delay_period, linestyle='dashed')    ax.legend()    ax.set_xlabel("Time [ms]")    plt.show()tool = Tool()net = DecisionMakingNet()mu0 = 40.coherence = 25.6IA_freqs = tool.generate_freqs(mu0 + mu0 / 100. * coherence)IB_freqs = tool.generate_freqs(mu0 - mu0 / 100. * coherence)def give_input():    i = bp.share['i']    net.IA.freqs[0] = IA_freqs[i]    net.IB.freqs[0] = IB_freqs[i]runner = bp.DSRunner(net, inputs=give_input, monitors=['A.spike', 'B.spike'])runner.run(tool.total_period)tool.visualize_results(runner.mon, IA_freqs, IB_freqs)ResultsStochastic Decision MakingA Rate Network of DMReduced ModelåŒ–ç®€åˆ°åªæœ‰ä¸¤ç¾¤ç¥žç»å…ƒï¼ŒåªæŽ¥å—å¤–ç•Œè¾“å…¥ä¿¡å·ï¼Œäº’ç›¸å½±å“å¯¹æ–¹Synaptic variables\(\begin{gathered}\frac{dS_{1}}{dt} =F(x_1)\gamma(1-S_1)-S_1/\tau_s \\\frac{dS_2}{dt} =F(x_2)\gamma(1-S_2)-S_2/\tau_s \end{gathered}\)Input current to each population\(\begin{gathered}x_{1} =J_{E}S_{1}+J_{I}S_{2}+I_{0}+I_{noise1}+J_{\text{ext }\mu_{1}} \\x_{2} =J_{E}S_{2}+J_{I}S_{1}+I_{0}+I_{noise2}+J_{\mathrm{ext}}\mu_{2} \end{gathered}\)Background input\(I_0+I_{noise}\\\begin{gathered}dI_{noise1} =-I_{noise1}\frac{dt}{\tau_{0}}+\sigma dW \\dI_{noise2} =-I_{noise2}\frac{dt}{\tau_{0}}+\sigma dW \end{gathered}\)Firing rates\(r_i=F(x_i)=\frac{ax_i-b}{1-\exp(-d(ax_i-b))}\)Coherence-dependent inputs\(\begin{array}{l}\mu_1=\mu_0\big(1+c'/100\big)\\\mu_2=\mu_0\big(1-c'/100\big)\end{array}\)\[\begin{aligned}&amp;\gamma,a,b,d,J_E,J_I,J_{\mathrm{ext}},I_0,\mu_0,\tau_{\mathrm{AMPA}},\sigma_{\mathrm{noise}}\\&amp;\text{are fixed parameters.}\end{aligned}\]class DecisionMakingRateModel(bp.dyn.NeuGroup):    def __init__(self, size, coherence, JE=0.2609, JI=0.0497, Jext=5.2e-4, I0=0.3255,                 gamma=6.41e-4, tau=100., tau_n=2., sigma_n=0.02, a=270., b=108., d=0.154,                 noise_freq=2400., method='exp_auto', **kwargs):        super(DecisionMakingRateModel, self).__init__(size, **kwargs)                # åˆå§‹åŒ–å‚æ•°        self.coherence = coherence        self.JE = JE        self.JI = JI        self.Jext = Jext        self.I0 = I0        self.gamma = gamma        self.tau = tau        self.tau_n = tau_n        self.sigma_n = sigma_n        self.a = a        self.b = b        self.d = d                # åˆå§‹åŒ–å˜é‡        self.s1 = bm.Variable(bm.zeros(self.num) + 0.15)        self.s2 = bm.Variable(bm.zeros(self.num) + 0.15)        self.r1 = bm.Variable(bm.zeros(self.num))        self.r2 = bm.Variable(bm.zeros(self.num))        self.mu0 = bm.Variable(bm.zeros(self.num))        self.I1_noise = bm.Variable(bm.zeros(self.num))        self.I2_noise = bm.Variable(bm.zeros(self.num))                # å™ªå£°è¾“å…¥çš„ç¥žç»å…ƒ        self.noise1 = bp.dyn.PoissonGroup(self.num, freqs=noise_freq)        self.noise2 = bp.dyn.PoissonGroup(self.num, freqs=noise_freq)                # å®šä¹‰ç§¯åˆ†å‡½æ•°        self.integral = bp.odeint(self.derivative, method=method)            @property    def derivative(self):        return bp.JointEq([self.ds1, self.ds2, self.dI1noise, self.dI2noise])            def ds1(self, s1, t, s2, mu0):        I1 = self.Jext * mu0 * (1. + self.coherence / 100.)        x1 = self.JE * s1 - self.JI * s2 + self.I0 + I1 + self.I1_noise        r1 = (self.a * x1 - self.b) / (1. - bm.exp(-self.d * (self.a * x1 - self.b)))        return - s1 / self.tau + (1. - s1) * self.gamma * r1        def ds2(self, s2, t, s1, mu0):        I2=self.Jext*mu0*(1.- self.coherence / 100.)        x2 = self.JE * s2 - self.JI * s1 + self.I0 + I2 + self.I2_noise        r2 = (self.a * x2 - self.b) / (1. - bm.exp(-self.d * (self.a * x2 - self.b)))         return - s2 / self.tau + (1. - s2) * self.gamma * r2    def dI1noise(self, I1_noise, t, noise1):        return (- I1_noise + noise1.spike * bm.sqrt(self.tau_n * self.sigma_n * self.sigma_n)) / self.tau_n        def dI2noise(self, I2_noise, t, noise2):        return (- I2_noise + noise2.spike * bm.sqrt(self.tau_n * self.sigma_n * self.sigma_n)) / self.tau_n            def update(self, tdi):        # æ›´æ–°å™ªå£°ç¥žç»å…ƒä»¥äº§ç”Ÿæ–°çš„éšæœºå‘æ”¾ self.noise1.update(tdi) self.noise2.update(tdi)        # æ›´æ–°s1ã€s2ã€I1_noiseã€I2_noise        integral = self.integral(self.s1, self.s2, self.I1_noise, self.I2_noise, tdi.t, mu0=self.mu0,                             noise1=self.noise1, noise2=self.noise2, dt=tdi.dt)        self.s1.value, self.s2.value, self.I1_noise.value, self.I2_noise.value = integral                # ç”¨æ›´æ–°åŽçš„s1ã€s2è®¡ç®—r1ã€r2        I1 = self.Jext * self.mu0 * (1. + self.coherence / 100.)        x1 = self.JE * self.s1 + self.JI * self.s2 + self.I0 + I1 + self.I1_noise        self.r1.value = (self.a * x1 - self.b) / (1. - bm.exp(-self.d * (self.a * x1 - self.b)))        I2 = self.Jext * self.mu0 * (1. - self.coherence / 100.)        x2 = self.JE * self.s2 + self.JI * self.s1 + self.I0 + I2 + self.I2_noise        self.r2.value = (self.a * x2 - self.b) / (1. - bm.exp(-self.d * (self.a * x2 - self.b)))                # é‡ç½®å¤–éƒ¨è¾“å…¥         self.mu0[:] = 0.# å®šä¹‰å„ä¸ªé˜¶æ®µçš„æ—¶é•¿pre_stimulus_period, stimulus_period, delay_period = 100., 2000., 500.# ç”Ÿæˆæ¨¡åž‹dmnet = DecisionMakingRateModel(1, coherence=25.6, noise_freq=2400.)# å®šä¹‰ç”µæµéšæ—¶é—´çš„å˜åŒ–inputs, total_period = bp.inputs.constant_input([(0., pre_stimulus_period),                                                 (20., stimulus_period),                                                 (0., delay_period)])# è¿è¡Œæ•°å€¼æ¨¡æ‹Ÿrunner = bp.DSRunner(dmnet,                     monitors=['s1', 's2', 'r1', 'r2'],                     inputs=('mu0', inputs, 'iter'))runner.run(total_period)# å¯è§†åŒ–fig, gs = plt.subplots(2, 1, figsize=(6, 6), sharex='all')gs[0].plot(runner.mon.ts, runner.mon.s1, label='s1')gs[0].plot(runner.mon.ts, runner.mon.s2, label='s2')gs[0].axvline(pre_stimulus_period, 0., 1., linestyle='dashed', color=u'#444444')gs[0].axvline(pre_stimulus_period + stimulus_period, 0., 1., linestyle='dashed', color=u'#444444')gs[0].set_ylabel('gating variable $s$')gs[0].legend()gs[1].plot(runner.mon.ts, runner.mon.r1, label='r1')gs[1].plot(runner.mon.ts, runner.mon.r2, label='r2')gs[1].axvline(pre_stimulus_period, 0., 1., linestyle='dashed', color=u'#444444')gs[1].axvline(pre_stimulus_period + stimulus_period, 0., 1., linestyle='dashed', color=u'#444444')gs[1].set_xlabel('t (ms)')gs[1].set_ylabel('firing rate $r$')gs[1].legend()plt.subplots_adjust(hspace=0.1)plt.show()ResultsPhase Plane Analysiså› ä¸ºåªæœ‰ä¸¤ä¸ªvariableModel implementation@bp.odeintdef int_s1(s1, t, s2, coh=0.5, mu=20.):    x1 = JE * s1 + JI * s2 + Ib + JAext * mu * (1. + coh/100)    r1 = (a * x1 - b) / (1. - bm.exp(-d * (a * x1 - b)))    return - s1 / tau + (1. - s1) * gamma * r1@bp.odeintdef int_s2(s2, t, s1, coh=0.5, mu=20.):    x2 = JE * s2 + JI * s1 + Ib + JAext * mu * (1. - coh/100)    r2 = (a * x2 - b) / (1. - bm.exp(-d * (a * x2 - b)))    return - s2 / tau + (1. - s2) * gamma * r2Without / with inputåªå—æ‰°åŠ¨å½±å“ï¼Œæœ‰inputåŽä¸­é—´å˜å¾—ä¸ç¨³å®šï¼Œä½†å¦‚æžœå·²ç»é€‰æ‹©ï¼Œç½‘ç»œä»ç»´æŒä¹‹å‰é€‰æ‹©çš„ç»“æžœCoherenceç¨³å®šç‚¹å¯¹ç½‘ç»œçš„æ‹‰ä¼¸æ›´å¼ºReservoir Computingå¼•å…¥è®­ç»ƒå€¾å‘äºŽä½¿ç”¨RNNConnecting different units\(\begin{aligned}&amp;\textsf{Input to unit i from unit j:} \\&amp;&amp;&amp;I_{j\rightarrow i}=J_{ij}r_{j}(t) \\&amp;\textsf{Total input to unit i:} \\&amp;&amp;&amp;I_{i}^{(tot)}=\sum_{j=1}^{N}J_{ij}r_{j}(t)+I_{i}^{(ext)} \end{aligned}\)\[\textsf{Activation of unit i:}\\\tau\frac{dx_{i}}{dt}=-x_{i}+\sum_{j=1}^{N}J_{ij}\frac{\phi(x_{j})}{1}+I_{i}^{(ext)}(t)\]è®­ç»ƒèŒƒå¼Echo state machineEcho state machineç±»ä¼¼äººå·¥ç¥žç»ç½‘ç»œRNNï¼Œå¯ä»¥å¤„ç†temporalä¿¡æ¯\(\begin{aligned}&amp;\mathbf{x}(n+1) =f(\mathbf{W}^{\mathrm{in}}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n)+\mathbf{W}^{\mathrm{back}}\mathbf{y}(n))  \\&amp;\mathbf{y}(n+1) =\mathbf{W}^{\mathrm{out}}(\mathbf{u}(n+1),\mathbf{x}(n+1),\mathbf{y}(n)) \end{aligned}\)For an RNN, the state of its internal neurons reflects the historical information of the external inputs.åæ˜ çš„echoçš„åŽ†å²ä¿¡æ¯ï¼Œå”¯ä¸€ä¾èµ–åŽ†å²ä¿¡æ¯Assuming that the updates of the network are discrete, the external input at the ð‘›th moment is u(ð‘›) and the neuron state is x(ð‘›), then x(ð‘›) should be determined by u(ð‘›), u(ð‘› - 1), â€¦ uniquely determined. At this point, x(ð‘›) can be regarded as an â€œechoâ€ of the historical input signals.ä¸éœ€è¦è®­ç»ƒconnectionEcho state machine with leaky integratoræœ‰ä¸€ä¸ªleakyé¡¹ï¼Œå¼•å…¥decay###\[\begin{aligned}\hat{h}(n)=\tanh(W^{in}x(n)+W^{rec}h(n-1)+W^{fb}y(n-1)+b^{rec})\\h(n)=(1-\alpha)x(n-1)+\alpha\hat{h}(n)\end{aligned}\]where $h(n)$ is a vector of reservoir neuron activations, $W^{in}$ and $W^{rec}$ are the input and recurrent weight matrices respectively, and $\alpha\in(0,1]$ is the leaking rate. The model is also sometimes used without the leaky integration, which is a special case of $\alpha=1$The linear readout layer is defined as \(y(n)=W^{out}h(n)+b^{out}\)where $y(n)$ is network output, $W^{out}$ the output weight matrix, and $b^out$ is the output biasConstraints of echo state machineEcho state propertyTheorem 1For the echo state network defined above, the network will be echoey as long as the maximum singular value  $\sigma_{max}&lt;1$ of the recurrent connectivity matrix W .  Provement:\(\begin{aligned}d(\mathbf{x}(n+1),\mathbf{x}^{\prime}(n+1))&amp; =d(T(\mathbf{x}(n),\mathbf{u}(n+1)),T(\mathbf{x}'(n),\mathbf{u}(n+1)))  \\&amp;=d(f(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n)),f(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}'(n))) \\&amp;\leq d(\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}(n),\mathbf{W}^\mathrm{in}\mathbf{u}(n+1)+\mathbf{W}\mathbf{x}^{\prime}(n)) \\&amp;=d(\mathbf{W}\mathbf{x}(n),\mathbf{W}\mathbf{x}'(n)) \\&amp;=||\mathbf{W}(\mathbf{x}(n)-\mathbf{x}^{\prime}(n))|| \\&amp;\leq\sigma_{\max}(\mathbf{W})d(\mathbf{x}(n),\mathbf{x}'(n))\end{aligned}\)Theorem 2            For the echo state network defined above, as long as the spectral radius $      \lambda_{max}      $ of the recurrent connection matrix W &gt; 1, then the network must not be echogenic. The spectral radius of the matrix is the absolute value of the largest eigenvalue $\lambda_{max}$.      How to initializeUsing these two theorems, how should we initialize W so that the network has an echo property?If we scale W, i.e., multiply it by a scaling factor $\alpha$, then $\sigma_{max}&lt;1$ and $\lambda_{max}$ will also be scaled $\alpha$.\(\text{For any square matrix, we have}\sigma_{max}\geq|\lambda_{max}|.\\\text{Therefore we set}\alpha_{min}=1/\sigma_{max}(W),\alpha_{max}=1/|\lambda_{max}|(W).\mathrm{Then},\\\begin{array}{ll}\bullet&amp;\text{if}\alpha&lt;\alpha_{min}\text{,the network must have the echo state.}\\\bullet&amp;\text{if}\alpha&gt;\alpha_{max}\text{,the network will not have the echo state.}\\\bullet&amp;\text{if}\alpha_{min}\le\alpha\le\alpha_{max}\text{,the network may have the echo state.}\end{array}\)$\alpha$è®¾çš„ç•¥å°äºŽ1Global parameters of reservoirè¿™äº›è¶…å‚ä¼šå½±å“reservoir networkçš„æ€§èƒ½ï¼Œéœ€è¦æ‰‹åŠ¨è°ƒå‚ï¼Œå¾ˆéš¾è‡ªåŠ¨åŽ»è°ƒæ•´  The size $N_x$          General wisdom: the bigger the reservoir, the better the obtainable performance      Select global parameters with smaller reservoirs, then scale to bigger ones.        Sparsity  Distribution of nonzero elements:          Normal distribution      Uniform distribution      The width of the distributions does not matter        spectral radius of $W$          scales the width of the distribution of its nonzero elements      determines how fast the influence of an input dies out in a reservoir with time, and how stable the reservoir activations are      The spectral radius should be larger in tasks requiring longer memory of the input        Scaling(-s) to $W^{in}$:          For uniform distributed $W^{in}$, $\alpha$ in the range of the interval $[-a;a]$.      For normal distributed $W^{in}$, one may take the standard deviation as a scaling measure.      The leaking rate $\alpha$Training of echo state machineOffline learningThe advantage of the echo state network is that it does not train recurrent connections within the reservoir, but only the readout layer from the reservoir to the output.çº¿æ€§å±‚çš„ä¼˜åŒ–æ–¹æ³•æ˜¯ç®€å•çš„Ridge regression\(\begin{aligned}\epsilon_{\mathrm{train}}(n)&amp;=\mathbf{y}(n)-\mathbf{\hat{y}}(n)\\&amp;=\mathbf{y}(n)-\mathbf{W}^{\mathrm{out}}\mathbf{x}(n)\\&amp;L_{\mathrm{ridge}}=\frac{1}{N}\sum_{i=1}^{N}\epsilon_{\mathrm{train}}^{2}(i)+\alpha||\mathbf{W^{out}}||^{2}\\\\W^{out}&amp;=Y^{target}X^T(XX^T+\beta I)^{-1}\end{aligned}\)trainer = bp.OfflineTrainer(model, fit_method=bp.algorithms.RidgeRegression(1e-7), dt=dt)Online learningæ¥ä¸€ä¸ªsampleï¼Œè¿›è¡Œä¸€æ¬¡trainingï¼Œå¯¹è®­ç»ƒèµ„æºå¯ä»¥é¿å…ç“¶é¢ˆThe training data is passed to the trainer in a certain sequence (e.g., time series), and the trainer continuously learns based on the new incoming data.Recursive Least Squares (RLS) algorithm\(E(\mathbf{y},\mathbf{y}^\mathrm{target},n)=\frac{1}{N_\mathrm{y}}\sum_{i=1}^{N_\mathrm{y}}\sum_{j=1}^{n}\lambda^{n-j}\left(y_i(j)-y_i^\mathrm{target}(j)\right)^2,\)trainer = bp.OnlineTrainer(model, fit_method=bp.algorithms.RLS(), dt=dt)Datasetç»™å®štime sequenceï¼Œå¯ä»¥è®©ç½‘ç»œåŽ»é¢„æµ‹regressionç”¨åˆ°BrainPyé›†æˆçš„Neuromorphic and Cognitive DatasetsOther tasksMNIST dataset or Fashion MNISTTwo aspect:  Running time  Memory UsageEcho state machine programmingimport brainpy as bpimport brainpy.math as bmimport brainpy_datasets as bdimport matplotlib.pyplot as plt# enable x64 computationbm.set_environment(x64=True, mode=bm.batching_mode)bm.set_platform('cpu')Datasetdef plot_mackey_glass_series(ts, x_series, x_tau_series, num_sample):  plt.figure(figsize=(13, 5))  plt.subplot(121)  plt.title(f"Timeserie - {num_sample} timesteps")  plt.plot(ts[:num_sample], x_series[:num_sample], lw=2, color="lightgrey", zorder=0)  plt.scatter(ts[:num_sample], x_series[:num_sample], c=ts[:num_sample], cmap="viridis", s=6)  plt.xlabel("$t$")  plt.ylabel("$P(t)$")  ax = plt.subplot(122)  ax.margins(0.05)  plt.title(f"Phase diagram: $P(t) = f(P(t-\\tau))$")  plt.plot(x_tau_series[: num_sample], x_series[: num_sample], lw=1, color="lightgrey", zorder=0)  plt.scatter(x_tau_series[:num_sample], x_series[: num_sample], lw=0.5, c=ts[:num_sample], cmap="viridis", s=6)  plt.xlabel("$P(t-\\tau)$")  plt.ylabel("$P(t)$")  cbar = plt.colorbar()  cbar.ax.set_ylabel('$t$')  plt.tight_layout()  plt.show()dt = 0.1mg_data = bd.chaos.MackeyGlassEq(25000, dt=dt, tau=17, beta=0.2, gamma=0.1, n=10)ts = mg_data.tsxs = mg_data.xsys = mg_data.ysplot_mackey_glass_series(ts, xs, ys, num_sample=int(1000 / dt))Prediction of Mackey-Glass timeseriesPrepare the datadef get_data(t_warm, t_forcast, t_train, sample_rate=1):    warmup = int(t_warm / dt)  # warmup the reservoir    forecast = int(t_forcast / dt)  # predict 10 ms ahead    train_length = int(t_train / dt)    X_warm = xs[:warmup:sample_rate]    X_warm = bm.expand_dims(X_warm, 0)    X_train = xs[warmup: warmup+train_length: sample_rate]    X_train = bm.expand_dims(X_train, 0)    Y_train = xs[warmup+forecast: warmup+train_length+forecast: sample_rate]    Y_train = bm.expand_dims(Y_train, 0)    X_test = xs[warmup + train_length: -forecast: sample_rate]    X_test = bm.expand_dims(X_test, 0)    Y_test = xs[warmup + train_length + forecast::sample_rate]    Y_test = bm.expand_dims(Y_test, 0)    return X_warm, X_train, Y_train, X_test, Y_test# First warmup the reservoir using the first 100 ms# Then, train the network in 20000 ms to predict 1 ms chaotic series aheadx_warm, x_train, y_train, x_test, y_test = get_data(100, 1, 20000)sample = 3000fig = plt.figure(figsize=(15, 5))plt.plot(x_train[0, :sample], label="Training data")plt.plot(y_train[0, :sample], label="True prediction")plt.legend()plt.show()Prepare the ESNclass ESN(bp.DynamicalSystemNS):  def __init__(self, num_in, num_hidden, num_out, sr=1., leaky_rate=0.3,               Win_initializer=bp.init.Uniform(0, 0.2)):    super(ESN, self).__init__()    self.r = bp.layers.Reservoir(        num_in, num_hidden,        Win_initializer=Win_initializer,        spectral_radius=sr,        leaky_rate=leaky_rate,    )    self.o = bp.layers.Dense(num_hidden, num_out, mode=bm.training_mode)  def update(self, x):    return x &gt;&gt; self.r &gt;&gt; self.oTrain and testmodel = ESN(1, 100, 1)model.reset_state(1)trainer = bp.RidgeTrainer(model, alpha=1e-6)# warmup_ = trainer.predict(x_warm)# train_ = trainer.fit([x_train, y_train])Test the training datays_predict = trainer.predict(x_train)start, end = 1000, 6000plt.figure(figsize=(15, 7))plt.subplot(211)plt.plot(bm.as_numpy(ys_predict)[0, start:end, 0],         lw=3, label="ESN prediction")plt.plot(bm.as_numpy(y_train)[0, start:end, 0], linestyle="--",         lw=2, label="True value")plt.title(f'Mean Square Error: {bp.losses.mean_squared_error(ys_predict, y_train)}')plt.legend()plt.show()Test the testing datays_predict = trainer.predict(x_test)start, end = 1000, 6000plt.figure(figsize=(15, 7))plt.subplot(211)plt.plot(bm.as_numpy(ys_predict)[0, start:end, 0], lw=3, label="ESN prediction")plt.plot(bm.as_numpy(y_test)[0,start:end, 0], linestyle="--", lw=2, label="True value")plt.title(f'Mean Square Error: {bp.losses.mean_squared_error(ys_predict, y_test)}')plt.legend()plt.show()JIT connection operators  Just-in-time randomly generated matrix.  Support for Mat@Vec and Mat@Mat.  Support different random generation methods.(homogenous, uniform, normal)import math, randomdef jitconn_prob_homo(events, prob, weight, seed, outs):    random.seed(seed)    max_cdist= math.ceil(2/prob -1)    for event in  events:        if event:            post_i = random.randint(1, max_cdist)            outs[post_i] += weightApplicationsFrom the perspective of kernel methodsç»´åº¦æ‰©å¼ æ€æƒ³Non-linear SVMs: Kernel MappingKernel methods in neural system? ä¸Žç»´åº¦æ‰©å¼ çš„æ€æƒ³ç›¸ä¼¼Subcortical pathway for rapid motion processingThe first two stages of subcortical visual pathway:Retina -&gt; superior colliculusThe first two stages of primary auditory pathway:Inner Ear -&gt; Cochlear Nucleiç»´åº¦æ‰©å¼ åœ¨subcortical pathwayä¸­ä½“çŽ°ï¼Œreservoir èƒ½å¤Ÿé«˜ç»´å¤„ç†çš„æ›´ç®€å•Spatial-temporal tasksæ—¢æœ‰æ—¶é—´ä¿¡æ¯ï¼Œåˆæœ‰ç©ºé—´ä¿¡æ¯çš„datasetï¼Œä½¿ç”¨reservoiræ¥å¤„ç†é«˜ç»´ä¿¡æ¯ï¼Œåä½ Dimension expansionGait recognitioninputæ¥äº†å†åšè®¡ç®—Spatial-temporal taskslarge-scaleï¼Œéšsizeå¢žå¤§ï¼Œaccuracyå¢žå¤§Liquid state machineA liquid state machine (LSM) is a type of reservoir computer that uses a spiking neural network.ä¸ŽESNä¸€æ ·çš„èŒƒå¼ï¼Œéƒ½æ˜¯åŽ»åšdimension expansionå¾ˆéš¾åŽ»åˆ†æžæ€Žä¹ˆworkçš„]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
